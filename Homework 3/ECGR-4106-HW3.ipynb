{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea285927-44bf-48fd-8bb0-d714340924d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import datetime\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d27321-ac91-4af5-b793-0beb1f7b5cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/p1ch7/'\n",
    "cifar10 = datasets.CIFAR10(data_path, train = True, download = True, transform =  transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))]))\n",
    "cifar10_val = datasets.CIFAR10(data_path, train = False, download = True, transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ca8412-5117-4b45-a518-79d20365190f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size = 64, shuffle = True)\n",
    "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size = 64, shuffle = False)\n",
    "correct = 0\n",
    "total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07557a1-d176-4142-899a-2999c7057702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.activation import Tanh\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch.nn.modules.pooling import MaxPool2d\n",
    "from torch.nn.modules.conv import Conv2d\n",
    "model = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size= 3, padding= 1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 8, kernel_size= 3, padding= 1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32,10)\n",
    ")\n",
    "model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda77eb2-0bc9-4b43-a072-1a4483d65541",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-3\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn.to('cuda:0')\n",
    "n_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffa7cea-5818-4df8-b2af-1d9a98e7e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "  for epoch in range(1, n_epochs +1):\n",
    "    loss_train = 0.0\n",
    "    for imgs, labels in train_loader:\n",
    "      outputs = model(imgs.to('cuda:0'))\n",
    "      loss = loss_fn(outputs.to('cuda:0'), labels.to('cuda:0'))\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      loss_train += loss.item()\n",
    "      if epoch == 1 or epoch % 10 == 0:\n",
    "        print('Epoch {}, Training Loss {}'.format( epoch, loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683bc307-8ab7-4b7b-90a5-198ec7bf9795",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop(\n",
    "    n_epochs = n_epochs,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e289af8c-d243-438c-8508-99b32260dd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, train_loader, val_loader):\n",
    "  for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "      for imgs, labels in loader:\n",
    "        outputs = model(imgs.to('cuda:0'))\n",
    "        _, predicted = torch.max(outputs.to('cuda:0'), dim = 1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted.to('cuda:0') == labels.to('cuda:0')).sum())\n",
    "    print(\"Accuracy {}: {:.2f}\".format(name, correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290afc8f-4039-4b57-9b3d-2f5f75a6c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(\n",
    "        model = model, \n",
    "         train_loader = train_loader, \n",
    "         val_loader = val_loader\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b492820-50c6-4915-8ec8-a184ec5d84a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size= 3, padding= 1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 8, kernel_size= 3, padding= 1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(8, 16, kernel_size = 3, padding = 1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32,10)\n",
    ")\n",
    "model2.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f20eada-19c2-4d85-a4ee-1a029b71502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop(n_epochs = 600, optimizer = optimizer, loss_fn = loss_fn, model = model2, train_loader = train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630e94c5-3bc6-4d89-9bc9-7decc815fa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(\n",
    "        model = model2, \n",
    "         train_loader = train_loader, \n",
    "         val_loader = val_loader\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab50e6a8-3b60-416e-87e7-98f96f83623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57fa935-49d1-4543-ab37-2308e1dd55b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "  def __init__(self, n_chans):\n",
    "    super(ResBlock, self).__init__()\n",
    "    self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3, padding=1, bias=False)\n",
    "    self.btach_norm = nn.BatchNorm2d(num_features=n_chans)\n",
    "    torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu')\n",
    "    torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
    "    torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.conv(x)\n",
    "    out = torch.relu(out)\n",
    "    return out + x\n",
    "class ResNet10(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.resblocks = nn.Sequential(\n",
    "            *(n_blocks * [ResBlock(n_chans=n_chans1)]))\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9455e9fc-6988-4cce-b4e7-f4992aa0894e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e982b15-1370-4d97-a718-81bd1bb52105",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet10()\n",
    "model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6197956e-ed39-49fa-8d0f-ce7949496140",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop(n_epochs = n_epochs, optimizer = optimizer, model = model, loss_fn = loss_fn, train_loader = train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64eb607-8ee3-4257-96fe-683621fae279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, train_loader, val_loader):\n",
    "  for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "      for imgs, labels in loader:\n",
    "        outputs = model(imgs.to('cuda:0'))\n",
    "        _, predicted = torch.max(outputs.to('cuda:0'), dim = 1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted.to('cuda:0') == labels.to('cuda:0')).sum())\n",
    "    print(\"Accuracy {}: {:.2f}\".format(name, correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739051d4-eb14-4d5d-8bb9-c22800715e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(model = model, train_loader = train_loader, val_loader = val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7236143-316e-459f-833d-53707de985f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_l2reg(n_epochs, optimizer, model, loss_fn,\n",
    "train_loader):\n",
    "    device = 'cuda:0'\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            l2_lambda = 0.001\n",
    "            l2_norm = sum(p.pow(2.0).sum()\n",
    "                        for p in model.parameters())\n",
    "            loss = loss + l2_lambda * l2_norm\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5169acf-f39a-4c59-9299-5dc622f7b8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop_l2reg(n_epochs = n_epochs, optimizer = optimizer, model = model, loss_fn = loss_fn, train_loader = train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3f3680-9a1d-4ad6-8a1b-ec5a933d4325",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(model = model, train_loader = train_loader, val_loader = val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b56c5f-032b-44a0-81bf-cd6caa6cc6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "  def __init__(self, n_chans):\n",
    "    super(ResBlock, self).__init__()\n",
    "    self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3, padding=1, bias=False)\n",
    "    self.dropout = nn.Dropout2d(p = 0.3)\n",
    "  def forward(self, x):\n",
    "    out = self.conv(x)\n",
    "    out = self.dropout(out)\n",
    "    out = torch.relu(out)\n",
    "    return out + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73541a25-8f6b-4ab9-9dca-182fbd748c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet10D(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.resblocks = nn.Sequential(\n",
    "            *(n_blocks * [ResBlock(n_chans=n_chans1)]))\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "f230bf08-3f11-4988-b4b8-da9e8c441e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet10D()\n",
    "model.to('cuda:0')\n",
    "learning_rate = 3e-3\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn.to('cuda:0')\n",
    "n_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "098a5cd9-90dd-4043-99e9-47573e567bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss 0.005095358394905734\n",
      "Epoch 1, Training Loss 0.009420937589367332\n",
      "Epoch 1, Training Loss 0.012478083905661503\n",
      "Epoch 1, Training Loss 0.015497911311781316\n",
      "Epoch 1, Training Loss 0.018498081380448988\n",
      "Epoch 1, Training Loss 0.021327377585193995\n",
      "Epoch 1, Training Loss 0.024282243550585968\n",
      "Epoch 1, Training Loss 0.027282910578696015\n",
      "Epoch 1, Training Loss 0.030177978908314425\n",
      "Epoch 1, Training Loss 0.03310792037593129\n",
      "Epoch 1, Training Loss 0.03613773117894712\n",
      "Epoch 1, Training Loss 0.039028957676704584\n",
      "Epoch 1, Training Loss 0.04187473891031407\n",
      "Epoch 1, Training Loss 0.04479264847152983\n",
      "Epoch 1, Training Loss 0.04762487948093268\n",
      "Epoch 1, Training Loss 0.05051382819709875\n",
      "Epoch 1, Training Loss 0.05338879924296113\n",
      "Epoch 1, Training Loss 0.056274268938147506\n",
      "Epoch 1, Training Loss 0.05912391029660354\n",
      "Epoch 1, Training Loss 0.062033231301075964\n",
      "Epoch 1, Training Loss 0.06492223520108197\n",
      "Epoch 1, Training Loss 0.06777564918293673\n",
      "Epoch 1, Training Loss 0.07080416179374051\n",
      "Epoch 1, Training Loss 0.07362446425211094\n",
      "Epoch 1, Training Loss 0.07657730487911292\n",
      "Epoch 1, Training Loss 0.07936949498208283\n",
      "Epoch 1, Training Loss 0.082232335949188\n",
      "Epoch 1, Training Loss 0.08517251356178537\n",
      "Epoch 1, Training Loss 0.08802711353887377\n",
      "Epoch 1, Training Loss 0.09089409908675172\n",
      "Epoch 1, Training Loss 0.0937167375593844\n",
      "Epoch 1, Training Loss 0.09652431358766678\n",
      "Epoch 1, Training Loss 0.09938068188669737\n",
      "Epoch 1, Training Loss 0.10221388943664864\n",
      "Epoch 1, Training Loss 0.10508697721964258\n",
      "Epoch 1, Training Loss 0.10795919486628774\n",
      "Epoch 1, Training Loss 0.11088940646032543\n",
      "Epoch 1, Training Loss 0.11378747514446678\n",
      "Epoch 1, Training Loss 0.11658329975879406\n",
      "Epoch 1, Training Loss 0.1193923181889917\n",
      "Epoch 1, Training Loss 0.1222282547475127\n",
      "Epoch 1, Training Loss 0.12496700555162357\n",
      "Epoch 1, Training Loss 0.12776590521683168\n",
      "Epoch 1, Training Loss 0.13060252288418353\n",
      "Epoch 1, Training Loss 0.1334877822100354\n",
      "Epoch 1, Training Loss 0.13631046458583354\n",
      "Epoch 1, Training Loss 0.1390625936600863\n",
      "Epoch 1, Training Loss 0.14194274893807024\n",
      "Epoch 1, Training Loss 0.14469379476269187\n",
      "Epoch 1, Training Loss 0.14754374008959212\n",
      "Epoch 1, Training Loss 0.15031492679625216\n",
      "Epoch 1, Training Loss 0.15305446572315967\n",
      "Epoch 1, Training Loss 0.15588016217321995\n",
      "Epoch 1, Training Loss 0.15876916759764143\n",
      "Epoch 1, Training Loss 0.161562439425827\n",
      "Epoch 1, Training Loss 0.16451395868950183\n",
      "Epoch 1, Training Loss 0.16737154743555563\n",
      "Epoch 1, Training Loss 0.17013871913675763\n",
      "Epoch 1, Training Loss 0.17297823166908205\n",
      "Epoch 1, Training Loss 0.17578994557070915\n",
      "Epoch 1, Training Loss 0.1787406831141323\n",
      "Epoch 1, Training Loss 0.18154657344379083\n",
      "Epoch 1, Training Loss 0.18443173886564992\n",
      "Epoch 1, Training Loss 0.1872882193616589\n",
      "Epoch 1, Training Loss 0.19000546035864163\n",
      "Epoch 1, Training Loss 0.19290488302860115\n",
      "Epoch 1, Training Loss 0.195777292141829\n",
      "Epoch 1, Training Loss 0.19845035435903408\n",
      "Epoch 1, Training Loss 0.20114656787394258\n",
      "Epoch 1, Training Loss 0.20394460837859327\n",
      "Epoch 1, Training Loss 0.20676431021726954\n",
      "Epoch 1, Training Loss 0.2096227794656973\n",
      "Epoch 1, Training Loss 0.2124048596452874\n",
      "Epoch 1, Training Loss 0.21514142626691657\n",
      "Epoch 1, Training Loss 0.21775777839943575\n",
      "Epoch 1, Training Loss 0.22077776586917966\n",
      "Epoch 1, Training Loss 0.22360664621338516\n",
      "Epoch 1, Training Loss 0.2263080567655051\n",
      "Epoch 1, Training Loss 0.22899267953984878\n",
      "Epoch 1, Training Loss 0.23179788388254696\n",
      "Epoch 1, Training Loss 0.23459380209598396\n",
      "Epoch 1, Training Loss 0.23731094248154583\n",
      "Epoch 1, Training Loss 0.2401312943309774\n",
      "Epoch 1, Training Loss 0.2428959644663974\n",
      "Epoch 1, Training Loss 0.24575625081806232\n",
      "Epoch 1, Training Loss 0.24854230667319138\n",
      "Epoch 1, Training Loss 0.25144068908203593\n",
      "Epoch 1, Training Loss 0.2542286574688104\n",
      "Epoch 1, Training Loss 0.2571331018682026\n",
      "Epoch 1, Training Loss 0.25984278542306416\n",
      "Epoch 1, Training Loss 0.26258904305870273\n",
      "Epoch 1, Training Loss 0.265475784421272\n",
      "Epoch 1, Training Loss 0.26828088784766624\n",
      "Epoch 1, Training Loss 0.2708925036213282\n",
      "Epoch 1, Training Loss 0.27368302784307535\n",
      "Epoch 1, Training Loss 0.2765703878134413\n",
      "Epoch 1, Training Loss 0.2792223685842646\n",
      "Epoch 1, Training Loss 0.28206867879004127\n",
      "Epoch 1, Training Loss 0.28463506454701926\n",
      "Epoch 1, Training Loss 0.2871736820091677\n",
      "Epoch 1, Training Loss 0.28992109332243193\n",
      "Epoch 1, Training Loss 0.2926609505472891\n",
      "Epoch 1, Training Loss 0.2953364329264902\n",
      "Epoch 1, Training Loss 0.29817189844063174\n",
      "Epoch 1, Training Loss 0.30079837025278977\n",
      "Epoch 1, Training Loss 0.30361040489143115\n",
      "Epoch 1, Training Loss 0.3062460722825716\n",
      "Epoch 1, Training Loss 0.3089061852001473\n",
      "Epoch 1, Training Loss 0.3115242910202202\n",
      "Epoch 1, Training Loss 0.3141200757392532\n",
      "Epoch 1, Training Loss 0.31678922203800564\n",
      "Epoch 1, Training Loss 0.31953190369984075\n",
      "Epoch 1, Training Loss 0.3222521517587745\n",
      "Epoch 1, Training Loss 0.32493009058105976\n",
      "Epoch 1, Training Loss 0.3274804626584358\n",
      "Epoch 1, Training Loss 0.32995829374893854\n",
      "Epoch 1, Training Loss 0.33265875062674205\n",
      "Epoch 1, Training Loss 0.3351544702754301\n",
      "Epoch 1, Training Loss 0.3378995989289735\n",
      "Epoch 1, Training Loss 0.34044892281827416\n",
      "Epoch 1, Training Loss 0.3432971422019822\n",
      "Epoch 1, Training Loss 0.34583999006949423\n",
      "Epoch 1, Training Loss 0.3484993459623488\n",
      "Epoch 1, Training Loss 0.35134164024801817\n",
      "Epoch 1, Training Loss 0.35405582052362544\n",
      "Epoch 1, Training Loss 0.3567884413482588\n",
      "Epoch 1, Training Loss 0.35952618268444714\n",
      "Epoch 1, Training Loss 0.3621446555837646\n",
      "Epoch 1, Training Loss 0.36465315212069266\n",
      "Epoch 1, Training Loss 0.3672990644984233\n",
      "Epoch 1, Training Loss 0.3700868688580935\n",
      "Epoch 1, Training Loss 0.37276658270974905\n",
      "Epoch 1, Training Loss 0.37551881910285073\n",
      "Epoch 1, Training Loss 0.378214930786806\n",
      "Epoch 1, Training Loss 0.3808765854981854\n",
      "Epoch 1, Training Loss 0.3835023603475917\n",
      "Epoch 1, Training Loss 0.3863132690529689\n",
      "Epoch 1, Training Loss 0.3889546536118783\n",
      "Epoch 1, Training Loss 0.3915641315452888\n",
      "Epoch 1, Training Loss 0.394320612825701\n",
      "Epoch 1, Training Loss 0.3969280035294535\n",
      "Epoch 1, Training Loss 0.39963662304231884\n",
      "Epoch 1, Training Loss 0.4021345541605254\n",
      "Epoch 1, Training Loss 0.40491637244553824\n",
      "Epoch 1, Training Loss 0.40767814741110253\n",
      "Epoch 1, Training Loss 0.41035718381252434\n",
      "Epoch 1, Training Loss 0.41289789627884965\n",
      "Epoch 1, Training Loss 0.41548331893618456\n",
      "Epoch 1, Training Loss 0.4178720788882517\n",
      "Epoch 1, Training Loss 0.4206332922591578\n",
      "Epoch 1, Training Loss 0.4232929173637839\n",
      "Epoch 1, Training Loss 0.4259835701159504\n",
      "Epoch 1, Training Loss 0.4285189957569932\n",
      "Epoch 1, Training Loss 0.43103828210659956\n",
      "Epoch 1, Training Loss 0.4336500091625906\n",
      "Epoch 1, Training Loss 0.4363438433698376\n",
      "Epoch 1, Training Loss 0.4389606900227344\n",
      "Epoch 1, Training Loss 0.44156694930532703\n",
      "Epoch 1, Training Loss 0.44403617324121775\n",
      "Epoch 1, Training Loss 0.4466259194457013\n",
      "Epoch 1, Training Loss 0.4491924853885875\n",
      "Epoch 1, Training Loss 0.45196556206554406\n",
      "Epoch 1, Training Loss 0.4546016404390945\n",
      "Epoch 1, Training Loss 0.4572973368722764\n",
      "Epoch 1, Training Loss 0.4598305380862692\n",
      "Epoch 1, Training Loss 0.46229550295778554\n",
      "Epoch 1, Training Loss 0.4647208897354048\n",
      "Epoch 1, Training Loss 0.4670995736061155\n",
      "Epoch 1, Training Loss 0.4695710794395193\n",
      "Epoch 1, Training Loss 0.4721706157450176\n",
      "Epoch 1, Training Loss 0.47458519472185606\n",
      "Epoch 1, Training Loss 0.47720084653790956\n",
      "Epoch 1, Training Loss 0.4796968789966515\n",
      "Epoch 1, Training Loss 0.48214828175352054\n",
      "Epoch 1, Training Loss 0.4846821232220096\n",
      "Epoch 1, Training Loss 0.48711346436644454\n",
      "Epoch 1, Training Loss 0.4894661046659855\n",
      "Epoch 1, Training Loss 0.49229105018898656\n",
      "Epoch 1, Training Loss 0.49508020457099466\n",
      "Epoch 1, Training Loss 0.4977402491947574\n",
      "Epoch 1, Training Loss 0.5003755577384968\n",
      "Epoch 1, Training Loss 0.5029862149597129\n",
      "Epoch 1, Training Loss 0.505521977496574\n",
      "Epoch 1, Training Loss 0.5079560202101002\n",
      "Epoch 1, Training Loss 0.5106068073636125\n",
      "Epoch 1, Training Loss 0.5131249316513081\n",
      "Epoch 1, Training Loss 0.5157081686017458\n",
      "Epoch 1, Training Loss 0.5182584507386093\n",
      "Epoch 1, Training Loss 0.5209202423424977\n",
      "Epoch 1, Training Loss 0.5234525789080373\n",
      "Epoch 1, Training Loss 0.5261343238908617\n",
      "Epoch 1, Training Loss 0.5284449994716498\n",
      "Epoch 1, Training Loss 0.5309582284039549\n",
      "Epoch 1, Training Loss 0.5336569169598162\n",
      "Epoch 1, Training Loss 0.5360382613928422\n",
      "Epoch 1, Training Loss 0.5385283416189501\n",
      "Epoch 1, Training Loss 0.5412162467646782\n",
      "Epoch 1, Training Loss 0.5438741569019034\n",
      "Epoch 1, Training Loss 0.5463310457251566\n",
      "Epoch 1, Training Loss 0.5489144552394253\n",
      "Epoch 1, Training Loss 0.5513753441288648\n",
      "Epoch 1, Training Loss 0.5538889739824377\n",
      "Epoch 1, Training Loss 0.5564895950619827\n",
      "Epoch 1, Training Loss 0.5591186068551924\n",
      "Epoch 1, Training Loss 0.5616527289685691\n",
      "Epoch 1, Training Loss 0.5641969132911214\n",
      "Epoch 1, Training Loss 0.5666528223725535\n",
      "Epoch 1, Training Loss 0.5692551831150299\n",
      "Epoch 1, Training Loss 0.5716535013045192\n",
      "Epoch 1, Training Loss 0.5740935171351713\n",
      "Epoch 1, Training Loss 0.5767172825001085\n",
      "Epoch 1, Training Loss 0.579183068269354\n",
      "Epoch 1, Training Loss 0.5816170500062615\n",
      "Epoch 1, Training Loss 0.5839700544886577\n",
      "Epoch 1, Training Loss 0.5863905361546274\n",
      "Epoch 1, Training Loss 0.5889963304905026\n",
      "Epoch 1, Training Loss 0.5915135623853834\n",
      "Epoch 1, Training Loss 0.5941029460838688\n",
      "Epoch 1, Training Loss 0.5966168207585659\n",
      "Epoch 1, Training Loss 0.5992118548554228\n",
      "Epoch 1, Training Loss 0.601745248298206\n",
      "Epoch 1, Training Loss 0.6043180978816488\n",
      "Epoch 1, Training Loss 0.6067917441468105\n",
      "Epoch 1, Training Loss 0.6091653925683492\n",
      "Epoch 1, Training Loss 0.6116527047608514\n",
      "Epoch 1, Training Loss 0.6139805393145822\n",
      "Epoch 1, Training Loss 0.6165634229055146\n",
      "Epoch 1, Training Loss 0.6190646952375427\n",
      "Epoch 1, Training Loss 0.6213650797943935\n",
      "Epoch 1, Training Loss 0.6237184877895638\n",
      "Epoch 1, Training Loss 0.6262653698701688\n",
      "Epoch 1, Training Loss 0.6286643606317622\n",
      "Epoch 1, Training Loss 0.6311681752314653\n",
      "Epoch 1, Training Loss 0.6336171288624444\n",
      "Epoch 1, Training Loss 0.6360475454488983\n",
      "Epoch 1, Training Loss 0.6384519669405945\n",
      "Epoch 1, Training Loss 0.641125908135758\n",
      "Epoch 1, Training Loss 0.6436377103676272\n",
      "Epoch 1, Training Loss 0.6462496721836002\n",
      "Epoch 1, Training Loss 0.6485356965943065\n",
      "Epoch 1, Training Loss 0.6508131399179053\n",
      "Epoch 1, Training Loss 0.6532276961809534\n",
      "Epoch 1, Training Loss 0.6557665421529804\n",
      "Epoch 1, Training Loss 0.6581947352270336\n",
      "Epoch 1, Training Loss 0.6606282890605195\n",
      "Epoch 1, Training Loss 0.66293980063075\n",
      "Epoch 1, Training Loss 0.6654278058225237\n",
      "Epoch 1, Training Loss 0.6678494068667712\n",
      "Epoch 1, Training Loss 0.6702799804680183\n",
      "Epoch 1, Training Loss 0.6726802058536988\n",
      "Epoch 1, Training Loss 0.6751300005046913\n",
      "Epoch 1, Training Loss 0.6776653275160534\n",
      "Epoch 1, Training Loss 0.6800700168475471\n",
      "Epoch 1, Training Loss 0.682621946115323\n",
      "Epoch 1, Training Loss 0.6850059896783756\n",
      "Epoch 1, Training Loss 0.687579219451036\n",
      "Epoch 1, Training Loss 0.6899416155522436\n",
      "Epoch 1, Training Loss 0.6922809086797183\n",
      "Epoch 1, Training Loss 0.6946477268053137\n",
      "Epoch 1, Training Loss 0.6971598830064545\n",
      "Epoch 1, Training Loss 0.6994576280379234\n",
      "Epoch 1, Training Loss 0.7019404993032861\n",
      "Epoch 1, Training Loss 0.7046094123664719\n",
      "Epoch 1, Training Loss 0.707085558673\n",
      "Epoch 1, Training Loss 0.709458401898289\n",
      "Epoch 1, Training Loss 0.7117843210239849\n",
      "Epoch 1, Training Loss 0.7141490683836096\n",
      "Epoch 1, Training Loss 0.7165657446512481\n",
      "Epoch 1, Training Loss 0.7191126418235662\n",
      "Epoch 1, Training Loss 0.7214311321678064\n",
      "Epoch 1, Training Loss 0.7237759011480814\n",
      "Epoch 1, Training Loss 0.7260423979490919\n",
      "Epoch 1, Training Loss 0.7285024311841296\n",
      "Epoch 1, Training Loss 0.7309482711965166\n",
      "Epoch 1, Training Loss 0.7330644231318207\n",
      "Epoch 1, Training Loss 0.7355258399263367\n",
      "Epoch 1, Training Loss 0.7377969041809707\n",
      "Epoch 1, Training Loss 0.7405032227411295\n",
      "Epoch 1, Training Loss 0.743058539564957\n",
      "Epoch 1, Training Loss 0.7455416164739662\n",
      "Epoch 1, Training Loss 0.7478669448886686\n",
      "Epoch 1, Training Loss 0.7504068167923051\n",
      "Epoch 1, Training Loss 0.752777476292437\n",
      "Epoch 1, Training Loss 0.7551112478346471\n",
      "Epoch 1, Training Loss 0.7576034937978096\n",
      "Epoch 1, Training Loss 0.7598039806651338\n",
      "Epoch 1, Training Loss 0.7622185285438967\n",
      "Epoch 1, Training Loss 0.7648021636716545\n",
      "Epoch 1, Training Loss 0.767115581523427\n",
      "Epoch 1, Training Loss 0.769738785598589\n",
      "Epoch 1, Training Loss 0.7720303027831075\n",
      "Epoch 1, Training Loss 0.7745143315371346\n",
      "Epoch 1, Training Loss 0.7768275916119061\n",
      "Epoch 1, Training Loss 0.7790969633080466\n",
      "Epoch 1, Training Loss 0.7818676271402013\n",
      "Epoch 1, Training Loss 0.7840998009647555\n",
      "Epoch 1, Training Loss 0.7865846307991106\n",
      "Epoch 1, Training Loss 0.7888930205189054\n",
      "Epoch 1, Training Loss 0.7912508357516335\n",
      "Epoch 1, Training Loss 0.7938063778840673\n",
      "Epoch 1, Training Loss 0.7961710959749149\n",
      "Epoch 1, Training Loss 0.7985536776235341\n",
      "Epoch 1, Training Loss 0.8010489429964129\n",
      "Epoch 1, Training Loss 0.8034831561395884\n",
      "Epoch 1, Training Loss 0.8058336325313734\n",
      "Epoch 1, Training Loss 0.8082672775249042\n",
      "Epoch 1, Training Loss 0.8108557468789923\n",
      "Epoch 1, Training Loss 0.8133250599931878\n",
      "Epoch 1, Training Loss 0.8156409306294473\n",
      "Epoch 1, Training Loss 0.8181886819317518\n",
      "Epoch 1, Training Loss 0.8205851141144248\n",
      "Epoch 1, Training Loss 0.8228731400826398\n",
      "Epoch 1, Training Loss 0.8256266612530975\n",
      "Epoch 1, Training Loss 0.8280737544874401\n",
      "Epoch 1, Training Loss 0.8302453984994718\n",
      "Epoch 1, Training Loss 0.8326559234458162\n",
      "Epoch 1, Training Loss 0.8351083124994927\n",
      "Epoch 1, Training Loss 0.837583053599843\n",
      "Epoch 1, Training Loss 0.8397543136116183\n",
      "Epoch 1, Training Loss 0.8419933613303983\n",
      "Epoch 1, Training Loss 0.8444537874073019\n",
      "Epoch 1, Training Loss 0.8466276253580742\n",
      "Epoch 1, Training Loss 0.8487895969539652\n",
      "Epoch 1, Training Loss 0.8511545110846419\n",
      "Epoch 1, Training Loss 0.853596375726373\n",
      "Epoch 1, Training Loss 0.8558446237498232\n",
      "Epoch 1, Training Loss 0.8582901382995078\n",
      "Epoch 1, Training Loss 0.8606315047844596\n",
      "Epoch 1, Training Loss 0.8627767154322866\n",
      "Epoch 1, Training Loss 0.8650266784231376\n",
      "Epoch 1, Training Loss 0.8673283706235764\n",
      "Epoch 1, Training Loss 0.8694748131515425\n",
      "Epoch 1, Training Loss 0.8717836662936393\n",
      "Epoch 1, Training Loss 0.8739071515819911\n",
      "Epoch 1, Training Loss 0.8764126431911498\n",
      "Epoch 1, Training Loss 0.8785420192781922\n",
      "Epoch 1, Training Loss 0.881145050458591\n",
      "Epoch 1, Training Loss 0.8835620146883113\n",
      "Epoch 1, Training Loss 0.8860590802434155\n",
      "Epoch 1, Training Loss 0.8885971202569849\n",
      "Epoch 1, Training Loss 0.8908594153116426\n",
      "Epoch 1, Training Loss 0.8932111473644481\n",
      "Epoch 1, Training Loss 0.8952251459326586\n",
      "Epoch 1, Training Loss 0.8975452140469076\n",
      "Epoch 1, Training Loss 0.8999188601818231\n",
      "Epoch 1, Training Loss 0.9022625820411135\n",
      "Epoch 1, Training Loss 0.9044110797860129\n",
      "Epoch 1, Training Loss 0.9066778425975224\n",
      "Epoch 1, Training Loss 0.9088922519513103\n",
      "Epoch 1, Training Loss 0.9109087650428342\n",
      "Epoch 1, Training Loss 0.91332286870693\n",
      "Epoch 1, Training Loss 0.9159178878645153\n",
      "Epoch 1, Training Loss 0.9180615986399638\n",
      "Epoch 1, Training Loss 0.9202159935860987\n",
      "Epoch 1, Training Loss 0.9224506735496814\n",
      "Epoch 1, Training Loss 0.9247119664536108\n",
      "Epoch 1, Training Loss 0.926941811581097\n",
      "Epoch 1, Training Loss 0.9292156885347098\n",
      "Epoch 1, Training Loss 0.9315642264797864\n",
      "Epoch 1, Training Loss 0.9337724142367273\n",
      "Epoch 1, Training Loss 0.9362710159452979\n",
      "Epoch 1, Training Loss 0.9384535811746212\n",
      "Epoch 1, Training Loss 0.9409166179654543\n",
      "Epoch 1, Training Loss 0.9433453791891523\n",
      "Epoch 1, Training Loss 0.9455569116660701\n",
      "Epoch 1, Training Loss 0.9477584390993923\n",
      "Epoch 1, Training Loss 0.9500766748662495\n",
      "Epoch 1, Training Loss 0.9524049835131906\n",
      "Epoch 1, Training Loss 0.954684402174352\n",
      "Epoch 1, Training Loss 0.9569193158308258\n",
      "Epoch 1, Training Loss 0.9590543377429933\n",
      "Epoch 1, Training Loss 0.9613750549533483\n",
      "Epoch 1, Training Loss 0.9636826981668887\n",
      "Epoch 1, Training Loss 0.9658254490179174\n",
      "Epoch 1, Training Loss 0.9681885875094577\n",
      "Epoch 1, Training Loss 0.9705816231420278\n",
      "Epoch 1, Training Loss 0.9731207162218021\n",
      "Epoch 1, Training Loss 0.9754390295814065\n",
      "Epoch 1, Training Loss 0.9776658926473554\n",
      "Epoch 1, Training Loss 0.9805220514917008\n",
      "Epoch 1, Training Loss 0.9834837724485666\n",
      "Epoch 1, Training Loss 0.9859707154276426\n",
      "Epoch 1, Training Loss 0.9883054532968175\n",
      "Epoch 1, Training Loss 0.9906065866465459\n",
      "Epoch 1, Training Loss 0.9928738979427406\n",
      "Epoch 1, Training Loss 0.9949439800608798\n",
      "Epoch 1, Training Loss 0.9973839195183171\n",
      "Epoch 1, Training Loss 0.99976251786932\n",
      "Epoch 1, Training Loss 1.0017766822939334\n",
      "Epoch 1, Training Loss 1.0042107794290918\n",
      "Epoch 1, Training Loss 1.006384476828758\n",
      "Epoch 1, Training Loss 1.0086606574790251\n",
      "Epoch 1, Training Loss 1.0108895182914441\n",
      "Epoch 1, Training Loss 1.0130611301383095\n",
      "Epoch 1, Training Loss 1.0154189080228586\n",
      "Epoch 1, Training Loss 1.0177039196119284\n",
      "Epoch 1, Training Loss 1.0198955601438537\n",
      "Epoch 1, Training Loss 1.0218459495802974\n",
      "Epoch 1, Training Loss 1.024306334650425\n",
      "Epoch 1, Training Loss 1.0265215566700987\n",
      "Epoch 1, Training Loss 1.0286918924287762\n",
      "Epoch 1, Training Loss 1.030968413176134\n",
      "Epoch 1, Training Loss 1.033334335555201\n",
      "Epoch 1, Training Loss 1.035666803264862\n",
      "Epoch 1, Training Loss 1.0378616167151409\n",
      "Epoch 1, Training Loss 1.0399217480588752\n",
      "Epoch 1, Training Loss 1.0420135735245921\n",
      "Epoch 1, Training Loss 1.044314262049887\n",
      "Epoch 1, Training Loss 1.0464522998656154\n",
      "Epoch 1, Training Loss 1.0488832068565253\n",
      "Epoch 1, Training Loss 1.051258225727569\n",
      "Epoch 1, Training Loss 1.053465736491601\n",
      "Epoch 1, Training Loss 1.055858890113928\n",
      "Epoch 1, Training Loss 1.0581355381499775\n",
      "Epoch 1, Training Loss 1.0604357365756998\n",
      "Epoch 1, Training Loss 1.062594839221681\n",
      "Epoch 1, Training Loss 1.0645970641194706\n",
      "Epoch 1, Training Loss 1.0669957466442566\n",
      "Epoch 1, Training Loss 1.069167482578541\n",
      "Epoch 1, Training Loss 1.0716169728037646\n",
      "Epoch 1, Training Loss 1.0736905464430904\n",
      "Epoch 1, Training Loss 1.0758838845640801\n",
      "Epoch 1, Training Loss 1.0779947554668807\n",
      "Epoch 1, Training Loss 1.0804400415066868\n",
      "Epoch 1, Training Loss 1.0824860322201038\n",
      "Epoch 1, Training Loss 1.0849305078806475\n",
      "Epoch 1, Training Loss 1.0871764060176548\n",
      "Epoch 1, Training Loss 1.0892515243471736\n",
      "Epoch 1, Training Loss 1.0913416235648152\n",
      "Epoch 1, Training Loss 1.09334593050925\n",
      "Epoch 1, Training Loss 1.0951882686151568\n",
      "Epoch 1, Training Loss 1.0974155940363168\n",
      "Epoch 1, Training Loss 1.0996719685661824\n",
      "Epoch 1, Training Loss 1.1017479887398918\n",
      "Epoch 1, Training Loss 1.1038103003026274\n",
      "Epoch 1, Training Loss 1.106400223034422\n",
      "Epoch 1, Training Loss 1.1086796988611636\n",
      "Epoch 1, Training Loss 1.110768151862542\n",
      "Epoch 1, Training Loss 1.113035457365958\n",
      "Epoch 1, Training Loss 1.115146723244806\n",
      "Epoch 1, Training Loss 1.1175154860672134\n",
      "Epoch 1, Training Loss 1.119694820450395\n",
      "Epoch 1, Training Loss 1.122031208956638\n",
      "Epoch 1, Training Loss 1.124172320756156\n",
      "Epoch 1, Training Loss 1.126282561922927\n",
      "Epoch 1, Training Loss 1.128379943425698\n",
      "Epoch 1, Training Loss 1.130619140079869\n",
      "Epoch 1, Training Loss 1.132655345875284\n",
      "Epoch 1, Training Loss 1.134740218482054\n",
      "Epoch 1, Training Loss 1.1369770478714458\n",
      "Epoch 1, Training Loss 1.1391853220627437\n",
      "Epoch 1, Training Loss 1.1415924961914492\n",
      "Epoch 1, Training Loss 1.1436480186174593\n",
      "Epoch 1, Training Loss 1.146067672983155\n",
      "Epoch 1, Training Loss 1.1484753765413522\n",
      "Epoch 1, Training Loss 1.1507128960336261\n",
      "Epoch 1, Training Loss 1.1530556951642341\n",
      "Epoch 1, Training Loss 1.1551830652729629\n",
      "Epoch 1, Training Loss 1.1572657969906508\n",
      "Epoch 1, Training Loss 1.1593432667310282\n",
      "Epoch 1, Training Loss 1.1616694376901593\n",
      "Epoch 1, Training Loss 1.1636937676793169\n",
      "Epoch 1, Training Loss 1.1657923258783873\n",
      "Epoch 1, Training Loss 1.1679735938301476\n",
      "Epoch 1, Training Loss 1.170354419809473\n",
      "Epoch 1, Training Loss 1.1722070826288988\n",
      "Epoch 1, Training Loss 1.174471698453664\n",
      "Epoch 1, Training Loss 1.176590029845762\n",
      "Epoch 1, Training Loss 1.1791097851055663\n",
      "Epoch 1, Training Loss 1.18142497219393\n",
      "Epoch 1, Training Loss 1.1835998147344955\n",
      "Epoch 1, Training Loss 1.1856991820932958\n",
      "Epoch 1, Training Loss 1.1880583491776606\n",
      "Epoch 1, Training Loss 1.1902693885061748\n",
      "Epoch 1, Training Loss 1.192478549145067\n",
      "Epoch 1, Training Loss 1.1946036646433194\n",
      "Epoch 1, Training Loss 1.1967915054172507\n",
      "Epoch 1, Training Loss 1.1991460157172453\n",
      "Epoch 1, Training Loss 1.201372434568527\n",
      "Epoch 1, Training Loss 1.203625870482696\n",
      "Epoch 1, Training Loss 1.2059758582993236\n",
      "Epoch 1, Training Loss 1.2081585723115964\n",
      "Epoch 1, Training Loss 1.2102388568851343\n",
      "Epoch 1, Training Loss 1.2125369714349128\n",
      "Epoch 1, Training Loss 1.214768040058253\n",
      "Epoch 1, Training Loss 1.2170822118859157\n",
      "Epoch 1, Training Loss 1.219546167137068\n",
      "Epoch 1, Training Loss 1.2218891801431662\n",
      "Epoch 1, Training Loss 1.2240576015408997\n",
      "Epoch 1, Training Loss 1.2260186801786008\n",
      "Epoch 1, Training Loss 1.2284694464920123\n",
      "Epoch 1, Training Loss 1.2305561043417361\n",
      "Epoch 1, Training Loss 1.2327751389244939\n",
      "Epoch 1, Training Loss 1.2349307102620448\n",
      "Epoch 1, Training Loss 1.237062790509685\n",
      "Epoch 1, Training Loss 1.2392529326936472\n",
      "Epoch 1, Training Loss 1.2416638068835755\n",
      "Epoch 1, Training Loss 1.243750541563839\n",
      "Epoch 1, Training Loss 1.245683095034431\n",
      "Epoch 1, Training Loss 1.2477652026564263\n",
      "Epoch 1, Training Loss 1.2498786126256294\n",
      "Epoch 1, Training Loss 1.2521476404136405\n",
      "Epoch 1, Training Loss 1.2544014723709478\n",
      "Epoch 1, Training Loss 1.2564123307957369\n",
      "Epoch 1, Training Loss 1.258593018402529\n",
      "Epoch 1, Training Loss 1.2606031236136357\n",
      "Epoch 1, Training Loss 1.2628120722063363\n",
      "Epoch 1, Training Loss 1.2650405962753783\n",
      "Epoch 1, Training Loss 1.2670632337060426\n",
      "Epoch 1, Training Loss 1.2692549006103555\n",
      "Epoch 1, Training Loss 1.2714740816894394\n",
      "Epoch 1, Training Loss 1.2734564804969846\n",
      "Epoch 1, Training Loss 1.275567685399214\n",
      "Epoch 1, Training Loss 1.2777102748146447\n",
      "Epoch 1, Training Loss 1.2798025826054156\n",
      "Epoch 1, Training Loss 1.2820507295601202\n",
      "Epoch 1, Training Loss 1.2844114071877717\n",
      "Epoch 1, Training Loss 1.286227654465629\n",
      "Epoch 1, Training Loss 1.288235555676853\n",
      "Epoch 1, Training Loss 1.2904099474477646\n",
      "Epoch 1, Training Loss 1.2923931666957142\n",
      "Epoch 1, Training Loss 1.2945618554759208\n",
      "Epoch 1, Training Loss 1.2969852144760854\n",
      "Epoch 1, Training Loss 1.299188076687591\n",
      "Epoch 1, Training Loss 1.3015118605645417\n",
      "Epoch 1, Training Loss 1.3037192417532586\n",
      "Epoch 1, Training Loss 1.3059300737612693\n",
      "Epoch 1, Training Loss 1.3079969568935501\n",
      "Epoch 1, Training Loss 1.3101634273443685\n",
      "Epoch 1, Training Loss 1.311976714969596\n",
      "Epoch 1, Training Loss 1.3140773116170292\n",
      "Epoch 1, Training Loss 1.3165593372891322\n",
      "Epoch 1, Training Loss 1.3189569635464407\n",
      "Epoch 1, Training Loss 1.321242569962426\n",
      "Epoch 1, Training Loss 1.3232325763653612\n",
      "Epoch 1, Training Loss 1.325234456440372\n",
      "Epoch 1, Training Loss 1.327382501586319\n",
      "Epoch 1, Training Loss 1.3293732386415877\n",
      "Epoch 1, Training Loss 1.331360627013399\n",
      "Epoch 1, Training Loss 1.333678374509982\n",
      "Epoch 1, Training Loss 1.335786389572846\n",
      "Epoch 1, Training Loss 1.3378580616563178\n",
      "Epoch 1, Training Loss 1.3401202600630349\n",
      "Epoch 1, Training Loss 1.3423778804976616\n",
      "Epoch 1, Training Loss 1.3444018757251828\n",
      "Epoch 1, Training Loss 1.3463150985405574\n",
      "Epoch 1, Training Loss 1.3484819113750897\n",
      "Epoch 1, Training Loss 1.350528635179905\n",
      "Epoch 1, Training Loss 1.3529963787559354\n",
      "Epoch 1, Training Loss 1.3553678910140796\n",
      "Epoch 1, Training Loss 1.3575807049146393\n",
      "Epoch 1, Training Loss 1.3598115674370086\n",
      "Epoch 1, Training Loss 1.3619052877511515\n",
      "Epoch 1, Training Loss 1.3638625196788623\n",
      "Epoch 1, Training Loss 1.3659093956203412\n",
      "Epoch 1, Training Loss 1.3680137324211237\n",
      "Epoch 1, Training Loss 1.3698946981478834\n",
      "Epoch 1, Training Loss 1.3719802744248335\n",
      "Epoch 1, Training Loss 1.3738945410074785\n",
      "Epoch 1, Training Loss 1.3757897270914843\n",
      "Epoch 1, Training Loss 1.3781228818551963\n",
      "Epoch 1, Training Loss 1.380172329790452\n",
      "Epoch 1, Training Loss 1.3822155611594316\n",
      "Epoch 1, Training Loss 1.3843236068630462\n",
      "Epoch 1, Training Loss 1.386586908489237\n",
      "Epoch 1, Training Loss 1.3887774596738693\n",
      "Epoch 1, Training Loss 1.3910248675919554\n",
      "Epoch 1, Training Loss 1.3930439391099583\n",
      "Epoch 1, Training Loss 1.3952743775399445\n",
      "Epoch 1, Training Loss 1.3973011093981125\n",
      "Epoch 1, Training Loss 1.3992055614890955\n",
      "Epoch 1, Training Loss 1.4013506163416616\n",
      "Epoch 1, Training Loss 1.4033837304700671\n",
      "Epoch 1, Training Loss 1.4055344305379922\n",
      "Epoch 1, Training Loss 1.407583590816049\n",
      "Epoch 1, Training Loss 1.4095565379428132\n",
      "Epoch 1, Training Loss 1.4112880810752244\n",
      "Epoch 1, Training Loss 1.4132100249190465\n",
      "Epoch 1, Training Loss 1.4152867457141047\n",
      "Epoch 1, Training Loss 1.417201849658166\n",
      "Epoch 1, Training Loss 1.4193013982699656\n",
      "Epoch 1, Training Loss 1.421441717220999\n",
      "Epoch 1, Training Loss 1.4234471115309868\n",
      "Epoch 1, Training Loss 1.4255534171143456\n",
      "Epoch 1, Training Loss 1.4276574529955148\n",
      "Epoch 1, Training Loss 1.4298940311612376\n",
      "Epoch 1, Training Loss 1.4322925902083707\n",
      "Epoch 1, Training Loss 1.4345684264931837\n",
      "Epoch 1, Training Loss 1.4367721062487044\n",
      "Epoch 1, Training Loss 1.4389540377785177\n",
      "Epoch 1, Training Loss 1.441053885785515\n",
      "Epoch 1, Training Loss 1.4428584310404784\n",
      "Epoch 1, Training Loss 1.4450131192841493\n",
      "Epoch 1, Training Loss 1.4470494621245147\n",
      "Epoch 1, Training Loss 1.449124082579942\n",
      "Epoch 1, Training Loss 1.451303275192485\n",
      "Epoch 1, Training Loss 1.4533744698290325\n",
      "Epoch 1, Training Loss 1.4554242385013023\n",
      "Epoch 1, Training Loss 1.4575090606499206\n",
      "Epoch 1, Training Loss 1.4594548230280961\n",
      "Epoch 1, Training Loss 1.4618199213081613\n",
      "Epoch 1, Training Loss 1.4638121054910334\n",
      "Epoch 1, Training Loss 1.4658162542011426\n",
      "Epoch 1, Training Loss 1.4677823411534205\n",
      "Epoch 1, Training Loss 1.4698685624105545\n",
      "Epoch 1, Training Loss 1.47168477599883\n",
      "Epoch 1, Training Loss 1.4733510303985127\n",
      "Epoch 1, Training Loss 1.4755528143909582\n",
      "Epoch 1, Training Loss 1.4777548300945544\n",
      "Epoch 1, Training Loss 1.4798125356359555\n",
      "Epoch 1, Training Loss 1.482018018470091\n",
      "Epoch 1, Training Loss 1.4843123980495325\n",
      "Epoch 1, Training Loss 1.4864120992553203\n",
      "Epoch 1, Training Loss 1.4888595406661558\n",
      "Epoch 1, Training Loss 1.4910809476967053\n",
      "Epoch 1, Training Loss 1.4935081938038701\n",
      "Epoch 1, Training Loss 1.4952234485570122\n",
      "Epoch 1, Training Loss 1.4975289742050268\n",
      "Epoch 1, Training Loss 1.4995158470195273\n",
      "Epoch 1, Training Loss 1.5016216640277287\n",
      "Epoch 1, Training Loss 1.5035696272045145\n",
      "Epoch 1, Training Loss 1.5055292820381692\n",
      "Epoch 1, Training Loss 1.507665547873358\n",
      "Epoch 1, Training Loss 1.5097078664223555\n",
      "Epoch 1, Training Loss 1.5118209170868329\n",
      "Epoch 1, Training Loss 1.5137381065836952\n",
      "Epoch 1, Training Loss 1.5158776639367613\n",
      "Epoch 1, Training Loss 1.5179050061708825\n",
      "Epoch 1, Training Loss 1.5198596185430542\n",
      "Epoch 1, Training Loss 1.5220984930882369\n",
      "Epoch 1, Training Loss 1.5243466565066286\n",
      "Epoch 1, Training Loss 1.5265340666331904\n",
      "Epoch 1, Training Loss 1.528607379292588\n",
      "Epoch 1, Training Loss 1.5306922991562377\n",
      "Epoch 1, Training Loss 1.532753405211222\n",
      "Epoch 1, Training Loss 1.5347396652107044\n",
      "Epoch 1, Training Loss 1.536756301474998\n",
      "Epoch 1, Training Loss 1.5391011857010823\n",
      "Epoch 1, Training Loss 1.5411813704254071\n",
      "Epoch 1, Training Loss 1.54329507308238\n",
      "Epoch 1, Training Loss 1.5454017096163366\n",
      "Epoch 1, Training Loss 1.547585605965246\n",
      "Epoch 1, Training Loss 1.5496412979062562\n",
      "Epoch 1, Training Loss 1.5516573173920516\n",
      "Epoch 1, Training Loss 1.5534544241093005\n",
      "Epoch 1, Training Loss 1.5553693794228536\n",
      "Epoch 1, Training Loss 1.5571407994345936\n",
      "Epoch 1, Training Loss 1.5592997229617576\n",
      "Epoch 1, Training Loss 1.5611695008509605\n",
      "Epoch 1, Training Loss 1.5631011482089987\n",
      "Epoch 1, Training Loss 1.5654391457357675\n",
      "Epoch 1, Training Loss 1.5674801871294866\n",
      "Epoch 1, Training Loss 1.5695859134349677\n",
      "Epoch 1, Training Loss 1.5713609380795217\n",
      "Epoch 1, Training Loss 1.5734659396778896\n",
      "Epoch 1, Training Loss 1.5755662677233175\n",
      "Epoch 1, Training Loss 1.5777572219633995\n",
      "Epoch 1, Training Loss 1.5799366578726513\n",
      "Epoch 1, Training Loss 1.582084955309358\n",
      "Epoch 1, Training Loss 1.5844031543378025\n",
      "Epoch 1, Training Loss 1.5864751451765484\n",
      "Epoch 1, Training Loss 1.5885082221092166\n",
      "Epoch 1, Training Loss 1.5904523080877027\n",
      "Epoch 1, Training Loss 1.5924457523523998\n",
      "Epoch 1, Training Loss 1.5943137815846202\n",
      "Epoch 1, Training Loss 1.5966116604597673\n",
      "Epoch 1, Training Loss 1.5986194445958832\n",
      "Epoch 1, Training Loss 1.6005929412744235\n",
      "Epoch 1, Training Loss 1.6026188330272275\n",
      "Epoch 1, Training Loss 1.6047894811386343\n",
      "Epoch 1, Training Loss 1.6068936025395113\n",
      "Epoch 1, Training Loss 1.6091192672624612\n",
      "Epoch 1, Training Loss 1.6113052189807453\n",
      "Epoch 1, Training Loss 1.6132871569574947\n",
      "Epoch 1, Training Loss 1.6153498433739937\n",
      "Epoch 1, Training Loss 1.617662838200474\n",
      "Epoch 1, Training Loss 1.6196502334321552\n",
      "Epoch 1, Training Loss 1.6216663635905137\n",
      "Epoch 1, Training Loss 1.6236935297546484\n",
      "Epoch 1, Training Loss 1.6256559841773088\n",
      "Epoch 1, Training Loss 1.6277234664048685\n",
      "Epoch 1, Training Loss 1.6297495313312695\n",
      "Epoch 1, Training Loss 1.631767065323832\n",
      "Epoch 1, Training Loss 1.6336218420501865\n",
      "Epoch 1, Training Loss 1.6355171743256356\n",
      "Epoch 1, Training Loss 1.6378534013962807\n",
      "Epoch 1, Training Loss 1.6398132626357895\n",
      "Epoch 1, Training Loss 1.6416265289192005\n",
      "Epoch 1, Training Loss 1.6439691841449884\n",
      "Epoch 1, Training Loss 1.646126635696577\n",
      "Epoch 1, Training Loss 1.6478670458964375\n",
      "Epoch 1, Training Loss 1.649972420976595\n",
      "Epoch 1, Training Loss 1.6520110833675354\n",
      "Epoch 1, Training Loss 1.6542336215143618\n",
      "Epoch 1, Training Loss 1.65622910697137\n",
      "Epoch 1, Training Loss 1.6583472915622584\n",
      "Epoch 1, Training Loss 1.6604566686903424\n",
      "Epoch 1, Training Loss 1.6625966339769875\n",
      "Epoch 1, Training Loss 1.6645503138642177\n",
      "Epoch 1, Training Loss 1.6665142446832584\n",
      "Epoch 1, Training Loss 1.6683111771598191\n",
      "Epoch 1, Training Loss 1.670435783655747\n",
      "Epoch 1, Training Loss 1.6725508067613977\n",
      "Epoch 1, Training Loss 1.6747276633596786\n",
      "Epoch 1, Training Loss 1.676743374151342\n",
      "Epoch 1, Training Loss 1.6788080644119732\n",
      "Epoch 1, Training Loss 1.6813196841713107\n",
      "Epoch 1, Training Loss 1.683322563653102\n",
      "Epoch 1, Training Loss 1.6855710813456484\n",
      "Epoch 1, Training Loss 1.6875181393245298\n",
      "Epoch 1, Training Loss 1.6894757557098212\n",
      "Epoch 1, Training Loss 1.691200288055498\n",
      "Epoch 1, Training Loss 1.6933563523890112\n",
      "Epoch 1, Training Loss 1.6955000648413168\n",
      "Epoch 1, Training Loss 1.6973281697848874\n",
      "Epoch 1, Training Loss 1.6994172793520077\n",
      "Epoch 1, Training Loss 1.7016846717471052\n",
      "Epoch 1, Training Loss 1.7038466325196464\n",
      "Epoch 1, Training Loss 1.70571520810237\n",
      "Epoch 1, Training Loss 1.7076995945952433\n",
      "Epoch 1, Training Loss 1.7096057916846117\n",
      "Epoch 1, Training Loss 1.7117105414495444\n",
      "Epoch 1, Training Loss 1.7139666662801563\n",
      "Epoch 1, Training Loss 1.7159735063457733\n",
      "Epoch 1, Training Loss 1.7182236659862196\n",
      "Epoch 1, Training Loss 1.7204239446183909\n",
      "Epoch 1, Training Loss 1.7225349333585072\n",
      "Epoch 1, Training Loss 1.724476510454017\n",
      "Epoch 1, Training Loss 1.7266084807913016\n",
      "Epoch 1, Training Loss 1.7287801549867596\n",
      "Epoch 1, Training Loss 1.7307001882806763\n",
      "Epoch 1, Training Loss 1.7329420316249817\n",
      "Epoch 1, Training Loss 1.7350631362336981\n",
      "Epoch 1, Training Loss 1.7371203283519696\n",
      "Epoch 1, Training Loss 1.739275602276063\n",
      "Epoch 1, Training Loss 1.7415034708464543\n",
      "Epoch 1, Training Loss 1.743354750716168\n",
      "Epoch 1, Training Loss 1.745454625400436\n",
      "Epoch 1, Training Loss 1.7475597824891815\n",
      "Epoch 1, Training Loss 1.749713781087295\n",
      "Epoch 1, Training Loss 1.751974329771593\n",
      "Epoch 1, Training Loss 1.75396218583407\n",
      "Epoch 1, Training Loss 1.75611652117556\n",
      "Epoch 1, Training Loss 1.7581992374966517\n",
      "Epoch 1, Training Loss 1.7601948906393612\n",
      "Epoch 1, Training Loss 1.7622545302066657\n",
      "Epoch 1, Training Loss 1.764105285525017\n",
      "Epoch 1, Training Loss 1.7659869050735708\n",
      "Epoch 1, Training Loss 1.7678060882231768\n",
      "Epoch 1, Training Loss 1.7697890588389638\n",
      "Epoch 1, Training Loss 1.7718429428232296\n",
      "Epoch 1, Training Loss 1.7738743128678989\n",
      "Epoch 1, Training Loss 1.7756976252016814\n",
      "Epoch 1, Training Loss 1.7777249698748674\n",
      "Epoch 1, Training Loss 1.780062539345773\n",
      "Epoch 1, Training Loss 1.7820323649269845\n",
      "Epoch 1, Training Loss 1.784368553125035\n",
      "Epoch 1, Training Loss 1.7862493651907156\n",
      "Epoch 1, Training Loss 1.78810381371042\n",
      "Epoch 1, Training Loss 1.7903194189681422\n",
      "Epoch 1, Training Loss 1.7924547073481334\n",
      "Epoch 1, Training Loss 1.7944374529601972\n",
      "Epoch 1, Training Loss 1.7964239949765413\n",
      "Epoch 1, Training Loss 1.7984038961817845\n",
      "Epoch 1, Training Loss 1.800289499942604\n",
      "Epoch 1, Training Loss 1.8024891773453149\n",
      "Epoch 1, Training Loss 1.8046311661410515\n",
      "Epoch 1, Training Loss 1.8065398806501227\n",
      "Epoch 1, Training Loss 1.808632862994738\n",
      "Epoch 1, Training Loss 1.8104358228576152\n",
      "Epoch 1, Training Loss 1.812641527951526\n",
      "Epoch 1, Training Loss 1.8146250781500737\n",
      "Epoch 1, Training Loss 1.8166402378655455\n",
      "Epoch 1, Training Loss 1.8185669157816016\n",
      "Epoch 1, Training Loss 1.820457115350172\n",
      "Epoch 1, Training Loss 1.8224853841240143\n",
      "Epoch 1, Training Loss 1.824767959392284\n",
      "Epoch 1, Training Loss 1.8266706204475345\n",
      "Epoch 1, Training Loss 1.8286136211946493\n",
      "Epoch 1, Training Loss 1.8307568665660556\n",
      "Epoch 1, Training Loss 1.8326532512979434\n",
      "Epoch 1, Training Loss 1.83467337085158\n",
      "Epoch 10, Training Loss 0.0011257396634582364\n",
      "Epoch 10, Training Loss 0.0022391004635549873\n",
      "Epoch 10, Training Loss 0.0035464443514109266\n",
      "Epoch 10, Training Loss 0.004767156927786825\n",
      "Epoch 10, Training Loss 0.006206174335821205\n",
      "Epoch 10, Training Loss 0.007287414863591304\n",
      "Epoch 10, Training Loss 0.008663419643631372\n",
      "Epoch 10, Training Loss 0.009823756678330015\n",
      "Epoch 10, Training Loss 0.010954392764269544\n",
      "Epoch 10, Training Loss 0.012248153195661656\n",
      "Epoch 10, Training Loss 0.013579924469408781\n",
      "Epoch 10, Training Loss 0.014838997665268685\n",
      "Epoch 10, Training Loss 0.0159085245083665\n",
      "Epoch 10, Training Loss 0.01708649255125724\n",
      "Epoch 10, Training Loss 0.018319809147159156\n",
      "Epoch 10, Training Loss 0.019384828293719864\n",
      "Epoch 10, Training Loss 0.020424100977685447\n",
      "Epoch 10, Training Loss 0.02167621315897578\n",
      "Epoch 10, Training Loss 0.023085974900009076\n",
      "Epoch 10, Training Loss 0.02446760965125335\n",
      "Epoch 10, Training Loss 0.025418079189022485\n",
      "Epoch 10, Training Loss 0.02677917030766187\n",
      "Epoch 10, Training Loss 0.027919447635445755\n",
      "Epoch 10, Training Loss 0.02931687502604921\n",
      "Epoch 10, Training Loss 0.03066744340960022\n",
      "Epoch 10, Training Loss 0.03202369999702629\n",
      "Epoch 10, Training Loss 0.03322581348516752\n",
      "Epoch 10, Training Loss 0.03477600879986268\n",
      "Epoch 10, Training Loss 0.035960352786666594\n",
      "Epoch 10, Training Loss 0.037348854572266874\n",
      "Epoch 10, Training Loss 0.03864621003265576\n",
      "Epoch 10, Training Loss 0.03991107379688936\n",
      "Epoch 10, Training Loss 0.04111189031235092\n",
      "Epoch 10, Training Loss 0.04239463851884808\n",
      "Epoch 10, Training Loss 0.04387768805789216\n",
      "Epoch 10, Training Loss 0.04498440713223899\n",
      "Epoch 10, Training Loss 0.04609521667060949\n",
      "Epoch 10, Training Loss 0.047430104916662816\n",
      "Epoch 10, Training Loss 0.04897315361920525\n",
      "Epoch 10, Training Loss 0.049994039947114634\n",
      "Epoch 10, Training Loss 0.05108672327092846\n",
      "Epoch 10, Training Loss 0.05241695595214434\n",
      "Epoch 10, Training Loss 0.05354978590060378\n",
      "Epoch 10, Training Loss 0.05488602980933226\n",
      "Epoch 10, Training Loss 0.05586197332043172\n",
      "Epoch 10, Training Loss 0.057182012235417086\n",
      "Epoch 10, Training Loss 0.05864947737025483\n",
      "Epoch 10, Training Loss 0.05973157500062148\n",
      "Epoch 10, Training Loss 0.06087267574142007\n",
      "Epoch 10, Training Loss 0.06198012729739899\n",
      "Epoch 10, Training Loss 0.06312494928879506\n",
      "Epoch 10, Training Loss 0.06426398010205125\n",
      "Epoch 10, Training Loss 0.06545692514580534\n",
      "Epoch 10, Training Loss 0.0664113659383086\n",
      "Epoch 10, Training Loss 0.06779098251591557\n",
      "Epoch 10, Training Loss 0.06863223431665269\n",
      "Epoch 10, Training Loss 0.06976139392999127\n",
      "Epoch 10, Training Loss 0.07105134515201345\n",
      "Epoch 10, Training Loss 0.0725520323304569\n",
      "Epoch 10, Training Loss 0.07382490583088087\n",
      "Epoch 10, Training Loss 0.07514845699910312\n",
      "Epoch 10, Training Loss 0.0763902679428725\n",
      "Epoch 10, Training Loss 0.07767562793038996\n",
      "Epoch 10, Training Loss 0.07865202198248081\n",
      "Epoch 10, Training Loss 0.07988257176430939\n",
      "Epoch 10, Training Loss 0.08122407566860813\n",
      "Epoch 10, Training Loss 0.08243459516474048\n",
      "Epoch 10, Training Loss 0.08366865552294894\n",
      "Epoch 10, Training Loss 0.08497738662888021\n",
      "Epoch 10, Training Loss 0.0865504387242105\n",
      "Epoch 10, Training Loss 0.08783763197376905\n",
      "Epoch 10, Training Loss 0.08898663132087044\n",
      "Epoch 10, Training Loss 0.09004162911259\n",
      "Epoch 10, Training Loss 0.09144686303480203\n",
      "Epoch 10, Training Loss 0.09288143616198274\n",
      "Epoch 10, Training Loss 0.09395473966818027\n",
      "Epoch 10, Training Loss 0.09525357289692325\n",
      "Epoch 10, Training Loss 0.09625497856713316\n",
      "Epoch 10, Training Loss 0.09741831252641996\n",
      "Epoch 10, Training Loss 0.09866164155933253\n",
      "Epoch 10, Training Loss 0.09995275865430417\n",
      "Epoch 10, Training Loss 0.10109060674981998\n",
      "Epoch 10, Training Loss 0.10237191301172652\n",
      "Epoch 10, Training Loss 0.10364358130928196\n",
      "Epoch 10, Training Loss 0.10473728073222557\n",
      "Epoch 10, Training Loss 0.10634284053007355\n",
      "Epoch 10, Training Loss 0.108000060329047\n",
      "Epoch 10, Training Loss 0.10921653746948827\n",
      "Epoch 10, Training Loss 0.1103831728553528\n",
      "Epoch 10, Training Loss 0.11150754961516242\n",
      "Epoch 10, Training Loss 0.11291492808505398\n",
      "Epoch 10, Training Loss 0.11398180678982259\n",
      "Epoch 10, Training Loss 0.11523138135290512\n",
      "Epoch 10, Training Loss 0.11650263378992105\n",
      "Epoch 10, Training Loss 0.11758131551010834\n",
      "Epoch 10, Training Loss 0.11878798982066571\n",
      "Epoch 10, Training Loss 0.11991798824361523\n",
      "Epoch 10, Training Loss 0.12100289171309117\n",
      "Epoch 10, Training Loss 0.12195785804782681\n",
      "Epoch 10, Training Loss 0.12299827419583449\n",
      "Epoch 10, Training Loss 0.12431427310494815\n",
      "Epoch 10, Training Loss 0.12582579049307976\n",
      "Epoch 10, Training Loss 0.12715823708287896\n",
      "Epoch 10, Training Loss 0.12836891252671362\n",
      "Epoch 10, Training Loss 0.12951497257213154\n",
      "Epoch 10, Training Loss 0.1306113463533504\n",
      "Epoch 10, Training Loss 0.13185391577003558\n",
      "Epoch 10, Training Loss 0.13315563700388156\n",
      "Epoch 10, Training Loss 0.1345293091233734\n",
      "Epoch 10, Training Loss 0.13575432039892582\n",
      "Epoch 10, Training Loss 0.1369600748009694\n",
      "Epoch 10, Training Loss 0.1382761362111172\n",
      "Epoch 10, Training Loss 0.13937286015056893\n",
      "Epoch 10, Training Loss 0.1407062988299543\n",
      "Epoch 10, Training Loss 0.14207729742959943\n",
      "Epoch 10, Training Loss 0.14317486719097322\n",
      "Epoch 10, Training Loss 0.1443776334338176\n",
      "Epoch 10, Training Loss 0.14568987389659638\n",
      "Epoch 10, Training Loss 0.14709464028058455\n",
      "Epoch 10, Training Loss 0.14816037049074002\n",
      "Epoch 10, Training Loss 0.14943837563095191\n",
      "Epoch 10, Training Loss 0.1506745379294276\n",
      "Epoch 10, Training Loss 0.15226785407956603\n",
      "Epoch 10, Training Loss 0.15337759042944749\n",
      "Epoch 10, Training Loss 0.1548963486386077\n",
      "Epoch 10, Training Loss 0.15635574260331175\n",
      "Epoch 10, Training Loss 0.1576025361752571\n",
      "Epoch 10, Training Loss 0.15904818494301623\n",
      "Epoch 10, Training Loss 0.16033452650165314\n",
      "Epoch 10, Training Loss 0.16170382461584437\n",
      "Epoch 10, Training Loss 0.16290087475801063\n",
      "Epoch 10, Training Loss 0.16420006332799908\n",
      "Epoch 10, Training Loss 0.16533014102055288\n",
      "Epoch 10, Training Loss 0.16644604393588308\n",
      "Epoch 10, Training Loss 0.1680124841840066\n",
      "Epoch 10, Training Loss 0.1694220988189473\n",
      "Epoch 10, Training Loss 0.17034975052489648\n",
      "Epoch 10, Training Loss 0.17152005495012873\n",
      "Epoch 10, Training Loss 0.17306817126700946\n",
      "Epoch 10, Training Loss 0.17436508678109444\n",
      "Epoch 10, Training Loss 0.17564196110991262\n",
      "Epoch 10, Training Loss 0.17736927048324624\n",
      "Epoch 10, Training Loss 0.1787103762102249\n",
      "Epoch 10, Training Loss 0.17977622547722838\n",
      "Epoch 10, Training Loss 0.18093440775066386\n",
      "Epoch 10, Training Loss 0.1823388716906233\n",
      "Epoch 10, Training Loss 0.1834714584948157\n",
      "Epoch 10, Training Loss 0.18449367289348026\n",
      "Epoch 10, Training Loss 0.18567152668143172\n",
      "Epoch 10, Training Loss 0.18693097419750965\n",
      "Epoch 10, Training Loss 0.18816840389500494\n",
      "Epoch 10, Training Loss 0.1893167518593771\n",
      "Epoch 10, Training Loss 0.19060532897329696\n",
      "Epoch 10, Training Loss 0.19177364098751332\n",
      "Epoch 10, Training Loss 0.19288867994037737\n",
      "Epoch 10, Training Loss 0.19413966222492327\n",
      "Epoch 10, Training Loss 0.19523754205240312\n",
      "Epoch 10, Training Loss 0.19642485651518682\n",
      "Epoch 10, Training Loss 0.1977992772751147\n",
      "Epoch 10, Training Loss 0.19904971770618274\n",
      "Epoch 10, Training Loss 0.20046343118943216\n",
      "Epoch 10, Training Loss 0.20162225112585766\n",
      "Epoch 10, Training Loss 0.2025976522499338\n",
      "Epoch 10, Training Loss 0.2039392009720473\n",
      "Epoch 10, Training Loss 0.20500117586091962\n",
      "Epoch 10, Training Loss 0.20630777903529993\n",
      "Epoch 10, Training Loss 0.2074987621563475\n",
      "Epoch 10, Training Loss 0.2088060563482592\n",
      "Epoch 10, Training Loss 0.20995242478292617\n",
      "Epoch 10, Training Loss 0.21130377290498875\n",
      "Epoch 10, Training Loss 0.21244356905103035\n",
      "Epoch 10, Training Loss 0.2137757787466659\n",
      "Epoch 10, Training Loss 0.21487718500444652\n",
      "Epoch 10, Training Loss 0.21607185759202904\n",
      "Epoch 10, Training Loss 0.21738564137302702\n",
      "Epoch 10, Training Loss 0.21861234269178736\n",
      "Epoch 10, Training Loss 0.2196498286846044\n",
      "Epoch 10, Training Loss 0.22099582885232422\n",
      "Epoch 10, Training Loss 0.2222277780475519\n",
      "Epoch 10, Training Loss 0.22352834118296727\n",
      "Epoch 10, Training Loss 0.22464921925683765\n",
      "Epoch 10, Training Loss 0.2256998836689288\n",
      "Epoch 10, Training Loss 0.226793544219278\n",
      "Epoch 10, Training Loss 0.2278283428962883\n",
      "Epoch 10, Training Loss 0.2292233127767168\n",
      "Epoch 10, Training Loss 0.23041727605378232\n",
      "Epoch 10, Training Loss 0.23144429457157165\n",
      "Epoch 10, Training Loss 0.23294348096298745\n",
      "Epoch 10, Training Loss 0.23409126504607822\n",
      "Epoch 10, Training Loss 0.23520756720581934\n",
      "Epoch 10, Training Loss 0.23668372028928888\n",
      "Epoch 10, Training Loss 0.23798949677316125\n",
      "Epoch 10, Training Loss 0.2391548652935516\n",
      "Epoch 10, Training Loss 0.24035052204375987\n",
      "Epoch 10, Training Loss 0.24173956072848776\n",
      "Epoch 10, Training Loss 0.2429813657270368\n",
      "Epoch 10, Training Loss 0.24405458645747446\n",
      "Epoch 10, Training Loss 0.2451574907583349\n",
      "Epoch 10, Training Loss 0.24612223034929437\n",
      "Epoch 10, Training Loss 0.24749337757944756\n",
      "Epoch 10, Training Loss 0.2490123171940484\n",
      "Epoch 10, Training Loss 0.25016132469677255\n",
      "Epoch 10, Training Loss 0.2513311929105188\n",
      "Epoch 10, Training Loss 0.25309134444312364\n",
      "Epoch 10, Training Loss 0.254542492692123\n",
      "Epoch 10, Training Loss 0.2559484476628511\n",
      "Epoch 10, Training Loss 0.25731172586036155\n",
      "Epoch 10, Training Loss 0.25824591204943254\n",
      "Epoch 10, Training Loss 0.2592453564067021\n",
      "Epoch 10, Training Loss 0.26045717096999477\n",
      "Epoch 10, Training Loss 0.26181288440818984\n",
      "Epoch 10, Training Loss 0.2631807459132446\n",
      "Epoch 10, Training Loss 0.26415970471813854\n",
      "Epoch 10, Training Loss 0.2655197206665488\n",
      "Epoch 10, Training Loss 0.2666346212024884\n",
      "Epoch 10, Training Loss 0.26795467642871923\n",
      "Epoch 10, Training Loss 0.26928455971391\n",
      "Epoch 10, Training Loss 0.27051075126813806\n",
      "Epoch 10, Training Loss 0.27187755223735216\n",
      "Epoch 10, Training Loss 0.2729495888018547\n",
      "Epoch 10, Training Loss 0.27413461587922955\n",
      "Epoch 10, Training Loss 0.2752094536333743\n",
      "Epoch 10, Training Loss 0.276312692467209\n",
      "Epoch 10, Training Loss 0.2775070536929323\n",
      "Epoch 10, Training Loss 0.2784410733396135\n",
      "Epoch 10, Training Loss 0.2797162687534566\n",
      "Epoch 10, Training Loss 0.2810068614495075\n",
      "Epoch 10, Training Loss 0.2822637216514334\n",
      "Epoch 10, Training Loss 0.2833265266607485\n",
      "Epoch 10, Training Loss 0.2843485820628798\n",
      "Epoch 10, Training Loss 0.2853429569765125\n",
      "Epoch 10, Training Loss 0.28683192627814114\n",
      "Epoch 10, Training Loss 0.28801136408620476\n",
      "Epoch 10, Training Loss 0.28889247332997336\n",
      "Epoch 10, Training Loss 0.29015914002038024\n",
      "Epoch 10, Training Loss 0.29121445397586776\n",
      "Epoch 10, Training Loss 0.29243096404368313\n",
      "Epoch 10, Training Loss 0.293630899294563\n",
      "Epoch 10, Training Loss 0.2947178673561272\n",
      "Epoch 10, Training Loss 0.29617312131330487\n",
      "Epoch 10, Training Loss 0.2975994514687287\n",
      "Epoch 10, Training Loss 0.2987555153382099\n",
      "Epoch 10, Training Loss 0.3000520198698849\n",
      "Epoch 10, Training Loss 0.3013313320439185\n",
      "Epoch 10, Training Loss 0.30301112553957477\n",
      "Epoch 10, Training Loss 0.3041875730542576\n",
      "Epoch 10, Training Loss 0.30536958567626643\n",
      "Epoch 10, Training Loss 0.30681423869584223\n",
      "Epoch 10, Training Loss 0.3080779791183179\n",
      "Epoch 10, Training Loss 0.30991000851706774\n",
      "Epoch 10, Training Loss 0.31112690426199635\n",
      "Epoch 10, Training Loss 0.312511532507894\n",
      "Epoch 10, Training Loss 0.3136446058292828\n",
      "Epoch 10, Training Loss 0.3150358916548512\n",
      "Epoch 10, Training Loss 0.31643693724556654\n",
      "Epoch 10, Training Loss 0.3178052716242993\n",
      "Epoch 10, Training Loss 0.3189444696659322\n",
      "Epoch 10, Training Loss 0.3203082315604705\n",
      "Epoch 10, Training Loss 0.32165498250280805\n",
      "Epoch 10, Training Loss 0.3228362588321461\n",
      "Epoch 10, Training Loss 0.3240266711358219\n",
      "Epoch 10, Training Loss 0.3250591587989836\n",
      "Epoch 10, Training Loss 0.326098408006951\n",
      "Epoch 10, Training Loss 0.32718204697379677\n",
      "Epoch 10, Training Loss 0.3284550896843376\n",
      "Epoch 10, Training Loss 0.3297316268124544\n",
      "Epoch 10, Training Loss 0.3305887997607746\n",
      "Epoch 10, Training Loss 0.3315994678555852\n",
      "Epoch 10, Training Loss 0.33317270836866725\n",
      "Epoch 10, Training Loss 0.3345236545023711\n",
      "Epoch 10, Training Loss 0.3355718374709644\n",
      "Epoch 10, Training Loss 0.33674458218047687\n",
      "Epoch 10, Training Loss 0.33816679626169716\n",
      "Epoch 10, Training Loss 0.33937792072210776\n",
      "Epoch 10, Training Loss 0.3407246683107313\n",
      "Epoch 10, Training Loss 0.3419036876667491\n",
      "Epoch 10, Training Loss 0.34297647058506453\n",
      "Epoch 10, Training Loss 0.3439651175837992\n",
      "Epoch 10, Training Loss 0.3452581400456636\n",
      "Epoch 10, Training Loss 0.3466872226856554\n",
      "Epoch 10, Training Loss 0.3480337004527411\n",
      "Epoch 10, Training Loss 0.3493505698030867\n",
      "Epoch 10, Training Loss 0.3507744273566224\n",
      "Epoch 10, Training Loss 0.3519458106106809\n",
      "Epoch 10, Training Loss 0.3533659232851794\n",
      "Epoch 10, Training Loss 0.35436825625731816\n",
      "Epoch 10, Training Loss 0.3557330096316764\n",
      "Epoch 10, Training Loss 0.35673429853166155\n",
      "Epoch 10, Training Loss 0.35790833175334785\n",
      "Epoch 10, Training Loss 0.35923454638027474\n",
      "Epoch 10, Training Loss 0.35997461990627183\n",
      "Epoch 10, Training Loss 0.36113316925895184\n",
      "Epoch 10, Training Loss 0.36253577417424876\n",
      "Epoch 10, Training Loss 0.363920660503685\n",
      "Epoch 10, Training Loss 0.36501413172163316\n",
      "Epoch 10, Training Loss 0.3662600391508673\n",
      "Epoch 10, Training Loss 0.3672783277985995\n",
      "Epoch 10, Training Loss 0.36842291083787104\n",
      "Epoch 10, Training Loss 0.3696896824080621\n",
      "Epoch 10, Training Loss 0.3710177240469267\n",
      "Epoch 10, Training Loss 0.37231041525330993\n",
      "Epoch 10, Training Loss 0.3736105209116436\n",
      "Epoch 10, Training Loss 0.37502112473978105\n",
      "Epoch 10, Training Loss 0.3765616569372699\n",
      "Epoch 10, Training Loss 0.3776173745579732\n",
      "Epoch 10, Training Loss 0.3784976413335337\n",
      "Epoch 10, Training Loss 0.37953933311240445\n",
      "Epoch 10, Training Loss 0.38069151612498875\n",
      "Epoch 10, Training Loss 0.3816623005568219\n",
      "Epoch 10, Training Loss 0.3828160520404806\n",
      "Epoch 10, Training Loss 0.3840247884278407\n",
      "Epoch 10, Training Loss 0.38543996687435433\n",
      "Epoch 10, Training Loss 0.3865741659765658\n",
      "Epoch 10, Training Loss 0.3875434507646829\n",
      "Epoch 10, Training Loss 0.38891197600023214\n",
      "Epoch 10, Training Loss 0.3899949609166216\n",
      "Epoch 10, Training Loss 0.391361319195584\n",
      "Epoch 10, Training Loss 0.39268863719442615\n",
      "Epoch 10, Training Loss 0.39380425763557025\n",
      "Epoch 10, Training Loss 0.3955403029766229\n",
      "Epoch 10, Training Loss 0.3968157659253806\n",
      "Epoch 10, Training Loss 0.39807588311717335\n",
      "Epoch 10, Training Loss 0.3992918539230171\n",
      "Epoch 10, Training Loss 0.40073164512434273\n",
      "Epoch 10, Training Loss 0.40205931114723614\n",
      "Epoch 10, Training Loss 0.4032995061343893\n",
      "Epoch 10, Training Loss 0.4046178168195593\n",
      "Epoch 10, Training Loss 0.4062244156589898\n",
      "Epoch 10, Training Loss 0.4075847256671437\n",
      "Epoch 10, Training Loss 0.40853213867567995\n",
      "Epoch 10, Training Loss 0.40995235180915773\n",
      "Epoch 10, Training Loss 0.41133923344599926\n",
      "Epoch 10, Training Loss 0.41269864923203997\n",
      "Epoch 10, Training Loss 0.41388464766695066\n",
      "Epoch 10, Training Loss 0.4153860287593149\n",
      "Epoch 10, Training Loss 0.4162999955589509\n",
      "Epoch 10, Training Loss 0.4175130846097951\n",
      "Epoch 10, Training Loss 0.4186603883495721\n",
      "Epoch 10, Training Loss 0.4197443023971889\n",
      "Epoch 10, Training Loss 0.4208945445240001\n",
      "Epoch 10, Training Loss 0.4223720361966916\n",
      "Epoch 10, Training Loss 0.42356821193414573\n",
      "Epoch 10, Training Loss 0.4247267708906432\n",
      "Epoch 10, Training Loss 0.4258267011331475\n",
      "Epoch 10, Training Loss 0.426866477331542\n",
      "Epoch 10, Training Loss 0.4279207770171983\n",
      "Epoch 10, Training Loss 0.42900526493101776\n",
      "Epoch 10, Training Loss 0.430459786863888\n",
      "Epoch 10, Training Loss 0.43180915080677823\n",
      "Epoch 10, Training Loss 0.4330672099614692\n",
      "Epoch 10, Training Loss 0.43427322671541474\n",
      "Epoch 10, Training Loss 0.4353496313399976\n",
      "Epoch 10, Training Loss 0.4368413734009199\n",
      "Epoch 10, Training Loss 0.4382231657767235\n",
      "Epoch 10, Training Loss 0.43940558267371427\n",
      "Epoch 10, Training Loss 0.44064422687301247\n",
      "Epoch 10, Training Loss 0.4419014225225619\n",
      "Epoch 10, Training Loss 0.4430902700899812\n",
      "Epoch 10, Training Loss 0.44449984219373034\n",
      "Epoch 10, Training Loss 0.44573089876748107\n",
      "Epoch 10, Training Loss 0.4472084389165844\n",
      "Epoch 10, Training Loss 0.4484991489926263\n",
      "Epoch 10, Training Loss 0.44966905539297997\n",
      "Epoch 10, Training Loss 0.45083258912691376\n",
      "Epoch 10, Training Loss 0.4523632847287161\n",
      "Epoch 10, Training Loss 0.45382048826083504\n",
      "Epoch 10, Training Loss 0.45488242839303467\n",
      "Epoch 10, Training Loss 0.4559434452630065\n",
      "Epoch 10, Training Loss 0.45740210171550744\n",
      "Epoch 10, Training Loss 0.458675921420612\n",
      "Epoch 10, Training Loss 0.4596529667029905\n",
      "Epoch 10, Training Loss 0.4606656471024389\n",
      "Epoch 10, Training Loss 0.46181959363505665\n",
      "Epoch 10, Training Loss 0.4628933278648445\n",
      "Epoch 10, Training Loss 0.4639679929789375\n",
      "Epoch 10, Training Loss 0.46488220010267195\n",
      "Epoch 10, Training Loss 0.4658887837549\n",
      "Epoch 10, Training Loss 0.4672012992222291\n",
      "Epoch 10, Training Loss 0.46837571705393777\n",
      "Epoch 10, Training Loss 0.4696252621958018\n",
      "Epoch 10, Training Loss 0.47080192808300025\n",
      "Epoch 10, Training Loss 0.4717960498674446\n",
      "Epoch 10, Training Loss 0.47296973155892413\n",
      "Epoch 10, Training Loss 0.47425579536906287\n",
      "Epoch 10, Training Loss 0.4752770852097465\n",
      "Epoch 10, Training Loss 0.47661431419575\n",
      "Epoch 10, Training Loss 0.47767618222309804\n",
      "Epoch 10, Training Loss 0.47893689431802694\n",
      "Epoch 10, Training Loss 0.4802355536109651\n",
      "Epoch 10, Training Loss 0.4815709462860966\n",
      "Epoch 10, Training Loss 0.4829508470147467\n",
      "Epoch 10, Training Loss 0.4841392874107946\n",
      "Epoch 10, Training Loss 0.48563101255070523\n",
      "Epoch 10, Training Loss 0.48676710665378425\n",
      "Epoch 10, Training Loss 0.4883726547136331\n",
      "Epoch 10, Training Loss 0.4892565284848518\n",
      "Epoch 10, Training Loss 0.49031454286611903\n",
      "Epoch 10, Training Loss 0.4915979692850576\n",
      "Epoch 10, Training Loss 0.49303596045659936\n",
      "Epoch 10, Training Loss 0.49419298280230567\n",
      "Epoch 10, Training Loss 0.49538267268549147\n",
      "Epoch 10, Training Loss 0.49707716825368153\n",
      "Epoch 10, Training Loss 0.49835653362981497\n",
      "Epoch 10, Training Loss 0.4998873589593736\n",
      "Epoch 10, Training Loss 0.501444389600583\n",
      "Epoch 10, Training Loss 0.5026290314581693\n",
      "Epoch 10, Training Loss 0.5037145330320538\n",
      "Epoch 10, Training Loss 0.5048475270076176\n",
      "Epoch 10, Training Loss 0.5062761113161931\n",
      "Epoch 10, Training Loss 0.50752580950937\n",
      "Epoch 10, Training Loss 0.5086731921376475\n",
      "Epoch 10, Training Loss 0.5100485911149808\n",
      "Epoch 10, Training Loss 0.5114625878346241\n",
      "Epoch 10, Training Loss 0.5123542386400121\n",
      "Epoch 10, Training Loss 0.5134332945279758\n",
      "Epoch 10, Training Loss 0.5145795389514445\n",
      "Epoch 10, Training Loss 0.5159773460739409\n",
      "Epoch 10, Training Loss 0.5170823393575371\n",
      "Epoch 10, Training Loss 0.5185767886278879\n",
      "Epoch 10, Training Loss 0.5197476113543791\n",
      "Epoch 10, Training Loss 0.5209721084445944\n",
      "Epoch 10, Training Loss 0.5220619810511694\n",
      "Epoch 10, Training Loss 0.5231615197475609\n",
      "Epoch 10, Training Loss 0.5245211827937905\n",
      "Epoch 10, Training Loss 0.5259840132482826\n",
      "Epoch 10, Training Loss 0.5274328340197463\n",
      "Epoch 10, Training Loss 0.5287181703788241\n",
      "Epoch 10, Training Loss 0.5297082369894628\n",
      "Epoch 10, Training Loss 0.5306765543073034\n",
      "Epoch 10, Training Loss 0.5317614868168941\n",
      "Epoch 10, Training Loss 0.5329924667887675\n",
      "Epoch 10, Training Loss 0.5343104018579663\n",
      "Epoch 10, Training Loss 0.5353546980244425\n",
      "Epoch 10, Training Loss 0.5365772109354854\n",
      "Epoch 10, Training Loss 0.537613498737745\n",
      "Epoch 10, Training Loss 0.5390067056316854\n",
      "Epoch 10, Training Loss 0.540307249864349\n",
      "Epoch 10, Training Loss 0.5413095480035943\n",
      "Epoch 10, Training Loss 0.5422187988715403\n",
      "Epoch 10, Training Loss 0.5436142564124768\n",
      "Epoch 10, Training Loss 0.5448787497437518\n",
      "Epoch 10, Training Loss 0.5459953504602623\n",
      "Epoch 10, Training Loss 0.5474459756823147\n",
      "Epoch 10, Training Loss 0.5487432704709679\n",
      "Epoch 10, Training Loss 0.5499906975899815\n",
      "Epoch 10, Training Loss 0.5517335096588525\n",
      "Epoch 10, Training Loss 0.5534561101128074\n",
      "Epoch 10, Training Loss 0.5548879754207933\n",
      "Epoch 10, Training Loss 0.5560183906189317\n",
      "Epoch 10, Training Loss 0.5572627290435459\n",
      "Epoch 10, Training Loss 0.5585882049387373\n",
      "Epoch 10, Training Loss 0.559812736175859\n",
      "Epoch 10, Training Loss 0.5610897534186273\n",
      "Epoch 10, Training Loss 0.5627183549087066\n",
      "Epoch 10, Training Loss 0.5637786760354591\n",
      "Epoch 10, Training Loss 0.5650285855126198\n",
      "Epoch 10, Training Loss 0.5663276368852161\n",
      "Epoch 10, Training Loss 0.5674666933086522\n",
      "Epoch 10, Training Loss 0.568797281178672\n",
      "Epoch 10, Training Loss 0.5699649835029221\n",
      "Epoch 10, Training Loss 0.571102971158674\n",
      "Epoch 10, Training Loss 0.5725034641487824\n",
      "Epoch 10, Training Loss 0.5738514933134894\n",
      "Epoch 10, Training Loss 0.5752251343349056\n",
      "Epoch 10, Training Loss 0.5764673240196979\n",
      "Epoch 10, Training Loss 0.5777008503751682\n",
      "Epoch 10, Training Loss 0.5788269370718075\n",
      "Epoch 10, Training Loss 0.580101592918796\n",
      "Epoch 10, Training Loss 0.5813234651180179\n",
      "Epoch 10, Training Loss 0.5824604304245365\n",
      "Epoch 10, Training Loss 0.5835402594198047\n",
      "Epoch 10, Training Loss 0.5849527352301361\n",
      "Epoch 10, Training Loss 0.5859771343448278\n",
      "Epoch 10, Training Loss 0.5869072057554484\n",
      "Epoch 10, Training Loss 0.588278845219356\n",
      "Epoch 10, Training Loss 0.5893668631458526\n",
      "Epoch 10, Training Loss 0.5910400156779667\n",
      "Epoch 10, Training Loss 0.5923963179978569\n",
      "Epoch 10, Training Loss 0.5937010478180693\n",
      "Epoch 10, Training Loss 0.5951154594836028\n",
      "Epoch 10, Training Loss 0.5963798034983827\n",
      "Epoch 10, Training Loss 0.5974638782956107\n",
      "Epoch 10, Training Loss 0.5985386985189775\n",
      "Epoch 10, Training Loss 0.5998198159058076\n",
      "Epoch 10, Training Loss 0.6013482512567964\n",
      "Epoch 10, Training Loss 0.602805429026294\n",
      "Epoch 10, Training Loss 0.6040633323095034\n",
      "Epoch 10, Training Loss 0.6053425813727367\n",
      "Epoch 10, Training Loss 0.6063641065069477\n",
      "Epoch 10, Training Loss 0.6074949442730535\n",
      "Epoch 10, Training Loss 0.6089847855403295\n",
      "Epoch 10, Training Loss 0.6102664346432747\n",
      "Epoch 10, Training Loss 0.6116765087370373\n",
      "Epoch 10, Training Loss 0.6128809939107627\n",
      "Epoch 10, Training Loss 0.6139732191477285\n",
      "Epoch 10, Training Loss 0.6149782739636843\n",
      "Epoch 10, Training Loss 0.6159401305038911\n",
      "Epoch 10, Training Loss 0.6170848101911033\n",
      "Epoch 10, Training Loss 0.6183953152593139\n",
      "Epoch 10, Training Loss 0.6197000734336541\n",
      "Epoch 10, Training Loss 0.6208592712726739\n",
      "Epoch 10, Training Loss 0.6221042369180323\n",
      "Epoch 10, Training Loss 0.6234718409493146\n",
      "Epoch 10, Training Loss 0.6246395313069034\n",
      "Epoch 10, Training Loss 0.6259494692925602\n",
      "Epoch 10, Training Loss 0.6275178751219874\n",
      "Epoch 10, Training Loss 0.6286369623887874\n",
      "Epoch 10, Training Loss 0.6295187782753459\n",
      "Epoch 10, Training Loss 0.6309357537028125\n",
      "Epoch 10, Training Loss 0.6321879321961756\n",
      "Epoch 10, Training Loss 0.6337669087797785\n",
      "Epoch 10, Training Loss 0.6350467154741897\n",
      "Epoch 10, Training Loss 0.6362509487382592\n",
      "Epoch 10, Training Loss 0.6372428319185895\n",
      "Epoch 10, Training Loss 0.6384614817321758\n",
      "Epoch 10, Training Loss 0.6400995778915523\n",
      "Epoch 10, Training Loss 0.6411212558484138\n",
      "Epoch 10, Training Loss 0.6423755906274556\n",
      "Epoch 10, Training Loss 0.6437518135513491\n",
      "Epoch 10, Training Loss 0.6449252511076915\n",
      "Epoch 10, Training Loss 0.6460952495827395\n",
      "Epoch 10, Training Loss 0.6472623949617986\n",
      "Epoch 10, Training Loss 0.6485185613260245\n",
      "Epoch 10, Training Loss 0.6499453623733862\n",
      "Epoch 10, Training Loss 0.6513059585143233\n",
      "Epoch 10, Training Loss 0.6525868225433028\n",
      "Epoch 10, Training Loss 0.6539384479565389\n",
      "Epoch 10, Training Loss 0.6547754445039403\n",
      "Epoch 10, Training Loss 0.6559258608714394\n",
      "Epoch 10, Training Loss 0.6572756173513125\n",
      "Epoch 10, Training Loss 0.658383869149191\n",
      "Epoch 10, Training Loss 0.6596381551469378\n",
      "Epoch 10, Training Loss 0.6605211435376531\n",
      "Epoch 10, Training Loss 0.661847138191428\n",
      "Epoch 10, Training Loss 0.663410320306373\n",
      "Epoch 10, Training Loss 0.6647961638162813\n",
      "Epoch 10, Training Loss 0.6659187656991622\n",
      "Epoch 10, Training Loss 0.6671432210203937\n",
      "Epoch 10, Training Loss 0.6684167189976139\n",
      "Epoch 10, Training Loss 0.6694448312835011\n",
      "Epoch 10, Training Loss 0.6704290081625399\n",
      "Epoch 10, Training Loss 0.6716472024045637\n",
      "Epoch 10, Training Loss 0.6727013956860203\n",
      "Epoch 10, Training Loss 0.6739226094139811\n",
      "Epoch 10, Training Loss 0.675333354707874\n",
      "Epoch 10, Training Loss 0.6762871255197793\n",
      "Epoch 10, Training Loss 0.6776497486759635\n",
      "Epoch 10, Training Loss 0.6790222398307927\n",
      "Epoch 10, Training Loss 0.6801433603629432\n",
      "Epoch 10, Training Loss 0.6812460569621962\n",
      "Epoch 10, Training Loss 0.6822161012140991\n",
      "Epoch 10, Training Loss 0.683465809163535\n",
      "Epoch 10, Training Loss 0.6844450588269002\n",
      "Epoch 10, Training Loss 0.6855260276276133\n",
      "Epoch 10, Training Loss 0.6865840621311646\n",
      "Epoch 10, Training Loss 0.6875693654770132\n",
      "Epoch 10, Training Loss 0.6886864070544767\n",
      "Epoch 10, Training Loss 0.6898749478332832\n",
      "Epoch 10, Training Loss 0.6909339807527449\n",
      "Epoch 10, Training Loss 0.6917390891200746\n",
      "Epoch 10, Training Loss 0.6930865670561486\n",
      "Epoch 10, Training Loss 0.6943685080846558\n",
      "Epoch 10, Training Loss 0.6955946654919773\n",
      "Epoch 10, Training Loss 0.6970413139713999\n",
      "Epoch 10, Training Loss 0.6981810987605463\n",
      "Epoch 10, Training Loss 0.6994408080949808\n",
      "Epoch 10, Training Loss 0.7008168403144992\n",
      "Epoch 10, Training Loss 0.70213097227199\n",
      "Epoch 10, Training Loss 0.7030729737580584\n",
      "Epoch 10, Training Loss 0.7041051228485449\n",
      "Epoch 10, Training Loss 0.7054158067306899\n",
      "Epoch 10, Training Loss 0.7063526809215546\n",
      "Epoch 10, Training Loss 0.7074762118594421\n",
      "Epoch 10, Training Loss 0.708772569437466\n",
      "Epoch 10, Training Loss 0.7099702757642702\n",
      "Epoch 10, Training Loss 0.7111992259769488\n",
      "Epoch 10, Training Loss 0.7124549778526091\n",
      "Epoch 10, Training Loss 0.7137153942871581\n",
      "Epoch 10, Training Loss 0.715083352287712\n",
      "Epoch 10, Training Loss 0.7165190266526263\n",
      "Epoch 10, Training Loss 0.7178331110483546\n",
      "Epoch 10, Training Loss 0.7191938061238555\n",
      "Epoch 10, Training Loss 0.7204244036961089\n",
      "Epoch 10, Training Loss 0.7217444767580008\n",
      "Epoch 10, Training Loss 0.7228808807747443\n",
      "Epoch 10, Training Loss 0.7240147589112792\n",
      "Epoch 10, Training Loss 0.7252312267527861\n",
      "Epoch 10, Training Loss 0.7263817776499502\n",
      "Epoch 10, Training Loss 0.7274260270930922\n",
      "Epoch 10, Training Loss 0.7286665128624957\n",
      "Epoch 10, Training Loss 0.7299257129659433\n",
      "Epoch 10, Training Loss 0.7310509461423625\n",
      "Epoch 10, Training Loss 0.732337887863369\n",
      "Epoch 10, Training Loss 0.7335964951216413\n",
      "Epoch 10, Training Loss 0.735266155065478\n",
      "Epoch 10, Training Loss 0.7365032197416895\n",
      "Epoch 10, Training Loss 0.737709043123533\n",
      "Epoch 10, Training Loss 0.7388401814281483\n",
      "Epoch 10, Training Loss 0.7400380299829156\n",
      "Epoch 10, Training Loss 0.7410652631384027\n",
      "Epoch 10, Training Loss 0.7423263684563015\n",
      "Epoch 10, Training Loss 0.743591732000146\n",
      "Epoch 10, Training Loss 0.7448689141084471\n",
      "Epoch 10, Training Loss 0.745771348781293\n",
      "Epoch 10, Training Loss 0.746931581088649\n",
      "Epoch 10, Training Loss 0.7482865705819386\n",
      "Epoch 10, Training Loss 0.7498375297812245\n",
      "Epoch 10, Training Loss 0.7513720313911243\n",
      "Epoch 10, Training Loss 0.7524168036325508\n",
      "Epoch 10, Training Loss 0.753908605603001\n",
      "Epoch 10, Training Loss 0.755059125249648\n",
      "Epoch 10, Training Loss 0.7563232076747338\n",
      "Epoch 10, Training Loss 0.757465907451137\n",
      "Epoch 10, Training Loss 0.7591552965323943\n",
      "Epoch 10, Training Loss 0.7604036447032333\n",
      "Epoch 10, Training Loss 0.7617055811845433\n",
      "Epoch 10, Training Loss 0.7629181233513386\n",
      "Epoch 10, Training Loss 0.7639474384010295\n",
      "Epoch 10, Training Loss 0.7650976251915592\n",
      "Epoch 10, Training Loss 0.7663587637417152\n",
      "Epoch 10, Training Loss 0.7677482251468521\n",
      "Epoch 10, Training Loss 0.7689757651990027\n",
      "Epoch 10, Training Loss 0.7699718079755983\n",
      "Epoch 10, Training Loss 0.7711391009180747\n",
      "Epoch 10, Training Loss 0.7723593531972002\n",
      "Epoch 10, Training Loss 0.7736214492327113\n",
      "Epoch 10, Training Loss 0.7749821758636123\n",
      "Epoch 10, Training Loss 0.7764603018455798\n",
      "Epoch 10, Training Loss 0.7773322349466631\n",
      "Epoch 10, Training Loss 0.7787603457717944\n",
      "Epoch 10, Training Loss 0.7800646290907165\n",
      "Epoch 10, Training Loss 0.7813920882504309\n",
      "Epoch 10, Training Loss 0.782550060459415\n",
      "Epoch 10, Training Loss 0.783603720881445\n",
      "Epoch 10, Training Loss 0.78484744832034\n",
      "Epoch 10, Training Loss 0.7861520467359392\n",
      "Epoch 10, Training Loss 0.7874878720401803\n",
      "Epoch 10, Training Loss 0.788500308838037\n",
      "Epoch 10, Training Loss 0.789705772152947\n",
      "Epoch 10, Training Loss 0.7910482984826998\n",
      "Epoch 10, Training Loss 0.7922152599410328\n",
      "Epoch 10, Training Loss 0.7932133387269267\n",
      "Epoch 10, Training Loss 0.7947189948900276\n",
      "Epoch 10, Training Loss 0.7957998144504664\n",
      "Epoch 10, Training Loss 0.7971498869416659\n",
      "Epoch 10, Training Loss 0.7983606379965077\n",
      "Epoch 10, Training Loss 0.7994601678512895\n",
      "Epoch 10, Training Loss 0.8008868957267088\n",
      "Epoch 10, Training Loss 0.8022638950353999\n",
      "Epoch 10, Training Loss 0.8033204135077688\n",
      "Epoch 10, Training Loss 0.8044619426855346\n",
      "Epoch 10, Training Loss 0.8055344847462061\n",
      "Epoch 10, Training Loss 0.8064960139944121\n",
      "Epoch 10, Training Loss 0.8080801487426319\n",
      "Epoch 10, Training Loss 0.8091109204475228\n",
      "Epoch 10, Training Loss 0.8102691133156457\n",
      "Epoch 10, Training Loss 0.8111905255890868\n",
      "Epoch 10, Training Loss 0.8124416639737766\n",
      "Epoch 10, Training Loss 0.8134684080967818\n",
      "Epoch 10, Training Loss 0.8147453421064655\n",
      "Epoch 10, Training Loss 0.8161452856972394\n",
      "Epoch 10, Training Loss 0.8176782109091044\n",
      "Epoch 10, Training Loss 0.8188865140575887\n",
      "Epoch 10, Training Loss 0.8203272104568189\n",
      "Epoch 10, Training Loss 0.8213628220283772\n",
      "Epoch 10, Training Loss 0.8226223575032275\n",
      "Epoch 10, Training Loss 0.8236908190847968\n",
      "Epoch 10, Training Loss 0.8248356902385916\n",
      "Epoch 10, Training Loss 0.8262756794614865\n",
      "Epoch 10, Training Loss 0.8273913625561063\n",
      "Epoch 10, Training Loss 0.828521316542345\n",
      "Epoch 10, Training Loss 0.8294521706640873\n",
      "Epoch 10, Training Loss 0.8303021305357404\n",
      "Epoch 10, Training Loss 0.8315883431288288\n",
      "Epoch 10, Training Loss 0.8328518040497285\n",
      "Epoch 10, Training Loss 0.834089691376747\n",
      "Epoch 10, Training Loss 0.8351278035232174\n",
      "Epoch 10, Training Loss 0.8364222142702479\n",
      "Epoch 10, Training Loss 0.8376901857840741\n",
      "Epoch 10, Training Loss 0.8391341313224314\n",
      "Epoch 10, Training Loss 0.8404980805676306\n",
      "Epoch 10, Training Loss 0.8420794815815928\n",
      "Epoch 10, Training Loss 0.843407090134023\n",
      "Epoch 10, Training Loss 0.8446283358747088\n",
      "Epoch 10, Training Loss 0.8460879153607751\n",
      "Epoch 10, Training Loss 0.8471243162746624\n",
      "Epoch 10, Training Loss 0.8483029927134209\n",
      "Epoch 10, Training Loss 0.8496700938400406\n",
      "Epoch 10, Training Loss 0.8509105085716833\n",
      "Epoch 10, Training Loss 0.8519562563627882\n",
      "Epoch 10, Training Loss 0.8533073378645856\n",
      "Epoch 10, Training Loss 0.8543188275431123\n",
      "Epoch 10, Training Loss 0.8553176263104314\n",
      "Epoch 10, Training Loss 0.8564583166785862\n",
      "Epoch 10, Training Loss 0.8578988298430772\n",
      "Epoch 10, Training Loss 0.8591226701388883\n",
      "Epoch 10, Training Loss 0.8601146142958375\n",
      "Epoch 10, Training Loss 0.861357646753721\n",
      "Epoch 10, Training Loss 0.8625989781163842\n",
      "Epoch 10, Training Loss 0.8638552937208844\n",
      "Epoch 10, Training Loss 0.8649174082462135\n",
      "Epoch 10, Training Loss 0.8661017511659266\n",
      "Epoch 10, Training Loss 0.8675472095341938\n",
      "Epoch 10, Training Loss 0.8686866242714855\n",
      "Epoch 10, Training Loss 0.869943587523897\n",
      "Epoch 10, Training Loss 0.8712389921898123\n",
      "Epoch 10, Training Loss 0.8726686306316834\n",
      "Epoch 10, Training Loss 0.8738549103212478\n",
      "Epoch 10, Training Loss 0.8750362253707388\n",
      "Epoch 10, Training Loss 0.8765077186210076\n",
      "Epoch 10, Training Loss 0.8775733360244186\n",
      "Epoch 10, Training Loss 0.8787243732101168\n",
      "Epoch 10, Training Loss 0.8799555609598184\n",
      "Epoch 10, Training Loss 0.8812771468516201\n",
      "Epoch 10, Training Loss 0.8823945086325526\n",
      "Epoch 10, Training Loss 0.8836638747578691\n",
      "Epoch 10, Training Loss 0.8846380838652705\n",
      "Epoch 10, Training Loss 0.8857359133108192\n",
      "Epoch 10, Training Loss 0.8867863369414873\n",
      "Epoch 10, Training Loss 0.8880076191919234\n",
      "Epoch 10, Training Loss 0.8891301319727203\n",
      "Epoch 10, Training Loss 0.8904490803208802\n",
      "Epoch 10, Training Loss 0.8916988701314268\n",
      "Epoch 10, Training Loss 0.8927814251626544\n",
      "Epoch 10, Training Loss 0.8936887177664911\n",
      "Epoch 10, Training Loss 0.8950836920677243\n",
      "Epoch 10, Training Loss 0.8962567741303797\n",
      "Epoch 10, Training Loss 0.8977013620574151\n",
      "Epoch 10, Training Loss 0.8988309567389281\n",
      "Epoch 10, Training Loss 0.9004230153987475\n",
      "Epoch 10, Training Loss 0.901762429786765\n",
      "Epoch 10, Training Loss 0.9029206484175094\n",
      "Epoch 10, Training Loss 0.9041115871018461\n",
      "Epoch 10, Training Loss 0.9053688139256919\n",
      "Epoch 10, Training Loss 0.9065120042589925\n",
      "Epoch 10, Training Loss 0.9080087631712179\n",
      "Epoch 10, Training Loss 0.9089999631085359\n",
      "Epoch 10, Training Loss 0.9101613702828927\n",
      "Epoch 10, Training Loss 0.9113664617928703\n",
      "Epoch 10, Training Loss 0.9123634118253313\n",
      "Epoch 10, Training Loss 0.9134352457187974\n",
      "Epoch 10, Training Loss 0.9148476428692908\n",
      "Epoch 10, Training Loss 0.91614867156119\n",
      "Epoch 10, Training Loss 0.9172857136982481\n",
      "Epoch 10, Training Loss 0.9186300182586435\n",
      "Epoch 10, Training Loss 0.9199350253700296\n",
      "Epoch 10, Training Loss 0.9212415247317165\n",
      "Epoch 10, Training Loss 0.9225463915968795\n",
      "Epoch 10, Training Loss 0.923379061319639\n",
      "Epoch 10, Training Loss 0.9247326012462607\n",
      "Epoch 10, Training Loss 0.9258809294694524\n",
      "Epoch 10, Training Loss 0.9269411106548651\n",
      "Epoch 10, Training Loss 0.9282169207892454\n",
      "Epoch 10, Training Loss 0.9293908360973954\n",
      "Epoch 10, Training Loss 0.9304916819800502\n",
      "Epoch 10, Training Loss 0.9318271052959325\n",
      "Epoch 10, Training Loss 0.9333261669901631\n",
      "Epoch 10, Training Loss 0.934415695231284\n",
      "Epoch 10, Training Loss 0.935779816430548\n",
      "Epoch 10, Training Loss 0.9366700108856192\n",
      "Epoch 10, Training Loss 0.9376580927073194\n",
      "Epoch 10, Training Loss 0.9389236742425757\n",
      "Epoch 10, Training Loss 0.9401789767967771\n",
      "Epoch 10, Training Loss 0.9414702670653458\n",
      "Epoch 10, Training Loss 0.9425935712464325\n",
      "Epoch 10, Training Loss 0.9435926634637292\n",
      "Epoch 10, Training Loss 0.944980417066218\n",
      "Epoch 10, Training Loss 0.9462365249691107\n",
      "Epoch 10, Training Loss 0.9476001391477902\n",
      "Epoch 10, Training Loss 0.9486332505255404\n",
      "Epoch 10, Training Loss 0.9498363115903362\n",
      "Epoch 10, Training Loss 0.9512728445060418\n",
      "Epoch 10, Training Loss 0.9526570137504422\n",
      "Epoch 10, Training Loss 0.9538448046692802\n",
      "Epoch 10, Training Loss 0.9549529582185818\n",
      "Epoch 10, Training Loss 0.9561944353153639\n",
      "Epoch 10, Training Loss 0.9575072526169555\n",
      "Epoch 10, Training Loss 0.9587916014596934\n",
      "Epoch 10, Training Loss 0.9600162759156483\n",
      "Epoch 10, Training Loss 0.9613716974282813\n",
      "Epoch 10, Training Loss 0.9627260277643228\n",
      "Epoch 10, Training Loss 0.9642454859850657\n",
      "Epoch 20, Training Loss 0.0012029244771698856\n",
      "Epoch 20, Training Loss 0.0020302253610947553\n",
      "Epoch 20, Training Loss 0.003110389727765642\n",
      "Epoch 20, Training Loss 0.0037979298387951863\n",
      "Epoch 20, Training Loss 0.0045986074163480795\n",
      "Epoch 20, Training Loss 0.005475447687041729\n",
      "Epoch 20, Training Loss 0.006381841114414927\n",
      "Epoch 20, Training Loss 0.007329154502400352\n",
      "Epoch 20, Training Loss 0.00810099440767332\n",
      "Epoch 20, Training Loss 0.009147316141201713\n",
      "Epoch 20, Training Loss 0.010092428883018397\n",
      "Epoch 20, Training Loss 0.011258480417758912\n",
      "Epoch 20, Training Loss 0.012170511209751335\n",
      "Epoch 20, Training Loss 0.013172848678915702\n",
      "Epoch 20, Training Loss 0.014212844316916698\n",
      "Epoch 20, Training Loss 0.015126629406229004\n",
      "Epoch 20, Training Loss 0.01627592738632046\n",
      "Epoch 20, Training Loss 0.01757993524336754\n",
      "Epoch 20, Training Loss 0.018561079678937907\n",
      "Epoch 20, Training Loss 0.01930428679336977\n",
      "Epoch 20, Training Loss 0.020225126953685984\n",
      "Epoch 20, Training Loss 0.0212029221722537\n",
      "Epoch 20, Training Loss 0.022176301387874672\n",
      "Epoch 20, Training Loss 0.02317682495507438\n",
      "Epoch 20, Training Loss 0.02420860262173216\n",
      "Epoch 20, Training Loss 0.025130230371299606\n",
      "Epoch 20, Training Loss 0.0258755485724915\n",
      "Epoch 20, Training Loss 0.026855422255328245\n",
      "Epoch 20, Training Loss 0.027668865211784383\n",
      "Epoch 20, Training Loss 0.028664647313335057\n",
      "Epoch 20, Training Loss 0.029855943549319607\n",
      "Epoch 20, Training Loss 0.030768229071136632\n",
      "Epoch 20, Training Loss 0.03170788631109935\n",
      "Epoch 20, Training Loss 0.032720980284464025\n",
      "Epoch 20, Training Loss 0.03372710341077936\n",
      "Epoch 20, Training Loss 0.03482342063618438\n",
      "Epoch 20, Training Loss 0.03578830939119734\n",
      "Epoch 20, Training Loss 0.0367061979020648\n",
      "Epoch 20, Training Loss 0.03745677190668443\n",
      "Epoch 20, Training Loss 0.03844257297418307\n",
      "Epoch 20, Training Loss 0.039190474602267565\n",
      "Epoch 20, Training Loss 0.039981118934538665\n",
      "Epoch 20, Training Loss 0.04087150965810127\n",
      "Epoch 20, Training Loss 0.04180524256223303\n",
      "Epoch 20, Training Loss 0.04272418650214934\n",
      "Epoch 20, Training Loss 0.043737023077962345\n",
      "Epoch 20, Training Loss 0.045166408771749045\n",
      "Epoch 20, Training Loss 0.046726774071793424\n",
      "Epoch 20, Training Loss 0.04768024701291643\n",
      "Epoch 20, Training Loss 0.04885591981965867\n",
      "Epoch 20, Training Loss 0.04990521103829679\n",
      "Epoch 20, Training Loss 0.05056298800441615\n",
      "Epoch 20, Training Loss 0.051572278789851975\n",
      "Epoch 20, Training Loss 0.05237628538590258\n",
      "Epoch 20, Training Loss 0.05309178015155255\n",
      "Epoch 20, Training Loss 0.053974017005442354\n",
      "Epoch 20, Training Loss 0.05501338450805\n",
      "Epoch 20, Training Loss 0.05583961022174572\n",
      "Epoch 20, Training Loss 0.0568906589389762\n",
      "Epoch 20, Training Loss 0.05791558885513364\n",
      "Epoch 20, Training Loss 0.05903587545580266\n",
      "Epoch 20, Training Loss 0.05983377234710147\n",
      "Epoch 20, Training Loss 0.06059904850047568\n",
      "Epoch 20, Training Loss 0.06145177762526685\n",
      "Epoch 20, Training Loss 0.0627039664846552\n",
      "Epoch 20, Training Loss 0.06375989958148479\n",
      "Epoch 20, Training Loss 0.06457076139767151\n",
      "Epoch 20, Training Loss 0.06560208090126057\n",
      "Epoch 20, Training Loss 0.06648996868706725\n",
      "Epoch 20, Training Loss 0.0673420338527016\n",
      "Epoch 20, Training Loss 0.06859689562217049\n",
      "Epoch 20, Training Loss 0.0694841227263136\n",
      "Epoch 20, Training Loss 0.07037603679825277\n",
      "Epoch 20, Training Loss 0.07111119126419888\n",
      "Epoch 20, Training Loss 0.07183134647281579\n",
      "Epoch 20, Training Loss 0.0727104657446332\n",
      "Epoch 20, Training Loss 0.07366524221342238\n",
      "Epoch 20, Training Loss 0.07450652732263746\n",
      "Epoch 20, Training Loss 0.07569682727689328\n",
      "Epoch 20, Training Loss 0.07673342644101214\n",
      "Epoch 20, Training Loss 0.07791648061988908\n",
      "Epoch 20, Training Loss 0.07906639606446561\n",
      "Epoch 20, Training Loss 0.07985418768185179\n",
      "Epoch 20, Training Loss 0.08086524572213898\n",
      "Epoch 20, Training Loss 0.08150560174451764\n",
      "Epoch 20, Training Loss 0.0823947278130085\n",
      "Epoch 20, Training Loss 0.08329832957833624\n",
      "Epoch 20, Training Loss 0.08432760781339367\n",
      "Epoch 20, Training Loss 0.08554360293366416\n",
      "Epoch 20, Training Loss 0.08644045092870513\n",
      "Epoch 20, Training Loss 0.08754148713463103\n",
      "Epoch 20, Training Loss 0.08839690144104725\n",
      "Epoch 20, Training Loss 0.08915962961018847\n",
      "Epoch 20, Training Loss 0.08978829748185395\n",
      "Epoch 20, Training Loss 0.0907947623058963\n",
      "Epoch 20, Training Loss 0.09171398902488181\n",
      "Epoch 20, Training Loss 0.09272047884933785\n",
      "Epoch 20, Training Loss 0.09382164585011085\n",
      "Epoch 20, Training Loss 0.0945421475583635\n",
      "Epoch 20, Training Loss 0.09539651390536667\n",
      "Epoch 20, Training Loss 0.0963169407204289\n",
      "Epoch 20, Training Loss 0.09719663820303309\n",
      "Epoch 20, Training Loss 0.09826248800358199\n",
      "Epoch 20, Training Loss 0.09900809538638805\n",
      "Epoch 20, Training Loss 0.1000529686965601\n",
      "Epoch 20, Training Loss 0.10084755470990525\n",
      "Epoch 20, Training Loss 0.10157695122996864\n",
      "Epoch 20, Training Loss 0.1026524972275395\n",
      "Epoch 20, Training Loss 0.10349897990751145\n",
      "Epoch 20, Training Loss 0.10461583299100247\n",
      "Epoch 20, Training Loss 0.10565863987978767\n",
      "Epoch 20, Training Loss 0.10665954013004937\n",
      "Epoch 20, Training Loss 0.10766801626785942\n",
      "Epoch 20, Training Loss 0.10857903728704624\n",
      "Epoch 20, Training Loss 0.10942126387525397\n",
      "Epoch 20, Training Loss 0.11073644752697566\n",
      "Epoch 20, Training Loss 0.11171660558951785\n",
      "Epoch 20, Training Loss 0.11263566645210052\n",
      "Epoch 20, Training Loss 0.11366040413946751\n",
      "Epoch 20, Training Loss 0.11438422434775115\n",
      "Epoch 20, Training Loss 0.11535281362131124\n",
      "Epoch 20, Training Loss 0.11649268278685372\n",
      "Epoch 20, Training Loss 0.11754238125308396\n",
      "Epoch 20, Training Loss 0.11854329308890321\n",
      "Epoch 20, Training Loss 0.11940130377974352\n",
      "Epoch 20, Training Loss 0.12032537280446123\n",
      "Epoch 20, Training Loss 0.12142566128460038\n",
      "Epoch 20, Training Loss 0.12218760628529522\n",
      "Epoch 20, Training Loss 0.12332832805640861\n",
      "Epoch 20, Training Loss 0.12422307868442876\n",
      "Epoch 20, Training Loss 0.12513701476709313\n",
      "Epoch 20, Training Loss 0.12600069491149823\n",
      "Epoch 20, Training Loss 0.12699940046081154\n",
      "Epoch 20, Training Loss 0.12817694075272212\n",
      "Epoch 20, Training Loss 0.12905633823036233\n",
      "Epoch 20, Training Loss 0.12995383204401606\n",
      "Epoch 20, Training Loss 0.1308714805356682\n",
      "Epoch 20, Training Loss 0.13162910671490233\n",
      "Epoch 20, Training Loss 0.13272248898320796\n",
      "Epoch 20, Training Loss 0.1336362198795504\n",
      "Epoch 20, Training Loss 0.13490201178414132\n",
      "Epoch 20, Training Loss 0.13578502875764656\n",
      "Epoch 20, Training Loss 0.13655377516661155\n",
      "Epoch 20, Training Loss 0.13762588307375798\n",
      "Epoch 20, Training Loss 0.13861605890876497\n",
      "Epoch 20, Training Loss 0.13940705217973656\n",
      "Epoch 20, Training Loss 0.14027418268611058\n",
      "Epoch 20, Training Loss 0.14122915062148247\n",
      "Epoch 20, Training Loss 0.14201382366592621\n",
      "Epoch 20, Training Loss 0.14272774973183947\n",
      "Epoch 20, Training Loss 0.14324262437155788\n",
      "Epoch 20, Training Loss 0.14425362349318727\n",
      "Epoch 20, Training Loss 0.1451666399722209\n",
      "Epoch 20, Training Loss 0.14594352348228853\n",
      "Epoch 20, Training Loss 0.1467324196911224\n",
      "Epoch 20, Training Loss 0.14783028358846065\n",
      "Epoch 20, Training Loss 0.1489522029905368\n",
      "Epoch 20, Training Loss 0.14984633935534436\n",
      "Epoch 20, Training Loss 0.15066322558523748\n",
      "Epoch 20, Training Loss 0.15119463285369336\n",
      "Epoch 20, Training Loss 0.15221597856420385\n",
      "Epoch 20, Training Loss 0.15297823225903084\n",
      "Epoch 20, Training Loss 0.1539603565126429\n",
      "Epoch 20, Training Loss 0.15494033492282225\n",
      "Epoch 20, Training Loss 0.15601113030825126\n",
      "Epoch 20, Training Loss 0.15720653484392044\n",
      "Epoch 20, Training Loss 0.1581158678778602\n",
      "Epoch 20, Training Loss 0.15936881238999573\n",
      "Epoch 20, Training Loss 0.16067184431626058\n",
      "Epoch 20, Training Loss 0.1613403767957102\n",
      "Epoch 20, Training Loss 0.16224317991977458\n",
      "Epoch 20, Training Loss 0.16319794571765547\n",
      "Epoch 20, Training Loss 0.16441928772517786\n",
      "Epoch 20, Training Loss 0.16548535719399562\n",
      "Epoch 20, Training Loss 0.1666242902159996\n",
      "Epoch 20, Training Loss 0.16781870388161496\n",
      "Epoch 20, Training Loss 0.16868436165020595\n",
      "Epoch 20, Training Loss 0.16964314085290866\n",
      "Epoch 20, Training Loss 0.17082664633498473\n",
      "Epoch 20, Training Loss 0.1717467406750335\n",
      "Epoch 20, Training Loss 0.17281231699544755\n",
      "Epoch 20, Training Loss 0.17384804038288038\n",
      "Epoch 20, Training Loss 0.17473979301922157\n",
      "Epoch 20, Training Loss 0.17579960864980507\n",
      "Epoch 20, Training Loss 0.17667514950875432\n",
      "Epoch 20, Training Loss 0.177557334608739\n",
      "Epoch 20, Training Loss 0.17869830630776828\n",
      "Epoch 20, Training Loss 0.17964853635986747\n",
      "Epoch 20, Training Loss 0.18058354771503096\n",
      "Epoch 20, Training Loss 0.1814532390869487\n",
      "Epoch 20, Training Loss 0.1823189037154093\n",
      "Epoch 20, Training Loss 0.18328927868924788\n",
      "Epoch 20, Training Loss 0.18422843553983342\n",
      "Epoch 20, Training Loss 0.18501385592896005\n",
      "Epoch 20, Training Loss 0.18607262017019569\n",
      "Epoch 20, Training Loss 0.187003221391412\n",
      "Epoch 20, Training Loss 0.18850422641048042\n",
      "Epoch 20, Training Loss 0.1892719761566128\n",
      "Epoch 20, Training Loss 0.19054058213215655\n",
      "Epoch 20, Training Loss 0.1913406795934033\n",
      "Epoch 20, Training Loss 0.1920562519518006\n",
      "Epoch 20, Training Loss 0.19285927796760177\n",
      "Epoch 20, Training Loss 0.19382591865709065\n",
      "Epoch 20, Training Loss 0.19462903994885858\n",
      "Epoch 20, Training Loss 0.1954898881866499\n",
      "Epoch 20, Training Loss 0.19644799557945614\n",
      "Epoch 20, Training Loss 0.1975386494489582\n",
      "Epoch 20, Training Loss 0.19835919328510304\n",
      "Epoch 20, Training Loss 0.1992697261864572\n",
      "Epoch 20, Training Loss 0.20015639565942234\n",
      "Epoch 20, Training Loss 0.20103907200229137\n",
      "Epoch 20, Training Loss 0.20228482188318697\n",
      "Epoch 20, Training Loss 0.20334260188557607\n",
      "Epoch 20, Training Loss 0.20422242795262496\n",
      "Epoch 20, Training Loss 0.2048564004471235\n",
      "Epoch 20, Training Loss 0.20574778074498676\n",
      "Epoch 20, Training Loss 0.20662843738980305\n",
      "Epoch 20, Training Loss 0.20773603437501756\n",
      "Epoch 20, Training Loss 0.20863802582406632\n",
      "Epoch 20, Training Loss 0.2096469326854667\n",
      "Epoch 20, Training Loss 0.21037820774271054\n",
      "Epoch 20, Training Loss 0.21134869712392998\n",
      "Epoch 20, Training Loss 0.2124212762278974\n",
      "Epoch 20, Training Loss 0.2132586703428527\n",
      "Epoch 20, Training Loss 0.21439493472313942\n",
      "Epoch 20, Training Loss 0.21542999293188306\n",
      "Epoch 20, Training Loss 0.21625690158370817\n",
      "Epoch 20, Training Loss 0.21725006344373268\n",
      "Epoch 20, Training Loss 0.21847212459425183\n",
      "Epoch 20, Training Loss 0.21949595601662344\n",
      "Epoch 20, Training Loss 0.22017682353248985\n",
      "Epoch 20, Training Loss 0.2211881198389146\n",
      "Epoch 20, Training Loss 0.22200609861737322\n",
      "Epoch 20, Training Loss 0.22322386472731295\n",
      "Epoch 20, Training Loss 0.22409922707721094\n",
      "Epoch 20, Training Loss 0.22494865812913842\n",
      "Epoch 20, Training Loss 0.22585448576971087\n",
      "Epoch 20, Training Loss 0.22693228637775803\n",
      "Epoch 20, Training Loss 0.2280120586647707\n",
      "Epoch 20, Training Loss 0.2290736296597649\n",
      "Epoch 20, Training Loss 0.23003527041896224\n",
      "Epoch 20, Training Loss 0.23105111253231078\n",
      "Epoch 20, Training Loss 0.2318347139889017\n",
      "Epoch 20, Training Loss 0.23270445574275064\n",
      "Epoch 20, Training Loss 0.2337638736533387\n",
      "Epoch 20, Training Loss 0.234887990881415\n",
      "Epoch 20, Training Loss 0.23619465808124493\n",
      "Epoch 20, Training Loss 0.23714082891983754\n",
      "Epoch 20, Training Loss 0.23805743608328386\n",
      "Epoch 20, Training Loss 0.2390729250657894\n",
      "Epoch 20, Training Loss 0.23990684092197273\n",
      "Epoch 20, Training Loss 0.24117549072446115\n",
      "Epoch 20, Training Loss 0.24208857725038552\n",
      "Epoch 20, Training Loss 0.2428794369063414\n",
      "Epoch 20, Training Loss 0.243924004647433\n",
      "Epoch 20, Training Loss 0.24498305974713983\n",
      "Epoch 20, Training Loss 0.24594076240763946\n",
      "Epoch 20, Training Loss 0.24693845406822537\n",
      "Epoch 20, Training Loss 0.24804148276138793\n",
      "Epoch 20, Training Loss 0.24908826715501067\n",
      "Epoch 20, Training Loss 0.25030139378269617\n",
      "Epoch 20, Training Loss 0.25124790494704186\n",
      "Epoch 20, Training Loss 0.2520953035720474\n",
      "Epoch 20, Training Loss 0.25317271819809817\n",
      "Epoch 20, Training Loss 0.25432841293037395\n",
      "Epoch 20, Training Loss 0.25539382339438516\n",
      "Epoch 20, Training Loss 0.2565235206690591\n",
      "Epoch 20, Training Loss 0.25764639892846425\n",
      "Epoch 20, Training Loss 0.258745473745229\n",
      "Epoch 20, Training Loss 0.2597294646455809\n",
      "Epoch 20, Training Loss 0.2605873288400948\n",
      "Epoch 20, Training Loss 0.26151986210547445\n",
      "Epoch 20, Training Loss 0.262583406532512\n",
      "Epoch 20, Training Loss 0.2634298167265285\n",
      "Epoch 20, Training Loss 0.264621912747088\n",
      "Epoch 20, Training Loss 0.26591590367009876\n",
      "Epoch 20, Training Loss 0.2669693060848109\n",
      "Epoch 20, Training Loss 0.2683417342812814\n",
      "Epoch 20, Training Loss 0.26945430208045196\n",
      "Epoch 20, Training Loss 0.27020726385323895\n",
      "Epoch 20, Training Loss 0.270907085951027\n",
      "Epoch 20, Training Loss 0.2718753550211182\n",
      "Epoch 20, Training Loss 0.27289007257317643\n",
      "Epoch 20, Training Loss 0.27399157494535226\n",
      "Epoch 20, Training Loss 0.2749297383343777\n",
      "Epoch 20, Training Loss 0.27588619676697285\n",
      "Epoch 20, Training Loss 0.277010531300474\n",
      "Epoch 20, Training Loss 0.27794569021905474\n",
      "Epoch 20, Training Loss 0.278863789327919\n",
      "Epoch 20, Training Loss 0.2797687937841391\n",
      "Epoch 20, Training Loss 0.28072140657383465\n",
      "Epoch 20, Training Loss 0.28136442422561936\n",
      "Epoch 20, Training Loss 0.2824130422623871\n",
      "Epoch 20, Training Loss 0.28346607965581555\n",
      "Epoch 20, Training Loss 0.28397539654351256\n",
      "Epoch 20, Training Loss 0.2850629233033456\n",
      "Epoch 20, Training Loss 0.28578817196514295\n",
      "Epoch 20, Training Loss 0.2867466680838934\n",
      "Epoch 20, Training Loss 0.2879193583717736\n",
      "Epoch 20, Training Loss 0.28899655538751645\n",
      "Epoch 20, Training Loss 0.2898874978732575\n",
      "Epoch 20, Training Loss 0.2906459558497914\n",
      "Epoch 20, Training Loss 0.29192383309154557\n",
      "Epoch 20, Training Loss 0.2929561505537204\n",
      "Epoch 20, Training Loss 0.29406148507771895\n",
      "Epoch 20, Training Loss 0.29522058314374644\n",
      "Epoch 20, Training Loss 0.29625865336879137\n",
      "Epoch 20, Training Loss 0.29717959657959314\n",
      "Epoch 20, Training Loss 0.29801364368794825\n",
      "Epoch 20, Training Loss 0.29947397684502175\n",
      "Epoch 20, Training Loss 0.3004650905004243\n",
      "Epoch 20, Training Loss 0.3012830413058591\n",
      "Epoch 20, Training Loss 0.30222524035617215\n",
      "Epoch 20, Training Loss 0.3030477172273504\n",
      "Epoch 20, Training Loss 0.30402760905073123\n",
      "Epoch 20, Training Loss 0.3047973510554379\n",
      "Epoch 20, Training Loss 0.30565320393618417\n",
      "Epoch 20, Training Loss 0.3064942138121866\n",
      "Epoch 20, Training Loss 0.3073683801057089\n",
      "Epoch 20, Training Loss 0.30822933246107664\n",
      "Epoch 20, Training Loss 0.3091608372033405\n",
      "Epoch 20, Training Loss 0.3099776696975884\n",
      "Epoch 20, Training Loss 0.3110923568153625\n",
      "Epoch 20, Training Loss 0.31222709853325964\n",
      "Epoch 20, Training Loss 0.3134669996131107\n",
      "Epoch 20, Training Loss 0.3144333512734269\n",
      "Epoch 20, Training Loss 0.3155031289590899\n",
      "Epoch 20, Training Loss 0.3167562177571494\n",
      "Epoch 20, Training Loss 0.3177050852867039\n",
      "Epoch 20, Training Loss 0.31868417884992517\n",
      "Epoch 20, Training Loss 0.31962757083156224\n",
      "Epoch 20, Training Loss 0.32060919949770583\n",
      "Epoch 20, Training Loss 0.321554083043657\n",
      "Epoch 20, Training Loss 0.3222695288755705\n",
      "Epoch 20, Training Loss 0.32321113569047444\n",
      "Epoch 20, Training Loss 0.3240347904012636\n",
      "Epoch 20, Training Loss 0.32462788432302986\n",
      "Epoch 20, Training Loss 0.32547898087507626\n",
      "Epoch 20, Training Loss 0.32646032455174817\n",
      "Epoch 20, Training Loss 0.32746426357180264\n",
      "Epoch 20, Training Loss 0.32843459433759264\n",
      "Epoch 20, Training Loss 0.32921962981181374\n",
      "Epoch 20, Training Loss 0.3301703806804574\n",
      "Epoch 20, Training Loss 0.3312644781282796\n",
      "Epoch 20, Training Loss 0.3323729062248069\n",
      "Epoch 20, Training Loss 0.3333915490704729\n",
      "Epoch 20, Training Loss 0.3343124143074236\n",
      "Epoch 20, Training Loss 0.3350109462161808\n",
      "Epoch 20, Training Loss 0.336125859328548\n",
      "Epoch 20, Training Loss 0.33688843467503865\n",
      "Epoch 20, Training Loss 0.33819096354419925\n",
      "Epoch 20, Training Loss 0.33920966104015976\n",
      "Epoch 20, Training Loss 0.34037953588511327\n",
      "Epoch 20, Training Loss 0.3412120948590891\n",
      "Epoch 20, Training Loss 0.3421324876796864\n",
      "Epoch 20, Training Loss 0.34316666470006907\n",
      "Epoch 20, Training Loss 0.3442096593968399\n",
      "Epoch 20, Training Loss 0.34541571769110685\n",
      "Epoch 20, Training Loss 0.34617522134043066\n",
      "Epoch 20, Training Loss 0.34695476647990436\n",
      "Epoch 20, Training Loss 0.3480241890529842\n",
      "Epoch 20, Training Loss 0.349065449346057\n",
      "Epoch 20, Training Loss 0.350504162175881\n",
      "Epoch 20, Training Loss 0.35169736835200466\n",
      "Epoch 20, Training Loss 0.35270490430657514\n",
      "Epoch 20, Training Loss 0.3539522056613127\n",
      "Epoch 20, Training Loss 0.3550947790636736\n",
      "Epoch 20, Training Loss 0.35582198202610016\n",
      "Epoch 20, Training Loss 0.3568227708415912\n",
      "Epoch 20, Training Loss 0.3581032084915644\n",
      "Epoch 20, Training Loss 0.35914433196834894\n",
      "Epoch 20, Training Loss 0.3606728337838522\n",
      "Epoch 20, Training Loss 0.361832793068398\n",
      "Epoch 20, Training Loss 0.3625620455311997\n",
      "Epoch 20, Training Loss 0.36343548300168704\n",
      "Epoch 20, Training Loss 0.3643067152527592\n",
      "Epoch 20, Training Loss 0.3651111552401272\n",
      "Epoch 20, Training Loss 0.3657799613521532\n",
      "Epoch 20, Training Loss 0.3666710647399468\n",
      "Epoch 20, Training Loss 0.3678634273807716\n",
      "Epoch 20, Training Loss 0.36921908715954216\n",
      "Epoch 20, Training Loss 0.3704050448163391\n",
      "Epoch 20, Training Loss 0.371482188325099\n",
      "Epoch 20, Training Loss 0.3722319956249593\n",
      "Epoch 20, Training Loss 0.37319475416179815\n",
      "Epoch 20, Training Loss 0.3741930719760373\n",
      "Epoch 20, Training Loss 0.3751206126283197\n",
      "Epoch 20, Training Loss 0.37611898676971034\n",
      "Epoch 20, Training Loss 0.3768870458197411\n",
      "Epoch 20, Training Loss 0.3778517838100643\n",
      "Epoch 20, Training Loss 0.3788817078637345\n",
      "Epoch 20, Training Loss 0.3799259536482794\n",
      "Epoch 20, Training Loss 0.38073781121264944\n",
      "Epoch 20, Training Loss 0.38142465062610936\n",
      "Epoch 20, Training Loss 0.38229242237784977\n",
      "Epoch 20, Training Loss 0.38331064143601584\n",
      "Epoch 20, Training Loss 0.3841246439291693\n",
      "Epoch 20, Training Loss 0.38481606870813445\n",
      "Epoch 20, Training Loss 0.3857146747353132\n",
      "Epoch 20, Training Loss 0.38671308340471416\n",
      "Epoch 20, Training Loss 0.38793425086666555\n",
      "Epoch 20, Training Loss 0.3891942147022623\n",
      "Epoch 20, Training Loss 0.3902570780966898\n",
      "Epoch 20, Training Loss 0.3912095483535391\n",
      "Epoch 20, Training Loss 0.39181956484951935\n",
      "Epoch 20, Training Loss 0.3926105674956461\n",
      "Epoch 20, Training Loss 0.3937271134094204\n",
      "Epoch 20, Training Loss 0.3946530704989153\n",
      "Epoch 20, Training Loss 0.3956423765024566\n",
      "Epoch 20, Training Loss 0.39661671594738046\n",
      "Epoch 20, Training Loss 0.3975003400193456\n",
      "Epoch 20, Training Loss 0.39860642989120826\n",
      "Epoch 20, Training Loss 0.39993373767646684\n",
      "Epoch 20, Training Loss 0.4011734805982131\n",
      "Epoch 20, Training Loss 0.4021122962846171\n",
      "Epoch 20, Training Loss 0.40307752986240875\n",
      "Epoch 20, Training Loss 0.4039301714857521\n",
      "Epoch 20, Training Loss 0.4046492086880652\n",
      "Epoch 20, Training Loss 0.40574613964313744\n",
      "Epoch 20, Training Loss 0.4067009117673425\n",
      "Epoch 20, Training Loss 0.408086190176437\n",
      "Epoch 20, Training Loss 0.4092561429190209\n",
      "Epoch 20, Training Loss 0.4102627043147831\n",
      "Epoch 20, Training Loss 0.41131321567556134\n",
      "Epoch 20, Training Loss 0.4123376918494549\n",
      "Epoch 20, Training Loss 0.4133182835319768\n",
      "Epoch 20, Training Loss 0.4143283943004925\n",
      "Epoch 20, Training Loss 0.41545363060196344\n",
      "Epoch 20, Training Loss 0.41652314479241287\n",
      "Epoch 20, Training Loss 0.41777020902432443\n",
      "Epoch 20, Training Loss 0.4188533559860781\n",
      "Epoch 20, Training Loss 0.4199505854979196\n",
      "Epoch 20, Training Loss 0.42090894746810886\n",
      "Epoch 20, Training Loss 0.42192554980745095\n",
      "Epoch 20, Training Loss 0.42291325776625777\n",
      "Epoch 20, Training Loss 0.423814541429205\n",
      "Epoch 20, Training Loss 0.42490712719042895\n",
      "Epoch 20, Training Loss 0.4257289643406563\n",
      "Epoch 20, Training Loss 0.42655817333542173\n",
      "Epoch 20, Training Loss 0.4278079039986481\n",
      "Epoch 20, Training Loss 0.42875975854409015\n",
      "Epoch 20, Training Loss 0.42981598592932574\n",
      "Epoch 20, Training Loss 0.4307921401146428\n",
      "Epoch 20, Training Loss 0.4315584173516544\n",
      "Epoch 20, Training Loss 0.4325763540118552\n",
      "Epoch 20, Training Loss 0.43352395978272723\n",
      "Epoch 20, Training Loss 0.43461783862937137\n",
      "Epoch 20, Training Loss 0.43547519222092446\n",
      "Epoch 20, Training Loss 0.4364614939064626\n",
      "Epoch 20, Training Loss 0.4375074858327046\n",
      "Epoch 20, Training Loss 0.43839678698031187\n",
      "Epoch 20, Training Loss 0.43943721006440994\n",
      "Epoch 20, Training Loss 0.4403060149887334\n",
      "Epoch 20, Training Loss 0.44114543078348156\n",
      "Epoch 20, Training Loss 0.4418428907995029\n",
      "Epoch 20, Training Loss 0.44291808511442543\n",
      "Epoch 20, Training Loss 0.4439728088924647\n",
      "Epoch 20, Training Loss 0.44488346313729005\n",
      "Epoch 20, Training Loss 0.4455712748991559\n",
      "Epoch 20, Training Loss 0.44636822589065717\n",
      "Epoch 20, Training Loss 0.44753421233285723\n",
      "Epoch 20, Training Loss 0.44864387520591315\n",
      "Epoch 20, Training Loss 0.4495650204017644\n",
      "Epoch 20, Training Loss 0.45034089726407817\n",
      "Epoch 20, Training Loss 0.45130223900918154\n",
      "Epoch 20, Training Loss 0.4521710760224506\n",
      "Epoch 20, Training Loss 0.4531200200776615\n",
      "Epoch 20, Training Loss 0.4542789723333495\n",
      "Epoch 20, Training Loss 0.45502771071308407\n",
      "Epoch 20, Training Loss 0.45581407715444977\n",
      "Epoch 20, Training Loss 0.4569309947207151\n",
      "Epoch 20, Training Loss 0.45804908494357865\n",
      "Epoch 20, Training Loss 0.45894823274801455\n",
      "Epoch 20, Training Loss 0.45993334904808525\n",
      "Epoch 20, Training Loss 0.46126007907988165\n",
      "Epoch 20, Training Loss 0.4620378938553583\n",
      "Epoch 20, Training Loss 0.46314975722214147\n",
      "Epoch 20, Training Loss 0.4639312906567093\n",
      "Epoch 20, Training Loss 0.46487217284072085\n",
      "Epoch 20, Training Loss 0.4658056954517389\n",
      "Epoch 20, Training Loss 0.46661454359131393\n",
      "Epoch 20, Training Loss 0.4676403232928737\n",
      "Epoch 20, Training Loss 0.4685720010562931\n",
      "Epoch 20, Training Loss 0.46931780165875964\n",
      "Epoch 20, Training Loss 0.47010087947863755\n",
      "Epoch 20, Training Loss 0.47114170965788615\n",
      "Epoch 20, Training Loss 0.47193039389674923\n",
      "Epoch 20, Training Loss 0.47338124534205706\n",
      "Epoch 20, Training Loss 0.4742957713735073\n",
      "Epoch 20, Training Loss 0.47521863328983716\n",
      "Epoch 20, Training Loss 0.4761147852367757\n",
      "Epoch 20, Training Loss 0.47713704925516376\n",
      "Epoch 20, Training Loss 0.4779155574872366\n",
      "Epoch 20, Training Loss 0.4787320658145353\n",
      "Epoch 20, Training Loss 0.47949133020685153\n",
      "Epoch 20, Training Loss 0.48054490518539456\n",
      "Epoch 20, Training Loss 0.4813009915525651\n",
      "Epoch 20, Training Loss 0.48241900669796695\n",
      "Epoch 20, Training Loss 0.4835464835852918\n",
      "Epoch 20, Training Loss 0.4845016778201398\n",
      "Epoch 20, Training Loss 0.4851966192731467\n",
      "Epoch 20, Training Loss 0.4864559457506365\n",
      "Epoch 20, Training Loss 0.4873209306422402\n",
      "Epoch 20, Training Loss 0.4882006144051052\n",
      "Epoch 20, Training Loss 0.48918618612429676\n",
      "Epoch 20, Training Loss 0.49011338454530673\n",
      "Epoch 20, Training Loss 0.49128604061005976\n",
      "Epoch 20, Training Loss 0.49219424001243717\n",
      "Epoch 20, Training Loss 0.4933611748697203\n",
      "Epoch 20, Training Loss 0.4943553651766399\n",
      "Epoch 20, Training Loss 0.49538738720709713\n",
      "Epoch 20, Training Loss 0.49632447390147794\n",
      "Epoch 20, Training Loss 0.4970629810905822\n",
      "Epoch 20, Training Loss 0.4982882476295047\n",
      "Epoch 20, Training Loss 0.4993245181296488\n",
      "Epoch 20, Training Loss 0.49999963623635907\n",
      "Epoch 20, Training Loss 0.5009351063643575\n",
      "Epoch 20, Training Loss 0.5019226598998775\n",
      "Epoch 20, Training Loss 0.5029395583569242\n",
      "Epoch 20, Training Loss 0.5037518867370113\n",
      "Epoch 20, Training Loss 0.5048612442696491\n",
      "Epoch 20, Training Loss 0.5058498776248653\n",
      "Epoch 20, Training Loss 0.5071370719415148\n",
      "Epoch 20, Training Loss 0.5079787735210355\n",
      "Epoch 20, Training Loss 0.5089345861350179\n",
      "Epoch 20, Training Loss 0.509951243963083\n",
      "Epoch 20, Training Loss 0.5108261288660566\n",
      "Epoch 20, Training Loss 0.5120188157500514\n",
      "Epoch 20, Training Loss 0.5129696625806487\n",
      "Epoch 20, Training Loss 0.5136802132096132\n",
      "Epoch 20, Training Loss 0.5146911051648352\n",
      "Epoch 20, Training Loss 0.5156646379653145\n",
      "Epoch 20, Training Loss 0.5165601415783548\n",
      "Epoch 20, Training Loss 0.5174898782273387\n",
      "Epoch 20, Training Loss 0.5184231270533388\n",
      "Epoch 20, Training Loss 0.5194313748337119\n",
      "Epoch 20, Training Loss 0.5203807467161237\n",
      "Epoch 20, Training Loss 0.5217760824562644\n",
      "Epoch 20, Training Loss 0.5228311158430851\n",
      "Epoch 20, Training Loss 0.5239350572800088\n",
      "Epoch 20, Training Loss 0.5247745306214409\n",
      "Epoch 20, Training Loss 0.5257173113886963\n",
      "Epoch 20, Training Loss 0.5264457982138294\n",
      "Epoch 20, Training Loss 0.5274677956119522\n",
      "Epoch 20, Training Loss 0.5282810282752947\n",
      "Epoch 20, Training Loss 0.5291286059428969\n",
      "Epoch 20, Training Loss 0.5299144118566952\n",
      "Epoch 20, Training Loss 0.5308627282338374\n",
      "Epoch 20, Training Loss 0.5316064224752319\n",
      "Epoch 20, Training Loss 0.5325900097865888\n",
      "Epoch 20, Training Loss 0.5340059946488847\n",
      "Epoch 20, Training Loss 0.5350184835055295\n",
      "Epoch 20, Training Loss 0.5359615144293631\n",
      "Epoch 20, Training Loss 0.5366599635623605\n",
      "Epoch 20, Training Loss 0.5376622792323837\n",
      "Epoch 20, Training Loss 0.5384934679092959\n",
      "Epoch 20, Training Loss 0.5397095014448361\n",
      "Epoch 20, Training Loss 0.540626254723505\n",
      "Epoch 20, Training Loss 0.5415476601370766\n",
      "Epoch 20, Training Loss 0.5426271318855798\n",
      "Epoch 20, Training Loss 0.5435577296768613\n",
      "Epoch 20, Training Loss 0.5447248706351155\n",
      "Epoch 20, Training Loss 0.5455144360623396\n",
      "Epoch 20, Training Loss 0.5463522708858065\n",
      "Epoch 20, Training Loss 0.5474748342009761\n",
      "Epoch 20, Training Loss 0.5481585224190026\n",
      "Epoch 20, Training Loss 0.5491632927028115\n",
      "Epoch 20, Training Loss 0.5502175449791467\n",
      "Epoch 20, Training Loss 0.5511426562467194\n",
      "Epoch 20, Training Loss 0.5521038540870028\n",
      "Epoch 20, Training Loss 0.5531641541768217\n",
      "Epoch 20, Training Loss 0.5540568572099861\n",
      "Epoch 20, Training Loss 0.55496041621546\n",
      "Epoch 20, Training Loss 0.5559172751882192\n",
      "Epoch 20, Training Loss 0.5570521371824967\n",
      "Epoch 20, Training Loss 0.5579203025383108\n",
      "Epoch 20, Training Loss 0.5591834100997052\n",
      "Epoch 20, Training Loss 0.560175225214885\n",
      "Epoch 20, Training Loss 0.5613507874252851\n",
      "Epoch 20, Training Loss 0.5623979126782064\n",
      "Epoch 20, Training Loss 0.5632252490047909\n",
      "Epoch 20, Training Loss 0.5643402710366432\n",
      "Epoch 20, Training Loss 0.5648738831434104\n",
      "Epoch 20, Training Loss 0.5659847052582084\n",
      "Epoch 20, Training Loss 0.5669091460116379\n",
      "Epoch 20, Training Loss 0.5676469784182356\n",
      "Epoch 20, Training Loss 0.5685826733975154\n",
      "Epoch 20, Training Loss 0.5698099647412824\n",
      "Epoch 20, Training Loss 0.5706079375484715\n",
      "Epoch 20, Training Loss 0.5715443143225691\n",
      "Epoch 20, Training Loss 0.5722901240715286\n",
      "Epoch 20, Training Loss 0.5731847161603401\n",
      "Epoch 20, Training Loss 0.5742408483458297\n",
      "Epoch 20, Training Loss 0.5751950975955294\n",
      "Epoch 20, Training Loss 0.5761363351207864\n",
      "Epoch 20, Training Loss 0.5774637900121377\n",
      "Epoch 20, Training Loss 0.5782274303914946\n",
      "Epoch 20, Training Loss 0.5793029096958887\n",
      "Epoch 20, Training Loss 0.5803457746649032\n",
      "Epoch 20, Training Loss 0.5816301422579514\n",
      "Epoch 20, Training Loss 0.5829119845043362\n",
      "Epoch 20, Training Loss 0.5839700264013027\n",
      "Epoch 20, Training Loss 0.5849740210625217\n",
      "Epoch 20, Training Loss 0.5860222234674122\n",
      "Epoch 20, Training Loss 0.5869312368314284\n",
      "Epoch 20, Training Loss 0.587753629554873\n",
      "Epoch 20, Training Loss 0.5887286906580791\n",
      "Epoch 20, Training Loss 0.5895658481075331\n",
      "Epoch 20, Training Loss 0.5908261648453105\n",
      "Epoch 20, Training Loss 0.5917978858780069\n",
      "Epoch 20, Training Loss 0.5927067519072682\n",
      "Epoch 20, Training Loss 0.5936565106863256\n",
      "Epoch 20, Training Loss 0.594614068146252\n",
      "Epoch 20, Training Loss 0.5962034514569261\n",
      "Epoch 20, Training Loss 0.597006957511158\n",
      "Epoch 20, Training Loss 0.5978986898727734\n",
      "Epoch 20, Training Loss 0.5988944536051177\n",
      "Epoch 20, Training Loss 0.5996173154896177\n",
      "Epoch 20, Training Loss 0.6003530730143227\n",
      "Epoch 20, Training Loss 0.6015259787783294\n",
      "Epoch 20, Training Loss 0.6025074579755364\n",
      "Epoch 20, Training Loss 0.6036749916994358\n",
      "Epoch 20, Training Loss 0.6049602242458202\n",
      "Epoch 20, Training Loss 0.6060215003426422\n",
      "Epoch 20, Training Loss 0.606964483780934\n",
      "Epoch 20, Training Loss 0.607762183184209\n",
      "Epoch 20, Training Loss 0.608797223785954\n",
      "Epoch 20, Training Loss 0.6097416227202281\n",
      "Epoch 20, Training Loss 0.6107142796678007\n",
      "Epoch 20, Training Loss 0.6119125874527275\n",
      "Epoch 20, Training Loss 0.6131530716977156\n",
      "Epoch 20, Training Loss 0.6139580976703892\n",
      "Epoch 20, Training Loss 0.6150732920374102\n",
      "Epoch 20, Training Loss 0.6160820922659486\n",
      "Epoch 20, Training Loss 0.6173400626996594\n",
      "Epoch 20, Training Loss 0.6184701573894457\n",
      "Epoch 20, Training Loss 0.6194504034869811\n",
      "Epoch 20, Training Loss 0.6204461066238106\n",
      "Epoch 20, Training Loss 0.6212790281800054\n",
      "Epoch 20, Training Loss 0.6222118759322959\n",
      "Epoch 20, Training Loss 0.6231055161760896\n",
      "Epoch 20, Training Loss 0.624272020462224\n",
      "Epoch 20, Training Loss 0.6252255009492035\n",
      "Epoch 20, Training Loss 0.6260431277401307\n",
      "Epoch 20, Training Loss 0.6273625882918877\n",
      "Epoch 20, Training Loss 0.6284758862860672\n",
      "Epoch 20, Training Loss 0.6295157808553228\n",
      "Epoch 20, Training Loss 0.6302471342675217\n",
      "Epoch 20, Training Loss 0.6312449680036291\n",
      "Epoch 20, Training Loss 0.6321921554748969\n",
      "Epoch 20, Training Loss 0.6331349650536047\n",
      "Epoch 20, Training Loss 0.6340189627598009\n",
      "Epoch 20, Training Loss 0.6350367748752579\n",
      "Epoch 20, Training Loss 0.6361930481232035\n",
      "Epoch 20, Training Loss 0.6371026289508775\n",
      "Epoch 20, Training Loss 0.6380342200512776\n",
      "Epoch 20, Training Loss 0.6391470950963857\n",
      "Epoch 20, Training Loss 0.6401662109681713\n",
      "Epoch 20, Training Loss 0.6410422195177858\n",
      "Epoch 20, Training Loss 0.6423470513213931\n",
      "Epoch 20, Training Loss 0.6432758929098353\n",
      "Epoch 20, Training Loss 0.6442640709983724\n",
      "Epoch 20, Training Loss 0.6454174198076853\n",
      "Epoch 20, Training Loss 0.646662641631063\n",
      "Epoch 20, Training Loss 0.6476400242665844\n",
      "Epoch 20, Training Loss 0.6486739156877294\n",
      "Epoch 20, Training Loss 0.6500412200189307\n",
      "Epoch 20, Training Loss 0.6509361082254468\n",
      "Epoch 20, Training Loss 0.6520860032428561\n",
      "Epoch 20, Training Loss 0.6530392158138173\n",
      "Epoch 20, Training Loss 0.653892686033188\n",
      "Epoch 20, Training Loss 0.654769641854574\n",
      "Epoch 20, Training Loss 0.6557747890501071\n",
      "Epoch 20, Training Loss 0.657235431511079\n",
      "Epoch 20, Training Loss 0.658220218766071\n",
      "Epoch 20, Training Loss 0.6591187548988006\n",
      "Epoch 20, Training Loss 0.6599251512447586\n",
      "Epoch 20, Training Loss 0.6609756873772882\n",
      "Epoch 20, Training Loss 0.661901595380605\n",
      "Epoch 20, Training Loss 0.6629196915708845\n",
      "Epoch 20, Training Loss 0.663837862555938\n",
      "Epoch 20, Training Loss 0.6649179737205091\n",
      "Epoch 20, Training Loss 0.6657927084685592\n",
      "Epoch 20, Training Loss 0.6666762866556187\n",
      "Epoch 20, Training Loss 0.6674627075186166\n",
      "Epoch 20, Training Loss 0.6685527813480333\n",
      "Epoch 20, Training Loss 0.6694841632224104\n",
      "Epoch 20, Training Loss 0.6704579112703538\n",
      "Epoch 20, Training Loss 0.6713419470869367\n",
      "Epoch 20, Training Loss 0.6722960534226864\n",
      "Epoch 20, Training Loss 0.6732367170055199\n",
      "Epoch 20, Training Loss 0.6742229946052937\n",
      "Epoch 20, Training Loss 0.6749681489318228\n",
      "Epoch 20, Training Loss 0.6756058967174472\n",
      "Epoch 20, Training Loss 0.6766205725767424\n",
      "Epoch 20, Training Loss 0.6778765370321396\n",
      "Epoch 20, Training Loss 0.6788502168624907\n",
      "Epoch 20, Training Loss 0.6797922188821046\n",
      "Epoch 20, Training Loss 0.6805760531169375\n",
      "Epoch 20, Training Loss 0.6817762279297079\n",
      "Epoch 20, Training Loss 0.6828173633731539\n",
      "Epoch 20, Training Loss 0.683786226431732\n",
      "Epoch 20, Training Loss 0.6848591415930891\n",
      "Epoch 20, Training Loss 0.6858554307152244\n",
      "Epoch 20, Training Loss 0.6868747408737612\n",
      "Epoch 20, Training Loss 0.6876564531222634\n",
      "Epoch 20, Training Loss 0.6887705897736123\n",
      "Epoch 20, Training Loss 0.690023569880849\n",
      "Epoch 20, Training Loss 0.6910431711265194\n",
      "Epoch 20, Training Loss 0.6919537632514143\n",
      "Epoch 20, Training Loss 0.6929233080285895\n",
      "Epoch 20, Training Loss 0.6939464656593245\n",
      "Epoch 20, Training Loss 0.6951783783448017\n",
      "Epoch 20, Training Loss 0.6964030243704081\n",
      "Epoch 20, Training Loss 0.6973590500976729\n",
      "Epoch 20, Training Loss 0.6984710055391502\n",
      "Epoch 20, Training Loss 0.6996491116940823\n",
      "Epoch 20, Training Loss 0.7005406609734001\n",
      "Epoch 20, Training Loss 0.7016123551541887\n",
      "Epoch 20, Training Loss 0.7024478218744478\n",
      "Epoch 20, Training Loss 0.7036141486424009\n",
      "Epoch 20, Training Loss 0.7046897138476067\n",
      "Epoch 20, Training Loss 0.7057459778950342\n",
      "Epoch 20, Training Loss 0.7067887597834058\n",
      "Epoch 20, Training Loss 0.7079958235058943\n",
      "Epoch 20, Training Loss 0.7089031356984697\n",
      "Epoch 20, Training Loss 0.7099608549529024\n",
      "Epoch 20, Training Loss 0.7110714318654726\n",
      "Epoch 20, Training Loss 0.7121370888274648\n",
      "Epoch 20, Training Loss 0.7131404775335356\n",
      "Epoch 20, Training Loss 0.7141195412944344\n",
      "Epoch 20, Training Loss 0.715198025526598\n",
      "Epoch 20, Training Loss 0.716271207460662\n",
      "Epoch 20, Training Loss 0.717282579682977\n",
      "Epoch 20, Training Loss 0.718244617781066\n",
      "Epoch 20, Training Loss 0.7195549687308729\n",
      "Epoch 20, Training Loss 0.7208577283965353\n",
      "Epoch 20, Training Loss 0.7218074611080881\n",
      "Epoch 20, Training Loss 0.7228728243152199\n",
      "Epoch 20, Training Loss 0.7237282736831919\n",
      "Epoch 20, Training Loss 0.7251205680620335\n",
      "Epoch 20, Training Loss 0.7262050105482721\n",
      "Epoch 20, Training Loss 0.7272059608755819\n",
      "Epoch 20, Training Loss 0.7286119958781221\n",
      "Epoch 20, Training Loss 0.7295340650221881\n",
      "Epoch 20, Training Loss 0.7304094598421356\n",
      "Epoch 20, Training Loss 0.7310989279576274\n",
      "Epoch 20, Training Loss 0.732108118284084\n",
      "Epoch 20, Training Loss 0.7332440246553982\n",
      "Epoch 20, Training Loss 0.7342230641018704\n",
      "Epoch 20, Training Loss 0.73485621375501\n",
      "Epoch 20, Training Loss 0.7359686842964738\n",
      "Epoch 20, Training Loss 0.7369501765274331\n",
      "Epoch 20, Training Loss 0.7380540398380641\n",
      "Epoch 20, Training Loss 0.739079286756418\n",
      "Epoch 20, Training Loss 0.7399239121648051\n",
      "Epoch 20, Training Loss 0.7410983171152032\n",
      "Epoch 20, Training Loss 0.7423492205874694\n",
      "Epoch 20, Training Loss 0.7430426163594132\n",
      "Epoch 20, Training Loss 0.7439392741074038\n",
      "Epoch 20, Training Loss 0.7449545021099813\n",
      "Epoch 20, Training Loss 0.7462686407749001\n",
      "Epoch 20, Training Loss 0.7472253778706426\n",
      "Epoch 20, Training Loss 0.7480104824771052\n",
      "Epoch 20, Training Loss 0.7487990334820565\n",
      "Epoch 20, Training Loss 0.7497398219907375\n",
      "Epoch 20, Training Loss 0.7509543805780923\n",
      "Epoch 20, Training Loss 0.7518520048054893\n",
      "Epoch 20, Training Loss 0.7528627775514217\n",
      "Epoch 20, Training Loss 0.7538345927167731\n",
      "Epoch 20, Training Loss 0.7546362700822103\n",
      "Epoch 20, Training Loss 0.7554427730610304\n",
      "Epoch 20, Training Loss 0.7565387398995402\n",
      "Epoch 20, Training Loss 0.7580005674410963\n",
      "Epoch 20, Training Loss 0.7590908368530176\n",
      "Epoch 20, Training Loss 0.7601090697071436\n",
      "Epoch 20, Training Loss 0.760936263119778\n",
      "Epoch 20, Training Loss 0.7618615747717641\n",
      "Epoch 20, Training Loss 0.7629823649630827\n",
      "Epoch 20, Training Loss 0.7642386339966903\n",
      "Epoch 20, Training Loss 0.7651942233600275\n",
      "Epoch 20, Training Loss 0.7663723372894785\n",
      "Epoch 30, Training Loss 0.0008924595840141902\n",
      "Epoch 30, Training Loss 0.0015071623999139538\n",
      "Epoch 30, Training Loss 0.0022286398865072926\n",
      "Epoch 30, Training Loss 0.002910682726699068\n",
      "Epoch 30, Training Loss 0.0035813360491677012\n",
      "Epoch 30, Training Loss 0.0044239268202306056\n",
      "Epoch 30, Training Loss 0.005378267153754564\n",
      "Epoch 30, Training Loss 0.006080464824386265\n",
      "Epoch 30, Training Loss 0.006991530890050141\n",
      "Epoch 30, Training Loss 0.007798573207062528\n",
      "Epoch 30, Training Loss 0.00847985399196215\n",
      "Epoch 30, Training Loss 0.009462081067397467\n",
      "Epoch 30, Training Loss 0.010395456648543668\n",
      "Epoch 30, Training Loss 0.011063353370522598\n",
      "Epoch 30, Training Loss 0.011861702479669809\n",
      "Epoch 30, Training Loss 0.012597709170082952\n",
      "Epoch 30, Training Loss 0.013443636200617037\n",
      "Epoch 30, Training Loss 0.014193626430333423\n",
      "Epoch 30, Training Loss 0.014891975104351482\n",
      "Epoch 30, Training Loss 0.015670126470763362\n",
      "Epoch 30, Training Loss 0.016484056401740558\n",
      "Epoch 30, Training Loss 0.0170452872581799\n",
      "Epoch 30, Training Loss 0.01787490342431666\n",
      "Epoch 30, Training Loss 0.01850965908726158\n",
      "Epoch 30, Training Loss 0.01891464985849912\n",
      "Epoch 30, Training Loss 0.01941772087303269\n",
      "Epoch 30, Training Loss 0.019922924278032446\n",
      "Epoch 30, Training Loss 0.020775126107513447\n",
      "Epoch 30, Training Loss 0.021548385853352753\n",
      "Epoch 30, Training Loss 0.022331369693017068\n",
      "Epoch 30, Training Loss 0.02308709096268315\n",
      "Epoch 30, Training Loss 0.024069841369948424\n",
      "Epoch 30, Training Loss 0.02486367554158506\n",
      "Epoch 30, Training Loss 0.025419935888951393\n",
      "Epoch 30, Training Loss 0.0264574244732747\n",
      "Epoch 30, Training Loss 0.027075337212713782\n",
      "Epoch 30, Training Loss 0.027930752433779293\n",
      "Epoch 30, Training Loss 0.028576964726838308\n",
      "Epoch 30, Training Loss 0.029264432428133154\n",
      "Epoch 30, Training Loss 0.030124805505623294\n",
      "Epoch 30, Training Loss 0.031099276964926658\n",
      "Epoch 30, Training Loss 0.03186269752357317\n",
      "Epoch 30, Training Loss 0.0324701040678317\n",
      "Epoch 30, Training Loss 0.03329573838454683\n",
      "Epoch 30, Training Loss 0.034061128487977224\n",
      "Epoch 30, Training Loss 0.03490800591533446\n",
      "Epoch 30, Training Loss 0.03595838800568105\n",
      "Epoch 30, Training Loss 0.036727877033640964\n",
      "Epoch 30, Training Loss 0.03774086970959783\n",
      "Epoch 30, Training Loss 0.03835950227802062\n",
      "Epoch 30, Training Loss 0.03917303708050867\n",
      "Epoch 30, Training Loss 0.04018684005950723\n",
      "Epoch 30, Training Loss 0.04085645815143195\n",
      "Epoch 30, Training Loss 0.041697081054567986\n",
      "Epoch 30, Training Loss 0.04228608992398548\n",
      "Epoch 30, Training Loss 0.04326375983560177\n",
      "Epoch 30, Training Loss 0.0442383790869847\n",
      "Epoch 30, Training Loss 0.04531193503638363\n",
      "Epoch 30, Training Loss 0.04617676207476565\n",
      "Epoch 30, Training Loss 0.04696989028959933\n",
      "Epoch 30, Training Loss 0.04767963183505456\n",
      "Epoch 30, Training Loss 0.04852380258652865\n",
      "Epoch 30, Training Loss 0.04937129961255261\n",
      "Epoch 30, Training Loss 0.05017812691076332\n",
      "Epoch 30, Training Loss 0.05091835425028106\n",
      "Epoch 30, Training Loss 0.051977433740635355\n",
      "Epoch 30, Training Loss 0.05277856048720572\n",
      "Epoch 30, Training Loss 0.05357829285094805\n",
      "Epoch 30, Training Loss 0.05449453872792861\n",
      "Epoch 30, Training Loss 0.055228073502440585\n",
      "Epoch 30, Training Loss 0.05607334640629761\n",
      "Epoch 30, Training Loss 0.05707855640774798\n",
      "Epoch 30, Training Loss 0.05761182110023011\n",
      "Epoch 30, Training Loss 0.05862182378768921\n",
      "Epoch 30, Training Loss 0.05923762768888108\n",
      "Epoch 30, Training Loss 0.06016107277034798\n",
      "Epoch 30, Training Loss 0.06094050777080419\n",
      "Epoch 30, Training Loss 0.06174716052344388\n",
      "Epoch 30, Training Loss 0.06238367280844227\n",
      "Epoch 30, Training Loss 0.06364939565701253\n",
      "Epoch 30, Training Loss 0.06451544420950858\n",
      "Epoch 30, Training Loss 0.0654331590894543\n",
      "Epoch 30, Training Loss 0.06600732297238791\n",
      "Epoch 30, Training Loss 0.06667638442400471\n",
      "Epoch 30, Training Loss 0.0675046727480486\n",
      "Epoch 30, Training Loss 0.06862221570575938\n",
      "Epoch 30, Training Loss 0.0695944871286602\n",
      "Epoch 30, Training Loss 0.07051650009801626\n",
      "Epoch 30, Training Loss 0.07114856844515446\n",
      "Epoch 30, Training Loss 0.0720910197862274\n",
      "Epoch 30, Training Loss 0.07289249775812144\n",
      "Epoch 30, Training Loss 0.07351594157231128\n",
      "Epoch 30, Training Loss 0.07427621832893938\n",
      "Epoch 30, Training Loss 0.07521650949707422\n",
      "Epoch 30, Training Loss 0.07600296694604332\n",
      "Epoch 30, Training Loss 0.07682361337534911\n",
      "Epoch 30, Training Loss 0.07753443596003305\n",
      "Epoch 30, Training Loss 0.0784214648901654\n",
      "Epoch 30, Training Loss 0.07913864863193248\n",
      "Epoch 30, Training Loss 0.07994157830467614\n",
      "Epoch 30, Training Loss 0.08097966537451196\n",
      "Epoch 30, Training Loss 0.08184909172680067\n",
      "Epoch 30, Training Loss 0.08243359099416171\n",
      "Epoch 30, Training Loss 0.08329896941361829\n",
      "Epoch 30, Training Loss 0.0839989577489131\n",
      "Epoch 30, Training Loss 0.08503801434698617\n",
      "Epoch 30, Training Loss 0.08586448236652043\n",
      "Epoch 30, Training Loss 0.0869461262546232\n",
      "Epoch 30, Training Loss 0.08780215680599213\n",
      "Epoch 30, Training Loss 0.08876662383146604\n",
      "Epoch 30, Training Loss 0.08968334921333186\n",
      "Epoch 30, Training Loss 0.0906555166710978\n",
      "Epoch 30, Training Loss 0.09152607352989714\n",
      "Epoch 30, Training Loss 0.09231960380931034\n",
      "Epoch 30, Training Loss 0.09326355723316407\n",
      "Epoch 30, Training Loss 0.09437037790980181\n",
      "Epoch 30, Training Loss 0.09524293899383691\n",
      "Epoch 30, Training Loss 0.09615167171296561\n",
      "Epoch 30, Training Loss 0.096841245622891\n",
      "Epoch 30, Training Loss 0.09795265440898174\n",
      "Epoch 30, Training Loss 0.09874333685163951\n",
      "Epoch 30, Training Loss 0.09964739609404903\n",
      "Epoch 30, Training Loss 0.1004846085749014\n",
      "Epoch 30, Training Loss 0.10118321312205567\n",
      "Epoch 30, Training Loss 0.10216546489302154\n",
      "Epoch 30, Training Loss 0.10284697555977365\n",
      "Epoch 30, Training Loss 0.1036281197348519\n",
      "Epoch 30, Training Loss 0.10455065786533649\n",
      "Epoch 30, Training Loss 0.10516307455347017\n",
      "Epoch 30, Training Loss 0.10593220038944498\n",
      "Epoch 30, Training Loss 0.10677491311374528\n",
      "Epoch 30, Training Loss 0.10753169369971965\n",
      "Epoch 30, Training Loss 0.10828932536684949\n",
      "Epoch 30, Training Loss 0.1090499707271376\n",
      "Epoch 30, Training Loss 0.10977915649676262\n",
      "Epoch 30, Training Loss 0.11053016938059532\n",
      "Epoch 30, Training Loss 0.11129668153003049\n",
      "Epoch 30, Training Loss 0.11218101613204498\n",
      "Epoch 30, Training Loss 0.11289197111221226\n",
      "Epoch 30, Training Loss 0.11383531583696985\n",
      "Epoch 30, Training Loss 0.11483816954943225\n",
      "Epoch 30, Training Loss 0.11597604409355641\n",
      "Epoch 30, Training Loss 0.11696505801909415\n",
      "Epoch 30, Training Loss 0.11775014677163585\n",
      "Epoch 30, Training Loss 0.11846361276896103\n",
      "Epoch 30, Training Loss 0.11927937344669381\n",
      "Epoch 30, Training Loss 0.11998579794031275\n",
      "Epoch 30, Training Loss 0.12078628892941243\n",
      "Epoch 30, Training Loss 0.12154636903644522\n",
      "Epoch 30, Training Loss 0.12239897064388255\n",
      "Epoch 30, Training Loss 0.12315423329315527\n",
      "Epoch 30, Training Loss 0.12396899486899071\n",
      "Epoch 30, Training Loss 0.12436815978163648\n",
      "Epoch 30, Training Loss 0.12521796240983413\n",
      "Epoch 30, Training Loss 0.12607155984167553\n",
      "Epoch 30, Training Loss 0.126967811028061\n",
      "Epoch 30, Training Loss 0.12792750724288812\n",
      "Epoch 30, Training Loss 0.12881860903004552\n",
      "Epoch 30, Training Loss 0.12942977451607393\n",
      "Epoch 30, Training Loss 0.13044571861281723\n",
      "Epoch 30, Training Loss 0.13140510514264217\n",
      "Epoch 30, Training Loss 0.13218120624647117\n",
      "Epoch 30, Training Loss 0.13300741092323343\n",
      "Epoch 30, Training Loss 0.133717081049824\n",
      "Epoch 30, Training Loss 0.13457023396211512\n",
      "Epoch 30, Training Loss 0.13575166197079222\n",
      "Epoch 30, Training Loss 0.13726746220417949\n",
      "Epoch 30, Training Loss 0.13811764723199713\n",
      "Epoch 30, Training Loss 0.13917100658197232\n",
      "Epoch 30, Training Loss 0.13992373816802373\n",
      "Epoch 30, Training Loss 0.14080894283016623\n",
      "Epoch 30, Training Loss 0.14154820734887477\n",
      "Epoch 30, Training Loss 0.14227622579735563\n",
      "Epoch 30, Training Loss 0.14312022871068678\n",
      "Epoch 30, Training Loss 0.14391725874313002\n",
      "Epoch 30, Training Loss 0.14469618916206653\n",
      "Epoch 30, Training Loss 0.14538502571223033\n",
      "Epoch 30, Training Loss 0.14622206593413486\n",
      "Epoch 30, Training Loss 0.14709008586071337\n",
      "Epoch 30, Training Loss 0.14813876746560606\n",
      "Epoch 30, Training Loss 0.14928886164789615\n",
      "Epoch 30, Training Loss 0.1499371263377197\n",
      "Epoch 30, Training Loss 0.15073455226085986\n",
      "Epoch 30, Training Loss 0.1515625366926803\n",
      "Epoch 30, Training Loss 0.152429688159767\n",
      "Epoch 30, Training Loss 0.15314180055237792\n",
      "Epoch 30, Training Loss 0.1539965572259615\n",
      "Epoch 30, Training Loss 0.15473251040939176\n",
      "Epoch 30, Training Loss 0.15546876153982508\n",
      "Epoch 30, Training Loss 0.15604665825891373\n",
      "Epoch 30, Training Loss 0.1568094361239992\n",
      "Epoch 30, Training Loss 0.15784916430330642\n",
      "Epoch 30, Training Loss 0.15864036146484678\n",
      "Epoch 30, Training Loss 0.15934949614050442\n",
      "Epoch 30, Training Loss 0.16025942857460598\n",
      "Epoch 30, Training Loss 0.16115809039539084\n",
      "Epoch 30, Training Loss 0.16187348485450306\n",
      "Epoch 30, Training Loss 0.16258110502339385\n",
      "Epoch 30, Training Loss 0.16334057353494114\n",
      "Epoch 30, Training Loss 0.16425561222731305\n",
      "Epoch 30, Training Loss 0.16515599640891376\n",
      "Epoch 30, Training Loss 0.16598030273109446\n",
      "Epoch 30, Training Loss 0.16688516625510458\n",
      "Epoch 30, Training Loss 0.16769754692264224\n",
      "Epoch 30, Training Loss 0.16827679591258163\n",
      "Epoch 30, Training Loss 0.16887817007806294\n",
      "Epoch 30, Training Loss 0.16963301153134203\n",
      "Epoch 30, Training Loss 0.17044048945007423\n",
      "Epoch 30, Training Loss 0.17126063556622362\n",
      "Epoch 30, Training Loss 0.17227303692142068\n",
      "Epoch 30, Training Loss 0.17302485789789263\n",
      "Epoch 30, Training Loss 0.17375001974422913\n",
      "Epoch 30, Training Loss 0.17494705540444844\n",
      "Epoch 30, Training Loss 0.1757186068140942\n",
      "Epoch 30, Training Loss 0.1764058162031881\n",
      "Epoch 30, Training Loss 0.17706625769510292\n",
      "Epoch 30, Training Loss 0.17759824568963112\n",
      "Epoch 30, Training Loss 0.17828306266109048\n",
      "Epoch 30, Training Loss 0.17908758046986806\n",
      "Epoch 30, Training Loss 0.17980716448000936\n",
      "Epoch 30, Training Loss 0.18056768674374846\n",
      "Epoch 30, Training Loss 0.18148793870835658\n",
      "Epoch 30, Training Loss 0.18231379475130144\n",
      "Epoch 30, Training Loss 0.18311122334216867\n",
      "Epoch 30, Training Loss 0.18393049109012574\n",
      "Epoch 30, Training Loss 0.1846394786596908\n",
      "Epoch 30, Training Loss 0.18551437552932584\n",
      "Epoch 30, Training Loss 0.18643250428807095\n",
      "Epoch 30, Training Loss 0.18732052355471168\n",
      "Epoch 30, Training Loss 0.1880261845448438\n",
      "Epoch 30, Training Loss 0.18886107236832914\n",
      "Epoch 30, Training Loss 0.1897304946046961\n",
      "Epoch 30, Training Loss 0.19039751509266437\n",
      "Epoch 30, Training Loss 0.1912156265714894\n",
      "Epoch 30, Training Loss 0.19201140582104168\n",
      "Epoch 30, Training Loss 0.19311724385946913\n",
      "Epoch 30, Training Loss 0.1940764098825967\n",
      "Epoch 30, Training Loss 0.19487478033356045\n",
      "Epoch 30, Training Loss 0.1955606955701433\n",
      "Epoch 30, Training Loss 0.1968701638833946\n",
      "Epoch 30, Training Loss 0.1975290656394666\n",
      "Epoch 30, Training Loss 0.19809773648181533\n",
      "Epoch 30, Training Loss 0.1987056295051599\n",
      "Epoch 30, Training Loss 0.19928332370565371\n",
      "Epoch 30, Training Loss 0.19995904517600604\n",
      "Epoch 30, Training Loss 0.2007745826793144\n",
      "Epoch 30, Training Loss 0.2016168230634821\n",
      "Epoch 30, Training Loss 0.2028136337961992\n",
      "Epoch 30, Training Loss 0.2035723915490348\n",
      "Epoch 30, Training Loss 0.20448314449976168\n",
      "Epoch 30, Training Loss 0.2052650097995768\n",
      "Epoch 30, Training Loss 0.20626551309205077\n",
      "Epoch 30, Training Loss 0.2072344582404017\n",
      "Epoch 30, Training Loss 0.20826200763587757\n",
      "Epoch 30, Training Loss 0.20899161261975613\n",
      "Epoch 30, Training Loss 0.2098643658563609\n",
      "Epoch 30, Training Loss 0.21049141651376738\n",
      "Epoch 30, Training Loss 0.21152512591970546\n",
      "Epoch 30, Training Loss 0.21238350704350434\n",
      "Epoch 30, Training Loss 0.21302960165168927\n",
      "Epoch 30, Training Loss 0.21390913381143603\n",
      "Epoch 30, Training Loss 0.21468806468769716\n",
      "Epoch 30, Training Loss 0.21555641983323695\n",
      "Epoch 30, Training Loss 0.21636277052295178\n",
      "Epoch 30, Training Loss 0.21706099167961598\n",
      "Epoch 30, Training Loss 0.21794110456543506\n",
      "Epoch 30, Training Loss 0.2189819867272511\n",
      "Epoch 30, Training Loss 0.21988112294612944\n",
      "Epoch 30, Training Loss 0.22056274977334014\n",
      "Epoch 30, Training Loss 0.2214203310363433\n",
      "Epoch 30, Training Loss 0.22218473728203103\n",
      "Epoch 30, Training Loss 0.2229752938079712\n",
      "Epoch 30, Training Loss 0.22374701252221452\n",
      "Epoch 30, Training Loss 0.22447086969757324\n",
      "Epoch 30, Training Loss 0.2251807276321494\n",
      "Epoch 30, Training Loss 0.22586606328597153\n",
      "Epoch 30, Training Loss 0.22675042951960697\n",
      "Epoch 30, Training Loss 0.22766534999355942\n",
      "Epoch 30, Training Loss 0.22835256959623693\n",
      "Epoch 30, Training Loss 0.2293601323805197\n",
      "Epoch 30, Training Loss 0.23028182141158893\n",
      "Epoch 30, Training Loss 0.23133739345061505\n",
      "Epoch 30, Training Loss 0.2320807706135923\n",
      "Epoch 30, Training Loss 0.2330750702973217\n",
      "Epoch 30, Training Loss 0.2339698161996539\n",
      "Epoch 30, Training Loss 0.23481218013769525\n",
      "Epoch 30, Training Loss 0.23584026208771464\n",
      "Epoch 30, Training Loss 0.23648669397282174\n",
      "Epoch 30, Training Loss 0.2372390776872635\n",
      "Epoch 30, Training Loss 0.23790087819556752\n",
      "Epoch 30, Training Loss 0.23860256998892634\n",
      "Epoch 30, Training Loss 0.2395410219497998\n",
      "Epoch 30, Training Loss 0.2404086324946045\n",
      "Epoch 30, Training Loss 0.24103947921329752\n",
      "Epoch 30, Training Loss 0.24184623745548756\n",
      "Epoch 30, Training Loss 0.2426860586685293\n",
      "Epoch 30, Training Loss 0.2431584757459743\n",
      "Epoch 30, Training Loss 0.24391002507160997\n",
      "Epoch 30, Training Loss 0.24481127191992366\n",
      "Epoch 30, Training Loss 0.24556682009221342\n",
      "Epoch 30, Training Loss 0.24621888903705666\n",
      "Epoch 30, Training Loss 0.24735906864980908\n",
      "Epoch 30, Training Loss 0.24824623858837216\n",
      "Epoch 30, Training Loss 0.2489513035320565\n",
      "Epoch 30, Training Loss 0.2496018982147012\n",
      "Epoch 30, Training Loss 0.2505539168634683\n",
      "Epoch 30, Training Loss 0.25138587780925625\n",
      "Epoch 30, Training Loss 0.2519351500074577\n",
      "Epoch 30, Training Loss 0.2527393469267794\n",
      "Epoch 30, Training Loss 0.2535609192860401\n",
      "Epoch 30, Training Loss 0.25410754624230175\n",
      "Epoch 30, Training Loss 0.2548630322946612\n",
      "Epoch 30, Training Loss 0.25591117722908857\n",
      "Epoch 30, Training Loss 0.2571494031287825\n",
      "Epoch 30, Training Loss 0.25775472076652606\n",
      "Epoch 30, Training Loss 0.2585203537093404\n",
      "Epoch 30, Training Loss 0.259380621373501\n",
      "Epoch 30, Training Loss 0.2600792163168378\n",
      "Epoch 30, Training Loss 0.2608900333914306\n",
      "Epoch 30, Training Loss 0.26192146783594583\n",
      "Epoch 30, Training Loss 0.2627064434768599\n",
      "Epoch 30, Training Loss 0.2634227395515003\n",
      "Epoch 30, Training Loss 0.2640217658122787\n",
      "Epoch 30, Training Loss 0.2651148178921941\n",
      "Epoch 30, Training Loss 0.2661243764412068\n",
      "Epoch 30, Training Loss 0.26707025367738035\n",
      "Epoch 30, Training Loss 0.2681300983480785\n",
      "Epoch 30, Training Loss 0.2693234904266684\n",
      "Epoch 30, Training Loss 0.27004546281474323\n",
      "Epoch 30, Training Loss 0.2709607444227199\n",
      "Epoch 30, Training Loss 0.2718415307952925\n",
      "Epoch 30, Training Loss 0.27258380824495154\n",
      "Epoch 30, Training Loss 0.2734678876598168\n",
      "Epoch 30, Training Loss 0.2742170729981664\n",
      "Epoch 30, Training Loss 0.27508933720228923\n",
      "Epoch 30, Training Loss 0.27611285459507456\n",
      "Epoch 30, Training Loss 0.276856890229313\n",
      "Epoch 30, Training Loss 0.27752969816060324\n",
      "Epoch 30, Training Loss 0.27831575339255127\n",
      "Epoch 30, Training Loss 0.2791351010960996\n",
      "Epoch 30, Training Loss 0.279777204403487\n",
      "Epoch 30, Training Loss 0.28038096702312265\n",
      "Epoch 30, Training Loss 0.28139967313203057\n",
      "Epoch 30, Training Loss 0.28207069498193843\n",
      "Epoch 30, Training Loss 0.28325737391591377\n",
      "Epoch 30, Training Loss 0.2840641880279307\n",
      "Epoch 30, Training Loss 0.2845993571726562\n",
      "Epoch 30, Training Loss 0.2852116042695692\n",
      "Epoch 30, Training Loss 0.2861046887114835\n",
      "Epoch 30, Training Loss 0.28665052415312403\n",
      "Epoch 30, Training Loss 0.2875935314485179\n",
      "Epoch 30, Training Loss 0.2885862219592799\n",
      "Epoch 30, Training Loss 0.2893250205404008\n",
      "Epoch 30, Training Loss 0.2902025091449928\n",
      "Epoch 30, Training Loss 0.2909829220579713\n",
      "Epoch 30, Training Loss 0.29168189193129235\n",
      "Epoch 30, Training Loss 0.29264626520521503\n",
      "Epoch 30, Training Loss 0.29324365531087226\n",
      "Epoch 30, Training Loss 0.29414050574497796\n",
      "Epoch 30, Training Loss 0.2949787696151782\n",
      "Epoch 30, Training Loss 0.2957834031270898\n",
      "Epoch 30, Training Loss 0.29676608745094457\n",
      "Epoch 30, Training Loss 0.29754435535891893\n",
      "Epoch 30, Training Loss 0.29836897068011486\n",
      "Epoch 30, Training Loss 0.299332309943026\n",
      "Epoch 30, Training Loss 0.3004419839443148\n",
      "Epoch 30, Training Loss 0.3010117645230135\n",
      "Epoch 30, Training Loss 0.3018813766253269\n",
      "Epoch 30, Training Loss 0.3027659519325437\n",
      "Epoch 30, Training Loss 0.3034414493900431\n",
      "Epoch 30, Training Loss 0.3042262702265664\n",
      "Epoch 30, Training Loss 0.30509083514170876\n",
      "Epoch 30, Training Loss 0.3060846502137611\n",
      "Epoch 30, Training Loss 0.30702286139321144\n",
      "Epoch 30, Training Loss 0.3080190345073295\n",
      "Epoch 30, Training Loss 0.3088596454438041\n",
      "Epoch 30, Training Loss 0.3097307164879406\n",
      "Epoch 30, Training Loss 0.3106226113522449\n",
      "Epoch 30, Training Loss 0.311638023123107\n",
      "Epoch 30, Training Loss 0.31229738151783226\n",
      "Epoch 30, Training Loss 0.3131704083107926\n",
      "Epoch 30, Training Loss 0.3137028586605321\n",
      "Epoch 30, Training Loss 0.3143098319278044\n",
      "Epoch 30, Training Loss 0.31546047101240327\n",
      "Epoch 30, Training Loss 0.31625385715833404\n",
      "Epoch 30, Training Loss 0.3173941996548792\n",
      "Epoch 30, Training Loss 0.3182772487935508\n",
      "Epoch 30, Training Loss 0.3191123316659952\n",
      "Epoch 30, Training Loss 0.31988385754168186\n",
      "Epoch 30, Training Loss 0.3207268100565352\n",
      "Epoch 30, Training Loss 0.321564283364874\n",
      "Epoch 30, Training Loss 0.3222918980719183\n",
      "Epoch 30, Training Loss 0.3229610707296435\n",
      "Epoch 30, Training Loss 0.32373779326143776\n",
      "Epoch 30, Training Loss 0.32446793933658646\n",
      "Epoch 30, Training Loss 0.3252060337139822\n",
      "Epoch 30, Training Loss 0.3258725663890009\n",
      "Epoch 30, Training Loss 0.3265759376308802\n",
      "Epoch 30, Training Loss 0.32738525178426364\n",
      "Epoch 30, Training Loss 0.3281542961402317\n",
      "Epoch 30, Training Loss 0.32890223923241696\n",
      "Epoch 30, Training Loss 0.3296394781078524\n",
      "Epoch 30, Training Loss 0.3302184740448242\n",
      "Epoch 30, Training Loss 0.330926051834965\n",
      "Epoch 30, Training Loss 0.33162361192886175\n",
      "Epoch 30, Training Loss 0.3324625219225579\n",
      "Epoch 30, Training Loss 0.33311190675286684\n",
      "Epoch 30, Training Loss 0.33391978368734765\n",
      "Epoch 30, Training Loss 0.33501166143380773\n",
      "Epoch 30, Training Loss 0.3359346646634514\n",
      "Epoch 30, Training Loss 0.3371143668051571\n",
      "Epoch 30, Training Loss 0.33780608427189196\n",
      "Epoch 30, Training Loss 0.33848852719492317\n",
      "Epoch 30, Training Loss 0.33916110234797153\n",
      "Epoch 30, Training Loss 0.34001220011955025\n",
      "Epoch 30, Training Loss 0.3406929597830224\n",
      "Epoch 30, Training Loss 0.34119220973585573\n",
      "Epoch 30, Training Loss 0.3419913709011224\n",
      "Epoch 30, Training Loss 0.3428266683350439\n",
      "Epoch 30, Training Loss 0.3436364560480923\n",
      "Epoch 30, Training Loss 0.34433029603470316\n",
      "Epoch 30, Training Loss 0.3451879949825804\n",
      "Epoch 30, Training Loss 0.3457485195392233\n",
      "Epoch 30, Training Loss 0.34667601964205425\n",
      "Epoch 30, Training Loss 0.3476963092375289\n",
      "Epoch 30, Training Loss 0.3486365937744565\n",
      "Epoch 30, Training Loss 0.3495352280795422\n",
      "Epoch 30, Training Loss 0.35058913309403394\n",
      "Epoch 30, Training Loss 0.3513968815965116\n",
      "Epoch 30, Training Loss 0.3519396661873669\n",
      "Epoch 30, Training Loss 0.3527689723636183\n",
      "Epoch 30, Training Loss 0.3535415819462608\n",
      "Epoch 30, Training Loss 0.3542530159739887\n",
      "Epoch 30, Training Loss 0.35500120137201246\n",
      "Epoch 30, Training Loss 0.3558988987713519\n",
      "Epoch 30, Training Loss 0.35651195914391665\n",
      "Epoch 30, Training Loss 0.357401123399015\n",
      "Epoch 30, Training Loss 0.3580097550397639\n",
      "Epoch 30, Training Loss 0.35884161057222225\n",
      "Epoch 30, Training Loss 0.3597052567983832\n",
      "Epoch 30, Training Loss 0.3602156364323233\n",
      "Epoch 30, Training Loss 0.3612774629955706\n",
      "Epoch 30, Training Loss 0.36189261219842966\n",
      "Epoch 30, Training Loss 0.3629312452376651\n",
      "Epoch 30, Training Loss 0.36398603448934874\n",
      "Epoch 30, Training Loss 0.3648654372262223\n",
      "Epoch 30, Training Loss 0.3657988337680812\n",
      "Epoch 30, Training Loss 0.3663363602688855\n",
      "Epoch 30, Training Loss 0.3671569313158465\n",
      "Epoch 30, Training Loss 0.36824343839417334\n",
      "Epoch 30, Training Loss 0.3686094099222242\n",
      "Epoch 30, Training Loss 0.3697140781242219\n",
      "Epoch 30, Training Loss 0.37049202468541576\n",
      "Epoch 30, Training Loss 0.3716115653133758\n",
      "Epoch 30, Training Loss 0.3721818410222183\n",
      "Epoch 30, Training Loss 0.37301419625806687\n",
      "Epoch 30, Training Loss 0.3739073106547451\n",
      "Epoch 30, Training Loss 0.3749961649517879\n",
      "Epoch 30, Training Loss 0.3759582931428309\n",
      "Epoch 30, Training Loss 0.3766124038897512\n",
      "Epoch 30, Training Loss 0.3773413183896438\n",
      "Epoch 30, Training Loss 0.3782314425691619\n",
      "Epoch 30, Training Loss 0.37913861184778724\n",
      "Epoch 30, Training Loss 0.38004983020255634\n",
      "Epoch 30, Training Loss 0.3808884667161176\n",
      "Epoch 30, Training Loss 0.38174810975104034\n",
      "Epoch 30, Training Loss 0.3828468482055323\n",
      "Epoch 30, Training Loss 0.3837242031188877\n",
      "Epoch 30, Training Loss 0.38441479640543613\n",
      "Epoch 30, Training Loss 0.3851856967372358\n",
      "Epoch 30, Training Loss 0.3857976856362789\n",
      "Epoch 30, Training Loss 0.38668177991419495\n",
      "Epoch 30, Training Loss 0.3873827107193525\n",
      "Epoch 30, Training Loss 0.38844511465495807\n",
      "Epoch 30, Training Loss 0.38916138572918485\n",
      "Epoch 30, Training Loss 0.3899261931248028\n",
      "Epoch 30, Training Loss 0.39058993093650357\n",
      "Epoch 30, Training Loss 0.3912542620507043\n",
      "Epoch 30, Training Loss 0.3920417968040842\n",
      "Epoch 30, Training Loss 0.39276553759032196\n",
      "Epoch 30, Training Loss 0.3936650052171229\n",
      "Epoch 30, Training Loss 0.39438248187532204\n",
      "Epoch 30, Training Loss 0.39508953526654206\n",
      "Epoch 30, Training Loss 0.39605281827852246\n",
      "Epoch 30, Training Loss 0.3971395881660759\n",
      "Epoch 30, Training Loss 0.39789232051433504\n",
      "Epoch 30, Training Loss 0.3985227260671918\n",
      "Epoch 30, Training Loss 0.39929108207335556\n",
      "Epoch 30, Training Loss 0.40012613651545154\n",
      "Epoch 30, Training Loss 0.40093740641765885\n",
      "Epoch 30, Training Loss 0.40162130477635755\n",
      "Epoch 30, Training Loss 0.4023286151078046\n",
      "Epoch 30, Training Loss 0.4031264800625994\n",
      "Epoch 30, Training Loss 0.40370321346213445\n",
      "Epoch 30, Training Loss 0.4043983819768252\n",
      "Epoch 30, Training Loss 0.4051674277047672\n",
      "Epoch 30, Training Loss 0.4059828430261758\n",
      "Epoch 30, Training Loss 0.40683162078985474\n",
      "Epoch 30, Training Loss 0.4077812255267292\n",
      "Epoch 30, Training Loss 0.4086392449067377\n",
      "Epoch 30, Training Loss 0.40944209584342245\n",
      "Epoch 30, Training Loss 0.41011309810459157\n",
      "Epoch 30, Training Loss 0.4108900999092995\n",
      "Epoch 30, Training Loss 0.4116697254616891\n",
      "Epoch 30, Training Loss 0.4123992091020965\n",
      "Epoch 30, Training Loss 0.4128886980702505\n",
      "Epoch 30, Training Loss 0.4135850537234865\n",
      "Epoch 30, Training Loss 0.4144016919309831\n",
      "Epoch 30, Training Loss 0.415103969557206\n",
      "Epoch 30, Training Loss 0.41597435080334355\n",
      "Epoch 30, Training Loss 0.4167528254220553\n",
      "Epoch 30, Training Loss 0.41743059075244554\n",
      "Epoch 30, Training Loss 0.418225528448439\n",
      "Epoch 30, Training Loss 0.4191497804030128\n",
      "Epoch 30, Training Loss 0.4201130932173156\n",
      "Epoch 30, Training Loss 0.4208799329636347\n",
      "Epoch 30, Training Loss 0.4214487239680327\n",
      "Epoch 30, Training Loss 0.4223943291722661\n",
      "Epoch 30, Training Loss 0.4232628350062748\n",
      "Epoch 30, Training Loss 0.42453125881417025\n",
      "Epoch 30, Training Loss 0.42528560079272143\n",
      "Epoch 30, Training Loss 0.4261520654344193\n",
      "Epoch 30, Training Loss 0.42712732600738934\n",
      "Epoch 30, Training Loss 0.4280268943218319\n",
      "Epoch 30, Training Loss 0.42869271761011285\n",
      "Epoch 30, Training Loss 0.4294803927621573\n",
      "Epoch 30, Training Loss 0.4303698117470802\n",
      "Epoch 30, Training Loss 0.4311073847743861\n",
      "Epoch 30, Training Loss 0.4318175253355899\n",
      "Epoch 30, Training Loss 0.43280985890447027\n",
      "Epoch 30, Training Loss 0.4334255635281048\n",
      "Epoch 30, Training Loss 0.43432398037532405\n",
      "Epoch 30, Training Loss 0.435038586254315\n",
      "Epoch 30, Training Loss 0.43585001728723727\n",
      "Epoch 30, Training Loss 0.43655198362782177\n",
      "Epoch 30, Training Loss 0.4375063321169685\n",
      "Epoch 30, Training Loss 0.4381303298275184\n",
      "Epoch 30, Training Loss 0.4388868888397046\n",
      "Epoch 30, Training Loss 0.43949516289069523\n",
      "Epoch 30, Training Loss 0.44015927631836715\n",
      "Epoch 30, Training Loss 0.4408644677123145\n",
      "Epoch 30, Training Loss 0.44188631518417615\n",
      "Epoch 30, Training Loss 0.4423632650729031\n",
      "Epoch 30, Training Loss 0.44319130270682333\n",
      "Epoch 30, Training Loss 0.4439990369560164\n",
      "Epoch 30, Training Loss 0.4446657320575031\n",
      "Epoch 30, Training Loss 0.44554105538236516\n",
      "Epoch 30, Training Loss 0.4462250810297554\n",
      "Epoch 30, Training Loss 0.4471107084885278\n",
      "Epoch 30, Training Loss 0.44786172853711315\n",
      "Epoch 30, Training Loss 0.44854188133078765\n",
      "Epoch 30, Training Loss 0.4493246157760815\n",
      "Epoch 30, Training Loss 0.450178678307082\n",
      "Epoch 30, Training Loss 0.45091673038194857\n",
      "Epoch 30, Training Loss 0.4516662734243876\n",
      "Epoch 30, Training Loss 0.4524629922474132\n",
      "Epoch 30, Training Loss 0.45324866645171513\n",
      "Epoch 30, Training Loss 0.45417427284943174\n",
      "Epoch 30, Training Loss 0.4548498209175246\n",
      "Epoch 30, Training Loss 0.45554643320610455\n",
      "Epoch 30, Training Loss 0.456624053780685\n",
      "Epoch 30, Training Loss 0.45753158106828284\n",
      "Epoch 30, Training Loss 0.45826024990862285\n",
      "Epoch 30, Training Loss 0.4590514116275036\n",
      "Epoch 30, Training Loss 0.4598212606461762\n",
      "Epoch 30, Training Loss 0.4606215705347183\n",
      "Epoch 30, Training Loss 0.4616132828280749\n",
      "Epoch 30, Training Loss 0.46242360759269246\n",
      "Epoch 30, Training Loss 0.4634214702926938\n",
      "Epoch 30, Training Loss 0.46414036923052404\n",
      "Epoch 30, Training Loss 0.46509758918486593\n",
      "Epoch 30, Training Loss 0.4660366868881313\n",
      "Epoch 30, Training Loss 0.4669034824042064\n",
      "Epoch 30, Training Loss 0.4679813083175503\n",
      "Epoch 30, Training Loss 0.46896426604531916\n",
      "Epoch 30, Training Loss 0.46983886816922354\n",
      "Epoch 30, Training Loss 0.470430533828028\n",
      "Epoch 30, Training Loss 0.47171213948513235\n",
      "Epoch 30, Training Loss 0.47241455247944886\n",
      "Epoch 30, Training Loss 0.47344676254655393\n",
      "Epoch 30, Training Loss 0.474353454683138\n",
      "Epoch 30, Training Loss 0.4750941811925005\n",
      "Epoch 30, Training Loss 0.4760162475926187\n",
      "Epoch 30, Training Loss 0.47678812103503193\n",
      "Epoch 30, Training Loss 0.47734798106086224\n",
      "Epoch 30, Training Loss 0.4782565343562904\n",
      "Epoch 30, Training Loss 0.47919868142403604\n",
      "Epoch 30, Training Loss 0.4800912554154311\n",
      "Epoch 30, Training Loss 0.4809044396023616\n",
      "Epoch 30, Training Loss 0.48179220650202176\n",
      "Epoch 30, Training Loss 0.48254549152710857\n",
      "Epoch 30, Training Loss 0.4834052132980903\n",
      "Epoch 30, Training Loss 0.4843062579326922\n",
      "Epoch 30, Training Loss 0.4852457094527876\n",
      "Epoch 30, Training Loss 0.48599886459767666\n",
      "Epoch 30, Training Loss 0.4868704633182272\n",
      "Epoch 30, Training Loss 0.4876794600883103\n",
      "Epoch 30, Training Loss 0.4882836786225019\n",
      "Epoch 30, Training Loss 0.4891223756553572\n",
      "Epoch 30, Training Loss 0.4901408486811401\n",
      "Epoch 30, Training Loss 0.49094809488872126\n",
      "Epoch 30, Training Loss 0.49180783876372725\n",
      "Epoch 30, Training Loss 0.4926255647178806\n",
      "Epoch 30, Training Loss 0.4933410485077392\n",
      "Epoch 30, Training Loss 0.49421614835329375\n",
      "Epoch 30, Training Loss 0.49520435365264676\n",
      "Epoch 30, Training Loss 0.49626513766815594\n",
      "Epoch 30, Training Loss 0.49731501121350263\n",
      "Epoch 30, Training Loss 0.49810738126030357\n",
      "Epoch 30, Training Loss 0.4988675552713292\n",
      "Epoch 30, Training Loss 0.49974794171350384\n",
      "Epoch 30, Training Loss 0.5006832591712932\n",
      "Epoch 30, Training Loss 0.5013384982905424\n",
      "Epoch 30, Training Loss 0.5020813218620427\n",
      "Epoch 30, Training Loss 0.5027246994283193\n",
      "Epoch 30, Training Loss 0.5035127812181898\n",
      "Epoch 30, Training Loss 0.5044867275925853\n",
      "Epoch 30, Training Loss 0.5051421204491344\n",
      "Epoch 30, Training Loss 0.5059591633889376\n",
      "Epoch 30, Training Loss 0.5066735362610244\n",
      "Epoch 30, Training Loss 0.5074861586246344\n",
      "Epoch 30, Training Loss 0.5082445303954737\n",
      "Epoch 30, Training Loss 0.5090733946436812\n",
      "Epoch 30, Training Loss 0.5098501154223977\n",
      "Epoch 30, Training Loss 0.5106443335180697\n",
      "Epoch 30, Training Loss 0.5115194935018145\n",
      "Epoch 30, Training Loss 0.512375949288878\n",
      "Epoch 30, Training Loss 0.5131443974459567\n",
      "Epoch 30, Training Loss 0.5141204712945787\n",
      "Epoch 30, Training Loss 0.5149178546103065\n",
      "Epoch 30, Training Loss 0.5157115510510056\n",
      "Epoch 30, Training Loss 0.5165921432130477\n",
      "Epoch 30, Training Loss 0.5173835941135426\n",
      "Epoch 30, Training Loss 0.5181247330535098\n",
      "Epoch 30, Training Loss 0.5188574361069428\n",
      "Epoch 30, Training Loss 0.5195494664599524\n",
      "Epoch 30, Training Loss 0.5202664235973602\n",
      "Epoch 30, Training Loss 0.521249666543263\n",
      "Epoch 30, Training Loss 0.5219356534273728\n",
      "Epoch 30, Training Loss 0.5229666124067038\n",
      "Epoch 30, Training Loss 0.523699374226353\n",
      "Epoch 30, Training Loss 0.5243434201725914\n",
      "Epoch 30, Training Loss 0.5254887922493088\n",
      "Epoch 30, Training Loss 0.526087552194705\n",
      "Epoch 30, Training Loss 0.5269657684027996\n",
      "Epoch 30, Training Loss 0.527670640279265\n",
      "Epoch 30, Training Loss 0.5287650654764127\n",
      "Epoch 30, Training Loss 0.52987026585185\n",
      "Epoch 30, Training Loss 0.5306793380804988\n",
      "Epoch 30, Training Loss 0.5315323636278777\n",
      "Epoch 30, Training Loss 0.5322788398512794\n",
      "Epoch 30, Training Loss 0.5329358213774079\n",
      "Epoch 30, Training Loss 0.5335043759449668\n",
      "Epoch 30, Training Loss 0.5341392010069259\n",
      "Epoch 30, Training Loss 0.5348461672015812\n",
      "Epoch 30, Training Loss 0.5357758031629235\n",
      "Epoch 30, Training Loss 0.5365339099903546\n",
      "Epoch 30, Training Loss 0.5374454471003979\n",
      "Epoch 30, Training Loss 0.538126232983816\n",
      "Epoch 30, Training Loss 0.5391196087955514\n",
      "Epoch 30, Training Loss 0.5400672326307467\n",
      "Epoch 30, Training Loss 0.5413105000010536\n",
      "Epoch 30, Training Loss 0.5419482639835923\n",
      "Epoch 30, Training Loss 0.5430562272858437\n",
      "Epoch 30, Training Loss 0.5440873836007569\n",
      "Epoch 30, Training Loss 0.5447319460189556\n",
      "Epoch 30, Training Loss 0.5457887308067068\n",
      "Epoch 30, Training Loss 0.5467087397794894\n",
      "Epoch 30, Training Loss 0.5475477394850358\n",
      "Epoch 30, Training Loss 0.5485032046085123\n",
      "Epoch 30, Training Loss 0.5491461272892135\n",
      "Epoch 30, Training Loss 0.5498668515621243\n",
      "Epoch 30, Training Loss 0.5506783418185875\n",
      "Epoch 30, Training Loss 0.5514672907721966\n",
      "Epoch 30, Training Loss 0.5525252477592214\n",
      "Epoch 30, Training Loss 0.5532205593403038\n",
      "Epoch 30, Training Loss 0.5538809903137519\n",
      "Epoch 30, Training Loss 0.5547831085179468\n",
      "Epoch 30, Training Loss 0.5555901224046107\n",
      "Epoch 30, Training Loss 0.5563741617495447\n",
      "Epoch 30, Training Loss 0.5573440969295209\n",
      "Epoch 30, Training Loss 0.5579103756591183\n",
      "Epoch 30, Training Loss 0.5586247124025584\n",
      "Epoch 30, Training Loss 0.5594680686588482\n",
      "Epoch 30, Training Loss 0.5603937226945482\n",
      "Epoch 30, Training Loss 0.5610059785187397\n",
      "Epoch 30, Training Loss 0.5618937128721295\n",
      "Epoch 30, Training Loss 0.5630136427595792\n",
      "Epoch 30, Training Loss 0.5637074647580876\n",
      "Epoch 30, Training Loss 0.5644764319786331\n",
      "Epoch 30, Training Loss 0.5652918295406014\n",
      "Epoch 30, Training Loss 0.5663078890355957\n",
      "Epoch 30, Training Loss 0.5670090366888534\n",
      "Epoch 30, Training Loss 0.5677438621859416\n",
      "Epoch 30, Training Loss 0.5688811174362821\n",
      "Epoch 30, Training Loss 0.5699083826807149\n",
      "Epoch 30, Training Loss 0.5706734550959619\n",
      "Epoch 30, Training Loss 0.5715388742935322\n",
      "Epoch 30, Training Loss 0.5724308478938954\n",
      "Epoch 30, Training Loss 0.5734563371943086\n",
      "Epoch 30, Training Loss 0.5742568462477315\n",
      "Epoch 30, Training Loss 0.575030536107395\n",
      "Epoch 30, Training Loss 0.575970728340966\n",
      "Epoch 30, Training Loss 0.5768457838260305\n",
      "Epoch 30, Training Loss 0.5777460949881302\n",
      "Epoch 30, Training Loss 0.5787161996831065\n",
      "Epoch 30, Training Loss 0.5795391655867667\n",
      "Epoch 30, Training Loss 0.5803124783060435\n",
      "Epoch 30, Training Loss 0.5810964312928412\n",
      "Epoch 30, Training Loss 0.5816715882943414\n",
      "Epoch 30, Training Loss 0.58266548972453\n",
      "Epoch 30, Training Loss 0.5837948123741028\n",
      "Epoch 30, Training Loss 0.5847619840937197\n",
      "Epoch 30, Training Loss 0.5856301903800891\n",
      "Epoch 30, Training Loss 0.5864776641587772\n",
      "Epoch 30, Training Loss 0.5872908420193835\n",
      "Epoch 30, Training Loss 0.5884001766095686\n",
      "Epoch 30, Training Loss 0.5894432526339045\n",
      "Epoch 30, Training Loss 0.5902231665294798\n",
      "Epoch 30, Training Loss 0.5912943464487105\n",
      "Epoch 30, Training Loss 0.5918837451873837\n",
      "Epoch 30, Training Loss 0.5926633637274623\n",
      "Epoch 30, Training Loss 0.5935694764337272\n",
      "Epoch 30, Training Loss 0.5943905688307779\n",
      "Epoch 30, Training Loss 0.5950061845809907\n",
      "Epoch 30, Training Loss 0.5959902313511695\n",
      "Epoch 30, Training Loss 0.5970106281892723\n",
      "Epoch 30, Training Loss 0.5979646772070004\n",
      "Epoch 30, Training Loss 0.5987254099162949\n",
      "Epoch 30, Training Loss 0.5994904783680616\n",
      "Epoch 30, Training Loss 0.6000144716800021\n",
      "Epoch 30, Training Loss 0.6008035446448095\n",
      "Epoch 30, Training Loss 0.6017585543491651\n",
      "Epoch 30, Training Loss 0.6029528190031685\n",
      "Epoch 30, Training Loss 0.6036391821130157\n",
      "Epoch 30, Training Loss 0.604288368083327\n",
      "Epoch 30, Training Loss 0.6049466272982795\n",
      "Epoch 30, Training Loss 0.6058781671783199\n",
      "Epoch 30, Training Loss 0.6068799149273606\n",
      "Epoch 30, Training Loss 0.6081214394334638\n",
      "Epoch 30, Training Loss 0.6089339417493557\n",
      "Epoch 30, Training Loss 0.6099335069927718\n",
      "Epoch 30, Training Loss 0.6113941563136133\n",
      "Epoch 30, Training Loss 0.612284365707956\n",
      "Epoch 30, Training Loss 0.6132061551980046\n",
      "Epoch 30, Training Loss 0.6140753863488927\n",
      "Epoch 30, Training Loss 0.6148441983077227\n",
      "Epoch 30, Training Loss 0.6156635662859968\n",
      "Epoch 30, Training Loss 0.6165309900899067\n",
      "Epoch 30, Training Loss 0.6175001020855306\n",
      "Epoch 30, Training Loss 0.6184132742836043\n",
      "Epoch 30, Training Loss 0.619303338706036\n",
      "Epoch 30, Training Loss 0.6201494324314015\n",
      "Epoch 30, Training Loss 0.6208117077189028\n",
      "Epoch 30, Training Loss 0.6215310681735158\n",
      "Epoch 30, Training Loss 0.6223927023999222\n",
      "Epoch 30, Training Loss 0.6235351611662399\n",
      "Epoch 30, Training Loss 0.6243914814327683\n",
      "Epoch 30, Training Loss 0.6252764080415296\n",
      "Epoch 30, Training Loss 0.6261695496490239\n",
      "Epoch 30, Training Loss 0.627109444545358\n",
      "Epoch 30, Training Loss 0.6279697573322165\n",
      "Epoch 30, Training Loss 0.6288470078993331\n",
      "Epoch 30, Training Loss 0.6295949971050863\n",
      "Epoch 30, Training Loss 0.6310122914402686\n",
      "Epoch 30, Training Loss 0.6316554861529099\n",
      "Epoch 30, Training Loss 0.632277797783732\n",
      "Epoch 30, Training Loss 0.6332705688598516\n",
      "Epoch 30, Training Loss 0.6339507830874694\n",
      "Epoch 30, Training Loss 0.6345910036655338\n",
      "Epoch 30, Training Loss 0.6354886615825126\n",
      "Epoch 30, Training Loss 0.6364178453260065\n",
      "Epoch 30, Training Loss 0.6373506747853116\n",
      "Epoch 30, Training Loss 0.637960610677824\n",
      "Epoch 30, Training Loss 0.6389575206181582\n",
      "Epoch 30, Training Loss 0.6400697068561374\n",
      "Epoch 30, Training Loss 0.640910114283147\n",
      "Epoch 30, Training Loss 0.6417589449440427\n",
      "Epoch 30, Training Loss 0.642480547890029\n",
      "Epoch 30, Training Loss 0.6432835469236764\n",
      "Epoch 30, Training Loss 0.6439592417929788\n",
      "Epoch 30, Training Loss 0.645814955272638\n",
      "Epoch 40, Training Loss 0.000780382119786099\n",
      "Epoch 40, Training Loss 0.001307132832534478\n",
      "Epoch 40, Training Loss 0.0020122111910749274\n",
      "Epoch 40, Training Loss 0.002682780518251307\n",
      "Epoch 40, Training Loss 0.003304088435819387\n",
      "Epoch 40, Training Loss 0.004083356817665002\n",
      "Epoch 40, Training Loss 0.0047400906262800215\n",
      "Epoch 40, Training Loss 0.005251383042091604\n",
      "Epoch 40, Training Loss 0.006223924972517106\n",
      "Epoch 40, Training Loss 0.007193251148514126\n",
      "Epoch 40, Training Loss 0.0077599107533159765\n",
      "Epoch 40, Training Loss 0.008461291947023337\n",
      "Epoch 40, Training Loss 0.009153237542532899\n",
      "Epoch 40, Training Loss 0.009894650343738858\n",
      "Epoch 40, Training Loss 0.010459969187026743\n",
      "Epoch 40, Training Loss 0.0113526512594784\n",
      "Epoch 40, Training Loss 0.011980589202907689\n",
      "Epoch 40, Training Loss 0.012654782370533175\n",
      "Epoch 40, Training Loss 0.013423211243756287\n",
      "Epoch 40, Training Loss 0.014066620670316164\n",
      "Epoch 40, Training Loss 0.014642159842774082\n",
      "Epoch 40, Training Loss 0.01553396758673441\n",
      "Epoch 40, Training Loss 0.0164816963779347\n",
      "Epoch 40, Training Loss 0.017136435031585986\n",
      "Epoch 40, Training Loss 0.017692299030930794\n",
      "Epoch 40, Training Loss 0.018424944842562956\n",
      "Epoch 40, Training Loss 0.019226752697964155\n",
      "Epoch 40, Training Loss 0.019818963030415117\n",
      "Epoch 40, Training Loss 0.020702528641047074\n",
      "Epoch 40, Training Loss 0.0214500572446667\n",
      "Epoch 40, Training Loss 0.022076812356024447\n",
      "Epoch 40, Training Loss 0.02264003333685648\n",
      "Epoch 40, Training Loss 0.023254683484201847\n",
      "Epoch 40, Training Loss 0.023802359771850468\n",
      "Epoch 40, Training Loss 0.024357955626514563\n",
      "Epoch 40, Training Loss 0.02505971266485541\n",
      "Epoch 40, Training Loss 0.02570277947903899\n",
      "Epoch 40, Training Loss 0.026654159397725256\n",
      "Epoch 40, Training Loss 0.027254473713352857\n",
      "Epoch 40, Training Loss 0.028124824166297913\n",
      "Epoch 40, Training Loss 0.028852720204216746\n",
      "Epoch 40, Training Loss 0.02949498105994271\n",
      "Epoch 40, Training Loss 0.03034755515168085\n",
      "Epoch 40, Training Loss 0.03088656201234559\n",
      "Epoch 40, Training Loss 0.031496530908452884\n",
      "Epoch 40, Training Loss 0.0323347441680596\n",
      "Epoch 40, Training Loss 0.03327012709949328\n",
      "Epoch 40, Training Loss 0.03373724736673448\n",
      "Epoch 40, Training Loss 0.03426823587826146\n",
      "Epoch 40, Training Loss 0.03487304801983602\n",
      "Epoch 40, Training Loss 0.03571162729159645\n",
      "Epoch 40, Training Loss 0.03640727302454926\n",
      "Epoch 40, Training Loss 0.03705615777036418\n",
      "Epoch 40, Training Loss 0.037902950135338336\n",
      "Epoch 40, Training Loss 0.038387572445223096\n",
      "Epoch 40, Training Loss 0.03917619997583082\n",
      "Epoch 40, Training Loss 0.039887095961119515\n",
      "Epoch 40, Training Loss 0.04039663514670204\n",
      "Epoch 40, Training Loss 0.04101721187839118\n",
      "Epoch 40, Training Loss 0.04158985763407119\n",
      "Epoch 40, Training Loss 0.042420390278786954\n",
      "Epoch 40, Training Loss 0.0430224034792322\n",
      "Epoch 40, Training Loss 0.04413881562554928\n",
      "Epoch 40, Training Loss 0.044928282804196445\n",
      "Epoch 40, Training Loss 0.045680044709569045\n",
      "Epoch 40, Training Loss 0.04631436042621008\n",
      "Epoch 40, Training Loss 0.04697511655747738\n",
      "Epoch 40, Training Loss 0.047493502535783425\n",
      "Epoch 40, Training Loss 0.04826764286021747\n",
      "Epoch 40, Training Loss 0.0489019926475442\n",
      "Epoch 40, Training Loss 0.04951149636827162\n",
      "Epoch 40, Training Loss 0.05031727265823833\n",
      "Epoch 40, Training Loss 0.05106659405066839\n",
      "Epoch 40, Training Loss 0.05165055184565542\n",
      "Epoch 40, Training Loss 0.05229414641247381\n",
      "Epoch 40, Training Loss 0.05265624462948431\n",
      "Epoch 40, Training Loss 0.05330226576084371\n",
      "Epoch 40, Training Loss 0.054144746621551415\n",
      "Epoch 40, Training Loss 0.05481896402738284\n",
      "Epoch 40, Training Loss 0.05564074690841958\n",
      "Epoch 40, Training Loss 0.05616963226014696\n",
      "Epoch 40, Training Loss 0.056753969672695755\n",
      "Epoch 40, Training Loss 0.057182355914884214\n",
      "Epoch 40, Training Loss 0.05798275719213364\n",
      "Epoch 40, Training Loss 0.05877817172528533\n",
      "Epoch 40, Training Loss 0.05929260721901798\n",
      "Epoch 40, Training Loss 0.059928789163184594\n",
      "Epoch 40, Training Loss 0.06053901736236289\n",
      "Epoch 40, Training Loss 0.06141416339770607\n",
      "Epoch 40, Training Loss 0.06216609771446804\n",
      "Epoch 40, Training Loss 0.06286052147598217\n",
      "Epoch 40, Training Loss 0.06352568789363822\n",
      "Epoch 40, Training Loss 0.06431084608330447\n",
      "Epoch 40, Training Loss 0.06508041651505034\n",
      "Epoch 40, Training Loss 0.0657699864614955\n",
      "Epoch 40, Training Loss 0.06663887271338412\n",
      "Epoch 40, Training Loss 0.06711404120830623\n",
      "Epoch 40, Training Loss 0.0676575459711387\n",
      "Epoch 40, Training Loss 0.0683522480909172\n",
      "Epoch 40, Training Loss 0.06881245014155307\n",
      "Epoch 40, Training Loss 0.06962932775849881\n",
      "Epoch 40, Training Loss 0.0704606620170881\n",
      "Epoch 40, Training Loss 0.07114216628129524\n",
      "Epoch 40, Training Loss 0.07181340642749806\n",
      "Epoch 40, Training Loss 0.0724298383878625\n",
      "Epoch 40, Training Loss 0.0732080764172937\n",
      "Epoch 40, Training Loss 0.07384030974429587\n",
      "Epoch 40, Training Loss 0.07425542720748335\n",
      "Epoch 40, Training Loss 0.0747568004042901\n",
      "Epoch 40, Training Loss 0.07558921051909552\n",
      "Epoch 40, Training Loss 0.07616136487944962\n",
      "Epoch 40, Training Loss 0.07679002901629718\n",
      "Epoch 40, Training Loss 0.07739642769326945\n",
      "Epoch 40, Training Loss 0.0782367488383637\n",
      "Epoch 40, Training Loss 0.07902934800480943\n",
      "Epoch 40, Training Loss 0.0801578175152659\n",
      "Epoch 40, Training Loss 0.08083887352510487\n",
      "Epoch 40, Training Loss 0.08156292079507238\n",
      "Epoch 40, Training Loss 0.0821715443564193\n",
      "Epoch 40, Training Loss 0.08279879238751843\n",
      "Epoch 40, Training Loss 0.08339519406218662\n",
      "Epoch 40, Training Loss 0.08413584221659413\n",
      "Epoch 40, Training Loss 0.08471584442021597\n",
      "Epoch 40, Training Loss 0.08532118347599683\n",
      "Epoch 40, Training Loss 0.08615868231829475\n",
      "Epoch 40, Training Loss 0.08668712818104288\n",
      "Epoch 40, Training Loss 0.08720958739747782\n",
      "Epoch 40, Training Loss 0.08812429635878415\n",
      "Epoch 40, Training Loss 0.08903457338700209\n",
      "Epoch 40, Training Loss 0.08987005901001298\n",
      "Epoch 40, Training Loss 0.09051254121086481\n",
      "Epoch 40, Training Loss 0.09115687161302932\n",
      "Epoch 40, Training Loss 0.09190308159726965\n",
      "Epoch 40, Training Loss 0.09255421897182074\n",
      "Epoch 40, Training Loss 0.09329592728096506\n",
      "Epoch 40, Training Loss 0.09403923485437622\n",
      "Epoch 40, Training Loss 0.09464072003541395\n",
      "Epoch 40, Training Loss 0.09519144827904909\n",
      "Epoch 40, Training Loss 0.09595999209319844\n",
      "Epoch 40, Training Loss 0.0965918862758695\n",
      "Epoch 40, Training Loss 0.09720869892088653\n",
      "Epoch 40, Training Loss 0.09791694204215809\n",
      "Epoch 40, Training Loss 0.09855479245905376\n",
      "Epoch 40, Training Loss 0.09918151197531035\n",
      "Epoch 40, Training Loss 0.09985023645488807\n",
      "Epoch 40, Training Loss 0.10039345013058704\n",
      "Epoch 40, Training Loss 0.10103212186442617\n",
      "Epoch 40, Training Loss 0.10172514255394412\n",
      "Epoch 40, Training Loss 0.10243135599224158\n",
      "Epoch 40, Training Loss 0.10309081370263454\n",
      "Epoch 40, Training Loss 0.1037070320542816\n",
      "Epoch 40, Training Loss 0.10428478216271267\n",
      "Epoch 40, Training Loss 0.10492524489417406\n",
      "Epoch 40, Training Loss 0.1056637978157424\n",
      "Epoch 40, Training Loss 0.10625765055341793\n",
      "Epoch 40, Training Loss 0.10677359617122299\n",
      "Epoch 40, Training Loss 0.10719286545615672\n",
      "Epoch 40, Training Loss 0.10792806809363158\n",
      "Epoch 40, Training Loss 0.10876877674513766\n",
      "Epoch 40, Training Loss 0.1094590367944649\n",
      "Epoch 40, Training Loss 0.11017713312755155\n",
      "Epoch 40, Training Loss 0.11091612043130733\n",
      "Epoch 40, Training Loss 0.11154302825098453\n",
      "Epoch 40, Training Loss 0.11254579712972616\n",
      "Epoch 40, Training Loss 0.11304406707396593\n",
      "Epoch 40, Training Loss 0.11367599719473163\n",
      "Epoch 40, Training Loss 0.11438802178101161\n",
      "Epoch 40, Training Loss 0.11505659862095133\n",
      "Epoch 40, Training Loss 0.11565892276404154\n",
      "Epoch 40, Training Loss 0.11633983563126811\n",
      "Epoch 40, Training Loss 0.11728829324550336\n",
      "Epoch 40, Training Loss 0.11808189219983338\n",
      "Epoch 40, Training Loss 0.11891704359475304\n",
      "Epoch 40, Training Loss 0.11972799054954363\n",
      "Epoch 40, Training Loss 0.12042438004480298\n",
      "Epoch 40, Training Loss 0.12104914358357334\n",
      "Epoch 40, Training Loss 0.12167121267989468\n",
      "Epoch 40, Training Loss 0.12273121238364589\n",
      "Epoch 40, Training Loss 0.12349102282158249\n",
      "Epoch 40, Training Loss 0.12397282042771654\n",
      "Epoch 40, Training Loss 0.1245955660977327\n",
      "Epoch 40, Training Loss 0.1255043434822346\n",
      "Epoch 40, Training Loss 0.1262031330934266\n",
      "Epoch 40, Training Loss 0.12668700440003133\n",
      "Epoch 40, Training Loss 0.1271358234498202\n",
      "Epoch 40, Training Loss 0.12793563249166054\n",
      "Epoch 40, Training Loss 0.1286566028052279\n",
      "Epoch 40, Training Loss 0.12946191361493162\n",
      "Epoch 40, Training Loss 0.13006502172678633\n",
      "Epoch 40, Training Loss 0.130721173818459\n",
      "Epoch 40, Training Loss 0.13129378557967408\n",
      "Epoch 40, Training Loss 0.13211522729652922\n",
      "Epoch 40, Training Loss 0.13283558586216948\n",
      "Epoch 40, Training Loss 0.13359004392495852\n",
      "Epoch 40, Training Loss 0.13437399622576926\n",
      "Epoch 40, Training Loss 0.1353503321976308\n",
      "Epoch 40, Training Loss 0.13600790893177853\n",
      "Epoch 40, Training Loss 0.13670010872356728\n",
      "Epoch 40, Training Loss 0.13730061572530997\n",
      "Epoch 40, Training Loss 0.1382097741374579\n",
      "Epoch 40, Training Loss 0.13873614549941723\n",
      "Epoch 40, Training Loss 0.1395375134847353\n",
      "Epoch 40, Training Loss 0.1406502988942139\n",
      "Epoch 40, Training Loss 0.14132370515857512\n",
      "Epoch 40, Training Loss 0.14203489093524416\n",
      "Epoch 40, Training Loss 0.14285811621819616\n",
      "Epoch 40, Training Loss 0.14373848291919056\n",
      "Epoch 40, Training Loss 0.14429433121705604\n",
      "Epoch 40, Training Loss 0.144944963476542\n",
      "Epoch 40, Training Loss 0.14546133452059362\n",
      "Epoch 40, Training Loss 0.1464326906844478\n",
      "Epoch 40, Training Loss 0.1472190139848558\n",
      "Epoch 40, Training Loss 0.14774848288282408\n",
      "Epoch 40, Training Loss 0.14831628050188275\n",
      "Epoch 40, Training Loss 0.14906028633379875\n",
      "Epoch 40, Training Loss 0.14975348263597854\n",
      "Epoch 40, Training Loss 0.1506404282568056\n",
      "Epoch 40, Training Loss 0.15147490822293264\n",
      "Epoch 40, Training Loss 0.15239621180555094\n",
      "Epoch 40, Training Loss 0.15323027720689164\n",
      "Epoch 40, Training Loss 0.15374921178421402\n",
      "Epoch 40, Training Loss 0.1543302556209247\n",
      "Epoch 40, Training Loss 0.15476403124344623\n",
      "Epoch 40, Training Loss 0.1556297216345282\n",
      "Epoch 40, Training Loss 0.1563118205732092\n",
      "Epoch 40, Training Loss 0.15679185336355664\n",
      "Epoch 40, Training Loss 0.1574424029235035\n",
      "Epoch 40, Training Loss 0.15830305306350484\n",
      "Epoch 40, Training Loss 0.15886780996914104\n",
      "Epoch 40, Training Loss 0.15933687626705756\n",
      "Epoch 40, Training Loss 0.1599278060905159\n",
      "Epoch 40, Training Loss 0.16031918017303243\n",
      "Epoch 40, Training Loss 0.16094559526352017\n",
      "Epoch 40, Training Loss 0.16166194088166327\n",
      "Epoch 40, Training Loss 0.16224051661351147\n",
      "Epoch 40, Training Loss 0.16281977901830696\n",
      "Epoch 40, Training Loss 0.16381884303391742\n",
      "Epoch 40, Training Loss 0.16448428049264358\n",
      "Epoch 40, Training Loss 0.16535197815779226\n",
      "Epoch 40, Training Loss 0.16596781289028695\n",
      "Epoch 40, Training Loss 0.16653415758896362\n",
      "Epoch 40, Training Loss 0.1672235322577874\n",
      "Epoch 40, Training Loss 0.16781179530693746\n",
      "Epoch 40, Training Loss 0.16839310881274436\n",
      "Epoch 40, Training Loss 0.16924323606521577\n",
      "Epoch 40, Training Loss 0.1700963133664997\n",
      "Epoch 40, Training Loss 0.17068152384989707\n",
      "Epoch 40, Training Loss 0.17122191651855284\n",
      "Epoch 40, Training Loss 0.17218087865111162\n",
      "Epoch 40, Training Loss 0.17289283532468255\n",
      "Epoch 40, Training Loss 0.173404448134515\n",
      "Epoch 40, Training Loss 0.17437954346084839\n",
      "Epoch 40, Training Loss 0.17510157415781485\n",
      "Epoch 40, Training Loss 0.17555119958527557\n",
      "Epoch 40, Training Loss 0.17622191319837593\n",
      "Epoch 40, Training Loss 0.17711179964530194\n",
      "Epoch 40, Training Loss 0.1778258624512826\n",
      "Epoch 40, Training Loss 0.1785657458826709\n",
      "Epoch 40, Training Loss 0.1793174528328659\n",
      "Epoch 40, Training Loss 0.17990363250150704\n",
      "Epoch 40, Training Loss 0.18054119377489894\n",
      "Epoch 40, Training Loss 0.18120786921142618\n",
      "Epoch 40, Training Loss 0.1816914909712189\n",
      "Epoch 40, Training Loss 0.18202072157121985\n",
      "Epoch 40, Training Loss 0.182615728367625\n",
      "Epoch 40, Training Loss 0.1834149962419744\n",
      "Epoch 40, Training Loss 0.18428819681829808\n",
      "Epoch 40, Training Loss 0.18502890838838904\n",
      "Epoch 40, Training Loss 0.18555042334377309\n",
      "Epoch 40, Training Loss 0.18615022274996618\n",
      "Epoch 40, Training Loss 0.18690942708031297\n",
      "Epoch 40, Training Loss 0.1878900487175988\n",
      "Epoch 40, Training Loss 0.18860026351783588\n",
      "Epoch 40, Training Loss 0.18914335032405755\n",
      "Epoch 40, Training Loss 0.18976724879516055\n",
      "Epoch 40, Training Loss 0.19059966775157566\n",
      "Epoch 40, Training Loss 0.1912080488927529\n",
      "Epoch 40, Training Loss 0.1918657529918129\n",
      "Epoch 40, Training Loss 0.19243685230422203\n",
      "Epoch 40, Training Loss 0.19310040162195025\n",
      "Epoch 40, Training Loss 0.19410715456051594\n",
      "Epoch 40, Training Loss 0.19488476666495624\n",
      "Epoch 40, Training Loss 0.1958125317874162\n",
      "Epoch 40, Training Loss 0.19645091376798537\n",
      "Epoch 40, Training Loss 0.19729506165322747\n",
      "Epoch 40, Training Loss 0.19787905588174415\n",
      "Epoch 40, Training Loss 0.19861678836290794\n",
      "Epoch 40, Training Loss 0.19935231966435757\n",
      "Epoch 40, Training Loss 0.2000385389456054\n",
      "Epoch 40, Training Loss 0.20069958220052597\n",
      "Epoch 40, Training Loss 0.20149464215463994\n",
      "Epoch 40, Training Loss 0.20206206061346146\n",
      "Epoch 40, Training Loss 0.20285279923082922\n",
      "Epoch 40, Training Loss 0.20346568604869306\n",
      "Epoch 40, Training Loss 0.20429633393921814\n",
      "Epoch 40, Training Loss 0.2049029516365827\n",
      "Epoch 40, Training Loss 0.2057624891057344\n",
      "Epoch 40, Training Loss 0.2064589311171066\n",
      "Epoch 40, Training Loss 0.207194831098437\n",
      "Epoch 40, Training Loss 0.2079032861134585\n",
      "Epoch 40, Training Loss 0.2085674200445185\n",
      "Epoch 40, Training Loss 0.20931615263147427\n",
      "Epoch 40, Training Loss 0.21006590627191013\n",
      "Epoch 40, Training Loss 0.2107516565667394\n",
      "Epoch 40, Training Loss 0.2113205178848008\n",
      "Epoch 40, Training Loss 0.212287202858559\n",
      "Epoch 40, Training Loss 0.2128499834738729\n",
      "Epoch 40, Training Loss 0.21344756626564523\n",
      "Epoch 40, Training Loss 0.2140918272306852\n",
      "Epoch 40, Training Loss 0.21491990823422552\n",
      "Epoch 40, Training Loss 0.21555816982408313\n",
      "Epoch 40, Training Loss 0.21629381568535513\n",
      "Epoch 40, Training Loss 0.2172773698406756\n",
      "Epoch 40, Training Loss 0.21815034000160138\n",
      "Epoch 40, Training Loss 0.21865146098386906\n",
      "Epoch 40, Training Loss 0.21924245738617296\n",
      "Epoch 40, Training Loss 0.21971789006229556\n",
      "Epoch 40, Training Loss 0.2203589761272416\n",
      "Epoch 40, Training Loss 0.22094288117745342\n",
      "Epoch 40, Training Loss 0.22152732217403323\n",
      "Epoch 40, Training Loss 0.22191471238727764\n",
      "Epoch 40, Training Loss 0.22262728111365873\n",
      "Epoch 40, Training Loss 0.22348240433294145\n",
      "Epoch 40, Training Loss 0.224035339892063\n",
      "Epoch 40, Training Loss 0.2245559450762961\n",
      "Epoch 40, Training Loss 0.22515079611555086\n",
      "Epoch 40, Training Loss 0.22574401439150885\n",
      "Epoch 40, Training Loss 0.22637312918367897\n",
      "Epoch 40, Training Loss 0.22689008933808796\n",
      "Epoch 40, Training Loss 0.2275325872404191\n",
      "Epoch 40, Training Loss 0.2282262888101056\n",
      "Epoch 40, Training Loss 0.22868589039348886\n",
      "Epoch 40, Training Loss 0.22950285551188243\n",
      "Epoch 40, Training Loss 0.23011483389245885\n",
      "Epoch 40, Training Loss 0.2305927385607034\n",
      "Epoch 40, Training Loss 0.23125947672692712\n",
      "Epoch 40, Training Loss 0.23186823226454312\n",
      "Epoch 40, Training Loss 0.23253333115059396\n",
      "Epoch 40, Training Loss 0.2330109645491061\n",
      "Epoch 40, Training Loss 0.2339250023865029\n",
      "Epoch 40, Training Loss 0.2345712836593618\n",
      "Epoch 40, Training Loss 0.23551741486315228\n",
      "Epoch 40, Training Loss 0.23659488970361403\n",
      "Epoch 40, Training Loss 0.23742177305014237\n",
      "Epoch 40, Training Loss 0.23801656730492096\n",
      "Epoch 40, Training Loss 0.2387718991626559\n",
      "Epoch 40, Training Loss 0.2393102718283758\n",
      "Epoch 40, Training Loss 0.24014709535462167\n",
      "Epoch 40, Training Loss 0.24141606429348822\n",
      "Epoch 40, Training Loss 0.2422234400763841\n",
      "Epoch 40, Training Loss 0.24276442216028032\n",
      "Epoch 40, Training Loss 0.24346687280765886\n",
      "Epoch 40, Training Loss 0.24414456831982068\n",
      "Epoch 40, Training Loss 0.24547830532731302\n",
      "Epoch 40, Training Loss 0.24663211961688897\n",
      "Epoch 40, Training Loss 0.24746455579919888\n",
      "Epoch 40, Training Loss 0.24820111028831024\n",
      "Epoch 40, Training Loss 0.24892291262783967\n",
      "Epoch 40, Training Loss 0.24954116485460334\n",
      "Epoch 40, Training Loss 0.25014599578459856\n",
      "Epoch 40, Training Loss 0.2506576316893253\n",
      "Epoch 40, Training Loss 0.25133569321364085\n",
      "Epoch 40, Training Loss 0.2519960212890449\n",
      "Epoch 40, Training Loss 0.2525645648427022\n",
      "Epoch 40, Training Loss 0.2532809599281272\n",
      "Epoch 40, Training Loss 0.2539895045025574\n",
      "Epoch 40, Training Loss 0.2545790820932754\n",
      "Epoch 40, Training Loss 0.255313295689995\n",
      "Epoch 40, Training Loss 0.2560111691274911\n",
      "Epoch 40, Training Loss 0.2567545722817521\n",
      "Epoch 40, Training Loss 0.25754563788623763\n",
      "Epoch 40, Training Loss 0.2583491072020567\n",
      "Epoch 40, Training Loss 0.25912265910212035\n",
      "Epoch 40, Training Loss 0.2597512386720199\n",
      "Epoch 40, Training Loss 0.2601350173544701\n",
      "Epoch 40, Training Loss 0.26081585499179333\n",
      "Epoch 40, Training Loss 0.2615553757647419\n",
      "Epoch 40, Training Loss 0.262289526631765\n",
      "Epoch 40, Training Loss 0.2627928578259085\n",
      "Epoch 40, Training Loss 0.26365016309349126\n",
      "Epoch 40, Training Loss 0.26444414261814275\n",
      "Epoch 40, Training Loss 0.2650206754046023\n",
      "Epoch 40, Training Loss 0.26561890969343505\n",
      "Epoch 40, Training Loss 0.2662785270101274\n",
      "Epoch 40, Training Loss 0.267031090376932\n",
      "Epoch 40, Training Loss 0.2674703955497888\n",
      "Epoch 40, Training Loss 0.26814191031943807\n",
      "Epoch 40, Training Loss 0.26910776402944186\n",
      "Epoch 40, Training Loss 0.26972312165800566\n",
      "Epoch 40, Training Loss 0.27042222796651105\n",
      "Epoch 40, Training Loss 0.27109811293042224\n",
      "Epoch 40, Training Loss 0.27160178551740966\n",
      "Epoch 40, Training Loss 0.2723827680282276\n",
      "Epoch 40, Training Loss 0.27310404299622604\n",
      "Epoch 40, Training Loss 0.2737343461464738\n",
      "Epoch 40, Training Loss 0.2742638740393207\n",
      "Epoch 40, Training Loss 0.2751604468773698\n",
      "Epoch 40, Training Loss 0.2761696031331406\n",
      "Epoch 40, Training Loss 0.2766146106869363\n",
      "Epoch 40, Training Loss 0.27734046141662255\n",
      "Epoch 40, Training Loss 0.2778333904569411\n",
      "Epoch 40, Training Loss 0.27849786257957254\n",
      "Epoch 40, Training Loss 0.2790142076704508\n",
      "Epoch 40, Training Loss 0.2799165630736924\n",
      "Epoch 40, Training Loss 0.28086528212518036\n",
      "Epoch 40, Training Loss 0.2815463146590211\n",
      "Epoch 40, Training Loss 0.28233942198936285\n",
      "Epoch 40, Training Loss 0.28302612915978104\n",
      "Epoch 40, Training Loss 0.2838291998409554\n",
      "Epoch 40, Training Loss 0.28436007200147184\n",
      "Epoch 40, Training Loss 0.28494325596505726\n",
      "Epoch 40, Training Loss 0.28574813056327497\n",
      "Epoch 40, Training Loss 0.286414838698514\n",
      "Epoch 40, Training Loss 0.2872362930680175\n",
      "Epoch 40, Training Loss 0.2880697057908758\n",
      "Epoch 40, Training Loss 0.28911434845698764\n",
      "Epoch 40, Training Loss 0.2899772937569167\n",
      "Epoch 40, Training Loss 0.290596328885354\n",
      "Epoch 40, Training Loss 0.29129156965733793\n",
      "Epoch 40, Training Loss 0.29212275048351044\n",
      "Epoch 40, Training Loss 0.29281872678595733\n",
      "Epoch 40, Training Loss 0.29339457381411893\n",
      "Epoch 40, Training Loss 0.2940637472340518\n",
      "Epoch 40, Training Loss 0.2947938416315162\n",
      "Epoch 40, Training Loss 0.2953499142471177\n",
      "Epoch 40, Training Loss 0.29609217988255687\n",
      "Epoch 40, Training Loss 0.29691380010846324\n",
      "Epoch 40, Training Loss 0.2972699802778566\n",
      "Epoch 40, Training Loss 0.29785721926280606\n",
      "Epoch 40, Training Loss 0.2985682005391401\n",
      "Epoch 40, Training Loss 0.29922888715706214\n",
      "Epoch 40, Training Loss 0.3002425825123287\n",
      "Epoch 40, Training Loss 0.3008396896483648\n",
      "Epoch 40, Training Loss 0.3014061343868065\n",
      "Epoch 40, Training Loss 0.3021150750043752\n",
      "Epoch 40, Training Loss 0.3029260596884486\n",
      "Epoch 40, Training Loss 0.3039793523452471\n",
      "Epoch 40, Training Loss 0.30465641240482133\n",
      "Epoch 40, Training Loss 0.3053716756879826\n",
      "Epoch 40, Training Loss 0.30602073254030376\n",
      "Epoch 40, Training Loss 0.306857407969587\n",
      "Epoch 40, Training Loss 0.30789914174610394\n",
      "Epoch 40, Training Loss 0.3087293597514672\n",
      "Epoch 40, Training Loss 0.30957067466300464\n",
      "Epoch 40, Training Loss 0.31019406805715294\n",
      "Epoch 40, Training Loss 0.3107393201812149\n",
      "Epoch 40, Training Loss 0.3112964544378583\n",
      "Epoch 40, Training Loss 0.312149522745091\n",
      "Epoch 40, Training Loss 0.31275860065846794\n",
      "Epoch 40, Training Loss 0.31331625698929855\n",
      "Epoch 40, Training Loss 0.3138453909731887\n",
      "Epoch 40, Training Loss 0.3145996654201347\n",
      "Epoch 40, Training Loss 0.3154392219184305\n",
      "Epoch 40, Training Loss 0.31610951380199176\n",
      "Epoch 40, Training Loss 0.3169653411487789\n",
      "Epoch 40, Training Loss 0.317853469944671\n",
      "Epoch 40, Training Loss 0.3185478117688538\n",
      "Epoch 40, Training Loss 0.31921854451336823\n",
      "Epoch 40, Training Loss 0.3197376332853151\n",
      "Epoch 40, Training Loss 0.3206265870186374\n",
      "Epoch 40, Training Loss 0.3211953782898081\n",
      "Epoch 40, Training Loss 0.3218624881847435\n",
      "Epoch 40, Training Loss 0.32257513381788494\n",
      "Epoch 40, Training Loss 0.32352236397278583\n",
      "Epoch 40, Training Loss 0.32398893781330274\n",
      "Epoch 40, Training Loss 0.3247928094223637\n",
      "Epoch 40, Training Loss 0.32551294450869644\n",
      "Epoch 40, Training Loss 0.3262257329033464\n",
      "Epoch 40, Training Loss 0.32721214991091463\n",
      "Epoch 40, Training Loss 0.32767178251615264\n",
      "Epoch 40, Training Loss 0.32834396635175056\n",
      "Epoch 40, Training Loss 0.32898625403718873\n",
      "Epoch 40, Training Loss 0.3296839966798377\n",
      "Epoch 40, Training Loss 0.33035153287755864\n",
      "Epoch 40, Training Loss 0.3312133715280791\n",
      "Epoch 40, Training Loss 0.33195776608593935\n",
      "Epoch 40, Training Loss 0.33266912290202383\n",
      "Epoch 40, Training Loss 0.3334303617172534\n",
      "Epoch 40, Training Loss 0.3344652334137646\n",
      "Epoch 40, Training Loss 0.3352359743679271\n",
      "Epoch 40, Training Loss 0.3361428546936006\n",
      "Epoch 40, Training Loss 0.33688968450517\n",
      "Epoch 40, Training Loss 0.33754734835966166\n",
      "Epoch 40, Training Loss 0.33829553192838685\n",
      "Epoch 40, Training Loss 0.33916076949185425\n",
      "Epoch 40, Training Loss 0.33986871359903187\n",
      "Epoch 40, Training Loss 0.34070905844878663\n",
      "Epoch 40, Training Loss 0.3412075638771057\n",
      "Epoch 40, Training Loss 0.3418254907173879\n",
      "Epoch 40, Training Loss 0.34275017804501917\n",
      "Epoch 40, Training Loss 0.34350528459414803\n",
      "Epoch 40, Training Loss 0.3442754554931465\n",
      "Epoch 40, Training Loss 0.3448064866120858\n",
      "Epoch 40, Training Loss 0.34545696643002505\n",
      "Epoch 40, Training Loss 0.3461346049290484\n",
      "Epoch 40, Training Loss 0.34689052688801075\n",
      "Epoch 40, Training Loss 0.3474372211091049\n",
      "Epoch 40, Training Loss 0.34801343136736196\n",
      "Epoch 40, Training Loss 0.3488788828825402\n",
      "Epoch 40, Training Loss 0.34950408945455574\n",
      "Epoch 40, Training Loss 0.34998566201885645\n",
      "Epoch 40, Training Loss 0.3508597655826822\n",
      "Epoch 40, Training Loss 0.3515888229965249\n",
      "Epoch 40, Training Loss 0.35219092800489166\n",
      "Epoch 40, Training Loss 0.3528373880916849\n",
      "Epoch 40, Training Loss 0.3536720803326658\n",
      "Epoch 40, Training Loss 0.3543432563009774\n",
      "Epoch 40, Training Loss 0.3549958299797819\n",
      "Epoch 40, Training Loss 0.35574089307004536\n",
      "Epoch 40, Training Loss 0.3563982611879363\n",
      "Epoch 40, Training Loss 0.35712793332231624\n",
      "Epoch 40, Training Loss 0.35776613995699624\n",
      "Epoch 40, Training Loss 0.3586220248504673\n",
      "Epoch 40, Training Loss 0.3592798635554131\n",
      "Epoch 40, Training Loss 0.3599777354684937\n",
      "Epoch 40, Training Loss 0.360780646772031\n",
      "Epoch 40, Training Loss 0.36145964184837875\n",
      "Epoch 40, Training Loss 0.3623449704454988\n",
      "Epoch 40, Training Loss 0.3630657567621192\n",
      "Epoch 40, Training Loss 0.36371894081687683\n",
      "Epoch 40, Training Loss 0.3643989949808706\n",
      "Epoch 40, Training Loss 0.36512743371069584\n",
      "Epoch 40, Training Loss 0.36563877814718526\n",
      "Epoch 40, Training Loss 0.3664743036336606\n",
      "Epoch 40, Training Loss 0.367329390648076\n",
      "Epoch 40, Training Loss 0.3678881442912704\n",
      "Epoch 40, Training Loss 0.36874238822771155\n",
      "Epoch 40, Training Loss 0.369614024296441\n",
      "Epoch 40, Training Loss 0.37036150289923336\n",
      "Epoch 40, Training Loss 0.3711154072181038\n",
      "Epoch 40, Training Loss 0.37160761364738043\n",
      "Epoch 40, Training Loss 0.37235377355457266\n",
      "Epoch 40, Training Loss 0.37290949627871406\n",
      "Epoch 40, Training Loss 0.37358626083034996\n",
      "Epoch 40, Training Loss 0.3742602820439107\n",
      "Epoch 40, Training Loss 0.374920371929398\n",
      "Epoch 40, Training Loss 0.37583656971107055\n",
      "Epoch 40, Training Loss 0.3767296223689223\n",
      "Epoch 40, Training Loss 0.3774029468483937\n",
      "Epoch 40, Training Loss 0.37818496085493764\n",
      "Epoch 40, Training Loss 0.37889003524999787\n",
      "Epoch 40, Training Loss 0.37961357725245876\n",
      "Epoch 40, Training Loss 0.3802137222055279\n",
      "Epoch 40, Training Loss 0.3808609520459114\n",
      "Epoch 40, Training Loss 0.3816177735624411\n",
      "Epoch 40, Training Loss 0.38238297867805454\n",
      "Epoch 40, Training Loss 0.38308985630417114\n",
      "Epoch 40, Training Loss 0.38373111668602583\n",
      "Epoch 40, Training Loss 0.38423053443889177\n",
      "Epoch 40, Training Loss 0.38515353461970453\n",
      "Epoch 40, Training Loss 0.38586446498056204\n",
      "Epoch 40, Training Loss 0.3868434538926615\n",
      "Epoch 40, Training Loss 0.38765839245313266\n",
      "Epoch 40, Training Loss 0.3883788845575679\n",
      "Epoch 40, Training Loss 0.38890010183272156\n",
      "Epoch 40, Training Loss 0.38941971439382306\n",
      "Epoch 40, Training Loss 0.39040733104014336\n",
      "Epoch 40, Training Loss 0.3912363947581147\n",
      "Epoch 40, Training Loss 0.39202412745684306\n",
      "Epoch 40, Training Loss 0.39293493894512393\n",
      "Epoch 40, Training Loss 0.39354761104906916\n",
      "Epoch 40, Training Loss 0.3941820034438082\n",
      "Epoch 40, Training Loss 0.3948443904709633\n",
      "Epoch 40, Training Loss 0.3957718356186167\n",
      "Epoch 40, Training Loss 0.3963842971626755\n",
      "Epoch 40, Training Loss 0.3972527789871406\n",
      "Epoch 40, Training Loss 0.39815279090648414\n",
      "Epoch 40, Training Loss 0.3989267483772829\n",
      "Epoch 40, Training Loss 0.39959863213169605\n",
      "Epoch 40, Training Loss 0.40046863623744694\n",
      "Epoch 40, Training Loss 0.40102220000818256\n",
      "Epoch 40, Training Loss 0.40175635613443905\n",
      "Epoch 40, Training Loss 0.4023325284728614\n",
      "Epoch 40, Training Loss 0.4030695482135734\n",
      "Epoch 40, Training Loss 0.4037897752221588\n",
      "Epoch 40, Training Loss 0.40451184768810905\n",
      "Epoch 40, Training Loss 0.40512169928044617\n",
      "Epoch 40, Training Loss 0.4057904171288166\n",
      "Epoch 40, Training Loss 0.40643988343913234\n",
      "Epoch 40, Training Loss 0.4071206790025887\n",
      "Epoch 40, Training Loss 0.4077905487755071\n",
      "Epoch 40, Training Loss 0.40854831253323715\n",
      "Epoch 40, Training Loss 0.409263503635326\n",
      "Epoch 40, Training Loss 0.41025284271868295\n",
      "Epoch 40, Training Loss 0.4110852648001498\n",
      "Epoch 40, Training Loss 0.4117605705624041\n",
      "Epoch 40, Training Loss 0.41224512484524867\n",
      "Epoch 40, Training Loss 0.41293251312449764\n",
      "Epoch 40, Training Loss 0.41361099759788467\n",
      "Epoch 40, Training Loss 0.41432184121950205\n",
      "Epoch 40, Training Loss 0.41516069420005963\n",
      "Epoch 40, Training Loss 0.4158970385484988\n",
      "Epoch 40, Training Loss 0.41660570568593264\n",
      "Epoch 40, Training Loss 0.41741721774153695\n",
      "Epoch 40, Training Loss 0.4180812196582175\n",
      "Epoch 40, Training Loss 0.41866237607301043\n",
      "Epoch 40, Training Loss 0.4191710698939955\n",
      "Epoch 40, Training Loss 0.4199322907211226\n",
      "Epoch 40, Training Loss 0.42054941808171287\n",
      "Epoch 40, Training Loss 0.42131122146421074\n",
      "Epoch 40, Training Loss 0.42191032505096376\n",
      "Epoch 40, Training Loss 0.4227644282076365\n",
      "Epoch 40, Training Loss 0.4233552047511196\n",
      "Epoch 40, Training Loss 0.42411876807127463\n",
      "Epoch 40, Training Loss 0.42502463648996086\n",
      "Epoch 40, Training Loss 0.4255139692055295\n",
      "Epoch 40, Training Loss 0.4263149355835927\n",
      "Epoch 40, Training Loss 0.4272064686278858\n",
      "Epoch 40, Training Loss 0.42796955815971355\n",
      "Epoch 40, Training Loss 0.4285258977385738\n",
      "Epoch 40, Training Loss 0.42910887426732447\n",
      "Epoch 40, Training Loss 0.42983357528286514\n",
      "Epoch 40, Training Loss 0.43042049772294283\n",
      "Epoch 40, Training Loss 0.4311066421553912\n",
      "Epoch 40, Training Loss 0.4318925324455856\n",
      "Epoch 40, Training Loss 0.4325416749700561\n",
      "Epoch 40, Training Loss 0.43318110483381755\n",
      "Epoch 40, Training Loss 0.4338311518702056\n",
      "Epoch 40, Training Loss 0.434445021059507\n",
      "Epoch 40, Training Loss 0.4351132410337858\n",
      "Epoch 40, Training Loss 0.43577748243613623\n",
      "Epoch 40, Training Loss 0.43646858480122996\n",
      "Epoch 40, Training Loss 0.4374242212690051\n",
      "Epoch 40, Training Loss 0.43810850355173925\n",
      "Epoch 40, Training Loss 0.43893665818454664\n",
      "Epoch 40, Training Loss 0.4396966321922629\n",
      "Epoch 40, Training Loss 0.44052299319783134\n",
      "Epoch 40, Training Loss 0.44152874901623984\n",
      "Epoch 40, Training Loss 0.4424646343187908\n",
      "Epoch 40, Training Loss 0.44340581757485714\n",
      "Epoch 40, Training Loss 0.44410339641906416\n",
      "Epoch 40, Training Loss 0.44475233650116053\n",
      "Epoch 40, Training Loss 0.4454262709754812\n",
      "Epoch 40, Training Loss 0.4460504459374396\n",
      "Epoch 40, Training Loss 0.44692726021684953\n",
      "Epoch 40, Training Loss 0.4475922958396585\n",
      "Epoch 40, Training Loss 0.4483375869062551\n",
      "Epoch 40, Training Loss 0.44909973686460947\n",
      "Epoch 40, Training Loss 0.4498911849449358\n",
      "Epoch 40, Training Loss 0.45084618771320106\n",
      "Epoch 40, Training Loss 0.45164782963597866\n",
      "Epoch 40, Training Loss 0.4524325868281562\n",
      "Epoch 40, Training Loss 0.45319796107766575\n",
      "Epoch 40, Training Loss 0.4539669913327907\n",
      "Epoch 40, Training Loss 0.45469352088468457\n",
      "Epoch 40, Training Loss 0.45537264576500947\n",
      "Epoch 40, Training Loss 0.45590888836499677\n",
      "Epoch 40, Training Loss 0.45645074470116354\n",
      "Epoch 40, Training Loss 0.4569709318906755\n",
      "Epoch 40, Training Loss 0.4578919510935883\n",
      "Epoch 40, Training Loss 0.45842798099950754\n",
      "Epoch 40, Training Loss 0.4590815325908344\n",
      "Epoch 40, Training Loss 0.459931560222755\n",
      "Epoch 40, Training Loss 0.46081066364065154\n",
      "Epoch 40, Training Loss 0.46121428670633174\n",
      "Epoch 40, Training Loss 0.4617073817554947\n",
      "Epoch 40, Training Loss 0.462458241740456\n",
      "Epoch 40, Training Loss 0.4633095844856004\n",
      "Epoch 40, Training Loss 0.4638019377160865\n",
      "Epoch 40, Training Loss 0.46439671070526933\n",
      "Epoch 40, Training Loss 0.4653512278709875\n",
      "Epoch 40, Training Loss 0.46612983008327386\n",
      "Epoch 40, Training Loss 0.46683003801061673\n",
      "Epoch 40, Training Loss 0.46741185320155393\n",
      "Epoch 40, Training Loss 0.4680608749923194\n",
      "Epoch 40, Training Loss 0.46879766180234794\n",
      "Epoch 40, Training Loss 0.4692472940515679\n",
      "Epoch 40, Training Loss 0.4700278582627816\n",
      "Epoch 40, Training Loss 0.470691494624633\n",
      "Epoch 40, Training Loss 0.47155872604731097\n",
      "Epoch 40, Training Loss 0.4725251203912603\n",
      "Epoch 40, Training Loss 0.4735740248656944\n",
      "Epoch 40, Training Loss 0.4742818413030766\n",
      "Epoch 40, Training Loss 0.47482089930788024\n",
      "Epoch 40, Training Loss 0.4753184134088209\n",
      "Epoch 40, Training Loss 0.4763457857434402\n",
      "Epoch 40, Training Loss 0.47701795883190906\n",
      "Epoch 40, Training Loss 0.47798724712618174\n",
      "Epoch 40, Training Loss 0.4785792412965194\n",
      "Epoch 40, Training Loss 0.4795386350673178\n",
      "Epoch 40, Training Loss 0.48015095872799757\n",
      "Epoch 40, Training Loss 0.48100232586378944\n",
      "Epoch 40, Training Loss 0.4815621082968724\n",
      "Epoch 40, Training Loss 0.48227058305307424\n",
      "Epoch 40, Training Loss 0.4829830177833357\n",
      "Epoch 40, Training Loss 0.48359539567509574\n",
      "Epoch 40, Training Loss 0.48439023230234374\n",
      "Epoch 40, Training Loss 0.4852397912527289\n",
      "Epoch 40, Training Loss 0.48596777517319945\n",
      "Epoch 40, Training Loss 0.4865404477967021\n",
      "Epoch 40, Training Loss 0.48731202123415135\n",
      "Epoch 40, Training Loss 0.4879644460537854\n",
      "Epoch 40, Training Loss 0.48869649116950264\n",
      "Epoch 40, Training Loss 0.48959919977980804\n",
      "Epoch 40, Training Loss 0.4904143655544047\n",
      "Epoch 40, Training Loss 0.4914722471590847\n",
      "Epoch 40, Training Loss 0.49234047227198513\n",
      "Epoch 40, Training Loss 0.4929620535172465\n",
      "Epoch 40, Training Loss 0.4939055070090477\n",
      "Epoch 40, Training Loss 0.4945461792714151\n",
      "Epoch 40, Training Loss 0.4952331280616848\n",
      "Epoch 40, Training Loss 0.4959384504791416\n",
      "Epoch 40, Training Loss 0.4967329438842471\n",
      "Epoch 40, Training Loss 0.49767073203840523\n",
      "Epoch 40, Training Loss 0.498237688980444\n",
      "Epoch 40, Training Loss 0.49888643287026974\n",
      "Epoch 40, Training Loss 0.4996892478307495\n",
      "Epoch 40, Training Loss 0.5003776794199444\n",
      "Epoch 40, Training Loss 0.5013494949663997\n",
      "Epoch 40, Training Loss 0.5021563774484503\n",
      "Epoch 40, Training Loss 0.5028080726066209\n",
      "Epoch 40, Training Loss 0.5031714284282816\n",
      "Epoch 40, Training Loss 0.5041182532411097\n",
      "Epoch 40, Training Loss 0.5046849609412196\n",
      "Epoch 40, Training Loss 0.505453452925243\n",
      "Epoch 40, Training Loss 0.5062072775934053\n",
      "Epoch 40, Training Loss 0.5068265966823339\n",
      "Epoch 40, Training Loss 0.5072745126302894\n",
      "Epoch 40, Training Loss 0.508011134651006\n",
      "Epoch 40, Training Loss 0.5085861570466205\n",
      "Epoch 40, Training Loss 0.5092203561454782\n",
      "Epoch 40, Training Loss 0.5102956807217025\n",
      "Epoch 40, Training Loss 0.5109214678673488\n",
      "Epoch 40, Training Loss 0.511419960300026\n",
      "Epoch 40, Training Loss 0.5121843465758712\n",
      "Epoch 40, Training Loss 0.5129266442240351\n",
      "Epoch 40, Training Loss 0.5135344683056902\n",
      "Epoch 40, Training Loss 0.5143963278407027\n",
      "Epoch 40, Training Loss 0.5150624185114565\n",
      "Epoch 40, Training Loss 0.5157797533227965\n",
      "Epoch 40, Training Loss 0.5166269446272984\n",
      "Epoch 40, Training Loss 0.5175009540584691\n",
      "Epoch 40, Training Loss 0.5180527506124638\n",
      "Epoch 40, Training Loss 0.5192526607867092\n",
      "Epoch 40, Training Loss 0.5202455849141416\n",
      "Epoch 40, Training Loss 0.5208266420132669\n",
      "Epoch 40, Training Loss 0.5217231020445714\n",
      "Epoch 40, Training Loss 0.5223009404928788\n",
      "Epoch 40, Training Loss 0.5229073728213225\n",
      "Epoch 40, Training Loss 0.5237398692180434\n",
      "Epoch 40, Training Loss 0.524626494101856\n",
      "Epoch 40, Training Loss 0.5253378762232374\n",
      "Epoch 40, Training Loss 0.5258426861766049\n",
      "Epoch 40, Training Loss 0.5265005536167823\n",
      "Epoch 40, Training Loss 0.5270117434013225\n",
      "Epoch 40, Training Loss 0.5276140795110742\n",
      "Epoch 40, Training Loss 0.5284929701967922\n",
      "Epoch 40, Training Loss 0.5292297785772997\n",
      "Epoch 40, Training Loss 0.5297005328604633\n",
      "Epoch 40, Training Loss 0.5302645750057972\n",
      "Epoch 40, Training Loss 0.5308357158966381\n",
      "Epoch 40, Training Loss 0.5315826125538258\n",
      "Epoch 40, Training Loss 0.5320586237456183\n",
      "Epoch 40, Training Loss 0.5325853072316445\n",
      "Epoch 40, Training Loss 0.5334561531196165\n",
      "Epoch 40, Training Loss 0.534383655051746\n",
      "Epoch 40, Training Loss 0.5351498506563094\n",
      "Epoch 40, Training Loss 0.5356634049997915\n",
      "Epoch 40, Training Loss 0.5362526948952004\n",
      "Epoch 40, Training Loss 0.5370108267230451\n",
      "Epoch 40, Training Loss 0.5377265081533691\n",
      "Epoch 40, Training Loss 0.5385692929063002\n",
      "Epoch 40, Training Loss 0.5392346764769396\n",
      "Epoch 40, Training Loss 0.5398337864662375\n",
      "Epoch 40, Training Loss 0.5402491278279468\n",
      "Epoch 40, Training Loss 0.5407332876682891\n",
      "Epoch 40, Training Loss 0.5414731130956689\n",
      "Epoch 40, Training Loss 0.5423452203612193\n",
      "Epoch 40, Training Loss 0.5433578574672684\n",
      "Epoch 40, Training Loss 0.5438746272602959\n",
      "Epoch 40, Training Loss 0.5446341225253347\n",
      "Epoch 40, Training Loss 0.5453854182644573\n",
      "Epoch 40, Training Loss 0.5461804090101091\n",
      "Epoch 40, Training Loss 0.5470365227183418\n",
      "Epoch 40, Training Loss 0.5475982498863469\n",
      "Epoch 40, Training Loss 0.5480239593311954\n",
      "Epoch 40, Training Loss 0.5486588388147866\n",
      "Epoch 40, Training Loss 0.5496903621327237\n",
      "Epoch 40, Training Loss 0.5508687607467632\n",
      "Epoch 40, Training Loss 0.5516339119742898\n",
      "Epoch 40, Training Loss 0.5524832546101202\n",
      "Epoch 40, Training Loss 0.5538047328781899\n",
      "Epoch 50, Training Loss 0.0009351924557210235\n",
      "Epoch 50, Training Loss 0.0013066040890296097\n",
      "Epoch 50, Training Loss 0.0019668739317628124\n",
      "Epoch 50, Training Loss 0.0029358478153453152\n",
      "Epoch 50, Training Loss 0.0034910086399453986\n",
      "Epoch 50, Training Loss 0.004107606914037329\n",
      "Epoch 50, Training Loss 0.0046110734762743\n",
      "Epoch 50, Training Loss 0.005262916121641388\n",
      "Epoch 50, Training Loss 0.0059254346296305545\n",
      "Epoch 50, Training Loss 0.0064975955068607765\n",
      "Epoch 50, Training Loss 0.007306970179538288\n",
      "Epoch 50, Training Loss 0.0077263770048575636\n",
      "Epoch 50, Training Loss 0.008343715420769303\n",
      "Epoch 50, Training Loss 0.008786292598985345\n",
      "Epoch 50, Training Loss 0.0093424987914922\n",
      "Epoch 50, Training Loss 0.010369679507087259\n",
      "Epoch 50, Training Loss 0.010990134056876688\n",
      "Epoch 50, Training Loss 0.01182400547634915\n",
      "Epoch 50, Training Loss 0.01240597493813166\n",
      "Epoch 50, Training Loss 0.013393936331009926\n",
      "Epoch 50, Training Loss 0.013921186137382332\n",
      "Epoch 50, Training Loss 0.014516371290397156\n",
      "Epoch 50, Training Loss 0.014937240449363924\n",
      "Epoch 50, Training Loss 0.015406469554852342\n",
      "Epoch 50, Training Loss 0.015912568210945714\n",
      "Epoch 50, Training Loss 0.016312852463758815\n",
      "Epoch 50, Training Loss 0.01693310826788168\n",
      "Epoch 50, Training Loss 0.017480734326040654\n",
      "Epoch 50, Training Loss 0.017920795265975814\n",
      "Epoch 50, Training Loss 0.018430851609505656\n",
      "Epoch 50, Training Loss 0.01886560483966642\n",
      "Epoch 50, Training Loss 0.019303199999472675\n",
      "Epoch 50, Training Loss 0.01982244243249869\n",
      "Epoch 50, Training Loss 0.02061582713023476\n",
      "Epoch 50, Training Loss 0.021079899747963145\n",
      "Epoch 50, Training Loss 0.021636432012938477\n",
      "Epoch 50, Training Loss 0.02205672143670299\n",
      "Epoch 50, Training Loss 0.022710372877242924\n",
      "Epoch 50, Training Loss 0.023441261723828134\n",
      "Epoch 50, Training Loss 0.024198403520047512\n",
      "Epoch 50, Training Loss 0.024750879010580994\n",
      "Epoch 50, Training Loss 0.025224880641683593\n",
      "Epoch 50, Training Loss 0.02569463894800152\n",
      "Epoch 50, Training Loss 0.026175345834868642\n",
      "Epoch 50, Training Loss 0.026970660747469538\n",
      "Epoch 50, Training Loss 0.027642106003773488\n",
      "Epoch 50, Training Loss 0.02822104759533387\n",
      "Epoch 50, Training Loss 0.02895754049806034\n",
      "Epoch 50, Training Loss 0.0296562257630136\n",
      "Epoch 50, Training Loss 0.03023561747635112\n",
      "Epoch 50, Training Loss 0.030638812219395357\n",
      "Epoch 50, Training Loss 0.03132312049341324\n",
      "Epoch 50, Training Loss 0.03196366950679008\n",
      "Epoch 50, Training Loss 0.032782211861646995\n",
      "Epoch 50, Training Loss 0.03334781226447171\n",
      "Epoch 50, Training Loss 0.03379710449282166\n",
      "Epoch 50, Training Loss 0.03438880056371469\n",
      "Epoch 50, Training Loss 0.03495437032578851\n",
      "Epoch 50, Training Loss 0.03538997650451368\n",
      "Epoch 50, Training Loss 0.036162966276373706\n",
      "Epoch 50, Training Loss 0.03669285347394626\n",
      "Epoch 50, Training Loss 0.03732160236829382\n",
      "Epoch 50, Training Loss 0.03769904104492548\n",
      "Epoch 50, Training Loss 0.03824901493156657\n",
      "Epoch 50, Training Loss 0.03893547941504232\n",
      "Epoch 50, Training Loss 0.039677736437534125\n",
      "Epoch 50, Training Loss 0.040318437435133074\n",
      "Epoch 50, Training Loss 0.04076338435530358\n",
      "Epoch 50, Training Loss 0.04140730644278514\n",
      "Epoch 50, Training Loss 0.04189611037673853\n",
      "Epoch 50, Training Loss 0.042468187030014176\n",
      "Epoch 50, Training Loss 0.043010507855573886\n",
      "Epoch 50, Training Loss 0.04357318290511666\n",
      "Epoch 50, Training Loss 0.04418212952821151\n",
      "Epoch 50, Training Loss 0.04469897790485636\n",
      "Epoch 50, Training Loss 0.04544262580402062\n",
      "Epoch 50, Training Loss 0.046185204401955275\n",
      "Epoch 50, Training Loss 0.04677133570851572\n",
      "Epoch 50, Training Loss 0.047205449408277526\n",
      "Epoch 50, Training Loss 0.047943721761179094\n",
      "Epoch 50, Training Loss 0.048334088433733984\n",
      "Epoch 50, Training Loss 0.049035459642520034\n",
      "Epoch 50, Training Loss 0.04977906615380436\n",
      "Epoch 50, Training Loss 0.05012755015926898\n",
      "Epoch 50, Training Loss 0.050631362001609316\n",
      "Epoch 50, Training Loss 0.05113595491632476\n",
      "Epoch 50, Training Loss 0.05166810598519757\n",
      "Epoch 50, Training Loss 0.052331289138330524\n",
      "Epoch 50, Training Loss 0.05292448950240679\n",
      "Epoch 50, Training Loss 0.05373947021296567\n",
      "Epoch 50, Training Loss 0.05450832615118197\n",
      "Epoch 50, Training Loss 0.05529928603745483\n",
      "Epoch 50, Training Loss 0.05588982183762523\n",
      "Epoch 50, Training Loss 0.05648587605989803\n",
      "Epoch 50, Training Loss 0.057128882202346\n",
      "Epoch 50, Training Loss 0.05769557199057411\n",
      "Epoch 50, Training Loss 0.05825909728284382\n",
      "Epoch 50, Training Loss 0.05897225359516681\n",
      "Epoch 50, Training Loss 0.059383772942416195\n",
      "Epoch 50, Training Loss 0.0599988743548503\n",
      "Epoch 50, Training Loss 0.060415763379362844\n",
      "Epoch 50, Training Loss 0.060898174715164066\n",
      "Epoch 50, Training Loss 0.06140701758587147\n",
      "Epoch 50, Training Loss 0.062016569516238046\n",
      "Epoch 50, Training Loss 0.0626984109049258\n",
      "Epoch 50, Training Loss 0.06322249095610645\n",
      "Epoch 50, Training Loss 0.06395057952769882\n",
      "Epoch 50, Training Loss 0.06446658189186964\n",
      "Epoch 50, Training Loss 0.06496286849536555\n",
      "Epoch 50, Training Loss 0.06535793696065693\n",
      "Epoch 50, Training Loss 0.06602255939065343\n",
      "Epoch 50, Training Loss 0.06676624067451643\n",
      "Epoch 50, Training Loss 0.06737132435259612\n",
      "Epoch 50, Training Loss 0.06791754204141515\n",
      "Epoch 50, Training Loss 0.06867850226971804\n",
      "Epoch 50, Training Loss 0.06921035455315924\n",
      "Epoch 50, Training Loss 0.07009985013995938\n",
      "Epoch 50, Training Loss 0.07084038929866097\n",
      "Epoch 50, Training Loss 0.07163750325017573\n",
      "Epoch 50, Training Loss 0.07224364662566758\n",
      "Epoch 50, Training Loss 0.07278147111158542\n",
      "Epoch 50, Training Loss 0.07323300708895145\n",
      "Epoch 50, Training Loss 0.07376595363592553\n",
      "Epoch 50, Training Loss 0.07443676404940808\n",
      "Epoch 50, Training Loss 0.07513985014937417\n",
      "Epoch 50, Training Loss 0.07563536814259142\n",
      "Epoch 50, Training Loss 0.07636124532088599\n",
      "Epoch 50, Training Loss 0.07686460044835229\n",
      "Epoch 50, Training Loss 0.07740350559239498\n",
      "Epoch 50, Training Loss 0.0779828741346174\n",
      "Epoch 50, Training Loss 0.0785568545541495\n",
      "Epoch 50, Training Loss 0.0790680833637257\n",
      "Epoch 50, Training Loss 0.0796073539482663\n",
      "Epoch 50, Training Loss 0.08015604919332373\n",
      "Epoch 50, Training Loss 0.08065895770516847\n",
      "Epoch 50, Training Loss 0.08152637685961125\n",
      "Epoch 50, Training Loss 0.08215724198562105\n",
      "Epoch 50, Training Loss 0.08275093881370467\n",
      "Epoch 50, Training Loss 0.08360392487872287\n",
      "Epoch 50, Training Loss 0.08426845271873962\n",
      "Epoch 50, Training Loss 0.08482883176992616\n",
      "Epoch 50, Training Loss 0.08568325192879533\n",
      "Epoch 50, Training Loss 0.08605523114009282\n",
      "Epoch 50, Training Loss 0.08654169421976485\n",
      "Epoch 50, Training Loss 0.08720594720767282\n",
      "Epoch 50, Training Loss 0.08785984545107693\n",
      "Epoch 50, Training Loss 0.08843702295094805\n",
      "Epoch 50, Training Loss 0.08930467130125636\n",
      "Epoch 50, Training Loss 0.08987038957950709\n",
      "Epoch 50, Training Loss 0.09034488496877958\n",
      "Epoch 50, Training Loss 0.09106801194912942\n",
      "Epoch 50, Training Loss 0.09159138227057884\n",
      "Epoch 50, Training Loss 0.09220746148120412\n",
      "Epoch 50, Training Loss 0.09281345424444779\n",
      "Epoch 50, Training Loss 0.09312848497153547\n",
      "Epoch 50, Training Loss 0.09384815884596856\n",
      "Epoch 50, Training Loss 0.09444315089365406\n",
      "Epoch 50, Training Loss 0.09493112059124291\n",
      "Epoch 50, Training Loss 0.09567404965229352\n",
      "Epoch 50, Training Loss 0.09626578177561236\n",
      "Epoch 50, Training Loss 0.09668492845943212\n",
      "Epoch 50, Training Loss 0.09696339943524822\n",
      "Epoch 50, Training Loss 0.09766547408555169\n",
      "Epoch 50, Training Loss 0.09816446305845705\n",
      "Epoch 50, Training Loss 0.09886547869733532\n",
      "Epoch 50, Training Loss 0.09930123491665287\n",
      "Epoch 50, Training Loss 0.09984792010558535\n",
      "Epoch 50, Training Loss 0.10040313302708403\n",
      "Epoch 50, Training Loss 0.10132998243317275\n",
      "Epoch 50, Training Loss 0.10200904046787936\n",
      "Epoch 50, Training Loss 0.10284143145127064\n",
      "Epoch 50, Training Loss 0.1032837612930771\n",
      "Epoch 50, Training Loss 0.10374487909819465\n",
      "Epoch 50, Training Loss 0.10441248801053332\n",
      "Epoch 50, Training Loss 0.10530149822344866\n",
      "Epoch 50, Training Loss 0.10595766052870495\n",
      "Epoch 50, Training Loss 0.10665393858919363\n",
      "Epoch 50, Training Loss 0.10716409135200179\n",
      "Epoch 50, Training Loss 0.10775520723036792\n",
      "Epoch 50, Training Loss 0.10844265008369065\n",
      "Epoch 50, Training Loss 0.10897580517070068\n",
      "Epoch 50, Training Loss 0.10970152678239681\n",
      "Epoch 50, Training Loss 0.11036959827860908\n",
      "Epoch 50, Training Loss 0.11095388950136921\n",
      "Epoch 50, Training Loss 0.11154060599291721\n",
      "Epoch 50, Training Loss 0.11188672482967377\n",
      "Epoch 50, Training Loss 0.11260072467729564\n",
      "Epoch 50, Training Loss 0.11318778968832986\n",
      "Epoch 50, Training Loss 0.1140680689640972\n",
      "Epoch 50, Training Loss 0.11471051892356189\n",
      "Epoch 50, Training Loss 0.1154560005420919\n",
      "Epoch 50, Training Loss 0.1162076900377298\n",
      "Epoch 50, Training Loss 0.11662739351429903\n",
      "Epoch 50, Training Loss 0.11742643280254911\n",
      "Epoch 50, Training Loss 0.1178501566581409\n",
      "Epoch 50, Training Loss 0.11853601823529929\n",
      "Epoch 50, Training Loss 0.11899766116343496\n",
      "Epoch 50, Training Loss 0.1194005499181845\n",
      "Epoch 50, Training Loss 0.11993216389737775\n",
      "Epoch 50, Training Loss 0.12036521946225325\n",
      "Epoch 50, Training Loss 0.12092310746612452\n",
      "Epoch 50, Training Loss 0.12158465271105852\n",
      "Epoch 50, Training Loss 0.12226934978724135\n",
      "Epoch 50, Training Loss 0.12283826964285673\n",
      "Epoch 50, Training Loss 0.12332894388214706\n",
      "Epoch 50, Training Loss 0.12391366952520502\n",
      "Epoch 50, Training Loss 0.12475663423538208\n",
      "Epoch 50, Training Loss 0.12535364590489956\n",
      "Epoch 50, Training Loss 0.1260668670048799\n",
      "Epoch 50, Training Loss 0.12655557173749674\n",
      "Epoch 50, Training Loss 0.12711761533604254\n",
      "Epoch 50, Training Loss 0.12777124863603842\n",
      "Epoch 50, Training Loss 0.12835967212991642\n",
      "Epoch 50, Training Loss 0.1291046085412545\n",
      "Epoch 50, Training Loss 0.12968482644966497\n",
      "Epoch 50, Training Loss 0.13023230616393908\n",
      "Epoch 50, Training Loss 0.1309235025092464\n",
      "Epoch 50, Training Loss 0.13174575933105195\n",
      "Epoch 50, Training Loss 0.13261170033603678\n",
      "Epoch 50, Training Loss 0.13335445576616564\n",
      "Epoch 50, Training Loss 0.1337506995557824\n",
      "Epoch 50, Training Loss 0.1342919932302002\n",
      "Epoch 50, Training Loss 0.1346403312347734\n",
      "Epoch 50, Training Loss 0.1353862870989553\n",
      "Epoch 50, Training Loss 0.13616809081238554\n",
      "Epoch 50, Training Loss 0.13693650184994768\n",
      "Epoch 50, Training Loss 0.13743129734645415\n",
      "Epoch 50, Training Loss 0.13782310024704164\n",
      "Epoch 50, Training Loss 0.1382424387404376\n",
      "Epoch 50, Training Loss 0.13864071103160644\n",
      "Epoch 50, Training Loss 0.13911803028620112\n",
      "Epoch 50, Training Loss 0.13975341183602658\n",
      "Epoch 50, Training Loss 0.1405525322231795\n",
      "Epoch 50, Training Loss 0.1411830728392467\n",
      "Epoch 50, Training Loss 0.14195439269018295\n",
      "Epoch 50, Training Loss 0.1425404499101517\n",
      "Epoch 50, Training Loss 0.1432381776135291\n",
      "Epoch 50, Training Loss 0.14388892923474617\n",
      "Epoch 50, Training Loss 0.14473187664280768\n",
      "Epoch 50, Training Loss 0.14506506653088133\n",
      "Epoch 50, Training Loss 0.14573040383551128\n",
      "Epoch 50, Training Loss 0.1461267005985655\n",
      "Epoch 50, Training Loss 0.14688415299443638\n",
      "Epoch 50, Training Loss 0.14748380216948517\n",
      "Epoch 50, Training Loss 0.14809240324570394\n",
      "Epoch 50, Training Loss 0.14873934238005782\n",
      "Epoch 50, Training Loss 0.1492721266530054\n",
      "Epoch 50, Training Loss 0.14986551413908028\n",
      "Epoch 50, Training Loss 0.15041448545577885\n",
      "Epoch 50, Training Loss 0.15074461115443188\n",
      "Epoch 50, Training Loss 0.15132620233251615\n",
      "Epoch 50, Training Loss 0.15186141008306342\n",
      "Epoch 50, Training Loss 0.15239920579563931\n",
      "Epoch 50, Training Loss 0.1529963305386741\n",
      "Epoch 50, Training Loss 0.1534779462439325\n",
      "Epoch 50, Training Loss 0.15416014289764493\n",
      "Epoch 50, Training Loss 0.15502960415904785\n",
      "Epoch 50, Training Loss 0.1556583012994903\n",
      "Epoch 50, Training Loss 0.15628298130029303\n",
      "Epoch 50, Training Loss 0.15676117427361286\n",
      "Epoch 50, Training Loss 0.15755525997380163\n",
      "Epoch 50, Training Loss 0.1582096923723855\n",
      "Epoch 50, Training Loss 0.15886144202840907\n",
      "Epoch 50, Training Loss 0.1594315924303001\n",
      "Epoch 50, Training Loss 0.16025091406634395\n",
      "Epoch 50, Training Loss 0.16099319174466536\n",
      "Epoch 50, Training Loss 0.16147013747936015\n",
      "Epoch 50, Training Loss 0.1619424648830653\n",
      "Epoch 50, Training Loss 0.16243963804848663\n",
      "Epoch 50, Training Loss 0.16304209786455345\n",
      "Epoch 50, Training Loss 0.1636511166306103\n",
      "Epoch 50, Training Loss 0.16433251532904633\n",
      "Epoch 50, Training Loss 0.16481731694830043\n",
      "Epoch 50, Training Loss 0.16538792829531843\n",
      "Epoch 50, Training Loss 0.16588263202201375\n",
      "Epoch 50, Training Loss 0.16646503228360734\n",
      "Epoch 50, Training Loss 0.16706398106596965\n",
      "Epoch 50, Training Loss 0.1674815151849976\n",
      "Epoch 50, Training Loss 0.1680843622788139\n",
      "Epoch 50, Training Loss 0.16881920911771867\n",
      "Epoch 50, Training Loss 0.16933200197756443\n",
      "Epoch 50, Training Loss 0.16982878592160658\n",
      "Epoch 50, Training Loss 0.17059448167033817\n",
      "Epoch 50, Training Loss 0.17104459445342382\n",
      "Epoch 50, Training Loss 0.17161918479158444\n",
      "Epoch 50, Training Loss 0.17219470342254395\n",
      "Epoch 50, Training Loss 0.17281571465075168\n",
      "Epoch 50, Training Loss 0.17356201068824514\n",
      "Epoch 50, Training Loss 0.17410294341919064\n",
      "Epoch 50, Training Loss 0.17461565544690621\n",
      "Epoch 50, Training Loss 0.17537887241986708\n",
      "Epoch 50, Training Loss 0.17615105638571102\n",
      "Epoch 50, Training Loss 0.17686945752567038\n",
      "Epoch 50, Training Loss 0.17730971633473322\n",
      "Epoch 50, Training Loss 0.17773794293251183\n",
      "Epoch 50, Training Loss 0.17803954060577676\n",
      "Epoch 50, Training Loss 0.17837673299910162\n",
      "Epoch 50, Training Loss 0.17899201898013845\n",
      "Epoch 50, Training Loss 0.17954584140606852\n",
      "Epoch 50, Training Loss 0.17991098273745584\n",
      "Epoch 50, Training Loss 0.18038851617242369\n",
      "Epoch 50, Training Loss 0.18113792079793828\n",
      "Epoch 50, Training Loss 0.18164420699524453\n",
      "Epoch 50, Training Loss 0.18239521141857137\n",
      "Epoch 50, Training Loss 0.18283751088639963\n",
      "Epoch 50, Training Loss 0.18315395482284638\n",
      "Epoch 50, Training Loss 0.1836612609303211\n",
      "Epoch 50, Training Loss 0.18439612636709457\n",
      "Epoch 50, Training Loss 0.18480752650505441\n",
      "Epoch 50, Training Loss 0.18549712831178283\n",
      "Epoch 50, Training Loss 0.1861092668703145\n",
      "Epoch 50, Training Loss 0.18697387538373927\n",
      "Epoch 50, Training Loss 0.18726779863505108\n",
      "Epoch 50, Training Loss 0.18789179992797733\n",
      "Epoch 50, Training Loss 0.1885282109917887\n",
      "Epoch 50, Training Loss 0.1891676851398195\n",
      "Epoch 50, Training Loss 0.18956447707112792\n",
      "Epoch 50, Training Loss 0.18999034227312678\n",
      "Epoch 50, Training Loss 0.19043903879802246\n",
      "Epoch 50, Training Loss 0.19078053766504274\n",
      "Epoch 50, Training Loss 0.19133601011827472\n",
      "Epoch 50, Training Loss 0.1918913109604355\n",
      "Epoch 50, Training Loss 0.1925712886749936\n",
      "Epoch 50, Training Loss 0.19311831110273786\n",
      "Epoch 50, Training Loss 0.19357861643252167\n",
      "Epoch 50, Training Loss 0.19401818437649465\n",
      "Epoch 50, Training Loss 0.19453093329506457\n",
      "Epoch 50, Training Loss 0.19527661468824156\n",
      "Epoch 50, Training Loss 0.19582897085515436\n",
      "Epoch 50, Training Loss 0.19618169521279347\n",
      "Epoch 50, Training Loss 0.1965701461143201\n",
      "Epoch 50, Training Loss 0.19720930265038825\n",
      "Epoch 50, Training Loss 0.1978307417057969\n",
      "Epoch 50, Training Loss 0.19845919551142036\n",
      "Epoch 50, Training Loss 0.19907322990924806\n",
      "Epoch 50, Training Loss 0.1997242906819219\n",
      "Epoch 50, Training Loss 0.20012465492843667\n",
      "Epoch 50, Training Loss 0.2007757560981204\n",
      "Epoch 50, Training Loss 0.20134068038457495\n",
      "Epoch 50, Training Loss 0.20205183498694768\n",
      "Epoch 50, Training Loss 0.20255770707679221\n",
      "Epoch 50, Training Loss 0.20294354901746717\n",
      "Epoch 50, Training Loss 0.2034600922823562\n",
      "Epoch 50, Training Loss 0.20388861515028092\n",
      "Epoch 50, Training Loss 0.2045360286065075\n",
      "Epoch 50, Training Loss 0.20515362659226294\n",
      "Epoch 50, Training Loss 0.2057539864498026\n",
      "Epoch 50, Training Loss 0.20620779723614988\n",
      "Epoch 50, Training Loss 0.2068602399676657\n",
      "Epoch 50, Training Loss 0.2074523935537509\n",
      "Epoch 50, Training Loss 0.20800244773897675\n",
      "Epoch 50, Training Loss 0.20870527308767714\n",
      "Epoch 50, Training Loss 0.2093816720082632\n",
      "Epoch 50, Training Loss 0.21038260091753566\n",
      "Epoch 50, Training Loss 0.2109235303709879\n",
      "Epoch 50, Training Loss 0.21155306346276226\n",
      "Epoch 50, Training Loss 0.2122217896954178\n",
      "Epoch 50, Training Loss 0.21309339946798048\n",
      "Epoch 50, Training Loss 0.21370770178182655\n",
      "Epoch 50, Training Loss 0.21441310628905624\n",
      "Epoch 50, Training Loss 0.21485664179105587\n",
      "Epoch 50, Training Loss 0.21567899297417886\n",
      "Epoch 50, Training Loss 0.21619348765334204\n",
      "Epoch 50, Training Loss 0.21695865732629585\n",
      "Epoch 50, Training Loss 0.21758613295262427\n",
      "Epoch 50, Training Loss 0.21811073549720636\n",
      "Epoch 50, Training Loss 0.21866957046796598\n",
      "Epoch 50, Training Loss 0.2192824811429319\n",
      "Epoch 50, Training Loss 0.2198367834548511\n",
      "Epoch 50, Training Loss 0.2204397554364046\n",
      "Epoch 50, Training Loss 0.2212387219338161\n",
      "Epoch 50, Training Loss 0.22178612577031032\n",
      "Epoch 50, Training Loss 0.222314843017122\n",
      "Epoch 50, Training Loss 0.22299707671413033\n",
      "Epoch 50, Training Loss 0.22343566979441193\n",
      "Epoch 50, Training Loss 0.2243319765457412\n",
      "Epoch 50, Training Loss 0.22495903139529022\n",
      "Epoch 50, Training Loss 0.22572893056723162\n",
      "Epoch 50, Training Loss 0.22615012846639393\n",
      "Epoch 50, Training Loss 0.22652612447433765\n",
      "Epoch 50, Training Loss 0.22719402287317358\n",
      "Epoch 50, Training Loss 0.2279526617978235\n",
      "Epoch 50, Training Loss 0.22860216339835732\n",
      "Epoch 50, Training Loss 0.2292217685819587\n",
      "Epoch 50, Training Loss 0.2297565958384053\n",
      "Epoch 50, Training Loss 0.23035303996804427\n",
      "Epoch 50, Training Loss 0.23085178385305283\n",
      "Epoch 50, Training Loss 0.23153466725593333\n",
      "Epoch 50, Training Loss 0.23219014837613802\n",
      "Epoch 50, Training Loss 0.23279118999038512\n",
      "Epoch 50, Training Loss 0.23328282251534865\n",
      "Epoch 50, Training Loss 0.23397752631198415\n",
      "Epoch 50, Training Loss 0.23476589133824838\n",
      "Epoch 50, Training Loss 0.23551043902364227\n",
      "Epoch 50, Training Loss 0.23612228051170975\n",
      "Epoch 50, Training Loss 0.23682943154174044\n",
      "Epoch 50, Training Loss 0.2373432934360431\n",
      "Epoch 50, Training Loss 0.23776587588555367\n",
      "Epoch 50, Training Loss 0.23820719206729507\n",
      "Epoch 50, Training Loss 0.23877539186526442\n",
      "Epoch 50, Training Loss 0.23914251097327913\n",
      "Epoch 50, Training Loss 0.2396336573240397\n",
      "Epoch 50, Training Loss 0.24036271085062294\n",
      "Epoch 50, Training Loss 0.24101849944542741\n",
      "Epoch 50, Training Loss 0.24186532732928195\n",
      "Epoch 50, Training Loss 0.24242700708796605\n",
      "Epoch 50, Training Loss 0.24297103567806352\n",
      "Epoch 50, Training Loss 0.2434829726167347\n",
      "Epoch 50, Training Loss 0.2439753755050547\n",
      "Epoch 50, Training Loss 0.2446908246906822\n",
      "Epoch 50, Training Loss 0.24513854924827586\n",
      "Epoch 50, Training Loss 0.24577715348862017\n",
      "Epoch 50, Training Loss 0.2463182022656931\n",
      "Epoch 50, Training Loss 0.24690111556931224\n",
      "Epoch 50, Training Loss 0.24770019647410457\n",
      "Epoch 50, Training Loss 0.24814450706514862\n",
      "Epoch 50, Training Loss 0.24865759287953682\n",
      "Epoch 50, Training Loss 0.24908826422051092\n",
      "Epoch 50, Training Loss 0.24983614168661025\n",
      "Epoch 50, Training Loss 0.2503797582653173\n",
      "Epoch 50, Training Loss 0.25109384637659465\n",
      "Epoch 50, Training Loss 0.25150480050869917\n",
      "Epoch 50, Training Loss 0.2519618009057496\n",
      "Epoch 50, Training Loss 0.2523839689810258\n",
      "Epoch 50, Training Loss 0.25288845427200923\n",
      "Epoch 50, Training Loss 0.25400777020112936\n",
      "Epoch 50, Training Loss 0.2545874915693117\n",
      "Epoch 50, Training Loss 0.2550635526476004\n",
      "Epoch 50, Training Loss 0.2556503040101522\n",
      "Epoch 50, Training Loss 0.25619756359883283\n",
      "Epoch 50, Training Loss 0.2567010396505561\n",
      "Epoch 50, Training Loss 0.25738850384569534\n",
      "Epoch 50, Training Loss 0.2579560470017021\n",
      "Epoch 50, Training Loss 0.25867821443873595\n",
      "Epoch 50, Training Loss 0.2593167224503539\n",
      "Epoch 50, Training Loss 0.25972596992312186\n",
      "Epoch 50, Training Loss 0.26035075644245537\n",
      "Epoch 50, Training Loss 0.2608930808885018\n",
      "Epoch 50, Training Loss 0.26143721996061026\n",
      "Epoch 50, Training Loss 0.26201301058539955\n",
      "Epoch 50, Training Loss 0.2624091129473713\n",
      "Epoch 50, Training Loss 0.26298598095279213\n",
      "Epoch 50, Training Loss 0.26409931416096893\n",
      "Epoch 50, Training Loss 0.26443352883734056\n",
      "Epoch 50, Training Loss 0.2652387114437035\n",
      "Epoch 50, Training Loss 0.26581569038846\n",
      "Epoch 50, Training Loss 0.26644595626674955\n",
      "Epoch 50, Training Loss 0.2673416537854373\n",
      "Epoch 50, Training Loss 0.2680538373682505\n",
      "Epoch 50, Training Loss 0.2685747578016023\n",
      "Epoch 50, Training Loss 0.26914439093121484\n",
      "Epoch 50, Training Loss 0.2699599249283676\n",
      "Epoch 50, Training Loss 0.2706164433370771\n",
      "Epoch 50, Training Loss 0.27130016745508784\n",
      "Epoch 50, Training Loss 0.2717985837432125\n",
      "Epoch 50, Training Loss 0.2724765125671616\n",
      "Epoch 50, Training Loss 0.273040079323532\n",
      "Epoch 50, Training Loss 0.2736738159528474\n",
      "Epoch 50, Training Loss 0.27409279113992707\n",
      "Epoch 50, Training Loss 0.27456278977034343\n",
      "Epoch 50, Training Loss 0.27520657389822517\n",
      "Epoch 50, Training Loss 0.275801545480633\n",
      "Epoch 50, Training Loss 0.27632402523856636\n",
      "Epoch 50, Training Loss 0.27693411189576855\n",
      "Epoch 50, Training Loss 0.27749686815854535\n",
      "Epoch 50, Training Loss 0.27799520273800093\n",
      "Epoch 50, Training Loss 0.278782178793112\n",
      "Epoch 50, Training Loss 0.27915399180501316\n",
      "Epoch 50, Training Loss 0.2795412871996155\n",
      "Epoch 50, Training Loss 0.28017271087145257\n",
      "Epoch 50, Training Loss 0.2808988946859184\n",
      "Epoch 50, Training Loss 0.28157345295104835\n",
      "Epoch 50, Training Loss 0.28217364931502914\n",
      "Epoch 50, Training Loss 0.2826816551673138\n",
      "Epoch 50, Training Loss 0.283322825913539\n",
      "Epoch 50, Training Loss 0.2838885654573855\n",
      "Epoch 50, Training Loss 0.2846567674213663\n",
      "Epoch 50, Training Loss 0.2851638659034544\n",
      "Epoch 50, Training Loss 0.28578012514754636\n",
      "Epoch 50, Training Loss 0.2863806855419408\n",
      "Epoch 50, Training Loss 0.2869100713592661\n",
      "Epoch 50, Training Loss 0.2876187940616437\n",
      "Epoch 50, Training Loss 0.2883427298968405\n",
      "Epoch 50, Training Loss 0.2891018160849886\n",
      "Epoch 50, Training Loss 0.28978347454382025\n",
      "Epoch 50, Training Loss 0.2906614526382188\n",
      "Epoch 50, Training Loss 0.2911965353866977\n",
      "Epoch 50, Training Loss 0.2918266595324592\n",
      "Epoch 50, Training Loss 0.2926033982230574\n",
      "Epoch 50, Training Loss 0.293160526191487\n",
      "Epoch 50, Training Loss 0.29386282622661736\n",
      "Epoch 50, Training Loss 0.29469588383689255\n",
      "Epoch 50, Training Loss 0.29511209442030134\n",
      "Epoch 50, Training Loss 0.2955988286172642\n",
      "Epoch 50, Training Loss 0.2964255451165197\n",
      "Epoch 50, Training Loss 0.2969721750453915\n",
      "Epoch 50, Training Loss 0.2976111755575365\n",
      "Epoch 50, Training Loss 0.2983762809763784\n",
      "Epoch 50, Training Loss 0.2992258799426696\n",
      "Epoch 50, Training Loss 0.300024626848033\n",
      "Epoch 50, Training Loss 0.30114124181782803\n",
      "Epoch 50, Training Loss 0.30173178695504316\n",
      "Epoch 50, Training Loss 0.30228499058262465\n",
      "Epoch 50, Training Loss 0.30287773042078825\n",
      "Epoch 50, Training Loss 0.303490538578814\n",
      "Epoch 50, Training Loss 0.30397916366072264\n",
      "Epoch 50, Training Loss 0.30455458202325475\n",
      "Epoch 50, Training Loss 0.3052143670256485\n",
      "Epoch 50, Training Loss 0.3057406516102574\n",
      "Epoch 50, Training Loss 0.3063099044363212\n",
      "Epoch 50, Training Loss 0.3069785114596872\n",
      "Epoch 50, Training Loss 0.3076219653991787\n",
      "Epoch 50, Training Loss 0.308042091786709\n",
      "Epoch 50, Training Loss 0.3084973252337912\n",
      "Epoch 50, Training Loss 0.3091385446088698\n",
      "Epoch 50, Training Loss 0.3098333179950714\n",
      "Epoch 50, Training Loss 0.31029556489661525\n",
      "Epoch 50, Training Loss 0.3106295285017594\n",
      "Epoch 50, Training Loss 0.3116023893399007\n",
      "Epoch 50, Training Loss 0.31252343140904554\n",
      "Epoch 50, Training Loss 0.31300197229208543\n",
      "Epoch 50, Training Loss 0.31361836564662815\n",
      "Epoch 50, Training Loss 0.3141640334787881\n",
      "Epoch 50, Training Loss 0.31497507723396084\n",
      "Epoch 50, Training Loss 0.3155876572251015\n",
      "Epoch 50, Training Loss 0.31621147200579536\n",
      "Epoch 50, Training Loss 0.31656063433803255\n",
      "Epoch 50, Training Loss 0.3170779516629856\n",
      "Epoch 50, Training Loss 0.317631265277143\n",
      "Epoch 50, Training Loss 0.3182347081887448\n",
      "Epoch 50, Training Loss 0.31884829242668494\n",
      "Epoch 50, Training Loss 0.3195263098953935\n",
      "Epoch 50, Training Loss 0.3202505936784208\n",
      "Epoch 50, Training Loss 0.32078157559685083\n",
      "Epoch 50, Training Loss 0.3214571009511533\n",
      "Epoch 50, Training Loss 0.321976613396269\n",
      "Epoch 50, Training Loss 0.32259895776391334\n",
      "Epoch 50, Training Loss 0.3233139954335854\n",
      "Epoch 50, Training Loss 0.3240184563276408\n",
      "Epoch 50, Training Loss 0.32459720443276796\n",
      "Epoch 50, Training Loss 0.32513606373001547\n",
      "Epoch 50, Training Loss 0.3257466584748929\n",
      "Epoch 50, Training Loss 0.3261493349547886\n",
      "Epoch 50, Training Loss 0.32666224134547633\n",
      "Epoch 50, Training Loss 0.32735843838328293\n",
      "Epoch 50, Training Loss 0.32785255498136096\n",
      "Epoch 50, Training Loss 0.32835035914045463\n",
      "Epoch 50, Training Loss 0.32890061523450914\n",
      "Epoch 50, Training Loss 0.32949945726967833\n",
      "Epoch 50, Training Loss 0.3302108515863833\n",
      "Epoch 50, Training Loss 0.33075236774923855\n",
      "Epoch 50, Training Loss 0.33121130861284787\n",
      "Epoch 50, Training Loss 0.33186335248105664\n",
      "Epoch 50, Training Loss 0.3325677517125064\n",
      "Epoch 50, Training Loss 0.33308731232915084\n",
      "Epoch 50, Training Loss 0.333560114778826\n",
      "Epoch 50, Training Loss 0.3343093974511032\n",
      "Epoch 50, Training Loss 0.33506516323370095\n",
      "Epoch 50, Training Loss 0.3357071097549575\n",
      "Epoch 50, Training Loss 0.3362133720189409\n",
      "Epoch 50, Training Loss 0.3369509192836254\n",
      "Epoch 50, Training Loss 0.33768764214442515\n",
      "Epoch 50, Training Loss 0.33858735863205114\n",
      "Epoch 50, Training Loss 0.3392218064774028\n",
      "Epoch 50, Training Loss 0.3398772323375468\n",
      "Epoch 50, Training Loss 0.340411484775031\n",
      "Epoch 50, Training Loss 0.3409309359005345\n",
      "Epoch 50, Training Loss 0.34156129221477166\n",
      "Epoch 50, Training Loss 0.34219584105264805\n",
      "Epoch 50, Training Loss 0.34307919354999766\n",
      "Epoch 50, Training Loss 0.34361253221474036\n",
      "Epoch 50, Training Loss 0.34407784902226285\n",
      "Epoch 50, Training Loss 0.3448682061546599\n",
      "Epoch 50, Training Loss 0.3455911374000637\n",
      "Epoch 50, Training Loss 0.34633735043313496\n",
      "Epoch 50, Training Loss 0.34697493156203835\n",
      "Epoch 50, Training Loss 0.3473414663616044\n",
      "Epoch 50, Training Loss 0.34783268584619703\n",
      "Epoch 50, Training Loss 0.34841857736220444\n",
      "Epoch 50, Training Loss 0.34915746329233166\n",
      "Epoch 50, Training Loss 0.34973595601975765\n",
      "Epoch 50, Training Loss 0.3503417742755407\n",
      "Epoch 50, Training Loss 0.3508137518258961\n",
      "Epoch 50, Training Loss 0.3514093579843526\n",
      "Epoch 50, Training Loss 0.3521627064251229\n",
      "Epoch 50, Training Loss 0.35276057656921084\n",
      "Epoch 50, Training Loss 0.35353294735217033\n",
      "Epoch 50, Training Loss 0.35417333050914435\n",
      "Epoch 50, Training Loss 0.3549330208231421\n",
      "Epoch 50, Training Loss 0.3556809815985467\n",
      "Epoch 50, Training Loss 0.356482319247997\n",
      "Epoch 50, Training Loss 0.3570229423701611\n",
      "Epoch 50, Training Loss 0.35745008286002955\n",
      "Epoch 50, Training Loss 0.35830699513330483\n",
      "Epoch 50, Training Loss 0.35884692220736647\n",
      "Epoch 50, Training Loss 0.35928937282098833\n",
      "Epoch 50, Training Loss 0.36001334867209117\n",
      "Epoch 50, Training Loss 0.3606505201524481\n",
      "Epoch 50, Training Loss 0.3617321183080868\n",
      "Epoch 50, Training Loss 0.3621686868502966\n",
      "Epoch 50, Training Loss 0.3627990124475621\n",
      "Epoch 50, Training Loss 0.363565063232656\n",
      "Epoch 50, Training Loss 0.36421157979904234\n",
      "Epoch 50, Training Loss 0.36494361127124114\n",
      "Epoch 50, Training Loss 0.3655544809444481\n",
      "Epoch 50, Training Loss 0.3661313462821419\n",
      "Epoch 50, Training Loss 0.36679196689287413\n",
      "Epoch 50, Training Loss 0.36746785673491483\n",
      "Epoch 50, Training Loss 0.3679265168011951\n",
      "Epoch 50, Training Loss 0.36871680960325937\n",
      "Epoch 50, Training Loss 0.369073626673435\n",
      "Epoch 50, Training Loss 0.3696246531689563\n",
      "Epoch 50, Training Loss 0.37040472644216876\n",
      "Epoch 50, Training Loss 0.3708544511471868\n",
      "Epoch 50, Training Loss 0.3712848250747032\n",
      "Epoch 50, Training Loss 0.3719598193226568\n",
      "Epoch 50, Training Loss 0.3725746842982519\n",
      "Epoch 50, Training Loss 0.3732975048710928\n",
      "Epoch 50, Training Loss 0.3740334878568454\n",
      "Epoch 50, Training Loss 0.37471970568989854\n",
      "Epoch 50, Training Loss 0.37559788104365854\n",
      "Epoch 50, Training Loss 0.37636291954066137\n",
      "Epoch 50, Training Loss 0.3772684179074929\n",
      "Epoch 50, Training Loss 0.3780317758126637\n",
      "Epoch 50, Training Loss 0.3787278781842698\n",
      "Epoch 50, Training Loss 0.37926147222671364\n",
      "Epoch 50, Training Loss 0.3799090307310719\n",
      "Epoch 50, Training Loss 0.38080447009000024\n",
      "Epoch 50, Training Loss 0.3813071343142663\n",
      "Epoch 50, Training Loss 0.38199022945845523\n",
      "Epoch 50, Training Loss 0.38265728112071984\n",
      "Epoch 50, Training Loss 0.3833939822585991\n",
      "Epoch 50, Training Loss 0.3840191695086487\n",
      "Epoch 50, Training Loss 0.38484775654190334\n",
      "Epoch 50, Training Loss 0.385459027898586\n",
      "Epoch 50, Training Loss 0.3861217669894933\n",
      "Epoch 50, Training Loss 0.3866817585723784\n",
      "Epoch 50, Training Loss 0.38714447213560726\n",
      "Epoch 50, Training Loss 0.3879188620068533\n",
      "Epoch 50, Training Loss 0.3886439003755369\n",
      "Epoch 50, Training Loss 0.3891462719501437\n",
      "Epoch 50, Training Loss 0.38985527933710984\n",
      "Epoch 50, Training Loss 0.39054029074776203\n",
      "Epoch 50, Training Loss 0.39101037657474313\n",
      "Epoch 50, Training Loss 0.39170045407531817\n",
      "Epoch 50, Training Loss 0.3923786987581521\n",
      "Epoch 50, Training Loss 0.39321722513269586\n",
      "Epoch 50, Training Loss 0.3938362628145291\n",
      "Epoch 50, Training Loss 0.3942810926595917\n",
      "Epoch 50, Training Loss 0.3948817136113906\n",
      "Epoch 50, Training Loss 0.39552341950366565\n",
      "Epoch 50, Training Loss 0.39601692881273187\n",
      "Epoch 50, Training Loss 0.3968261045491909\n",
      "Epoch 50, Training Loss 0.3975877464960908\n",
      "Epoch 50, Training Loss 0.39806543084818996\n",
      "Epoch 50, Training Loss 0.3986169206897926\n",
      "Epoch 50, Training Loss 0.3992742251633378\n",
      "Epoch 50, Training Loss 0.400082463841609\n",
      "Epoch 50, Training Loss 0.40078137593958385\n",
      "Epoch 50, Training Loss 0.40129865500170864\n",
      "Epoch 50, Training Loss 0.40203375931438584\n",
      "Epoch 50, Training Loss 0.4027940789833093\n",
      "Epoch 50, Training Loss 0.4031538853178854\n",
      "Epoch 50, Training Loss 0.4038958560551524\n",
      "Epoch 50, Training Loss 0.4043302757050985\n",
      "Epoch 50, Training Loss 0.4049877035800758\n",
      "Epoch 50, Training Loss 0.4054947459636747\n",
      "Epoch 50, Training Loss 0.4059692234791758\n",
      "Epoch 50, Training Loss 0.40656150370607597\n",
      "Epoch 50, Training Loss 0.40715139982340587\n",
      "Epoch 50, Training Loss 0.40760690896102536\n",
      "Epoch 50, Training Loss 0.40856770862398856\n",
      "Epoch 50, Training Loss 0.40902027121895107\n",
      "Epoch 50, Training Loss 0.40967156926689247\n",
      "Epoch 50, Training Loss 0.41014741666024296\n",
      "Epoch 50, Training Loss 0.41076202469561107\n",
      "Epoch 50, Training Loss 0.41107752458061403\n",
      "Epoch 50, Training Loss 0.4114903972276946\n",
      "Epoch 50, Training Loss 0.41233376042007486\n",
      "Epoch 50, Training Loss 0.41299675485057297\n",
      "Epoch 50, Training Loss 0.4137054098689038\n",
      "Epoch 50, Training Loss 0.41417849902301795\n",
      "Epoch 50, Training Loss 0.4148576453976009\n",
      "Epoch 50, Training Loss 0.41556664020813944\n",
      "Epoch 50, Training Loss 0.41616247525757843\n",
      "Epoch 50, Training Loss 0.41662569988109266\n",
      "Epoch 50, Training Loss 0.4173204965908509\n",
      "Epoch 50, Training Loss 0.4179392455483946\n",
      "Epoch 50, Training Loss 0.4187322393097841\n",
      "Epoch 50, Training Loss 0.4193623945536211\n",
      "Epoch 50, Training Loss 0.42060739350745746\n",
      "Epoch 50, Training Loss 0.42111384563738735\n",
      "Epoch 50, Training Loss 0.4215782378488185\n",
      "Epoch 50, Training Loss 0.42207080605999586\n",
      "Epoch 50, Training Loss 0.42246357585920397\n",
      "Epoch 50, Training Loss 0.42305297322590335\n",
      "Epoch 50, Training Loss 0.42368372973731105\n",
      "Epoch 50, Training Loss 0.4242942141145087\n",
      "Epoch 50, Training Loss 0.4249229203633335\n",
      "Epoch 50, Training Loss 0.4253830559113446\n",
      "Epoch 50, Training Loss 0.42579667273994604\n",
      "Epoch 50, Training Loss 0.42652768384465173\n",
      "Epoch 50, Training Loss 0.4272502437424477\n",
      "Epoch 50, Training Loss 0.4277437091864588\n",
      "Epoch 50, Training Loss 0.4284710653526399\n",
      "Epoch 50, Training Loss 0.4290284614657502\n",
      "Epoch 50, Training Loss 0.4296429489579652\n",
      "Epoch 50, Training Loss 0.4304898760050459\n",
      "Epoch 50, Training Loss 0.4312072395516174\n",
      "Epoch 50, Training Loss 0.43198371101218414\n",
      "Epoch 50, Training Loss 0.43259838478796925\n",
      "Epoch 50, Training Loss 0.43317856622473966\n",
      "Epoch 50, Training Loss 0.4339414580398813\n",
      "Epoch 50, Training Loss 0.4348125947101037\n",
      "Epoch 50, Training Loss 0.4353433899257494\n",
      "Epoch 50, Training Loss 0.4356867878333382\n",
      "Epoch 50, Training Loss 0.43632122710385285\n",
      "Epoch 50, Training Loss 0.4367760489206485\n",
      "Epoch 50, Training Loss 0.43761345515470673\n",
      "Epoch 50, Training Loss 0.43810770936939114\n",
      "Epoch 50, Training Loss 0.4388305736929559\n",
      "Epoch 50, Training Loss 0.4395668567598933\n",
      "Epoch 50, Training Loss 0.44078693243548694\n",
      "Epoch 50, Training Loss 0.441569797980511\n",
      "Epoch 50, Training Loss 0.4425827375305888\n",
      "Epoch 50, Training Loss 0.44290428275190047\n",
      "Epoch 50, Training Loss 0.4438509256257426\n",
      "Epoch 50, Training Loss 0.44456609096521\n",
      "Epoch 50, Training Loss 0.44521466373939955\n",
      "Epoch 50, Training Loss 0.44572261875242836\n",
      "Epoch 50, Training Loss 0.4465269405213768\n",
      "Epoch 50, Training Loss 0.44715710803675834\n",
      "Epoch 50, Training Loss 0.44803872757860463\n",
      "Epoch 50, Training Loss 0.44876735030537673\n",
      "Epoch 50, Training Loss 0.44930466285447024\n",
      "Epoch 50, Training Loss 0.44989881517789554\n",
      "Epoch 50, Training Loss 0.4505464123261859\n",
      "Epoch 50, Training Loss 0.45109473831970676\n",
      "Epoch 50, Training Loss 0.4519216834050615\n",
      "Epoch 50, Training Loss 0.45251605417722324\n",
      "Epoch 50, Training Loss 0.4532513541486257\n",
      "Epoch 50, Training Loss 0.4536377219745265\n",
      "Epoch 50, Training Loss 0.4540596360821858\n",
      "Epoch 50, Training Loss 0.4547264080904329\n",
      "Epoch 50, Training Loss 0.4552872857398084\n",
      "Epoch 50, Training Loss 0.45594858852646236\n",
      "Epoch 50, Training Loss 0.45651089977425385\n",
      "Epoch 50, Training Loss 0.4568470583852295\n",
      "Epoch 50, Training Loss 0.45740801465633274\n",
      "Epoch 50, Training Loss 0.4580309874642536\n",
      "Epoch 50, Training Loss 0.4587710533681733\n",
      "Epoch 50, Training Loss 0.4594447378383573\n",
      "Epoch 50, Training Loss 0.46000642689597576\n",
      "Epoch 50, Training Loss 0.46074908636415096\n",
      "Epoch 50, Training Loss 0.46164806084254817\n",
      "Epoch 50, Training Loss 0.46211404176166904\n",
      "Epoch 50, Training Loss 0.4628175137674107\n",
      "Epoch 50, Training Loss 0.463461085163114\n",
      "Epoch 50, Training Loss 0.46395199122788655\n",
      "Epoch 50, Training Loss 0.46443526054282325\n",
      "Epoch 50, Training Loss 0.4651331827616143\n",
      "Epoch 50, Training Loss 0.46572851052369607\n",
      "Epoch 50, Training Loss 0.46646398046742316\n",
      "Epoch 50, Training Loss 0.46714091125656576\n",
      "Epoch 50, Training Loss 0.4677264146945056\n",
      "Epoch 50, Training Loss 0.46819928940147393\n",
      "Epoch 50, Training Loss 0.4689057802833864\n",
      "Epoch 50, Training Loss 0.46950975502543435\n",
      "Epoch 50, Training Loss 0.47022271323996734\n",
      "Epoch 50, Training Loss 0.47066517208543274\n",
      "Epoch 50, Training Loss 0.4714913853751424\n",
      "Epoch 50, Training Loss 0.4722422220365471\n",
      "Epoch 50, Training Loss 0.4728962426142924\n",
      "Epoch 50, Training Loss 0.47338123082199973\n",
      "Epoch 50, Training Loss 0.47392406270784493\n",
      "Epoch 50, Training Loss 0.47484840498403513\n",
      "Epoch 50, Training Loss 0.47568400783459547\n",
      "Epoch 50, Training Loss 0.47628615346863445\n",
      "Epoch 50, Training Loss 0.477030523026081\n",
      "Epoch 50, Training Loss 0.4776139661021855\n",
      "Epoch 50, Training Loss 0.47813956298486654\n",
      "Epoch 50, Training Loss 0.47931153847433416\n",
      "Epoch 60, Training Loss 0.0005753188944228775\n",
      "Epoch 60, Training Loss 0.0011343855382231495\n",
      "Epoch 60, Training Loss 0.0014941670248270645\n",
      "Epoch 60, Training Loss 0.0019684042162297632\n",
      "Epoch 60, Training Loss 0.0026259728709755042\n",
      "Epoch 60, Training Loss 0.003273879551826536\n",
      "Epoch 60, Training Loss 0.0038336911774657268\n",
      "Epoch 60, Training Loss 0.004244657695445868\n",
      "Epoch 60, Training Loss 0.004601501747775261\n",
      "Epoch 60, Training Loss 0.005143916058113508\n",
      "Epoch 60, Training Loss 0.005698299811929083\n",
      "Epoch 60, Training Loss 0.006084415308959649\n",
      "Epoch 60, Training Loss 0.006441710473936232\n",
      "Epoch 60, Training Loss 0.007244419335099437\n",
      "Epoch 60, Training Loss 0.007829776352933606\n",
      "Epoch 60, Training Loss 0.008302945653191004\n",
      "Epoch 60, Training Loss 0.00869473186142914\n",
      "Epoch 60, Training Loss 0.009170528682296538\n",
      "Epoch 60, Training Loss 0.009612763400577828\n",
      "Epoch 60, Training Loss 0.010186963069164539\n",
      "Epoch 60, Training Loss 0.010920399442658095\n",
      "Epoch 60, Training Loss 0.011266844961649317\n",
      "Epoch 60, Training Loss 0.011667489624389297\n",
      "Epoch 60, Training Loss 0.012279566634646463\n",
      "Epoch 60, Training Loss 0.012715745803035434\n",
      "Epoch 60, Training Loss 0.01331391599019775\n",
      "Epoch 60, Training Loss 0.01376495767584847\n",
      "Epoch 60, Training Loss 0.014265844889004212\n",
      "Epoch 60, Training Loss 0.014761537542123624\n",
      "Epoch 60, Training Loss 0.01546726606386092\n",
      "Epoch 60, Training Loss 0.015850033029875792\n",
      "Epoch 60, Training Loss 0.016173014021895426\n",
      "Epoch 60, Training Loss 0.016477764357843667\n",
      "Epoch 60, Training Loss 0.016874190429439936\n",
      "Epoch 60, Training Loss 0.01729037319226643\n",
      "Epoch 60, Training Loss 0.01807661524132999\n",
      "Epoch 60, Training Loss 0.018482761900595692\n",
      "Epoch 60, Training Loss 0.01891086841254588\n",
      "Epoch 60, Training Loss 0.019417498365540027\n",
      "Epoch 60, Training Loss 0.019758201259024003\n",
      "Epoch 60, Training Loss 0.0203057502579811\n",
      "Epoch 60, Training Loss 0.020891789146853836\n",
      "Epoch 60, Training Loss 0.02156280345090515\n",
      "Epoch 60, Training Loss 0.022226318393064583\n",
      "Epoch 60, Training Loss 0.022523429197118715\n",
      "Epoch 60, Training Loss 0.02313933546280922\n",
      "Epoch 60, Training Loss 0.023645062961846666\n",
      "Epoch 60, Training Loss 0.024119462503496645\n",
      "Epoch 60, Training Loss 0.024573916135846502\n",
      "Epoch 60, Training Loss 0.025215126211990787\n",
      "Epoch 60, Training Loss 0.025705191218639577\n",
      "Epoch 60, Training Loss 0.026170620337471633\n",
      "Epoch 60, Training Loss 0.02665668474438855\n",
      "Epoch 60, Training Loss 0.027167601429897804\n",
      "Epoch 60, Training Loss 0.027548070644478662\n",
      "Epoch 60, Training Loss 0.02802923954356357\n",
      "Epoch 60, Training Loss 0.02853402887921199\n",
      "Epoch 60, Training Loss 0.02917714493201517\n",
      "Epoch 60, Training Loss 0.029629424786018897\n",
      "Epoch 60, Training Loss 0.030128293139550386\n",
      "Epoch 60, Training Loss 0.03064397179409671\n",
      "Epoch 60, Training Loss 0.031208010966820486\n",
      "Epoch 60, Training Loss 0.03170278031960168\n",
      "Epoch 60, Training Loss 0.03198783911402573\n",
      "Epoch 60, Training Loss 0.03227441150056737\n",
      "Epoch 60, Training Loss 0.032737543553952364\n",
      "Epoch 60, Training Loss 0.033239347626790976\n",
      "Epoch 60, Training Loss 0.0334793410985671\n",
      "Epoch 60, Training Loss 0.03411165666778374\n",
      "Epoch 60, Training Loss 0.03455202058529305\n",
      "Epoch 60, Training Loss 0.0352903078774662\n",
      "Epoch 60, Training Loss 0.035649936465198734\n",
      "Epoch 60, Training Loss 0.0360458275812971\n",
      "Epoch 60, Training Loss 0.0364661534004809\n",
      "Epoch 60, Training Loss 0.037063751703180624\n",
      "Epoch 60, Training Loss 0.03752962751385501\n",
      "Epoch 60, Training Loss 0.03798772686201593\n",
      "Epoch 60, Training Loss 0.038824561973819345\n",
      "Epoch 60, Training Loss 0.039295856052499904\n",
      "Epoch 60, Training Loss 0.03997425874099707\n",
      "Epoch 60, Training Loss 0.040412852621596794\n",
      "Epoch 60, Training Loss 0.040997361340333736\n",
      "Epoch 60, Training Loss 0.04136627582866517\n",
      "Epoch 60, Training Loss 0.04187667646142833\n",
      "Epoch 60, Training Loss 0.04238298956466758\n",
      "Epoch 60, Training Loss 0.042658974695236176\n",
      "Epoch 60, Training Loss 0.043270259695437255\n",
      "Epoch 60, Training Loss 0.04382244002102586\n",
      "Epoch 60, Training Loss 0.04436169376077555\n",
      "Epoch 60, Training Loss 0.0450934806405126\n",
      "Epoch 60, Training Loss 0.045409919984657744\n",
      "Epoch 60, Training Loss 0.045866331652454705\n",
      "Epoch 60, Training Loss 0.046283257415380016\n",
      "Epoch 60, Training Loss 0.046731655917051806\n",
      "Epoch 60, Training Loss 0.04743187139025125\n",
      "Epoch 60, Training Loss 0.04800265282392502\n",
      "Epoch 60, Training Loss 0.04851543512719367\n",
      "Epoch 60, Training Loss 0.04910117523063479\n",
      "Epoch 60, Training Loss 0.0494820827908833\n",
      "Epoch 60, Training Loss 0.049903459465869555\n",
      "Epoch 60, Training Loss 0.05052676196674557\n",
      "Epoch 60, Training Loss 0.051143367710473286\n",
      "Epoch 60, Training Loss 0.05141851500324581\n",
      "Epoch 60, Training Loss 0.05192574805310925\n",
      "Epoch 60, Training Loss 0.05230233686811784\n",
      "Epoch 60, Training Loss 0.052827401844131976\n",
      "Epoch 60, Training Loss 0.05323819774191093\n",
      "Epoch 60, Training Loss 0.05378468197477443\n",
      "Epoch 60, Training Loss 0.0541004661060965\n",
      "Epoch 60, Training Loss 0.05452032331996562\n",
      "Epoch 60, Training Loss 0.054983864297799744\n",
      "Epoch 60, Training Loss 0.055490832823469206\n",
      "Epoch 60, Training Loss 0.056116435476733596\n",
      "Epoch 60, Training Loss 0.056535185507648744\n",
      "Epoch 60, Training Loss 0.05688716019587139\n",
      "Epoch 60, Training Loss 0.0574965823603713\n",
      "Epoch 60, Training Loss 0.05789993129804007\n",
      "Epoch 60, Training Loss 0.05822507303465358\n",
      "Epoch 60, Training Loss 0.058626776594487603\n",
      "Epoch 60, Training Loss 0.05900518274139565\n",
      "Epoch 60, Training Loss 0.05945347144704341\n",
      "Epoch 60, Training Loss 0.05995160880524789\n",
      "Epoch 60, Training Loss 0.06047657008289986\n",
      "Epoch 60, Training Loss 0.06093993510507867\n",
      "Epoch 60, Training Loss 0.06141043553495651\n",
      "Epoch 60, Training Loss 0.06184737005120958\n",
      "Epoch 60, Training Loss 0.06239822981378916\n",
      "Epoch 60, Training Loss 0.06289743868362568\n",
      "Epoch 60, Training Loss 0.06314705357984508\n",
      "Epoch 60, Training Loss 0.06370444047024183\n",
      "Epoch 60, Training Loss 0.06420044719105791\n",
      "Epoch 60, Training Loss 0.06469434450197098\n",
      "Epoch 60, Training Loss 0.06510172631886914\n",
      "Epoch 60, Training Loss 0.06556631895282385\n",
      "Epoch 60, Training Loss 0.06605253004661911\n",
      "Epoch 60, Training Loss 0.066502594658176\n",
      "Epoch 60, Training Loss 0.06706680300290627\n",
      "Epoch 60, Training Loss 0.06764260601357121\n",
      "Epoch 60, Training Loss 0.06799196914943588\n",
      "Epoch 60, Training Loss 0.06829997780911452\n",
      "Epoch 60, Training Loss 0.068905923048706\n",
      "Epoch 60, Training Loss 0.06974272812952471\n",
      "Epoch 60, Training Loss 0.07021467073265548\n",
      "Epoch 60, Training Loss 0.07087940124370863\n",
      "Epoch 60, Training Loss 0.07138987238068714\n",
      "Epoch 60, Training Loss 0.07202996132547593\n",
      "Epoch 60, Training Loss 0.07249141941823618\n",
      "Epoch 60, Training Loss 0.07290900879732483\n",
      "Epoch 60, Training Loss 0.07334869168222408\n",
      "Epoch 60, Training Loss 0.0740189924073951\n",
      "Epoch 60, Training Loss 0.07452547372988118\n",
      "Epoch 60, Training Loss 0.07510772833357686\n",
      "Epoch 60, Training Loss 0.07568496059807366\n",
      "Epoch 60, Training Loss 0.07601702394311691\n",
      "Epoch 60, Training Loss 0.07686805845145374\n",
      "Epoch 60, Training Loss 0.0773404470604399\n",
      "Epoch 60, Training Loss 0.07793582566177754\n",
      "Epoch 60, Training Loss 0.07849323701904252\n",
      "Epoch 60, Training Loss 0.07893978244127214\n",
      "Epoch 60, Training Loss 0.07956899266185054\n",
      "Epoch 60, Training Loss 0.08001147136282738\n",
      "Epoch 60, Training Loss 0.08035086935667125\n",
      "Epoch 60, Training Loss 0.08096739842230097\n",
      "Epoch 60, Training Loss 0.08144197266196351\n",
      "Epoch 60, Training Loss 0.08214582340872806\n",
      "Epoch 60, Training Loss 0.08246635961944185\n",
      "Epoch 60, Training Loss 0.08293905730366402\n",
      "Epoch 60, Training Loss 0.08358248403233945\n",
      "Epoch 60, Training Loss 0.0838951191786305\n",
      "Epoch 60, Training Loss 0.08441759135259692\n",
      "Epoch 60, Training Loss 0.08485393283312279\n",
      "Epoch 60, Training Loss 0.08532588328699321\n",
      "Epoch 60, Training Loss 0.08582520420136659\n",
      "Epoch 60, Training Loss 0.08618197222347454\n",
      "Epoch 60, Training Loss 0.08648063513019201\n",
      "Epoch 60, Training Loss 0.08698574444064704\n",
      "Epoch 60, Training Loss 0.08756081531267337\n",
      "Epoch 60, Training Loss 0.08813249085412915\n",
      "Epoch 60, Training Loss 0.08864563646371407\n",
      "Epoch 60, Training Loss 0.08907780127452157\n",
      "Epoch 60, Training Loss 0.08988661641050177\n",
      "Epoch 60, Training Loss 0.0902142015945576\n",
      "Epoch 60, Training Loss 0.09097957317634008\n",
      "Epoch 60, Training Loss 0.09164452205990892\n",
      "Epoch 60, Training Loss 0.09238224387016443\n",
      "Epoch 60, Training Loss 0.09279779906925338\n",
      "Epoch 60, Training Loss 0.0934854445936125\n",
      "Epoch 60, Training Loss 0.09388242479023116\n",
      "Epoch 60, Training Loss 0.09414793415676298\n",
      "Epoch 60, Training Loss 0.0947933040578347\n",
      "Epoch 60, Training Loss 0.09538669882299346\n",
      "Epoch 60, Training Loss 0.09578882975270377\n",
      "Epoch 60, Training Loss 0.09621089866475376\n",
      "Epoch 60, Training Loss 0.09691739686386054\n",
      "Epoch 60, Training Loss 0.09739747935015222\n",
      "Epoch 60, Training Loss 0.09787529026684554\n",
      "Epoch 60, Training Loss 0.09826472777006266\n",
      "Epoch 60, Training Loss 0.09871724764328174\n",
      "Epoch 60, Training Loss 0.09922888065161913\n",
      "Epoch 60, Training Loss 0.09954768283973875\n",
      "Epoch 60, Training Loss 0.10020339163139348\n",
      "Epoch 60, Training Loss 0.10079782955405657\n",
      "Epoch 60, Training Loss 0.10118470451487299\n",
      "Epoch 60, Training Loss 0.10165603072060954\n",
      "Epoch 60, Training Loss 0.10190478685643058\n",
      "Epoch 60, Training Loss 0.10246752051974806\n",
      "Epoch 60, Training Loss 0.10280736566275891\n",
      "Epoch 60, Training Loss 0.10316403158714095\n",
      "Epoch 60, Training Loss 0.10378742749657473\n",
      "Epoch 60, Training Loss 0.10448706201503953\n",
      "Epoch 60, Training Loss 0.10501214364529265\n",
      "Epoch 60, Training Loss 0.10544512039788849\n",
      "Epoch 60, Training Loss 0.10604880847360777\n",
      "Epoch 60, Training Loss 0.1066547710153148\n",
      "Epoch 60, Training Loss 0.10736333136744511\n",
      "Epoch 60, Training Loss 0.10782161082529351\n",
      "Epoch 60, Training Loss 0.1086559575575087\n",
      "Epoch 60, Training Loss 0.10906453154352315\n",
      "Epoch 60, Training Loss 0.10958334294807576\n",
      "Epoch 60, Training Loss 0.11020936580646373\n",
      "Epoch 60, Training Loss 0.1106070670134881\n",
      "Epoch 60, Training Loss 0.11099936957935543\n",
      "Epoch 60, Training Loss 0.11136077228180892\n",
      "Epoch 60, Training Loss 0.11196354326918302\n",
      "Epoch 60, Training Loss 0.11246423558582125\n",
      "Epoch 60, Training Loss 0.11290106907143922\n",
      "Epoch 60, Training Loss 0.1134399555795028\n",
      "Epoch 60, Training Loss 0.11373140868704643\n",
      "Epoch 60, Training Loss 0.11440965413208813\n",
      "Epoch 60, Training Loss 0.11502022913578526\n",
      "Epoch 60, Training Loss 0.1156935863901892\n",
      "Epoch 60, Training Loss 0.11623488536195072\n",
      "Epoch 60, Training Loss 0.1167773971967685\n",
      "Epoch 60, Training Loss 0.11739436293120885\n",
      "Epoch 60, Training Loss 0.11788035774855968\n",
      "Epoch 60, Training Loss 0.11826811450750321\n",
      "Epoch 60, Training Loss 0.11869434633142198\n",
      "Epoch 60, Training Loss 0.11909638038452934\n",
      "Epoch 60, Training Loss 0.1193960427551928\n",
      "Epoch 60, Training Loss 0.11978138304884782\n",
      "Epoch 60, Training Loss 0.12046952972479184\n",
      "Epoch 60, Training Loss 0.12072506912833894\n",
      "Epoch 60, Training Loss 0.12150810289260981\n",
      "Epoch 60, Training Loss 0.12199392251651306\n",
      "Epoch 60, Training Loss 0.1225882237372191\n",
      "Epoch 60, Training Loss 0.12304754360862401\n",
      "Epoch 60, Training Loss 0.12380742958134702\n",
      "Epoch 60, Training Loss 0.12423414742702718\n",
      "Epoch 60, Training Loss 0.1248105616139634\n",
      "Epoch 60, Training Loss 0.12524417576277652\n",
      "Epoch 60, Training Loss 0.1256469688985659\n",
      "Epoch 60, Training Loss 0.12611183261170106\n",
      "Epoch 60, Training Loss 0.1263994857516435\n",
      "Epoch 60, Training Loss 0.12679963250218146\n",
      "Epoch 60, Training Loss 0.12704949566851492\n",
      "Epoch 60, Training Loss 0.12751022516690252\n",
      "Epoch 60, Training Loss 0.12799795217754895\n",
      "Epoch 60, Training Loss 0.128600889326209\n",
      "Epoch 60, Training Loss 0.12932700050227783\n",
      "Epoch 60, Training Loss 0.12977859749437293\n",
      "Epoch 60, Training Loss 0.13035510718593818\n",
      "Epoch 60, Training Loss 0.13095034081536486\n",
      "Epoch 60, Training Loss 0.13140875171593694\n",
      "Epoch 60, Training Loss 0.1318398607165917\n",
      "Epoch 60, Training Loss 0.13229654693161436\n",
      "Epoch 60, Training Loss 0.13299457921320215\n",
      "Epoch 60, Training Loss 0.13364119250374987\n",
      "Epoch 60, Training Loss 0.13413713260761004\n",
      "Epoch 60, Training Loss 0.1347939515548289\n",
      "Epoch 60, Training Loss 0.13531423863166434\n",
      "Epoch 60, Training Loss 0.13567586245058139\n",
      "Epoch 60, Training Loss 0.13630252998427053\n",
      "Epoch 60, Training Loss 0.13679431830449482\n",
      "Epoch 60, Training Loss 0.13726158384853007\n",
      "Epoch 60, Training Loss 0.1379212533764522\n",
      "Epoch 60, Training Loss 0.1383639168365837\n",
      "Epoch 60, Training Loss 0.13878425427943544\n",
      "Epoch 60, Training Loss 0.13920907807700775\n",
      "Epoch 60, Training Loss 0.13959363207716466\n",
      "Epoch 60, Training Loss 0.1401977164246847\n",
      "Epoch 60, Training Loss 0.1407585761621785\n",
      "Epoch 60, Training Loss 0.1413432936496137\n",
      "Epoch 60, Training Loss 0.14185228469350453\n",
      "Epoch 60, Training Loss 0.14226779819983046\n",
      "Epoch 60, Training Loss 0.14296417382290905\n",
      "Epoch 60, Training Loss 0.14371121590933228\n",
      "Epoch 60, Training Loss 0.14416044201616132\n",
      "Epoch 60, Training Loss 0.1449200109104671\n",
      "Epoch 60, Training Loss 0.14541661996594477\n",
      "Epoch 60, Training Loss 0.14586307125551926\n",
      "Epoch 60, Training Loss 0.1462286349262118\n",
      "Epoch 60, Training Loss 0.14655418539672252\n",
      "Epoch 60, Training Loss 0.1469449093160422\n",
      "Epoch 60, Training Loss 0.14742516223198313\n",
      "Epoch 60, Training Loss 0.14801483243094077\n",
      "Epoch 60, Training Loss 0.14856313441491797\n",
      "Epoch 60, Training Loss 0.14897445010026092\n",
      "Epoch 60, Training Loss 0.1495712791829158\n",
      "Epoch 60, Training Loss 0.14994961185299832\n",
      "Epoch 60, Training Loss 0.15079609941109975\n",
      "Epoch 60, Training Loss 0.15153245397312257\n",
      "Epoch 60, Training Loss 0.15201560712760062\n",
      "Epoch 60, Training Loss 0.1526727151039921\n",
      "Epoch 60, Training Loss 0.15324066767988304\n",
      "Epoch 60, Training Loss 0.15372192522372735\n",
      "Epoch 60, Training Loss 0.15430255445754132\n",
      "Epoch 60, Training Loss 0.15480281669846582\n",
      "Epoch 60, Training Loss 0.15544174548686313\n",
      "Epoch 60, Training Loss 0.15623576487497906\n",
      "Epoch 60, Training Loss 0.15679603367281691\n",
      "Epoch 60, Training Loss 0.1573489611144261\n",
      "Epoch 60, Training Loss 0.15786915748854122\n",
      "Epoch 60, Training Loss 0.1584191983351317\n",
      "Epoch 60, Training Loss 0.15904969238030636\n",
      "Epoch 60, Training Loss 0.15969468247326438\n",
      "Epoch 60, Training Loss 0.16021724554050304\n",
      "Epoch 60, Training Loss 0.1607792087261329\n",
      "Epoch 60, Training Loss 0.16127524330564166\n",
      "Epoch 60, Training Loss 0.1617384611455071\n",
      "Epoch 60, Training Loss 0.16218679649826814\n",
      "Epoch 60, Training Loss 0.16274228815913505\n",
      "Epoch 60, Training Loss 0.16316912490922167\n",
      "Epoch 60, Training Loss 0.163658493372333\n",
      "Epoch 60, Training Loss 0.16414816637554436\n",
      "Epoch 60, Training Loss 0.16455327014407845\n",
      "Epoch 60, Training Loss 0.1652090976114773\n",
      "Epoch 60, Training Loss 0.1656406479304099\n",
      "Epoch 60, Training Loss 0.16607578424617764\n",
      "Epoch 60, Training Loss 0.16650557790494636\n",
      "Epoch 60, Training Loss 0.16726174079777334\n",
      "Epoch 60, Training Loss 0.1677512200477788\n",
      "Epoch 60, Training Loss 0.168188834224668\n",
      "Epoch 60, Training Loss 0.16865608757337952\n",
      "Epoch 60, Training Loss 0.16943794136385784\n",
      "Epoch 60, Training Loss 0.16986446560877363\n",
      "Epoch 60, Training Loss 0.17030593438450334\n",
      "Epoch 60, Training Loss 0.17072809586668258\n",
      "Epoch 60, Training Loss 0.17133232945447688\n",
      "Epoch 60, Training Loss 0.17189907781836933\n",
      "Epoch 60, Training Loss 0.17251344362411963\n",
      "Epoch 60, Training Loss 0.17305821837747798\n",
      "Epoch 60, Training Loss 0.17374646116781722\n",
      "Epoch 60, Training Loss 0.1743642045446979\n",
      "Epoch 60, Training Loss 0.1750012322726762\n",
      "Epoch 60, Training Loss 0.1753439598566736\n",
      "Epoch 60, Training Loss 0.17583624547933374\n",
      "Epoch 60, Training Loss 0.17642447807828485\n",
      "Epoch 60, Training Loss 0.17700213413028157\n",
      "Epoch 60, Training Loss 0.1775042315387665\n",
      "Epoch 60, Training Loss 0.17802492910257692\n",
      "Epoch 60, Training Loss 0.17864131384417223\n",
      "Epoch 60, Training Loss 0.17942145716427538\n",
      "Epoch 60, Training Loss 0.18017476940017832\n",
      "Epoch 60, Training Loss 0.1806412088634718\n",
      "Epoch 60, Training Loss 0.18119794921115842\n",
      "Epoch 60, Training Loss 0.18170011952481308\n",
      "Epoch 60, Training Loss 0.18235296767462245\n",
      "Epoch 60, Training Loss 0.182718345862063\n",
      "Epoch 60, Training Loss 0.1833959140855333\n",
      "Epoch 60, Training Loss 0.18385039976871836\n",
      "Epoch 60, Training Loss 0.18443295068067053\n",
      "Epoch 60, Training Loss 0.18501253976770068\n",
      "Epoch 60, Training Loss 0.18532363030001942\n",
      "Epoch 60, Training Loss 0.18586986155613608\n",
      "Epoch 60, Training Loss 0.1861805799786392\n",
      "Epoch 60, Training Loss 0.1866582501536745\n",
      "Epoch 60, Training Loss 0.18710833862233345\n",
      "Epoch 60, Training Loss 0.1876453212879198\n",
      "Epoch 60, Training Loss 0.18830021314532555\n",
      "Epoch 60, Training Loss 0.18864355838435995\n",
      "Epoch 60, Training Loss 0.18937356733833738\n",
      "Epoch 60, Training Loss 0.19006633467000464\n",
      "Epoch 60, Training Loss 0.1906546641074483\n",
      "Epoch 60, Training Loss 0.19123176481489026\n",
      "Epoch 60, Training Loss 0.19178504786451758\n",
      "Epoch 60, Training Loss 0.19222833663987382\n",
      "Epoch 60, Training Loss 0.19262163891740466\n",
      "Epoch 60, Training Loss 0.19292667367116875\n",
      "Epoch 60, Training Loss 0.19322680159831596\n",
      "Epoch 60, Training Loss 0.19373649307300367\n",
      "Epoch 60, Training Loss 0.19412776433369694\n",
      "Epoch 60, Training Loss 0.19491147867325323\n",
      "Epoch 60, Training Loss 0.1952903498430996\n",
      "Epoch 60, Training Loss 0.19578359683837426\n",
      "Epoch 60, Training Loss 0.19661583759062126\n",
      "Epoch 60, Training Loss 0.19718236563836827\n",
      "Epoch 60, Training Loss 0.1978484062320741\n",
      "Epoch 60, Training Loss 0.1982376998304711\n",
      "Epoch 60, Training Loss 0.19858883025929752\n",
      "Epoch 60, Training Loss 0.19913586392960586\n",
      "Epoch 60, Training Loss 0.19962568072330616\n",
      "Epoch 60, Training Loss 0.20006516608207123\n",
      "Epoch 60, Training Loss 0.2004108901333321\n",
      "Epoch 60, Training Loss 0.20111753528608994\n",
      "Epoch 60, Training Loss 0.2015247918341471\n",
      "Epoch 60, Training Loss 0.20196904472606567\n",
      "Epoch 60, Training Loss 0.20260938977265297\n",
      "Epoch 60, Training Loss 0.20318901510266088\n",
      "Epoch 60, Training Loss 0.20374159798826402\n",
      "Epoch 60, Training Loss 0.2046491538586519\n",
      "Epoch 60, Training Loss 0.20513814537192854\n",
      "Epoch 60, Training Loss 0.20583742561623875\n",
      "Epoch 60, Training Loss 0.20633721551703066\n",
      "Epoch 60, Training Loss 0.206857055825803\n",
      "Epoch 60, Training Loss 0.2077628174591857\n",
      "Epoch 60, Training Loss 0.2084006821674764\n",
      "Epoch 60, Training Loss 0.20880065269558631\n",
      "Epoch 60, Training Loss 0.20932840052849191\n",
      "Epoch 60, Training Loss 0.20983487079896584\n",
      "Epoch 60, Training Loss 0.21066594499227642\n",
      "Epoch 60, Training Loss 0.21117893949417813\n",
      "Epoch 60, Training Loss 0.2116456846027728\n",
      "Epoch 60, Training Loss 0.2121897676907232\n",
      "Epoch 60, Training Loss 0.21263701712612607\n",
      "Epoch 60, Training Loss 0.21334424399582627\n",
      "Epoch 60, Training Loss 0.21368247253434433\n",
      "Epoch 60, Training Loss 0.21417748937597664\n",
      "Epoch 60, Training Loss 0.21471598913983617\n",
      "Epoch 60, Training Loss 0.21542655158302057\n",
      "Epoch 60, Training Loss 0.21605937661188643\n",
      "Epoch 60, Training Loss 0.21660836519259016\n",
      "Epoch 60, Training Loss 0.21724351499315417\n",
      "Epoch 60, Training Loss 0.2177789529685474\n",
      "Epoch 60, Training Loss 0.21817738345593138\n",
      "Epoch 60, Training Loss 0.21872295894662439\n",
      "Epoch 60, Training Loss 0.21921520786898216\n",
      "Epoch 60, Training Loss 0.21976184778277527\n",
      "Epoch 60, Training Loss 0.2202293594436877\n",
      "Epoch 60, Training Loss 0.22092669521984848\n",
      "Epoch 60, Training Loss 0.22171243497401552\n",
      "Epoch 60, Training Loss 0.2226469723884102\n",
      "Epoch 60, Training Loss 0.22321493148117724\n",
      "Epoch 60, Training Loss 0.22362778290077243\n",
      "Epoch 60, Training Loss 0.2243151355086995\n",
      "Epoch 60, Training Loss 0.22473911260781082\n",
      "Epoch 60, Training Loss 0.22541257070229792\n",
      "Epoch 60, Training Loss 0.22581388046750633\n",
      "Epoch 60, Training Loss 0.2262340755299534\n",
      "Epoch 60, Training Loss 0.22700905222493364\n",
      "Epoch 60, Training Loss 0.2273887840797529\n",
      "Epoch 60, Training Loss 0.227872452296107\n",
      "Epoch 60, Training Loss 0.22836761529107227\n",
      "Epoch 60, Training Loss 0.2289088426534172\n",
      "Epoch 60, Training Loss 0.22933922793782885\n",
      "Epoch 60, Training Loss 0.22995290550810601\n",
      "Epoch 60, Training Loss 0.23050580935938583\n",
      "Epoch 60, Training Loss 0.23115354492460066\n",
      "Epoch 60, Training Loss 0.23177750212380954\n",
      "Epoch 60, Training Loss 0.23247941328055413\n",
      "Epoch 60, Training Loss 0.2328231501800325\n",
      "Epoch 60, Training Loss 0.23364867946452192\n",
      "Epoch 60, Training Loss 0.23420746725462283\n",
      "Epoch 60, Training Loss 0.23467793910171064\n",
      "Epoch 60, Training Loss 0.23526149044942368\n",
      "Epoch 60, Training Loss 0.23587802717524112\n",
      "Epoch 60, Training Loss 0.23650667172334994\n",
      "Epoch 60, Training Loss 0.23721368845237795\n",
      "Epoch 60, Training Loss 0.23765374763923533\n",
      "Epoch 60, Training Loss 0.23804122165722005\n",
      "Epoch 60, Training Loss 0.23863607403033835\n",
      "Epoch 60, Training Loss 0.23939250339098903\n",
      "Epoch 60, Training Loss 0.2399402448092885\n",
      "Epoch 60, Training Loss 0.24050917811787037\n",
      "Epoch 60, Training Loss 0.2410876464546489\n",
      "Epoch 60, Training Loss 0.2416067776053458\n",
      "Epoch 60, Training Loss 0.24223538430984062\n",
      "Epoch 60, Training Loss 0.2426876554746762\n",
      "Epoch 60, Training Loss 0.243140388994723\n",
      "Epoch 60, Training Loss 0.24373227965725047\n",
      "Epoch 60, Training Loss 0.24460586027035017\n",
      "Epoch 60, Training Loss 0.24519578748575563\n",
      "Epoch 60, Training Loss 0.24567807854517645\n",
      "Epoch 60, Training Loss 0.24603682792628817\n",
      "Epoch 60, Training Loss 0.2465743493203007\n",
      "Epoch 60, Training Loss 0.24713264792547812\n",
      "Epoch 60, Training Loss 0.24778944324425725\n",
      "Epoch 60, Training Loss 0.24837578644456765\n",
      "Epoch 60, Training Loss 0.24893725209909937\n",
      "Epoch 60, Training Loss 0.24928039066550678\n",
      "Epoch 60, Training Loss 0.2498355215353429\n",
      "Epoch 60, Training Loss 0.25051379556317466\n",
      "Epoch 60, Training Loss 0.2509760507346724\n",
      "Epoch 60, Training Loss 0.25143370514406876\n",
      "Epoch 60, Training Loss 0.25215096800299863\n",
      "Epoch 60, Training Loss 0.2528173483889121\n",
      "Epoch 60, Training Loss 0.2535152770864689\n",
      "Epoch 60, Training Loss 0.253921795245784\n",
      "Epoch 60, Training Loss 0.25457688733516143\n",
      "Epoch 60, Training Loss 0.25502937100351314\n",
      "Epoch 60, Training Loss 0.25550695802168466\n",
      "Epoch 60, Training Loss 0.25598626172222444\n",
      "Epoch 60, Training Loss 0.2564153020529796\n",
      "Epoch 60, Training Loss 0.2569552698289342\n",
      "Epoch 60, Training Loss 0.2575299633319116\n",
      "Epoch 60, Training Loss 0.25801755447902947\n",
      "Epoch 60, Training Loss 0.2586228839309929\n",
      "Epoch 60, Training Loss 0.25933613391864635\n",
      "Epoch 60, Training Loss 0.25989420805364616\n",
      "Epoch 60, Training Loss 0.26025998125524474\n",
      "Epoch 60, Training Loss 0.26089771897972697\n",
      "Epoch 60, Training Loss 0.26144908656320914\n",
      "Epoch 60, Training Loss 0.26194201487943036\n",
      "Epoch 60, Training Loss 0.26279286318041783\n",
      "Epoch 60, Training Loss 0.26315405886725085\n",
      "Epoch 60, Training Loss 0.26348658811177134\n",
      "Epoch 60, Training Loss 0.2640396522172272\n",
      "Epoch 60, Training Loss 0.2646250550818565\n",
      "Epoch 60, Training Loss 0.2650684480128996\n",
      "Epoch 60, Training Loss 0.26584084852196066\n",
      "Epoch 60, Training Loss 0.2662389376927215\n",
      "Epoch 60, Training Loss 0.2668304783113472\n",
      "Epoch 60, Training Loss 0.2674418712782738\n",
      "Epoch 60, Training Loss 0.2678444785497073\n",
      "Epoch 60, Training Loss 0.2682377814751147\n",
      "Epoch 60, Training Loss 0.2686317743509627\n",
      "Epoch 60, Training Loss 0.2693000949366623\n",
      "Epoch 60, Training Loss 0.2696251280586738\n",
      "Epoch 60, Training Loss 0.27026942568590573\n",
      "Epoch 60, Training Loss 0.27060301878187054\n",
      "Epoch 60, Training Loss 0.27108498052944\n",
      "Epoch 60, Training Loss 0.2717359347454727\n",
      "Epoch 60, Training Loss 0.2723585615491928\n",
      "Epoch 60, Training Loss 0.2728547653769288\n",
      "Epoch 60, Training Loss 0.2733311667809706\n",
      "Epoch 60, Training Loss 0.27374006996450523\n",
      "Epoch 60, Training Loss 0.2743015276539661\n",
      "Epoch 60, Training Loss 0.27501171768245186\n",
      "Epoch 60, Training Loss 0.2756720428614665\n",
      "Epoch 60, Training Loss 0.2761415200274619\n",
      "Epoch 60, Training Loss 0.27665340509789677\n",
      "Epoch 60, Training Loss 0.2771148771390586\n",
      "Epoch 60, Training Loss 0.2777672987574202\n",
      "Epoch 60, Training Loss 0.27812198591430476\n",
      "Epoch 60, Training Loss 0.2786631609510888\n",
      "Epoch 60, Training Loss 0.27918634933355213\n",
      "Epoch 60, Training Loss 0.2796958340593921\n",
      "Epoch 60, Training Loss 0.28027823641705696\n",
      "Epoch 60, Training Loss 0.2807834667279897\n",
      "Epoch 60, Training Loss 0.28127729174349925\n",
      "Epoch 60, Training Loss 0.2817511175332777\n",
      "Epoch 60, Training Loss 0.2821053927931029\n",
      "Epoch 60, Training Loss 0.28276829301472517\n",
      "Epoch 60, Training Loss 0.2833659679574125\n",
      "Epoch 60, Training Loss 0.28402893357645825\n",
      "Epoch 60, Training Loss 0.284541894769882\n",
      "Epoch 60, Training Loss 0.2852743181692975\n",
      "Epoch 60, Training Loss 0.28587585216974054\n",
      "Epoch 60, Training Loss 0.28635332908700495\n",
      "Epoch 60, Training Loss 0.2870085409954381\n",
      "Epoch 60, Training Loss 0.2874094477051969\n",
      "Epoch 60, Training Loss 0.28787561326914124\n",
      "Epoch 60, Training Loss 0.2883116041341096\n",
      "Epoch 60, Training Loss 0.2888866442129435\n",
      "Epoch 60, Training Loss 0.28930430901248744\n",
      "Epoch 60, Training Loss 0.2898703477228694\n",
      "Epoch 60, Training Loss 0.2904667187567867\n",
      "Epoch 60, Training Loss 0.2911501220615624\n",
      "Epoch 60, Training Loss 0.2916854016120781\n",
      "Epoch 60, Training Loss 0.2922687714781298\n",
      "Epoch 60, Training Loss 0.29293806936658556\n",
      "Epoch 60, Training Loss 0.29343574879038364\n",
      "Epoch 60, Training Loss 0.2941108605707698\n",
      "Epoch 60, Training Loss 0.294696809328578\n",
      "Epoch 60, Training Loss 0.2952128234688583\n",
      "Epoch 60, Training Loss 0.2957530465844037\n",
      "Epoch 60, Training Loss 0.29615970575214956\n",
      "Epoch 60, Training Loss 0.29651105687822527\n",
      "Epoch 60, Training Loss 0.2971227029362298\n",
      "Epoch 60, Training Loss 0.2976277979552898\n",
      "Epoch 60, Training Loss 0.29808058677350774\n",
      "Epoch 60, Training Loss 0.2989028682908439\n",
      "Epoch 60, Training Loss 0.29961191561749523\n",
      "Epoch 60, Training Loss 0.30019603724903465\n",
      "Epoch 60, Training Loss 0.3010236947699581\n",
      "Epoch 60, Training Loss 0.301814742081458\n",
      "Epoch 60, Training Loss 0.3022366000525177\n",
      "Epoch 60, Training Loss 0.30283394648367185\n",
      "Epoch 60, Training Loss 0.30352930890401003\n",
      "Epoch 60, Training Loss 0.30422145328329653\n",
      "Epoch 60, Training Loss 0.30466857276228076\n",
      "Epoch 60, Training Loss 0.3054322918776966\n",
      "Epoch 60, Training Loss 0.3061212016760236\n",
      "Epoch 60, Training Loss 0.30663973613239615\n",
      "Epoch 60, Training Loss 0.30755646631617073\n",
      "Epoch 60, Training Loss 0.3082085619382846\n",
      "Epoch 60, Training Loss 0.30905192368247014\n",
      "Epoch 60, Training Loss 0.30952633149407405\n",
      "Epoch 60, Training Loss 0.3098997237241786\n",
      "Epoch 60, Training Loss 0.31038268785113876\n",
      "Epoch 60, Training Loss 0.3109368581105681\n",
      "Epoch 60, Training Loss 0.31143627353870046\n",
      "Epoch 60, Training Loss 0.31210620939502937\n",
      "Epoch 60, Training Loss 0.3125894896095366\n",
      "Epoch 60, Training Loss 0.3131016042950513\n",
      "Epoch 60, Training Loss 0.31356522103633416\n",
      "Epoch 60, Training Loss 0.3141155256449109\n",
      "Epoch 60, Training Loss 0.3148702263184216\n",
      "Epoch 60, Training Loss 0.31569387096806867\n",
      "Epoch 60, Training Loss 0.3164800496776695\n",
      "Epoch 60, Training Loss 0.3169913328326572\n",
      "Epoch 60, Training Loss 0.3174865751353371\n",
      "Epoch 60, Training Loss 0.3181710228933703\n",
      "Epoch 60, Training Loss 0.31868861880524996\n",
      "Epoch 60, Training Loss 0.31936951033065997\n",
      "Epoch 60, Training Loss 0.3198376254314352\n",
      "Epoch 60, Training Loss 0.3204086714274133\n",
      "Epoch 60, Training Loss 0.32108780469202325\n",
      "Epoch 60, Training Loss 0.32140882109360924\n",
      "Epoch 60, Training Loss 0.3219042140085374\n",
      "Epoch 60, Training Loss 0.3223382152064377\n",
      "Epoch 60, Training Loss 0.32273862145059856\n",
      "Epoch 60, Training Loss 0.3231586667773364\n",
      "Epoch 60, Training Loss 0.32366591057432886\n",
      "Epoch 60, Training Loss 0.3240735989892879\n",
      "Epoch 60, Training Loss 0.3245557487735053\n",
      "Epoch 60, Training Loss 0.3249425674452806\n",
      "Epoch 60, Training Loss 0.325421782805944\n",
      "Epoch 60, Training Loss 0.32581499784879975\n",
      "Epoch 60, Training Loss 0.32613736305319135\n",
      "Epoch 60, Training Loss 0.3266190598192422\n",
      "Epoch 60, Training Loss 0.3272079059191982\n",
      "Epoch 60, Training Loss 0.32772928226710585\n",
      "Epoch 60, Training Loss 0.3282072122406472\n",
      "Epoch 60, Training Loss 0.32896836660325984\n",
      "Epoch 60, Training Loss 0.3297349677785583\n",
      "Epoch 60, Training Loss 0.33028496262591206\n",
      "Epoch 60, Training Loss 0.3307384421567783\n",
      "Epoch 60, Training Loss 0.3312214934040823\n",
      "Epoch 60, Training Loss 0.3316999380583958\n",
      "Epoch 60, Training Loss 0.3322964413734653\n",
      "Epoch 60, Training Loss 0.3327396360923872\n",
      "Epoch 60, Training Loss 0.3332016310842751\n",
      "Epoch 60, Training Loss 0.333680910851492\n",
      "Epoch 60, Training Loss 0.3342033175137037\n",
      "Epoch 60, Training Loss 0.334606823008841\n",
      "Epoch 60, Training Loss 0.3351107100429742\n",
      "Epoch 60, Training Loss 0.33569834690036066\n",
      "Epoch 60, Training Loss 0.33630443349137634\n",
      "Epoch 60, Training Loss 0.33677347025374316\n",
      "Epoch 60, Training Loss 0.3373064286530475\n",
      "Epoch 60, Training Loss 0.33787136917452676\n",
      "Epoch 60, Training Loss 0.3386852613000004\n",
      "Epoch 60, Training Loss 0.33924339725004743\n",
      "Epoch 60, Training Loss 0.33970837921971253\n",
      "Epoch 60, Training Loss 0.34007736840440184\n",
      "Epoch 60, Training Loss 0.3404588433901977\n",
      "Epoch 60, Training Loss 0.34096554712490046\n",
      "Epoch 60, Training Loss 0.3416576582719298\n",
      "Epoch 60, Training Loss 0.34204231524635154\n",
      "Epoch 60, Training Loss 0.34254241262174323\n",
      "Epoch 60, Training Loss 0.34314013802258253\n",
      "Epoch 60, Training Loss 0.34351301126544126\n",
      "Epoch 60, Training Loss 0.3439103015662764\n",
      "Epoch 60, Training Loss 0.34467611332302506\n",
      "Epoch 60, Training Loss 0.34526726144277836\n",
      "Epoch 60, Training Loss 0.34578178809655596\n",
      "Epoch 60, Training Loss 0.34640246936503577\n",
      "Epoch 60, Training Loss 0.3468521397627528\n",
      "Epoch 60, Training Loss 0.3473089615364209\n",
      "Epoch 60, Training Loss 0.3478848405773073\n",
      "Epoch 60, Training Loss 0.34881927135884\n",
      "Epoch 60, Training Loss 0.34934377756036455\n",
      "Epoch 60, Training Loss 0.34988425224257247\n",
      "Epoch 60, Training Loss 0.35034134654361576\n",
      "Epoch 60, Training Loss 0.3508631536532241\n",
      "Epoch 60, Training Loss 0.3515572270278431\n",
      "Epoch 60, Training Loss 0.35225711955362576\n",
      "Epoch 60, Training Loss 0.3527790103155329\n",
      "Epoch 60, Training Loss 0.35342939958319336\n",
      "Epoch 60, Training Loss 0.35400302881551216\n",
      "Epoch 60, Training Loss 0.35470620508465317\n",
      "Epoch 60, Training Loss 0.35522045863939977\n",
      "Epoch 60, Training Loss 0.35587543327256543\n",
      "Epoch 60, Training Loss 0.3564096061355623\n",
      "Epoch 60, Training Loss 0.357310574168287\n",
      "Epoch 60, Training Loss 0.35795435440890927\n",
      "Epoch 60, Training Loss 0.35854922031121483\n",
      "Epoch 60, Training Loss 0.3593499074925852\n",
      "Epoch 60, Training Loss 0.35988928985488994\n",
      "Epoch 60, Training Loss 0.36017305190529664\n",
      "Epoch 60, Training Loss 0.36065214535083306\n",
      "Epoch 60, Training Loss 0.3610613287219306\n",
      "Epoch 60, Training Loss 0.361627457258494\n",
      "Epoch 60, Training Loss 0.36200026521826034\n",
      "Epoch 60, Training Loss 0.3626015188024782\n",
      "Epoch 60, Training Loss 0.3633497698265878\n",
      "Epoch 60, Training Loss 0.3637604044412103\n",
      "Epoch 60, Training Loss 0.36430336201511077\n",
      "Epoch 60, Training Loss 0.36486258563559376\n",
      "Epoch 60, Training Loss 0.3655004651688249\n",
      "Epoch 60, Training Loss 0.36590576821657095\n",
      "Epoch 60, Training Loss 0.36633198978879566\n",
      "Epoch 60, Training Loss 0.3668370782528692\n",
      "Epoch 60, Training Loss 0.36755184434792576\n",
      "Epoch 60, Training Loss 0.3680160644528506\n",
      "Epoch 60, Training Loss 0.36842693653329256\n",
      "Epoch 60, Training Loss 0.36904766932701516\n",
      "Epoch 60, Training Loss 0.3696130809690946\n",
      "Epoch 60, Training Loss 0.3701795497933007\n",
      "Epoch 60, Training Loss 0.37100260449416195\n",
      "Epoch 60, Training Loss 0.371367458015909\n",
      "Epoch 60, Training Loss 0.3719344465514583\n",
      "Epoch 60, Training Loss 0.3725315531920594\n",
      "Epoch 60, Training Loss 0.37294703003619334\n",
      "Epoch 60, Training Loss 0.37359364485115654\n",
      "Epoch 60, Training Loss 0.37389424031652757\n",
      "Epoch 60, Training Loss 0.3744436500170042\n",
      "Epoch 60, Training Loss 0.3749479077508687\n",
      "Epoch 60, Training Loss 0.3754596721638194\n",
      "Epoch 60, Training Loss 0.3760400736880729\n",
      "Epoch 60, Training Loss 0.37664214024305953\n",
      "Epoch 60, Training Loss 0.3772839831802851\n",
      "Epoch 60, Training Loss 0.3777344665106605\n",
      "Epoch 60, Training Loss 0.37811084171695175\n",
      "Epoch 60, Training Loss 0.37877679137927495\n",
      "Epoch 60, Training Loss 0.37947986795164435\n",
      "Epoch 60, Training Loss 0.38005331562608097\n",
      "Epoch 60, Training Loss 0.3805273587593947\n",
      "Epoch 60, Training Loss 0.38109199961890344\n",
      "Epoch 60, Training Loss 0.381542621632976\n",
      "Epoch 60, Training Loss 0.3819547932394935\n",
      "Epoch 60, Training Loss 0.38246318720795613\n",
      "Epoch 60, Training Loss 0.38292271058882593\n",
      "Epoch 60, Training Loss 0.3835178916640294\n",
      "Epoch 60, Training Loss 0.38410832544269463\n",
      "Epoch 60, Training Loss 0.38496047277432266\n",
      "Epoch 60, Training Loss 0.38535869521710575\n",
      "Epoch 60, Training Loss 0.3857586987107001\n",
      "Epoch 60, Training Loss 0.38654794591619535\n",
      "Epoch 60, Training Loss 0.3869227669046968\n",
      "Epoch 60, Training Loss 0.38741204005373103\n",
      "Epoch 60, Training Loss 0.3879550442366344\n",
      "Epoch 60, Training Loss 0.3884873471753981\n",
      "Epoch 60, Training Loss 0.38892659171462973\n",
      "Epoch 60, Training Loss 0.38939276028929465\n",
      "Epoch 60, Training Loss 0.3898815950164405\n",
      "Epoch 60, Training Loss 0.3903337002486524\n",
      "Epoch 60, Training Loss 0.3909872721909257\n",
      "Epoch 60, Training Loss 0.39155449079887944\n",
      "Epoch 60, Training Loss 0.3922604197812507\n",
      "Epoch 60, Training Loss 0.3928945175064799\n",
      "Epoch 60, Training Loss 0.3933732858704179\n",
      "Epoch 60, Training Loss 0.39436039900230935\n",
      "Epoch 60, Training Loss 0.39507316110079244\n",
      "Epoch 60, Training Loss 0.39581828928359636\n",
      "Epoch 60, Training Loss 0.39639640997742753\n",
      "Epoch 60, Training Loss 0.39688751646472364\n",
      "Epoch 60, Training Loss 0.3974007912685194\n",
      "Epoch 60, Training Loss 0.3980119383472311\n",
      "Epoch 60, Training Loss 0.3985352863741043\n",
      "Epoch 60, Training Loss 0.39909852530492845\n",
      "Epoch 60, Training Loss 0.3997002104892755\n",
      "Epoch 60, Training Loss 0.40024218149959584\n",
      "Epoch 60, Training Loss 0.4008523187292811\n",
      "Epoch 60, Training Loss 0.4012421913006726\n",
      "Epoch 60, Training Loss 0.40181522341945286\n",
      "Epoch 60, Training Loss 0.40232559169649773\n",
      "Epoch 60, Training Loss 0.4029273612572409\n",
      "Epoch 60, Training Loss 0.40365928038001975\n",
      "Epoch 60, Training Loss 0.4041443186266648\n",
      "Epoch 60, Training Loss 0.40462513477601053\n",
      "Epoch 60, Training Loss 0.4051569807712379\n",
      "Epoch 60, Training Loss 0.40577314999859654\n",
      "Epoch 60, Training Loss 0.40626204265353016\n",
      "Epoch 60, Training Loss 0.40666470831007606\n",
      "Epoch 60, Training Loss 0.4073436611601154\n",
      "Epoch 60, Training Loss 0.40776397356444305\n",
      "Epoch 60, Training Loss 0.4081711244705083\n",
      "Epoch 60, Training Loss 0.40882691832454615\n",
      "Epoch 60, Training Loss 0.4094066258967685\n",
      "Epoch 60, Training Loss 0.409796164163848\n",
      "Epoch 60, Training Loss 0.4102615437391774\n",
      "Epoch 60, Training Loss 0.4107184404378657\n",
      "Epoch 60, Training Loss 0.41113887319479453\n",
      "Epoch 60, Training Loss 0.41201609845661447\n",
      "Epoch 60, Training Loss 0.4127653458386736\n",
      "Epoch 60, Training Loss 0.4131406869958429\n",
      "Epoch 60, Training Loss 0.4133993246404411\n",
      "Epoch 60, Training Loss 0.41387015430594953\n",
      "Epoch 60, Training Loss 0.414333031438958\n",
      "Epoch 60, Training Loss 0.4150658533967974\n",
      "Epoch 60, Training Loss 0.4154753508355916\n",
      "Epoch 70, Training Loss 0.0005614393964752821\n",
      "Epoch 70, Training Loss 0.0011181464356839505\n",
      "Epoch 70, Training Loss 0.0016706224216524598\n",
      "Epoch 70, Training Loss 0.0021036188392078176\n",
      "Epoch 70, Training Loss 0.0024885256653246674\n",
      "Epoch 70, Training Loss 0.0029102170939945505\n",
      "Epoch 70, Training Loss 0.003263844667798113\n",
      "Epoch 70, Training Loss 0.003703780643775335\n",
      "Epoch 70, Training Loss 0.004228123542292953\n",
      "Epoch 70, Training Loss 0.0046416749353603935\n",
      "Epoch 70, Training Loss 0.0050011533681693894\n",
      "Epoch 70, Training Loss 0.005533442396641997\n",
      "Epoch 70, Training Loss 0.006047915086112059\n",
      "Epoch 70, Training Loss 0.006700104535997981\n",
      "Epoch 70, Training Loss 0.007129978798234554\n",
      "Epoch 70, Training Loss 0.00746795390268116\n",
      "Epoch 70, Training Loss 0.007968752661629407\n",
      "Epoch 70, Training Loss 0.008313482755895161\n",
      "Epoch 70, Training Loss 0.008568332475774428\n",
      "Epoch 70, Training Loss 0.00912831815154961\n",
      "Epoch 70, Training Loss 0.009659934646028387\n",
      "Epoch 70, Training Loss 0.010074994943635849\n",
      "Epoch 70, Training Loss 0.010383042685516045\n",
      "Epoch 70, Training Loss 0.01067341321036029\n",
      "Epoch 70, Training Loss 0.011097688866240898\n",
      "Epoch 70, Training Loss 0.011575005338777361\n",
      "Epoch 70, Training Loss 0.012209140069192022\n",
      "Epoch 70, Training Loss 0.0127278482327071\n",
      "Epoch 70, Training Loss 0.013136714068062775\n",
      "Epoch 70, Training Loss 0.013648925630180428\n",
      "Epoch 70, Training Loss 0.014056185493841196\n",
      "Epoch 70, Training Loss 0.01463778619952214\n",
      "Epoch 70, Training Loss 0.015177049695530815\n",
      "Epoch 70, Training Loss 0.015562967056661006\n",
      "Epoch 70, Training Loss 0.015930693491797925\n",
      "Epoch 70, Training Loss 0.016306523224124517\n",
      "Epoch 70, Training Loss 0.016538764909862558\n",
      "Epoch 70, Training Loss 0.016990512399874685\n",
      "Epoch 70, Training Loss 0.01739235067992564\n",
      "Epoch 70, Training Loss 0.01775417960894382\n",
      "Epoch 70, Training Loss 0.01822662599327619\n",
      "Epoch 70, Training Loss 0.018664262579072773\n",
      "Epoch 70, Training Loss 0.01896003769029437\n",
      "Epoch 70, Training Loss 0.019402293788502587\n",
      "Epoch 70, Training Loss 0.02016049459614717\n",
      "Epoch 70, Training Loss 0.020714697973502567\n",
      "Epoch 70, Training Loss 0.021077811870428606\n",
      "Epoch 70, Training Loss 0.021630210225539438\n",
      "Epoch 70, Training Loss 0.022175403773937077\n",
      "Epoch 70, Training Loss 0.022569335482614424\n",
      "Epoch 70, Training Loss 0.022985644657593556\n",
      "Epoch 70, Training Loss 0.02334567378549015\n",
      "Epoch 70, Training Loss 0.023665335915430123\n",
      "Epoch 70, Training Loss 0.024063497686477572\n",
      "Epoch 70, Training Loss 0.02444251859203324\n",
      "Epoch 70, Training Loss 0.024913329659672953\n",
      "Epoch 70, Training Loss 0.025467783460379256\n",
      "Epoch 70, Training Loss 0.025905021106648017\n",
      "Epoch 70, Training Loss 0.026333142100545145\n",
      "Epoch 70, Training Loss 0.02669874506304636\n",
      "Epoch 70, Training Loss 0.027075042219265648\n",
      "Epoch 70, Training Loss 0.02767376001457424\n",
      "Epoch 70, Training Loss 0.028051444236427317\n",
      "Epoch 70, Training Loss 0.02861849664498473\n",
      "Epoch 70, Training Loss 0.02896625659121272\n",
      "Epoch 70, Training Loss 0.02928825746030759\n",
      "Epoch 70, Training Loss 0.029683002749519884\n",
      "Epoch 70, Training Loss 0.02997160140815598\n",
      "Epoch 70, Training Loss 0.030465841179003802\n",
      "Epoch 70, Training Loss 0.030876700721128518\n",
      "Epoch 70, Training Loss 0.031211743040767778\n",
      "Epoch 70, Training Loss 0.03140350718937262\n",
      "Epoch 70, Training Loss 0.03217531531058309\n",
      "Epoch 70, Training Loss 0.032604827676587705\n",
      "Epoch 70, Training Loss 0.033049263841355855\n",
      "Epoch 70, Training Loss 0.033470825732821397\n",
      "Epoch 70, Training Loss 0.03382365295039418\n",
      "Epoch 70, Training Loss 0.03449984249251578\n",
      "Epoch 70, Training Loss 0.03514355329601356\n",
      "Epoch 70, Training Loss 0.035657666337764474\n",
      "Epoch 70, Training Loss 0.03600109404767566\n",
      "Epoch 70, Training Loss 0.03636785777633452\n",
      "Epoch 70, Training Loss 0.03676437474120303\n",
      "Epoch 70, Training Loss 0.03710050389284978\n",
      "Epoch 70, Training Loss 0.03741520412666414\n",
      "Epoch 70, Training Loss 0.03767957584098782\n",
      "Epoch 70, Training Loss 0.038041109338288415\n",
      "Epoch 70, Training Loss 0.03832924933842076\n",
      "Epoch 70, Training Loss 0.03879115149340666\n",
      "Epoch 70, Training Loss 0.03912350684023269\n",
      "Epoch 70, Training Loss 0.03980301899830704\n",
      "Epoch 70, Training Loss 0.040232938802455695\n",
      "Epoch 70, Training Loss 0.040793209734475214\n",
      "Epoch 70, Training Loss 0.04127287376872109\n",
      "Epoch 70, Training Loss 0.041649826690364065\n",
      "Epoch 70, Training Loss 0.04220131150139567\n",
      "Epoch 70, Training Loss 0.042466088938896\n",
      "Epoch 70, Training Loss 0.04276103591141493\n",
      "Epoch 70, Training Loss 0.043121382803715705\n",
      "Epoch 70, Training Loss 0.04371758474184729\n",
      "Epoch 70, Training Loss 0.04416047217672133\n",
      "Epoch 70, Training Loss 0.04454645977529419\n",
      "Epoch 70, Training Loss 0.04513863458886476\n",
      "Epoch 70, Training Loss 0.04576600190547421\n",
      "Epoch 70, Training Loss 0.046327353419397796\n",
      "Epoch 70, Training Loss 0.04677654297836601\n",
      "Epoch 70, Training Loss 0.04730719005893868\n",
      "Epoch 70, Training Loss 0.047745666292774706\n",
      "Epoch 70, Training Loss 0.048235932979589835\n",
      "Epoch 70, Training Loss 0.04870998030504607\n",
      "Epoch 70, Training Loss 0.049448981176099506\n",
      "Epoch 70, Training Loss 0.04974765978429629\n",
      "Epoch 70, Training Loss 0.050284482881693585\n",
      "Epoch 70, Training Loss 0.05061878951842828\n",
      "Epoch 70, Training Loss 0.05116795394045617\n",
      "Epoch 70, Training Loss 0.051606890471542585\n",
      "Epoch 70, Training Loss 0.05223957849356829\n",
      "Epoch 70, Training Loss 0.05255672390884755\n",
      "Epoch 70, Training Loss 0.052934801734774314\n",
      "Epoch 70, Training Loss 0.05333348721875559\n",
      "Epoch 70, Training Loss 0.053732651978959815\n",
      "Epoch 70, Training Loss 0.05428218435677116\n",
      "Epoch 70, Training Loss 0.05456731491305334\n",
      "Epoch 70, Training Loss 0.05495175618268645\n",
      "Epoch 70, Training Loss 0.05527449848935427\n",
      "Epoch 70, Training Loss 0.05568480676473559\n",
      "Epoch 70, Training Loss 0.05596484388689251\n",
      "Epoch 70, Training Loss 0.056455018308461474\n",
      "Epoch 70, Training Loss 0.05666956036825619\n",
      "Epoch 70, Training Loss 0.05697847693167684\n",
      "Epoch 70, Training Loss 0.05728893368826498\n",
      "Epoch 70, Training Loss 0.057770370084154025\n",
      "Epoch 70, Training Loss 0.05841574432980984\n",
      "Epoch 70, Training Loss 0.05885760529952891\n",
      "Epoch 70, Training Loss 0.0593542631363015\n",
      "Epoch 70, Training Loss 0.0596749593344186\n",
      "Epoch 70, Training Loss 0.06025138937527566\n",
      "Epoch 70, Training Loss 0.06052386987468471\n",
      "Epoch 70, Training Loss 0.06110753223795415\n",
      "Epoch 70, Training Loss 0.06153055695850221\n",
      "Epoch 70, Training Loss 0.06196675899312319\n",
      "Epoch 70, Training Loss 0.06244916476480796\n",
      "Epoch 70, Training Loss 0.06283441885276829\n",
      "Epoch 70, Training Loss 0.06316260004516148\n",
      "Epoch 70, Training Loss 0.06365353777966536\n",
      "Epoch 70, Training Loss 0.06392634254129952\n",
      "Epoch 70, Training Loss 0.06429427046605084\n",
      "Epoch 70, Training Loss 0.06499527833041023\n",
      "Epoch 70, Training Loss 0.06548105382248569\n",
      "Epoch 70, Training Loss 0.06577731880461773\n",
      "Epoch 70, Training Loss 0.06613606102097674\n",
      "Epoch 70, Training Loss 0.06665242528137953\n",
      "Epoch 70, Training Loss 0.06710392291969655\n",
      "Epoch 70, Training Loss 0.06751327214719695\n",
      "Epoch 70, Training Loss 0.06783366254757127\n",
      "Epoch 70, Training Loss 0.0682391247824025\n",
      "Epoch 70, Training Loss 0.06855292755472081\n",
      "Epoch 70, Training Loss 0.06894584083953477\n",
      "Epoch 70, Training Loss 0.06934534226689497\n",
      "Epoch 70, Training Loss 0.06987302641734443\n",
      "Epoch 70, Training Loss 0.07049622773514379\n",
      "Epoch 70, Training Loss 0.07117040756413394\n",
      "Epoch 70, Training Loss 0.07182638526267712\n",
      "Epoch 70, Training Loss 0.07224349730917255\n",
      "Epoch 70, Training Loss 0.07276428683334604\n",
      "Epoch 70, Training Loss 0.073198359450111\n",
      "Epoch 70, Training Loss 0.07364071227248063\n",
      "Epoch 70, Training Loss 0.0742804137108576\n",
      "Epoch 70, Training Loss 0.07473256745759178\n",
      "Epoch 70, Training Loss 0.07518670789878387\n",
      "Epoch 70, Training Loss 0.07568142096252392\n",
      "Epoch 70, Training Loss 0.07615985113489049\n",
      "Epoch 70, Training Loss 0.07649327997508866\n",
      "Epoch 70, Training Loss 0.07690291049535317\n",
      "Epoch 70, Training Loss 0.07723261778007078\n",
      "Epoch 70, Training Loss 0.0776519606561612\n",
      "Epoch 70, Training Loss 0.07810904958363994\n",
      "Epoch 70, Training Loss 0.07854605639529655\n",
      "Epoch 70, Training Loss 0.0790881383830629\n",
      "Epoch 70, Training Loss 0.07949352515932849\n",
      "Epoch 70, Training Loss 0.08005555301828457\n",
      "Epoch 70, Training Loss 0.08060493867110718\n",
      "Epoch 70, Training Loss 0.08120125604560004\n",
      "Epoch 70, Training Loss 0.08172378542325685\n",
      "Epoch 70, Training Loss 0.0822171174809146\n",
      "Epoch 70, Training Loss 0.0826247153074845\n",
      "Epoch 70, Training Loss 0.0830745074297766\n",
      "Epoch 70, Training Loss 0.08359948475190136\n",
      "Epoch 70, Training Loss 0.08396349889238167\n",
      "Epoch 70, Training Loss 0.08452447158906161\n",
      "Epoch 70, Training Loss 0.0850440950878441\n",
      "Epoch 70, Training Loss 0.08545208636604612\n",
      "Epoch 70, Training Loss 0.08609137308719518\n",
      "Epoch 70, Training Loss 0.08651911777913418\n",
      "Epoch 70, Training Loss 0.08689162882087785\n",
      "Epoch 70, Training Loss 0.08742883058307725\n",
      "Epoch 70, Training Loss 0.0878317287892027\n",
      "Epoch 70, Training Loss 0.08861189825303109\n",
      "Epoch 70, Training Loss 0.0889925327904694\n",
      "Epoch 70, Training Loss 0.08953956996693331\n",
      "Epoch 70, Training Loss 0.08989203612670264\n",
      "Epoch 70, Training Loss 0.0902549836717908\n",
      "Epoch 70, Training Loss 0.09066977803511998\n",
      "Epoch 70, Training Loss 0.09092532300278354\n",
      "Epoch 70, Training Loss 0.0913132339563516\n",
      "Epoch 70, Training Loss 0.09175004720535425\n",
      "Epoch 70, Training Loss 0.09230813201126235\n",
      "Epoch 70, Training Loss 0.09274168758441115\n",
      "Epoch 70, Training Loss 0.0930833468199386\n",
      "Epoch 70, Training Loss 0.09352041559908396\n",
      "Epoch 70, Training Loss 0.09376376293732992\n",
      "Epoch 70, Training Loss 0.09416362546060396\n",
      "Epoch 70, Training Loss 0.09449933215861431\n",
      "Epoch 70, Training Loss 0.09478041251449634\n",
      "Epoch 70, Training Loss 0.09525711803942385\n",
      "Epoch 70, Training Loss 0.0956300894165283\n",
      "Epoch 70, Training Loss 0.09628688946099537\n",
      "Epoch 70, Training Loss 0.09670696927763311\n",
      "Epoch 70, Training Loss 0.0971176924226839\n",
      "Epoch 70, Training Loss 0.09750116622204061\n",
      "Epoch 70, Training Loss 0.09806946666954118\n",
      "Epoch 70, Training Loss 0.09839104649508396\n",
      "Epoch 70, Training Loss 0.09882838566742284\n",
      "Epoch 70, Training Loss 0.09938775254484943\n",
      "Epoch 70, Training Loss 0.099801780965627\n",
      "Epoch 70, Training Loss 0.10014042608878192\n",
      "Epoch 70, Training Loss 0.10079874810965164\n",
      "Epoch 70, Training Loss 0.10144717469239783\n",
      "Epoch 70, Training Loss 0.10197863485807043\n",
      "Epoch 70, Training Loss 0.10242700942641939\n",
      "Epoch 70, Training Loss 0.10281862772029379\n",
      "Epoch 70, Training Loss 0.10317323065322379\n",
      "Epoch 70, Training Loss 0.1034923728812686\n",
      "Epoch 70, Training Loss 0.10386442507395659\n",
      "Epoch 70, Training Loss 0.1044470800272644\n",
      "Epoch 70, Training Loss 0.10492116428168534\n",
      "Epoch 70, Training Loss 0.1053494153844426\n",
      "Epoch 70, Training Loss 0.10561417837810638\n",
      "Epoch 70, Training Loss 0.10614705335377428\n",
      "Epoch 70, Training Loss 0.10661602681478881\n",
      "Epoch 70, Training Loss 0.10717722908843813\n",
      "Epoch 70, Training Loss 0.10748410331623634\n",
      "Epoch 70, Training Loss 0.10789772887211627\n",
      "Epoch 70, Training Loss 0.108458177817752\n",
      "Epoch 70, Training Loss 0.10885603584901755\n",
      "Epoch 70, Training Loss 0.10921054636425984\n",
      "Epoch 70, Training Loss 0.10952591785537008\n",
      "Epoch 70, Training Loss 0.1098431652730993\n",
      "Epoch 70, Training Loss 0.11018834003935689\n",
      "Epoch 70, Training Loss 0.11086912630387889\n",
      "Epoch 70, Training Loss 0.11119237677444277\n",
      "Epoch 70, Training Loss 0.11161195181900888\n",
      "Epoch 70, Training Loss 0.11197543691110123\n",
      "Epoch 70, Training Loss 0.11271222997123323\n",
      "Epoch 70, Training Loss 0.11307285876606432\n",
      "Epoch 70, Training Loss 0.11342766044466086\n",
      "Epoch 70, Training Loss 0.11387019688287355\n",
      "Epoch 70, Training Loss 0.11423355213287846\n",
      "Epoch 70, Training Loss 0.11468646141802868\n",
      "Epoch 70, Training Loss 0.11516952558475382\n",
      "Epoch 70, Training Loss 0.11569302803491388\n",
      "Epoch 70, Training Loss 0.11626458471007359\n",
      "Epoch 70, Training Loss 0.1166727466084768\n",
      "Epoch 70, Training Loss 0.11706736735294541\n",
      "Epoch 70, Training Loss 0.11760009099226779\n",
      "Epoch 70, Training Loss 0.11797114747488285\n",
      "Epoch 70, Training Loss 0.11826989814982085\n",
      "Epoch 70, Training Loss 0.11871579394239903\n",
      "Epoch 70, Training Loss 0.11914099164097511\n",
      "Epoch 70, Training Loss 0.11961720100677836\n",
      "Epoch 70, Training Loss 0.12008482585553928\n",
      "Epoch 70, Training Loss 0.12047676214248018\n",
      "Epoch 70, Training Loss 0.12104336938361072\n",
      "Epoch 70, Training Loss 0.12123420139026764\n",
      "Epoch 70, Training Loss 0.12153318732062264\n",
      "Epoch 70, Training Loss 0.12197692004387337\n",
      "Epoch 70, Training Loss 0.12261981337957675\n",
      "Epoch 70, Training Loss 0.12336787949209019\n",
      "Epoch 70, Training Loss 0.1236798864839327\n",
      "Epoch 70, Training Loss 0.12397630663250414\n",
      "Epoch 70, Training Loss 0.12437244901038191\n",
      "Epoch 70, Training Loss 0.12498240891243796\n",
      "Epoch 70, Training Loss 0.1253737143200377\n",
      "Epoch 70, Training Loss 0.12588326432897\n",
      "Epoch 70, Training Loss 0.12624234648997826\n",
      "Epoch 70, Training Loss 0.12666563352431787\n",
      "Epoch 70, Training Loss 0.1271430594117745\n",
      "Epoch 70, Training Loss 0.12765355821689375\n",
      "Epoch 70, Training Loss 0.12794571082152978\n",
      "Epoch 70, Training Loss 0.12856917151862093\n",
      "Epoch 70, Training Loss 0.12909065247954005\n",
      "Epoch 70, Training Loss 0.12950825378717973\n",
      "Epoch 70, Training Loss 0.1298902225311455\n",
      "Epoch 70, Training Loss 0.13061232830557373\n",
      "Epoch 70, Training Loss 0.1310618445086662\n",
      "Epoch 70, Training Loss 0.13150254974279868\n",
      "Epoch 70, Training Loss 0.13202302992496345\n",
      "Epoch 70, Training Loss 0.1323601811209603\n",
      "Epoch 70, Training Loss 0.1327669706262286\n",
      "Epoch 70, Training Loss 0.13317072204769115\n",
      "Epoch 70, Training Loss 0.13368491180565045\n",
      "Epoch 70, Training Loss 0.13425191909151005\n",
      "Epoch 70, Training Loss 0.13473019876595957\n",
      "Epoch 70, Training Loss 0.13516053385899196\n",
      "Epoch 70, Training Loss 0.13544071947827058\n",
      "Epoch 70, Training Loss 0.13599318730861634\n",
      "Epoch 70, Training Loss 0.13639720272072745\n",
      "Epoch 70, Training Loss 0.13702155276180228\n",
      "Epoch 70, Training Loss 0.13759443346801623\n",
      "Epoch 70, Training Loss 0.1380193963684999\n",
      "Epoch 70, Training Loss 0.13850719441690712\n",
      "Epoch 70, Training Loss 0.13889704469372244\n",
      "Epoch 70, Training Loss 0.13919832273517424\n",
      "Epoch 70, Training Loss 0.13968119398712198\n",
      "Epoch 70, Training Loss 0.139968923271617\n",
      "Epoch 70, Training Loss 0.14039078524426732\n",
      "Epoch 70, Training Loss 0.14093697915220504\n",
      "Epoch 70, Training Loss 0.14141586051343957\n",
      "Epoch 70, Training Loss 0.14183673912378222\n",
      "Epoch 70, Training Loss 0.1424357709486771\n",
      "Epoch 70, Training Loss 0.14274144586166151\n",
      "Epoch 70, Training Loss 0.14315616076483445\n",
      "Epoch 70, Training Loss 0.14357855746432033\n",
      "Epoch 70, Training Loss 0.14405934903246667\n",
      "Epoch 70, Training Loss 0.1445437023211318\n",
      "Epoch 70, Training Loss 0.14488488197555321\n",
      "Epoch 70, Training Loss 0.14529854968152084\n",
      "Epoch 70, Training Loss 0.14595234319758232\n",
      "Epoch 70, Training Loss 0.14641205838802832\n",
      "Epoch 70, Training Loss 0.1469358065358513\n",
      "Epoch 70, Training Loss 0.14744854760368156\n",
      "Epoch 70, Training Loss 0.1478919339416277\n",
      "Epoch 70, Training Loss 0.14828446227342576\n",
      "Epoch 70, Training Loss 0.14875528256377907\n",
      "Epoch 70, Training Loss 0.14917540075757618\n",
      "Epoch 70, Training Loss 0.1495575144734529\n",
      "Epoch 70, Training Loss 0.14996440687676524\n",
      "Epoch 70, Training Loss 0.15047224793973787\n",
      "Epoch 70, Training Loss 0.151001295984706\n",
      "Epoch 70, Training Loss 0.1516697477463566\n",
      "Epoch 70, Training Loss 0.15212762548261896\n",
      "Epoch 70, Training Loss 0.15247539145028804\n",
      "Epoch 70, Training Loss 0.1528170403960111\n",
      "Epoch 70, Training Loss 0.1533422556793903\n",
      "Epoch 70, Training Loss 0.15380100793469592\n",
      "Epoch 70, Training Loss 0.15403972175496314\n",
      "Epoch 70, Training Loss 0.15432318526765573\n",
      "Epoch 70, Training Loss 0.15463492236173976\n",
      "Epoch 70, Training Loss 0.1549357846188728\n",
      "Epoch 70, Training Loss 0.15532656246438967\n",
      "Epoch 70, Training Loss 0.1560092989136191\n",
      "Epoch 70, Training Loss 0.15641593422426286\n",
      "Epoch 70, Training Loss 0.1568269917879568\n",
      "Epoch 70, Training Loss 0.15732918039459706\n",
      "Epoch 70, Training Loss 0.15795472915977468\n",
      "Epoch 70, Training Loss 0.15842901848618637\n",
      "Epoch 70, Training Loss 0.15882450140193297\n",
      "Epoch 70, Training Loss 0.15920056212131325\n",
      "Epoch 70, Training Loss 0.15950406234130224\n",
      "Epoch 70, Training Loss 0.1599033380408421\n",
      "Epoch 70, Training Loss 0.1601076397444586\n",
      "Epoch 70, Training Loss 0.16075277442822372\n",
      "Epoch 70, Training Loss 0.16113827890142454\n",
      "Epoch 70, Training Loss 0.16171125092012498\n",
      "Epoch 70, Training Loss 0.16205914295695323\n",
      "Epoch 70, Training Loss 0.16245589106131697\n",
      "Epoch 70, Training Loss 0.16303448352362493\n",
      "Epoch 70, Training Loss 0.16360626466896222\n",
      "Epoch 70, Training Loss 0.16395680720696365\n",
      "Epoch 70, Training Loss 0.1644524009636296\n",
      "Epoch 70, Training Loss 0.16493275407177713\n",
      "Epoch 70, Training Loss 0.1652554737027649\n",
      "Epoch 70, Training Loss 0.16561763121953707\n",
      "Epoch 70, Training Loss 0.16620074052487493\n",
      "Epoch 70, Training Loss 0.16696272188287867\n",
      "Epoch 70, Training Loss 0.167282062951866\n",
      "Epoch 70, Training Loss 0.16765866175179592\n",
      "Epoch 70, Training Loss 0.16826758028753577\n",
      "Epoch 70, Training Loss 0.16860636489470596\n",
      "Epoch 70, Training Loss 0.16912955624977952\n",
      "Epoch 70, Training Loss 0.16960935203163216\n",
      "Epoch 70, Training Loss 0.17009482915749025\n",
      "Epoch 70, Training Loss 0.17050992779414673\n",
      "Epoch 70, Training Loss 0.17085760355453053\n",
      "Epoch 70, Training Loss 0.17122049987925897\n",
      "Epoch 70, Training Loss 0.1715184035508529\n",
      "Epoch 70, Training Loss 0.17217688975126846\n",
      "Epoch 70, Training Loss 0.17259648663308613\n",
      "Epoch 70, Training Loss 0.17295461599631687\n",
      "Epoch 70, Training Loss 0.1733239899053598\n",
      "Epoch 70, Training Loss 0.17374367223066442\n",
      "Epoch 70, Training Loss 0.17436286875658938\n",
      "Epoch 70, Training Loss 0.17478767609047463\n",
      "Epoch 70, Training Loss 0.17504688335196747\n",
      "Epoch 70, Training Loss 0.17564512834981885\n",
      "Epoch 70, Training Loss 0.17624325733965315\n",
      "Epoch 70, Training Loss 0.17689669429493682\n",
      "Epoch 70, Training Loss 0.17731156960472733\n",
      "Epoch 70, Training Loss 0.1778327610410388\n",
      "Epoch 70, Training Loss 0.17812330420593472\n",
      "Epoch 70, Training Loss 0.17864971474537153\n",
      "Epoch 70, Training Loss 0.179188816882003\n",
      "Epoch 70, Training Loss 0.17990844982588078\n",
      "Epoch 70, Training Loss 0.180373775970448\n",
      "Epoch 70, Training Loss 0.18090756147947457\n",
      "Epoch 70, Training Loss 0.1812197925413356\n",
      "Epoch 70, Training Loss 0.18166490703287635\n",
      "Epoch 70, Training Loss 0.18212615487063327\n",
      "Epoch 70, Training Loss 0.18257912665681766\n",
      "Epoch 70, Training Loss 0.18301474167715254\n",
      "Epoch 70, Training Loss 0.18349579723594744\n",
      "Epoch 70, Training Loss 0.1840548879655121\n",
      "Epoch 70, Training Loss 0.18461562929403447\n",
      "Epoch 70, Training Loss 0.184927779295103\n",
      "Epoch 70, Training Loss 0.18552443070713517\n",
      "Epoch 70, Training Loss 0.18587453055488484\n",
      "Epoch 70, Training Loss 0.18633417097275215\n",
      "Epoch 70, Training Loss 0.18673670427192507\n",
      "Epoch 70, Training Loss 0.18705605867954775\n",
      "Epoch 70, Training Loss 0.1874927765763629\n",
      "Epoch 70, Training Loss 0.18801485224986625\n",
      "Epoch 70, Training Loss 0.1883555792291146\n",
      "Epoch 70, Training Loss 0.1888817875167293\n",
      "Epoch 70, Training Loss 0.1892499694090975\n",
      "Epoch 70, Training Loss 0.18957846554572624\n",
      "Epoch 70, Training Loss 0.18990350857643826\n",
      "Epoch 70, Training Loss 0.19024496125366985\n",
      "Epoch 70, Training Loss 0.1907223768513221\n",
      "Epoch 70, Training Loss 0.19116160111582797\n",
      "Epoch 70, Training Loss 0.19174788261542236\n",
      "Epoch 70, Training Loss 0.19206801100688822\n",
      "Epoch 70, Training Loss 0.19243193745536877\n",
      "Epoch 70, Training Loss 0.1928146699695941\n",
      "Epoch 70, Training Loss 0.19338698591798772\n",
      "Epoch 70, Training Loss 0.1937868394472105\n",
      "Epoch 70, Training Loss 0.1941770721998666\n",
      "Epoch 70, Training Loss 0.19479912029736487\n",
      "Epoch 70, Training Loss 0.19519110208810747\n",
      "Epoch 70, Training Loss 0.19592672172943346\n",
      "Epoch 70, Training Loss 0.19610520795254452\n",
      "Epoch 70, Training Loss 0.19637174515620523\n",
      "Epoch 70, Training Loss 0.1968001405067761\n",
      "Epoch 70, Training Loss 0.19741000818169635\n",
      "Epoch 70, Training Loss 0.1980314930839002\n",
      "Epoch 70, Training Loss 0.19863862622424464\n",
      "Epoch 70, Training Loss 0.19907424684680636\n",
      "Epoch 70, Training Loss 0.19958866732504665\n",
      "Epoch 70, Training Loss 0.20010610606969165\n",
      "Epoch 70, Training Loss 0.20046007305459904\n",
      "Epoch 70, Training Loss 0.2009213759999751\n",
      "Epoch 70, Training Loss 0.20129540558818662\n",
      "Epoch 70, Training Loss 0.20175538648424857\n",
      "Epoch 70, Training Loss 0.20236123576188636\n",
      "Epoch 70, Training Loss 0.20297686877610432\n",
      "Epoch 70, Training Loss 0.2034742920218831\n",
      "Epoch 70, Training Loss 0.2038492511605363\n",
      "Epoch 70, Training Loss 0.2044544924631753\n",
      "Epoch 70, Training Loss 0.2049549873299001\n",
      "Epoch 70, Training Loss 0.20562989872587306\n",
      "Epoch 70, Training Loss 0.20602820760301313\n",
      "Epoch 70, Training Loss 0.20651999104510793\n",
      "Epoch 70, Training Loss 0.20698566300332394\n",
      "Epoch 70, Training Loss 0.20758446597534677\n",
      "Epoch 70, Training Loss 0.2080502408697172\n",
      "Epoch 70, Training Loss 0.20865072591987718\n",
      "Epoch 70, Training Loss 0.20905067099024877\n",
      "Epoch 70, Training Loss 0.2093854795788865\n",
      "Epoch 70, Training Loss 0.20993144261410168\n",
      "Epoch 70, Training Loss 0.21043802481478133\n",
      "Epoch 70, Training Loss 0.2108560817702042\n",
      "Epoch 70, Training Loss 0.2113981565551075\n",
      "Epoch 70, Training Loss 0.21198042182971144\n",
      "Epoch 70, Training Loss 0.21227787073006105\n",
      "Epoch 70, Training Loss 0.21283259610538288\n",
      "Epoch 70, Training Loss 0.21338881704660936\n",
      "Epoch 70, Training Loss 0.21422078797731864\n",
      "Epoch 70, Training Loss 0.21465429877076309\n",
      "Epoch 70, Training Loss 0.2152849467818999\n",
      "Epoch 70, Training Loss 0.21560374588307823\n",
      "Epoch 70, Training Loss 0.2159184620851446\n",
      "Epoch 70, Training Loss 0.2163445612849177\n",
      "Epoch 70, Training Loss 0.2167246362475483\n",
      "Epoch 70, Training Loss 0.21725849416630957\n",
      "Epoch 70, Training Loss 0.21790863724087206\n",
      "Epoch 70, Training Loss 0.2184851852714863\n",
      "Epoch 70, Training Loss 0.2189407184186494\n",
      "Epoch 70, Training Loss 0.21949770892291423\n",
      "Epoch 70, Training Loss 0.2198418773844114\n",
      "Epoch 70, Training Loss 0.22026805741631467\n",
      "Epoch 70, Training Loss 0.22073239913148343\n",
      "Epoch 70, Training Loss 0.22116121281976894\n",
      "Epoch 70, Training Loss 0.22175422811980747\n",
      "Epoch 70, Training Loss 0.22208206506107775\n",
      "Epoch 70, Training Loss 0.22265469077069436\n",
      "Epoch 70, Training Loss 0.2231333514346796\n",
      "Epoch 70, Training Loss 0.2236530964674852\n",
      "Epoch 70, Training Loss 0.22417394906434868\n",
      "Epoch 70, Training Loss 0.22450001802667022\n",
      "Epoch 70, Training Loss 0.2250187161671536\n",
      "Epoch 70, Training Loss 0.22538420689456604\n",
      "Epoch 70, Training Loss 0.22570784083184073\n",
      "Epoch 70, Training Loss 0.22617002519424004\n",
      "Epoch 70, Training Loss 0.2268709458620347\n",
      "Epoch 70, Training Loss 0.2273594824135151\n",
      "Epoch 70, Training Loss 0.22815109116723165\n",
      "Epoch 70, Training Loss 0.22870441731970634\n",
      "Epoch 70, Training Loss 0.22922481785116294\n",
      "Epoch 70, Training Loss 0.22956220906637514\n",
      "Epoch 70, Training Loss 0.2302292968572863\n",
      "Epoch 70, Training Loss 0.23083633788482613\n",
      "Epoch 70, Training Loss 0.23143445963368697\n",
      "Epoch 70, Training Loss 0.23211999833964936\n",
      "Epoch 70, Training Loss 0.23242072564790317\n",
      "Epoch 70, Training Loss 0.23300603956288998\n",
      "Epoch 70, Training Loss 0.23364840651793248\n",
      "Epoch 70, Training Loss 0.23424001729778013\n",
      "Epoch 70, Training Loss 0.23465399209724364\n",
      "Epoch 70, Training Loss 0.23520434137119356\n",
      "Epoch 70, Training Loss 0.23541239938696326\n",
      "Epoch 70, Training Loss 0.23582222857667356\n",
      "Epoch 70, Training Loss 0.2364260098703987\n",
      "Epoch 70, Training Loss 0.23695133613122393\n",
      "Epoch 70, Training Loss 0.23728929374300306\n",
      "Epoch 70, Training Loss 0.23774497544445344\n",
      "Epoch 70, Training Loss 0.23811571374345009\n",
      "Epoch 70, Training Loss 0.23873696697261326\n",
      "Epoch 70, Training Loss 0.2391835655969427\n",
      "Epoch 70, Training Loss 0.2395913388837329\n",
      "Epoch 70, Training Loss 0.23988031119565525\n",
      "Epoch 70, Training Loss 0.24024603931266633\n",
      "Epoch 70, Training Loss 0.24060239242699444\n",
      "Epoch 70, Training Loss 0.2409752604106198\n",
      "Epoch 70, Training Loss 0.24117544342947128\n",
      "Epoch 70, Training Loss 0.24147158863065799\n",
      "Epoch 70, Training Loss 0.24184294911982762\n",
      "Epoch 70, Training Loss 0.242214522264955\n",
      "Epoch 70, Training Loss 0.24266933794597836\n",
      "Epoch 70, Training Loss 0.24318534191078542\n",
      "Epoch 70, Training Loss 0.24382388517451103\n",
      "Epoch 70, Training Loss 0.24421709133764666\n",
      "Epoch 70, Training Loss 0.24463996411208302\n",
      "Epoch 70, Training Loss 0.24494042247533798\n",
      "Epoch 70, Training Loss 0.24529299797380671\n",
      "Epoch 70, Training Loss 0.24573775700977085\n",
      "Epoch 70, Training Loss 0.24618461272676886\n",
      "Epoch 70, Training Loss 0.2468959651220485\n",
      "Epoch 70, Training Loss 0.24733525836635428\n",
      "Epoch 70, Training Loss 0.2477224470137635\n",
      "Epoch 70, Training Loss 0.24812439352731266\n",
      "Epoch 70, Training Loss 0.24852867437826703\n",
      "Epoch 70, Training Loss 0.2490595246253111\n",
      "Epoch 70, Training Loss 0.2493993665670495\n",
      "Epoch 70, Training Loss 0.24981754410373586\n",
      "Epoch 70, Training Loss 0.25011583678710186\n",
      "Epoch 70, Training Loss 0.25043972293891564\n",
      "Epoch 70, Training Loss 0.2508665155571745\n",
      "Epoch 70, Training Loss 0.2511449750236538\n",
      "Epoch 70, Training Loss 0.2516992389584136\n",
      "Epoch 70, Training Loss 0.25244510648271923\n",
      "Epoch 70, Training Loss 0.25310200363245156\n",
      "Epoch 70, Training Loss 0.25357767232619893\n",
      "Epoch 70, Training Loss 0.2541325126043366\n",
      "Epoch 70, Training Loss 0.2547523873045926\n",
      "Epoch 70, Training Loss 0.255047877369177\n",
      "Epoch 70, Training Loss 0.25553525140142197\n",
      "Epoch 70, Training Loss 0.2560846832440332\n",
      "Epoch 70, Training Loss 0.25662979618896303\n",
      "Epoch 70, Training Loss 0.25705740665612015\n",
      "Epoch 70, Training Loss 0.25751743980152225\n",
      "Epoch 70, Training Loss 0.25802548803255687\n",
      "Epoch 70, Training Loss 0.2584715284159421\n",
      "Epoch 70, Training Loss 0.25893758841411535\n",
      "Epoch 70, Training Loss 0.25942357679081085\n",
      "Epoch 70, Training Loss 0.2597314819045689\n",
      "Epoch 70, Training Loss 0.2602199552309178\n",
      "Epoch 70, Training Loss 0.26064251271812505\n",
      "Epoch 70, Training Loss 0.2610564656041162\n",
      "Epoch 70, Training Loss 0.26160672131706686\n",
      "Epoch 70, Training Loss 0.26204982255121023\n",
      "Epoch 70, Training Loss 0.262518950473622\n",
      "Epoch 70, Training Loss 0.26292435367546424\n",
      "Epoch 70, Training Loss 0.2633813870379992\n",
      "Epoch 70, Training Loss 0.26413268117648564\n",
      "Epoch 70, Training Loss 0.26440736043559926\n",
      "Epoch 70, Training Loss 0.2651286934076063\n",
      "Epoch 70, Training Loss 0.26547742490192205\n",
      "Epoch 70, Training Loss 0.2659447546619588\n",
      "Epoch 70, Training Loss 0.266403809932949\n",
      "Epoch 70, Training Loss 0.2669074449430951\n",
      "Epoch 70, Training Loss 0.26737823127709387\n",
      "Epoch 70, Training Loss 0.2680538401693639\n",
      "Epoch 70, Training Loss 0.2687095176342808\n",
      "Epoch 70, Training Loss 0.2693754640191107\n",
      "Epoch 70, Training Loss 0.2699806039481212\n",
      "Epoch 70, Training Loss 0.2703157966513463\n",
      "Epoch 70, Training Loss 0.27094009829223004\n",
      "Epoch 70, Training Loss 0.27135184142367\n",
      "Epoch 70, Training Loss 0.2718354983974601\n",
      "Epoch 70, Training Loss 0.2723077137947387\n",
      "Epoch 70, Training Loss 0.27269072706818276\n",
      "Epoch 70, Training Loss 0.27316437697852664\n",
      "Epoch 70, Training Loss 0.2735420543786205\n",
      "Epoch 70, Training Loss 0.27400156708858203\n",
      "Epoch 70, Training Loss 0.27451473262989917\n",
      "Epoch 70, Training Loss 0.27486448502525346\n",
      "Epoch 70, Training Loss 0.2755727630556392\n",
      "Epoch 70, Training Loss 0.2759545603790856\n",
      "Epoch 70, Training Loss 0.27635597048894217\n",
      "Epoch 70, Training Loss 0.27682907492532144\n",
      "Epoch 70, Training Loss 0.2772399644603205\n",
      "Epoch 70, Training Loss 0.2776496891513505\n",
      "Epoch 70, Training Loss 0.27810409091546406\n",
      "Epoch 70, Training Loss 0.27843586565054895\n",
      "Epoch 70, Training Loss 0.2787248989009796\n",
      "Epoch 70, Training Loss 0.27903021999713407\n",
      "Epoch 70, Training Loss 0.27923578930937726\n",
      "Epoch 70, Training Loss 0.2797919911573\n",
      "Epoch 70, Training Loss 0.28040344060381966\n",
      "Epoch 70, Training Loss 0.28088192592191574\n",
      "Epoch 70, Training Loss 0.28143653220227915\n",
      "Epoch 70, Training Loss 0.2817741904569709\n",
      "Epoch 70, Training Loss 0.28224265884103067\n",
      "Epoch 70, Training Loss 0.2828474242973815\n",
      "Epoch 70, Training Loss 0.283377281425859\n",
      "Epoch 70, Training Loss 0.2837709256678896\n",
      "Epoch 70, Training Loss 0.28420343442493695\n",
      "Epoch 70, Training Loss 0.28447558664147504\n",
      "Epoch 70, Training Loss 0.28486362000560517\n",
      "Epoch 70, Training Loss 0.2854856642539544\n",
      "Epoch 70, Training Loss 0.2859372944783067\n",
      "Epoch 70, Training Loss 0.28645178816659983\n",
      "Epoch 70, Training Loss 0.2870004374124205\n",
      "Epoch 70, Training Loss 0.28747877661529403\n",
      "Epoch 70, Training Loss 0.28795969505291763\n",
      "Epoch 70, Training Loss 0.28850280152409885\n",
      "Epoch 70, Training Loss 0.2887868436477373\n",
      "Epoch 70, Training Loss 0.28946419830060066\n",
      "Epoch 70, Training Loss 0.28985641115461774\n",
      "Epoch 70, Training Loss 0.2902865963976097\n",
      "Epoch 70, Training Loss 0.29068782155775963\n",
      "Epoch 70, Training Loss 0.29093452499193306\n",
      "Epoch 70, Training Loss 0.2914502541427417\n",
      "Epoch 70, Training Loss 0.29197435321100534\n",
      "Epoch 70, Training Loss 0.2926204805941228\n",
      "Epoch 70, Training Loss 0.29290957682196744\n",
      "Epoch 70, Training Loss 0.29330239265852265\n",
      "Epoch 70, Training Loss 0.2935698095261288\n",
      "Epoch 70, Training Loss 0.29400147592929926\n",
      "Epoch 70, Training Loss 0.2943750086723996\n",
      "Epoch 70, Training Loss 0.29488669404440826\n",
      "Epoch 70, Training Loss 0.29520197565217154\n",
      "Epoch 70, Training Loss 0.2956461578874332\n",
      "Epoch 70, Training Loss 0.29601201516054476\n",
      "Epoch 70, Training Loss 0.2965988507851615\n",
      "Epoch 70, Training Loss 0.297216855713626\n",
      "Epoch 70, Training Loss 0.2977772756953678\n",
      "Epoch 70, Training Loss 0.29829764482386584\n",
      "Epoch 70, Training Loss 0.29877721388702805\n",
      "Epoch 70, Training Loss 0.2993240797954142\n",
      "Epoch 70, Training Loss 0.29972580359186357\n",
      "Epoch 70, Training Loss 0.3000890225972361\n",
      "Epoch 70, Training Loss 0.3005869398298471\n",
      "Epoch 70, Training Loss 0.3009921397127764\n",
      "Epoch 70, Training Loss 0.30144081847823184\n",
      "Epoch 70, Training Loss 0.30178468140875897\n",
      "Epoch 70, Training Loss 0.30219855513947697\n",
      "Epoch 70, Training Loss 0.3026243125653023\n",
      "Epoch 70, Training Loss 0.302925381361676\n",
      "Epoch 70, Training Loss 0.3033491805233919\n",
      "Epoch 70, Training Loss 0.3037341369692322\n",
      "Epoch 70, Training Loss 0.3041535941764827\n",
      "Epoch 70, Training Loss 0.30462353381202045\n",
      "Epoch 70, Training Loss 0.3051388456922053\n",
      "Epoch 70, Training Loss 0.3058001545765211\n",
      "Epoch 70, Training Loss 0.30636712737248073\n",
      "Epoch 70, Training Loss 0.3071404541926006\n",
      "Epoch 70, Training Loss 0.3077064369950453\n",
      "Epoch 70, Training Loss 0.3080554459329761\n",
      "Epoch 70, Training Loss 0.30852326342974173\n",
      "Epoch 70, Training Loss 0.30897663388868124\n",
      "Epoch 70, Training Loss 0.30943946483190105\n",
      "Epoch 70, Training Loss 0.3100192177935939\n",
      "Epoch 70, Training Loss 0.31041535864705627\n",
      "Epoch 70, Training Loss 0.311127694747637\n",
      "Epoch 70, Training Loss 0.31138647664004887\n",
      "Epoch 70, Training Loss 0.31184062847624655\n",
      "Epoch 70, Training Loss 0.3125882688957407\n",
      "Epoch 70, Training Loss 0.31318295617466385\n",
      "Epoch 70, Training Loss 0.31381784103181964\n",
      "Epoch 70, Training Loss 0.31425407934752875\n",
      "Epoch 70, Training Loss 0.3144968688259344\n",
      "Epoch 70, Training Loss 0.31500489823043804\n",
      "Epoch 70, Training Loss 0.31546701014499223\n",
      "Epoch 70, Training Loss 0.3158902035421118\n",
      "Epoch 70, Training Loss 0.3162396004056687\n",
      "Epoch 70, Training Loss 0.31678844367146797\n",
      "Epoch 70, Training Loss 0.3172152704747437\n",
      "Epoch 70, Training Loss 0.31778056690912415\n",
      "Epoch 70, Training Loss 0.3180065583961699\n",
      "Epoch 70, Training Loss 0.3185495363591272\n",
      "Epoch 70, Training Loss 0.31893960470357513\n",
      "Epoch 70, Training Loss 0.31936388352261785\n",
      "Epoch 70, Training Loss 0.3198837257369095\n",
      "Epoch 70, Training Loss 0.3203549778560543\n",
      "Epoch 70, Training Loss 0.32077948213614466\n",
      "Epoch 70, Training Loss 0.3214232440075606\n",
      "Epoch 70, Training Loss 0.32211319490543106\n",
      "Epoch 70, Training Loss 0.3227722942257476\n",
      "Epoch 70, Training Loss 0.3231698303004665\n",
      "Epoch 70, Training Loss 0.3237292387182146\n",
      "Epoch 70, Training Loss 0.324203814329851\n",
      "Epoch 70, Training Loss 0.3247298607245431\n",
      "Epoch 70, Training Loss 0.3252560695075928\n",
      "Epoch 70, Training Loss 0.32567044740061624\n",
      "Epoch 70, Training Loss 0.32618341072822166\n",
      "Epoch 70, Training Loss 0.3266998700359288\n",
      "Epoch 70, Training Loss 0.32725828114296773\n",
      "Epoch 70, Training Loss 0.3277950649866668\n",
      "Epoch 70, Training Loss 0.32824178819385025\n",
      "Epoch 70, Training Loss 0.32873264890726267\n",
      "Epoch 70, Training Loss 0.3293228275749994\n",
      "Epoch 70, Training Loss 0.329785513813081\n",
      "Epoch 70, Training Loss 0.3302600342408775\n",
      "Epoch 70, Training Loss 0.330785863063372\n",
      "Epoch 70, Training Loss 0.3313410364643997\n",
      "Epoch 70, Training Loss 0.33178240318051383\n",
      "Epoch 70, Training Loss 0.33227450522544133\n",
      "Epoch 70, Training Loss 0.33270768937476153\n",
      "Epoch 70, Training Loss 0.3330298473729807\n",
      "Epoch 70, Training Loss 0.3333815731622679\n",
      "Epoch 70, Training Loss 0.33375606617278153\n",
      "Epoch 70, Training Loss 0.33398307111028513\n",
      "Epoch 70, Training Loss 0.3344139918456297\n",
      "Epoch 70, Training Loss 0.3350735927748558\n",
      "Epoch 70, Training Loss 0.33575371210761085\n",
      "Epoch 70, Training Loss 0.33623924506518543\n",
      "Epoch 70, Training Loss 0.3368186969548235\n",
      "Epoch 70, Training Loss 0.3372950137538068\n",
      "Epoch 70, Training Loss 0.33790380987898466\n",
      "Epoch 70, Training Loss 0.33849660097561834\n",
      "Epoch 70, Training Loss 0.33894738396796426\n",
      "Epoch 70, Training Loss 0.33957517419553473\n",
      "Epoch 70, Training Loss 0.3399376715807354\n",
      "Epoch 70, Training Loss 0.3404471028758132\n",
      "Epoch 70, Training Loss 0.34085094957324247\n",
      "Epoch 70, Training Loss 0.3413287088503618\n",
      "Epoch 70, Training Loss 0.34179323372404896\n",
      "Epoch 70, Training Loss 0.3422738182384645\n",
      "Epoch 70, Training Loss 0.3427824463952533\n",
      "Epoch 70, Training Loss 0.3432772660156345\n",
      "Epoch 70, Training Loss 0.3436975221309211\n",
      "Epoch 70, Training Loss 0.3439930412546753\n",
      "Epoch 70, Training Loss 0.3445145695105843\n",
      "Epoch 70, Training Loss 0.3450828341724318\n",
      "Epoch 70, Training Loss 0.34563097876051196\n",
      "Epoch 70, Training Loss 0.34621162998401905\n",
      "Epoch 70, Training Loss 0.34677734064019244\n",
      "Epoch 70, Training Loss 0.3474009755779715\n",
      "Epoch 70, Training Loss 0.3478891105603074\n",
      "Epoch 70, Training Loss 0.34836796501560896\n",
      "Epoch 70, Training Loss 0.34870186398553726\n",
      "Epoch 70, Training Loss 0.34925313995164986\n",
      "Epoch 70, Training Loss 0.34974988014496805\n",
      "Epoch 70, Training Loss 0.35049484262381064\n",
      "Epoch 70, Training Loss 0.3508565082498219\n",
      "Epoch 70, Training Loss 0.3511909186992499\n",
      "Epoch 70, Training Loss 0.35166984083859815\n",
      "Epoch 70, Training Loss 0.35221903529161075\n",
      "Epoch 70, Training Loss 0.35280097773312913\n",
      "Epoch 70, Training Loss 0.3533264409626841\n",
      "Epoch 70, Training Loss 0.3540336267493875\n",
      "Epoch 70, Training Loss 0.354494082843861\n",
      "Epoch 70, Training Loss 0.3549569250296449\n",
      "Epoch 70, Training Loss 0.3559400605042572\n",
      "Epoch 70, Training Loss 0.3567775285533627\n",
      "Epoch 70, Training Loss 0.3572542344975045\n",
      "Epoch 70, Training Loss 0.35761518395312913\n",
      "Epoch 70, Training Loss 0.3578290994591115\n",
      "Epoch 70, Training Loss 0.3582214783600834\n",
      "Epoch 70, Training Loss 0.3589461653510018\n",
      "Epoch 70, Training Loss 0.35939960486596195\n",
      "Epoch 70, Training Loss 0.3599039523878976\n",
      "Epoch 80, Training Loss 0.0005007670511065237\n",
      "Epoch 80, Training Loss 0.0008712600335440673\n",
      "Epoch 80, Training Loss 0.0014457608504063637\n",
      "Epoch 80, Training Loss 0.0019095896759911267\n",
      "Epoch 80, Training Loss 0.002117253115872288\n",
      "Epoch 80, Training Loss 0.002558604968935632\n",
      "Epoch 80, Training Loss 0.0028829931107628376\n",
      "Epoch 80, Training Loss 0.003160443337981963\n",
      "Epoch 80, Training Loss 0.003701871313402415\n",
      "Epoch 80, Training Loss 0.00390779287994975\n",
      "Epoch 80, Training Loss 0.004163350786089592\n",
      "Epoch 80, Training Loss 0.004424242488563518\n",
      "Epoch 80, Training Loss 0.004825275115039952\n",
      "Epoch 80, Training Loss 0.005288300306900688\n",
      "Epoch 80, Training Loss 0.005612914557652096\n",
      "Epoch 80, Training Loss 0.006208196892152967\n",
      "Epoch 80, Training Loss 0.0064352840909262755\n",
      "Epoch 80, Training Loss 0.006685323727405284\n",
      "Epoch 80, Training Loss 0.007087974559010752\n",
      "Epoch 80, Training Loss 0.007625041867765929\n",
      "Epoch 80, Training Loss 0.00787131450212825\n",
      "Epoch 80, Training Loss 0.008380543774046251\n",
      "Epoch 80, Training Loss 0.008838711949565526\n",
      "Epoch 80, Training Loss 0.009317886951329459\n",
      "Epoch 80, Training Loss 0.0096898840363983\n",
      "Epoch 80, Training Loss 0.009913728403313386\n",
      "Epoch 80, Training Loss 0.01052722891273401\n",
      "Epoch 80, Training Loss 0.010945219251200976\n",
      "Epoch 80, Training Loss 0.011194146864706904\n",
      "Epoch 80, Training Loss 0.011475940094427074\n",
      "Epoch 80, Training Loss 0.011743360277636887\n",
      "Epoch 80, Training Loss 0.012148850256829616\n",
      "Epoch 80, Training Loss 0.012463897218942033\n",
      "Epoch 80, Training Loss 0.012683479663203745\n",
      "Epoch 80, Training Loss 0.012985778948687531\n",
      "Epoch 80, Training Loss 0.013296742878301675\n",
      "Epoch 80, Training Loss 0.013610248789762901\n",
      "Epoch 80, Training Loss 0.013948423592635737\n",
      "Epoch 80, Training Loss 0.014437117235130057\n",
      "Epoch 80, Training Loss 0.014902689267912179\n",
      "Epoch 80, Training Loss 0.015328464719950391\n",
      "Epoch 80, Training Loss 0.015543267073686166\n",
      "Epoch 80, Training Loss 0.015925134520244112\n",
      "Epoch 80, Training Loss 0.016383671871079204\n",
      "Epoch 80, Training Loss 0.01670138939948338\n",
      "Epoch 80, Training Loss 0.01694891091121737\n",
      "Epoch 80, Training Loss 0.017477099566965762\n",
      "Epoch 80, Training Loss 0.017981824076846432\n",
      "Epoch 80, Training Loss 0.018534295527678928\n",
      "Epoch 80, Training Loss 0.018750292718258052\n",
      "Epoch 80, Training Loss 0.019249711233331725\n",
      "Epoch 80, Training Loss 0.019620931285726444\n",
      "Epoch 80, Training Loss 0.020171449617351716\n",
      "Epoch 80, Training Loss 0.020669366163975748\n",
      "Epoch 80, Training Loss 0.021078080815427443\n",
      "Epoch 80, Training Loss 0.021388894018462248\n",
      "Epoch 80, Training Loss 0.021937562586248988\n",
      "Epoch 80, Training Loss 0.02228568919250727\n",
      "Epoch 80, Training Loss 0.02257787664909192\n",
      "Epoch 80, Training Loss 0.022927151102086773\n",
      "Epoch 80, Training Loss 0.02313497927411438\n",
      "Epoch 80, Training Loss 0.023470510550014808\n",
      "Epoch 80, Training Loss 0.023907337480646267\n",
      "Epoch 80, Training Loss 0.024186743170861395\n",
      "Epoch 80, Training Loss 0.024773060303667317\n",
      "Epoch 80, Training Loss 0.025094621645672548\n",
      "Epoch 80, Training Loss 0.025436713510309645\n",
      "Epoch 80, Training Loss 0.025960101705530415\n",
      "Epoch 80, Training Loss 0.02629919092902137\n",
      "Epoch 80, Training Loss 0.026599008542345003\n",
      "Epoch 80, Training Loss 0.027168747656942938\n",
      "Epoch 80, Training Loss 0.027561706255006667\n",
      "Epoch 80, Training Loss 0.028003732585693563\n",
      "Epoch 80, Training Loss 0.02847318097834697\n",
      "Epoch 80, Training Loss 0.02885297102772671\n",
      "Epoch 80, Training Loss 0.029326928145897664\n",
      "Epoch 80, Training Loss 0.029753579870056925\n",
      "Epoch 80, Training Loss 0.030147516190091057\n",
      "Epoch 80, Training Loss 0.030426856254220314\n",
      "Epoch 80, Training Loss 0.03077161607458768\n",
      "Epoch 80, Training Loss 0.03110606532038935\n",
      "Epoch 80, Training Loss 0.03147728434380363\n",
      "Epoch 80, Training Loss 0.03196186743810049\n",
      "Epoch 80, Training Loss 0.03238354004023935\n",
      "Epoch 80, Training Loss 0.03280543120544585\n",
      "Epoch 80, Training Loss 0.033144383002882416\n",
      "Epoch 80, Training Loss 0.03356378525495529\n",
      "Epoch 80, Training Loss 0.03383498465466073\n",
      "Epoch 80, Training Loss 0.03413762100745955\n",
      "Epoch 80, Training Loss 0.034544401137572726\n",
      "Epoch 80, Training Loss 0.03481042261242562\n",
      "Epoch 80, Training Loss 0.03524784974353697\n",
      "Epoch 80, Training Loss 0.035542818839135376\n",
      "Epoch 80, Training Loss 0.036043350291831415\n",
      "Epoch 80, Training Loss 0.03645660571963586\n",
      "Epoch 80, Training Loss 0.03679467972053591\n",
      "Epoch 80, Training Loss 0.037144090265721616\n",
      "Epoch 80, Training Loss 0.03765110860166647\n",
      "Epoch 80, Training Loss 0.03814992196190997\n",
      "Epoch 80, Training Loss 0.03844103796402817\n",
      "Epoch 80, Training Loss 0.03886740222154066\n",
      "Epoch 80, Training Loss 0.03917105377787519\n",
      "Epoch 80, Training Loss 0.0393770853309985\n",
      "Epoch 80, Training Loss 0.03994849169879313\n",
      "Epoch 80, Training Loss 0.040168237026847534\n",
      "Epoch 80, Training Loss 0.04065606762152499\n",
      "Epoch 80, Training Loss 0.041239857654589825\n",
      "Epoch 80, Training Loss 0.041719803400814076\n",
      "Epoch 80, Training Loss 0.04214966638237619\n",
      "Epoch 80, Training Loss 0.042501521442095035\n",
      "Epoch 80, Training Loss 0.04285241108949837\n",
      "Epoch 80, Training Loss 0.043248378578811655\n",
      "Epoch 80, Training Loss 0.043585714305300845\n",
      "Epoch 80, Training Loss 0.04385689596462128\n",
      "Epoch 80, Training Loss 0.044502453521237044\n",
      "Epoch 80, Training Loss 0.04486909030419786\n",
      "Epoch 80, Training Loss 0.045086870603549205\n",
      "Epoch 80, Training Loss 0.04530583417324154\n",
      "Epoch 80, Training Loss 0.045652813847412536\n",
      "Epoch 80, Training Loss 0.045973903420940994\n",
      "Epoch 80, Training Loss 0.04634739820609617\n",
      "Epoch 80, Training Loss 0.04671077129176206\n",
      "Epoch 80, Training Loss 0.04737425559317059\n",
      "Epoch 80, Training Loss 0.04761354526138062\n",
      "Epoch 80, Training Loss 0.04782641528512511\n",
      "Epoch 80, Training Loss 0.04825151309637767\n",
      "Epoch 80, Training Loss 0.048658379203523214\n",
      "Epoch 80, Training Loss 0.04906957609879086\n",
      "Epoch 80, Training Loss 0.04952290166369484\n",
      "Epoch 80, Training Loss 0.050038284543530105\n",
      "Epoch 80, Training Loss 0.05048983530772617\n",
      "Epoch 80, Training Loss 0.05083528305868359\n",
      "Epoch 80, Training Loss 0.051302043983088734\n",
      "Epoch 80, Training Loss 0.05170445891140062\n",
      "Epoch 80, Training Loss 0.052124564947984404\n",
      "Epoch 80, Training Loss 0.05240965316362698\n",
      "Epoch 80, Training Loss 0.052677681188449225\n",
      "Epoch 80, Training Loss 0.05304985125656323\n",
      "Epoch 80, Training Loss 0.05344903126092213\n",
      "Epoch 80, Training Loss 0.0538729767284125\n",
      "Epoch 80, Training Loss 0.05437711052729956\n",
      "Epoch 80, Training Loss 0.054857259630547155\n",
      "Epoch 80, Training Loss 0.055146534062559954\n",
      "Epoch 80, Training Loss 0.0555440292448339\n",
      "Epoch 80, Training Loss 0.05616702099361688\n",
      "Epoch 80, Training Loss 0.0564173511265184\n",
      "Epoch 80, Training Loss 0.05685198301320796\n",
      "Epoch 80, Training Loss 0.05720048127195719\n",
      "Epoch 80, Training Loss 0.057394952896763295\n",
      "Epoch 80, Training Loss 0.05782780338964804\n",
      "Epoch 80, Training Loss 0.058116211229578\n",
      "Epoch 80, Training Loss 0.0584944472136095\n",
      "Epoch 80, Training Loss 0.05905237630047762\n",
      "Epoch 80, Training Loss 0.05941334576405528\n",
      "Epoch 80, Training Loss 0.05970098390756056\n",
      "Epoch 80, Training Loss 0.05993702464625049\n",
      "Epoch 80, Training Loss 0.060147483940319636\n",
      "Epoch 80, Training Loss 0.060573441178902336\n",
      "Epoch 80, Training Loss 0.06076091179228805\n",
      "Epoch 80, Training Loss 0.06124698206820452\n",
      "Epoch 80, Training Loss 0.06164635334859419\n",
      "Epoch 80, Training Loss 0.06211302456114908\n",
      "Epoch 80, Training Loss 0.062347636598607765\n",
      "Epoch 80, Training Loss 0.06278293179657758\n",
      "Epoch 80, Training Loss 0.06306263356638686\n",
      "Epoch 80, Training Loss 0.06341377695274475\n",
      "Epoch 80, Training Loss 0.06391600008739536\n",
      "Epoch 80, Training Loss 0.06420572894765898\n",
      "Epoch 80, Training Loss 0.06464375223954925\n",
      "Epoch 80, Training Loss 0.06505617148735944\n",
      "Epoch 80, Training Loss 0.06557233700209567\n",
      "Epoch 80, Training Loss 0.06593932569636714\n",
      "Epoch 80, Training Loss 0.06629958066641523\n",
      "Epoch 80, Training Loss 0.06663821439456452\n",
      "Epoch 80, Training Loss 0.0671910035716908\n",
      "Epoch 80, Training Loss 0.06752997545330115\n",
      "Epoch 80, Training Loss 0.06787705497668527\n",
      "Epoch 80, Training Loss 0.06827493575985169\n",
      "Epoch 80, Training Loss 0.06859443109968434\n",
      "Epoch 80, Training Loss 0.06911855772175753\n",
      "Epoch 80, Training Loss 0.06947439093419049\n",
      "Epoch 80, Training Loss 0.06987655139945047\n",
      "Epoch 80, Training Loss 0.07053845099475987\n",
      "Epoch 80, Training Loss 0.0709960231238314\n",
      "Epoch 80, Training Loss 0.07138616265848165\n",
      "Epoch 80, Training Loss 0.07175223876143355\n",
      "Epoch 80, Training Loss 0.0721943464959064\n",
      "Epoch 80, Training Loss 0.0725475884688175\n",
      "Epoch 80, Training Loss 0.07300844739007828\n",
      "Epoch 80, Training Loss 0.07339118520164734\n",
      "Epoch 80, Training Loss 0.07372845301542745\n",
      "Epoch 80, Training Loss 0.07411756951485754\n",
      "Epoch 80, Training Loss 0.07461813835384291\n",
      "Epoch 80, Training Loss 0.07503758142213993\n",
      "Epoch 80, Training Loss 0.07531871168357332\n",
      "Epoch 80, Training Loss 0.0758102842608986\n",
      "Epoch 80, Training Loss 0.07617954258113871\n",
      "Epoch 80, Training Loss 0.07651608972750662\n",
      "Epoch 80, Training Loss 0.07678576671253995\n",
      "Epoch 80, Training Loss 0.07723216281827454\n",
      "Epoch 80, Training Loss 0.07757655479718963\n",
      "Epoch 80, Training Loss 0.07793480744752128\n",
      "Epoch 80, Training Loss 0.07822647614552238\n",
      "Epoch 80, Training Loss 0.0785341336751533\n",
      "Epoch 80, Training Loss 0.07906092787185288\n",
      "Epoch 80, Training Loss 0.0794821987524057\n",
      "Epoch 80, Training Loss 0.08037421556994738\n",
      "Epoch 80, Training Loss 0.08089918149706653\n",
      "Epoch 80, Training Loss 0.08126423719441495\n",
      "Epoch 80, Training Loss 0.08175146229126874\n",
      "Epoch 80, Training Loss 0.08196850334439436\n",
      "Epoch 80, Training Loss 0.08236038078889822\n",
      "Epoch 80, Training Loss 0.08275870666327075\n",
      "Epoch 80, Training Loss 0.08327089257709816\n",
      "Epoch 80, Training Loss 0.08357268196466329\n",
      "Epoch 80, Training Loss 0.08380636662397238\n",
      "Epoch 80, Training Loss 0.08418589333058013\n",
      "Epoch 80, Training Loss 0.08450043098548489\n",
      "Epoch 80, Training Loss 0.0848874081964688\n",
      "Epoch 80, Training Loss 0.0852420537368111\n",
      "Epoch 80, Training Loss 0.08553785884090702\n",
      "Epoch 80, Training Loss 0.0858898417228628\n",
      "Epoch 80, Training Loss 0.08612750903191164\n",
      "Epoch 80, Training Loss 0.08632253535339594\n",
      "Epoch 80, Training Loss 0.08660170752221666\n",
      "Epoch 80, Training Loss 0.08705728544908412\n",
      "Epoch 80, Training Loss 0.08742293928895155\n",
      "Epoch 80, Training Loss 0.08775384849904443\n",
      "Epoch 80, Training Loss 0.08811698975922812\n",
      "Epoch 80, Training Loss 0.08852509102400612\n",
      "Epoch 80, Training Loss 0.08900197307624476\n",
      "Epoch 80, Training Loss 0.0894108157785957\n",
      "Epoch 80, Training Loss 0.08976147421028303\n",
      "Epoch 80, Training Loss 0.09006623622706479\n",
      "Epoch 80, Training Loss 0.09047876062143184\n",
      "Epoch 80, Training Loss 0.09079329174040529\n",
      "Epoch 80, Training Loss 0.09105615891382822\n",
      "Epoch 80, Training Loss 0.09143342658915483\n",
      "Epoch 80, Training Loss 0.09172497623030791\n",
      "Epoch 80, Training Loss 0.09210591187791141\n",
      "Epoch 80, Training Loss 0.09262040926291205\n",
      "Epoch 80, Training Loss 0.09302578231943842\n",
      "Epoch 80, Training Loss 0.09354430522836382\n",
      "Epoch 80, Training Loss 0.09394296822721696\n",
      "Epoch 80, Training Loss 0.09429334441338048\n",
      "Epoch 80, Training Loss 0.09478854913921918\n",
      "Epoch 80, Training Loss 0.09517539018179144\n",
      "Epoch 80, Training Loss 0.09567448559700681\n",
      "Epoch 80, Training Loss 0.09590501753646699\n",
      "Epoch 80, Training Loss 0.09626936354219456\n",
      "Epoch 80, Training Loss 0.09693338199878287\n",
      "Epoch 80, Training Loss 0.09735193732373244\n",
      "Epoch 80, Training Loss 0.09764721705709271\n",
      "Epoch 80, Training Loss 0.09810567853014793\n",
      "Epoch 80, Training Loss 0.09842340770127524\n",
      "Epoch 80, Training Loss 0.09878461954691221\n",
      "Epoch 80, Training Loss 0.09912754987816677\n",
      "Epoch 80, Training Loss 0.09953460707079115\n",
      "Epoch 80, Training Loss 0.0998098395593331\n",
      "Epoch 80, Training Loss 0.10032742228501898\n",
      "Epoch 80, Training Loss 0.10063651659528312\n",
      "Epoch 80, Training Loss 0.10112491053769655\n",
      "Epoch 80, Training Loss 0.10144129877581316\n",
      "Epoch 80, Training Loss 0.10179934803101108\n",
      "Epoch 80, Training Loss 0.10213455433964425\n",
      "Epoch 80, Training Loss 0.10235033205250645\n",
      "Epoch 80, Training Loss 0.10268955527211698\n",
      "Epoch 80, Training Loss 0.10293916254625905\n",
      "Epoch 80, Training Loss 0.10327716112670386\n",
      "Epoch 80, Training Loss 0.10355840629095311\n",
      "Epoch 80, Training Loss 0.10389035660058946\n",
      "Epoch 80, Training Loss 0.10432951939304161\n",
      "Epoch 80, Training Loss 0.1048775260977428\n",
      "Epoch 80, Training Loss 0.10547515965255021\n",
      "Epoch 80, Training Loss 0.10587203134889797\n",
      "Epoch 80, Training Loss 0.10636540372734485\n",
      "Epoch 80, Training Loss 0.10688485580560801\n",
      "Epoch 80, Training Loss 0.10731650524965637\n",
      "Epoch 80, Training Loss 0.10774791417905437\n",
      "Epoch 80, Training Loss 0.10810393317962241\n",
      "Epoch 80, Training Loss 0.1083769765694428\n",
      "Epoch 80, Training Loss 0.10866667006326758\n",
      "Epoch 80, Training Loss 0.10898112795313301\n",
      "Epoch 80, Training Loss 0.10951396291289488\n",
      "Epoch 80, Training Loss 0.10997311826176046\n",
      "Epoch 80, Training Loss 0.11025756725188716\n",
      "Epoch 80, Training Loss 0.11053679702455735\n",
      "Epoch 80, Training Loss 0.1110900557216476\n",
      "Epoch 80, Training Loss 0.11141703738961987\n",
      "Epoch 80, Training Loss 0.11169776619623994\n",
      "Epoch 80, Training Loss 0.11201493466830315\n",
      "Epoch 80, Training Loss 0.1122636089239584\n",
      "Epoch 80, Training Loss 0.11269333764262822\n",
      "Epoch 80, Training Loss 0.11291719708220123\n",
      "Epoch 80, Training Loss 0.11324411914552875\n",
      "Epoch 80, Training Loss 0.11379124881590114\n",
      "Epoch 80, Training Loss 0.11417147519109803\n",
      "Epoch 80, Training Loss 0.11449142987542141\n",
      "Epoch 80, Training Loss 0.11476786034491361\n",
      "Epoch 80, Training Loss 0.11507891655882911\n",
      "Epoch 80, Training Loss 0.11568055951686772\n",
      "Epoch 80, Training Loss 0.11595518122929746\n",
      "Epoch 80, Training Loss 0.11629926203690527\n",
      "Epoch 80, Training Loss 0.11674955012776968\n",
      "Epoch 80, Training Loss 0.11714903505333245\n",
      "Epoch 80, Training Loss 0.11747869957819619\n",
      "Epoch 80, Training Loss 0.11774135600118076\n",
      "Epoch 80, Training Loss 0.11820847950780483\n",
      "Epoch 80, Training Loss 0.11839818217031792\n",
      "Epoch 80, Training Loss 0.1187135629413073\n",
      "Epoch 80, Training Loss 0.1191294285876062\n",
      "Epoch 80, Training Loss 0.11953958019118785\n",
      "Epoch 80, Training Loss 0.12001754991386247\n",
      "Epoch 80, Training Loss 0.12046136712784047\n",
      "Epoch 80, Training Loss 0.1207913528470432\n",
      "Epoch 80, Training Loss 0.12114718693601506\n",
      "Epoch 80, Training Loss 0.12144733561426782\n",
      "Epoch 80, Training Loss 0.12166895115238321\n",
      "Epoch 80, Training Loss 0.12209978404328646\n",
      "Epoch 80, Training Loss 0.12248872119523681\n",
      "Epoch 80, Training Loss 0.12311611686597394\n",
      "Epoch 80, Training Loss 0.12330601008042046\n",
      "Epoch 80, Training Loss 0.12374986395659045\n",
      "Epoch 80, Training Loss 0.12413217653246487\n",
      "Epoch 80, Training Loss 0.1246560911845673\n",
      "Epoch 80, Training Loss 0.12525824615565104\n",
      "Epoch 80, Training Loss 0.12586420332379353\n",
      "Epoch 80, Training Loss 0.12623925419414744\n",
      "Epoch 80, Training Loss 0.1267088346774011\n",
      "Epoch 80, Training Loss 0.12718171010846677\n",
      "Epoch 80, Training Loss 0.12750745585659887\n",
      "Epoch 80, Training Loss 0.12792498216299755\n",
      "Epoch 80, Training Loss 0.12833475437767974\n",
      "Epoch 80, Training Loss 0.12869806023662353\n",
      "Epoch 80, Training Loss 0.12911075730915264\n",
      "Epoch 80, Training Loss 0.12970176846017617\n",
      "Epoch 80, Training Loss 0.13005689701155934\n",
      "Epoch 80, Training Loss 0.130489401957568\n",
      "Epoch 80, Training Loss 0.13081997171844667\n",
      "Epoch 80, Training Loss 0.1312245784132072\n",
      "Epoch 80, Training Loss 0.13157817149711082\n",
      "Epoch 80, Training Loss 0.13193351472430218\n",
      "Epoch 80, Training Loss 0.1324902246980106\n",
      "Epoch 80, Training Loss 0.13289559154254396\n",
      "Epoch 80, Training Loss 0.13315273322107846\n",
      "Epoch 80, Training Loss 0.1337459954764227\n",
      "Epoch 80, Training Loss 0.13419841702484414\n",
      "Epoch 80, Training Loss 0.13453337999865833\n",
      "Epoch 80, Training Loss 0.1350127426559663\n",
      "Epoch 80, Training Loss 0.1353239622300543\n",
      "Epoch 80, Training Loss 0.13561905036344551\n",
      "Epoch 80, Training Loss 0.1359600114166889\n",
      "Epoch 80, Training Loss 0.13649532618120197\n",
      "Epoch 80, Training Loss 0.1368178833476113\n",
      "Epoch 80, Training Loss 0.13725580534209375\n",
      "Epoch 80, Training Loss 0.13760476534628807\n",
      "Epoch 80, Training Loss 0.13794447012874475\n",
      "Epoch 80, Training Loss 0.13821128736752683\n",
      "Epoch 80, Training Loss 0.13850087009351272\n",
      "Epoch 80, Training Loss 0.1388789653358862\n",
      "Epoch 80, Training Loss 0.1391867675134898\n",
      "Epoch 80, Training Loss 0.13945056402774722\n",
      "Epoch 80, Training Loss 0.13962101490448808\n",
      "Epoch 80, Training Loss 0.1399343837709988\n",
      "Epoch 80, Training Loss 0.1400936474580594\n",
      "Epoch 80, Training Loss 0.14070442322727358\n",
      "Epoch 80, Training Loss 0.1411368731037735\n",
      "Epoch 80, Training Loss 0.1416176493896548\n",
      "Epoch 80, Training Loss 0.14204116447654833\n",
      "Epoch 80, Training Loss 0.14230946596245023\n",
      "Epoch 80, Training Loss 0.1426322991242799\n",
      "Epoch 80, Training Loss 0.1430564663942208\n",
      "Epoch 80, Training Loss 0.1435494316012963\n",
      "Epoch 80, Training Loss 0.1439454420410154\n",
      "Epoch 80, Training Loss 0.14420792734836374\n",
      "Epoch 80, Training Loss 0.14470670343664907\n",
      "Epoch 80, Training Loss 0.14499110692297407\n",
      "Epoch 80, Training Loss 0.14558262795286106\n",
      "Epoch 80, Training Loss 0.14602961987638108\n",
      "Epoch 80, Training Loss 0.14648701269608325\n",
      "Epoch 80, Training Loss 0.14696344294968774\n",
      "Epoch 80, Training Loss 0.14734945467213537\n",
      "Epoch 80, Training Loss 0.14771240236966507\n",
      "Epoch 80, Training Loss 0.14816958752587017\n",
      "Epoch 80, Training Loss 0.1485772839821208\n",
      "Epoch 80, Training Loss 0.14884275527637633\n",
      "Epoch 80, Training Loss 0.14927513125683645\n",
      "Epoch 80, Training Loss 0.14960641425360194\n",
      "Epoch 80, Training Loss 0.14998074049306342\n",
      "Epoch 80, Training Loss 0.1504405026736162\n",
      "Epoch 80, Training Loss 0.15063868024769952\n",
      "Epoch 80, Training Loss 0.1508771559923811\n",
      "Epoch 80, Training Loss 0.15161563589444857\n",
      "Epoch 80, Training Loss 0.15204789613366432\n",
      "Epoch 80, Training Loss 0.15276677296747027\n",
      "Epoch 80, Training Loss 0.15300601404493727\n",
      "Epoch 80, Training Loss 0.15340147497099074\n",
      "Epoch 80, Training Loss 0.1537464045731308\n",
      "Epoch 80, Training Loss 0.15420856786048626\n",
      "Epoch 80, Training Loss 0.15460066905107034\n",
      "Epoch 80, Training Loss 0.1550804690631759\n",
      "Epoch 80, Training Loss 0.15539560897652147\n",
      "Epoch 80, Training Loss 0.15568573311771577\n",
      "Epoch 80, Training Loss 0.15618439479862029\n",
      "Epoch 80, Training Loss 0.15649360485012878\n",
      "Epoch 80, Training Loss 0.15677498502042286\n",
      "Epoch 80, Training Loss 0.15716668235523926\n",
      "Epoch 80, Training Loss 0.15745376540190728\n",
      "Epoch 80, Training Loss 0.15816827844399625\n",
      "Epoch 80, Training Loss 0.15861508530347854\n",
      "Epoch 80, Training Loss 0.15912623956913838\n",
      "Epoch 80, Training Loss 0.159555407874572\n",
      "Epoch 80, Training Loss 0.15999985749230666\n",
      "Epoch 80, Training Loss 0.16052160617869224\n",
      "Epoch 80, Training Loss 0.16086230796697498\n",
      "Epoch 80, Training Loss 0.1612996082286091\n",
      "Epoch 80, Training Loss 0.16163057780555448\n",
      "Epoch 80, Training Loss 0.16237434552377447\n",
      "Epoch 80, Training Loss 0.16279201924114886\n",
      "Epoch 80, Training Loss 0.1632390943977534\n",
      "Epoch 80, Training Loss 0.16354849780230876\n",
      "Epoch 80, Training Loss 0.16395232353902534\n",
      "Epoch 80, Training Loss 0.16434810802226177\n",
      "Epoch 80, Training Loss 0.16475780275852783\n",
      "Epoch 80, Training Loss 0.1650861726354455\n",
      "Epoch 80, Training Loss 0.16564210889208347\n",
      "Epoch 80, Training Loss 0.16606256203806918\n",
      "Epoch 80, Training Loss 0.1665839076690052\n",
      "Epoch 80, Training Loss 0.16710645065207005\n",
      "Epoch 80, Training Loss 0.16743406194174076\n",
      "Epoch 80, Training Loss 0.16789755859719518\n",
      "Epoch 80, Training Loss 0.16813361130254653\n",
      "Epoch 80, Training Loss 0.1684670930780718\n",
      "Epoch 80, Training Loss 0.16870273786889928\n",
      "Epoch 80, Training Loss 0.16906882155581812\n",
      "Epoch 80, Training Loss 0.16945483625087593\n",
      "Epoch 80, Training Loss 0.1696877612939576\n",
      "Epoch 80, Training Loss 0.17002161186369483\n",
      "Epoch 80, Training Loss 0.17024765870607722\n",
      "Epoch 80, Training Loss 0.1705245002913658\n",
      "Epoch 80, Training Loss 0.170830437441921\n",
      "Epoch 80, Training Loss 0.17118060169622418\n",
      "Epoch 80, Training Loss 0.17142534248359367\n",
      "Epoch 80, Training Loss 0.17168610377232438\n",
      "Epoch 80, Training Loss 0.17214647057416188\n",
      "Epoch 80, Training Loss 0.1726318861517455\n",
      "Epoch 80, Training Loss 0.17315577187806444\n",
      "Epoch 80, Training Loss 0.17352092940636607\n",
      "Epoch 80, Training Loss 0.17383963793821042\n",
      "Epoch 80, Training Loss 0.17427014429932056\n",
      "Epoch 80, Training Loss 0.17461282390234112\n",
      "Epoch 80, Training Loss 0.17509987834088334\n",
      "Epoch 80, Training Loss 0.1754853572801251\n",
      "Epoch 80, Training Loss 0.1759167290137857\n",
      "Epoch 80, Training Loss 0.1762519783871558\n",
      "Epoch 80, Training Loss 0.1765705129641401\n",
      "Epoch 80, Training Loss 0.17731002931628387\n",
      "Epoch 80, Training Loss 0.17770900146659377\n",
      "Epoch 80, Training Loss 0.17823505071956483\n",
      "Epoch 80, Training Loss 0.17868100581190471\n",
      "Epoch 80, Training Loss 0.17906508933933798\n",
      "Epoch 80, Training Loss 0.17949530655694435\n",
      "Epoch 80, Training Loss 0.17972174298275462\n",
      "Epoch 80, Training Loss 0.1801364881074642\n",
      "Epoch 80, Training Loss 0.1805602649364935\n",
      "Epoch 80, Training Loss 0.18079230768601304\n",
      "Epoch 80, Training Loss 0.18133083321249394\n",
      "Epoch 80, Training Loss 0.18166440713893423\n",
      "Epoch 80, Training Loss 0.18203795806068898\n",
      "Epoch 80, Training Loss 0.18265051712922734\n",
      "Epoch 80, Training Loss 0.18314523945379135\n",
      "Epoch 80, Training Loss 0.18357007491314198\n",
      "Epoch 80, Training Loss 0.18402366763185662\n",
      "Epoch 80, Training Loss 0.1843240921721434\n",
      "Epoch 80, Training Loss 0.18472121932241312\n",
      "Epoch 80, Training Loss 0.18509995066525076\n",
      "Epoch 80, Training Loss 0.18542996172786064\n",
      "Epoch 80, Training Loss 0.1857496421698414\n",
      "Epoch 80, Training Loss 0.18619516535716898\n",
      "Epoch 80, Training Loss 0.18655264089860574\n",
      "Epoch 80, Training Loss 0.18699004988917303\n",
      "Epoch 80, Training Loss 0.18748658352419542\n",
      "Epoch 80, Training Loss 0.18779837953693726\n",
      "Epoch 80, Training Loss 0.18831316065376677\n",
      "Epoch 80, Training Loss 0.18876270749761015\n",
      "Epoch 80, Training Loss 0.18937725417525567\n",
      "Epoch 80, Training Loss 0.1897056184118361\n",
      "Epoch 80, Training Loss 0.19001445063697103\n",
      "Epoch 80, Training Loss 0.19033947476493124\n",
      "Epoch 80, Training Loss 0.19075504417919442\n",
      "Epoch 80, Training Loss 0.19113673025842212\n",
      "Epoch 80, Training Loss 0.19152084000580147\n",
      "Epoch 80, Training Loss 0.19216961545102737\n",
      "Epoch 80, Training Loss 0.19262330920037712\n",
      "Epoch 80, Training Loss 0.1929336066555489\n",
      "Epoch 80, Training Loss 0.1933138302105772\n",
      "Epoch 80, Training Loss 0.19367885263755802\n",
      "Epoch 80, Training Loss 0.19408648529702135\n",
      "Epoch 80, Training Loss 0.19458340754365677\n",
      "Epoch 80, Training Loss 0.19506108568376287\n",
      "Epoch 80, Training Loss 0.19553081521673885\n",
      "Epoch 80, Training Loss 0.19586266891654494\n",
      "Epoch 80, Training Loss 0.1963260658752278\n",
      "Epoch 80, Training Loss 0.19667169354532077\n",
      "Epoch 80, Training Loss 0.19720300431827756\n",
      "Epoch 80, Training Loss 0.19753391924492844\n",
      "Epoch 80, Training Loss 0.1978953506063927\n",
      "Epoch 80, Training Loss 0.19805905895541087\n",
      "Epoch 80, Training Loss 0.1985363912437578\n",
      "Epoch 80, Training Loss 0.19899767780166758\n",
      "Epoch 80, Training Loss 0.19933129189645543\n",
      "Epoch 80, Training Loss 0.19998239390456768\n",
      "Epoch 80, Training Loss 0.20029272403939605\n",
      "Epoch 80, Training Loss 0.20068030091731445\n",
      "Epoch 80, Training Loss 0.20113898644133296\n",
      "Epoch 80, Training Loss 0.2015208633773772\n",
      "Epoch 80, Training Loss 0.202073089549761\n",
      "Epoch 80, Training Loss 0.20258474431912918\n",
      "Epoch 80, Training Loss 0.20290540941917073\n",
      "Epoch 80, Training Loss 0.20339005006015148\n",
      "Epoch 80, Training Loss 0.2036352823000125\n",
      "Epoch 80, Training Loss 0.2038906483013002\n",
      "Epoch 80, Training Loss 0.2041205891296077\n",
      "Epoch 80, Training Loss 0.20446130894410336\n",
      "Epoch 80, Training Loss 0.20499921753964462\n",
      "Epoch 80, Training Loss 0.20524644009445026\n",
      "Epoch 80, Training Loss 0.20568880217764385\n",
      "Epoch 80, Training Loss 0.20613176232713568\n",
      "Epoch 80, Training Loss 0.20653405949435272\n",
      "Epoch 80, Training Loss 0.20727228984960813\n",
      "Epoch 80, Training Loss 0.20770578013966456\n",
      "Epoch 80, Training Loss 0.20823266248568853\n",
      "Epoch 80, Training Loss 0.2086047860591308\n",
      "Epoch 80, Training Loss 0.20917013039826737\n",
      "Epoch 80, Training Loss 0.2096651160656034\n",
      "Epoch 80, Training Loss 0.20993365932379843\n",
      "Epoch 80, Training Loss 0.21023208981432268\n",
      "Epoch 80, Training Loss 0.2106210353505581\n",
      "Epoch 80, Training Loss 0.21122572401447978\n",
      "Epoch 80, Training Loss 0.21171004776759525\n",
      "Epoch 80, Training Loss 0.21232696994186362\n",
      "Epoch 80, Training Loss 0.21278919767388296\n",
      "Epoch 80, Training Loss 0.21323303516258668\n",
      "Epoch 80, Training Loss 0.21375501125364962\n",
      "Epoch 80, Training Loss 0.21416728095630247\n",
      "Epoch 80, Training Loss 0.2143859706075905\n",
      "Epoch 80, Training Loss 0.21477701103367158\n",
      "Epoch 80, Training Loss 0.21519795717561946\n",
      "Epoch 80, Training Loss 0.21561073252688284\n",
      "Epoch 80, Training Loss 0.21597723794334076\n",
      "Epoch 80, Training Loss 0.21631765500892458\n",
      "Epoch 80, Training Loss 0.21675061731768386\n",
      "Epoch 80, Training Loss 0.21708975105410647\n",
      "Epoch 80, Training Loss 0.21736538185335486\n",
      "Epoch 80, Training Loss 0.21781681485645607\n",
      "Epoch 80, Training Loss 0.2181658034434404\n",
      "Epoch 80, Training Loss 0.21860671557886216\n",
      "Epoch 80, Training Loss 0.21902214539477893\n",
      "Epoch 80, Training Loss 0.21931380014437848\n",
      "Epoch 80, Training Loss 0.2196856805354433\n",
      "Epoch 80, Training Loss 0.2201864290267915\n",
      "Epoch 80, Training Loss 0.2205185597509984\n",
      "Epoch 80, Training Loss 0.22104028366563266\n",
      "Epoch 80, Training Loss 0.22147009691314015\n",
      "Epoch 80, Training Loss 0.22199496543011094\n",
      "Epoch 80, Training Loss 0.22225920189067225\n",
      "Epoch 80, Training Loss 0.22266652501757492\n",
      "Epoch 80, Training Loss 0.22295411094985046\n",
      "Epoch 80, Training Loss 0.22341595670146405\n",
      "Epoch 80, Training Loss 0.22363823425510657\n",
      "Epoch 80, Training Loss 0.2240430950318151\n",
      "Epoch 80, Training Loss 0.22438703814659583\n",
      "Epoch 80, Training Loss 0.2246982232307839\n",
      "Epoch 80, Training Loss 0.225182794129757\n",
      "Epoch 80, Training Loss 0.2255079394487469\n",
      "Epoch 80, Training Loss 0.22586791228760233\n",
      "Epoch 80, Training Loss 0.2263814825612261\n",
      "Epoch 80, Training Loss 0.22660967240781735\n",
      "Epoch 80, Training Loss 0.22692166891930354\n",
      "Epoch 80, Training Loss 0.22749982715186562\n",
      "Epoch 80, Training Loss 0.22806564966202392\n",
      "Epoch 80, Training Loss 0.22854170460454035\n",
      "Epoch 80, Training Loss 0.22875006199645265\n",
      "Epoch 80, Training Loss 0.22899973823133943\n",
      "Epoch 80, Training Loss 0.22941103528070328\n",
      "Epoch 80, Training Loss 0.22974959240697534\n",
      "Epoch 80, Training Loss 0.23007801971624575\n",
      "Epoch 80, Training Loss 0.23045111277981487\n",
      "Epoch 80, Training Loss 0.23109110439067607\n",
      "Epoch 80, Training Loss 0.23136273409475755\n",
      "Epoch 80, Training Loss 0.23181085843030755\n",
      "Epoch 80, Training Loss 0.2324708075360264\n",
      "Epoch 80, Training Loss 0.23299605990081187\n",
      "Epoch 80, Training Loss 0.23376827465984829\n",
      "Epoch 80, Training Loss 0.2343495928913431\n",
      "Epoch 80, Training Loss 0.23474710084059658\n",
      "Epoch 80, Training Loss 0.2350771072537393\n",
      "Epoch 80, Training Loss 0.23532068495021757\n",
      "Epoch 80, Training Loss 0.23561625485606205\n",
      "Epoch 80, Training Loss 0.23609241940405057\n",
      "Epoch 80, Training Loss 0.23665572906775242\n",
      "Epoch 80, Training Loss 0.23696584490787648\n",
      "Epoch 80, Training Loss 0.23751117824517248\n",
      "Epoch 80, Training Loss 0.23791416566771315\n",
      "Epoch 80, Training Loss 0.2383336956276918\n",
      "Epoch 80, Training Loss 0.23881409584027727\n",
      "Epoch 80, Training Loss 0.2393512162368011\n",
      "Epoch 80, Training Loss 0.23975078420489646\n",
      "Epoch 80, Training Loss 0.2401104030744804\n",
      "Epoch 80, Training Loss 0.24057965519864236\n",
      "Epoch 80, Training Loss 0.241183326848785\n",
      "Epoch 80, Training Loss 0.24157421075550797\n",
      "Epoch 80, Training Loss 0.24191338294531073\n",
      "Epoch 80, Training Loss 0.24233073882206016\n",
      "Epoch 80, Training Loss 0.2425502956942524\n",
      "Epoch 80, Training Loss 0.2429888444902647\n",
      "Epoch 80, Training Loss 0.24335104309003372\n",
      "Epoch 80, Training Loss 0.24377396049173286\n",
      "Epoch 80, Training Loss 0.2443881956550776\n",
      "Epoch 80, Training Loss 0.2446141760520008\n",
      "Epoch 80, Training Loss 0.2450270395144782\n",
      "Epoch 80, Training Loss 0.24549926447746395\n",
      "Epoch 80, Training Loss 0.24590663409903837\n",
      "Epoch 80, Training Loss 0.2466720633037255\n",
      "Epoch 80, Training Loss 0.24706971138487083\n",
      "Epoch 80, Training Loss 0.24738417762091092\n",
      "Epoch 80, Training Loss 0.24790360546096815\n",
      "Epoch 80, Training Loss 0.2481381544447921\n",
      "Epoch 80, Training Loss 0.24849118616269983\n",
      "Epoch 80, Training Loss 0.24888383507576134\n",
      "Epoch 80, Training Loss 0.24941205269540362\n",
      "Epoch 80, Training Loss 0.24960057597483515\n",
      "Epoch 80, Training Loss 0.2499280526205097\n",
      "Epoch 80, Training Loss 0.2503176747685503\n",
      "Epoch 80, Training Loss 0.2508994565747888\n",
      "Epoch 80, Training Loss 0.25130778242407553\n",
      "Epoch 80, Training Loss 0.25182925103722936\n",
      "Epoch 80, Training Loss 0.2524434703847636\n",
      "Epoch 80, Training Loss 0.25293930191213215\n",
      "Epoch 80, Training Loss 0.2534595207713754\n",
      "Epoch 80, Training Loss 0.25388844970547025\n",
      "Epoch 80, Training Loss 0.2543391394798103\n",
      "Epoch 80, Training Loss 0.2546198226492423\n",
      "Epoch 80, Training Loss 0.25503014468247326\n",
      "Epoch 80, Training Loss 0.25536831487398925\n",
      "Epoch 80, Training Loss 0.25572290439206313\n",
      "Epoch 80, Training Loss 0.2564194648505172\n",
      "Epoch 80, Training Loss 0.2567708751620234\n",
      "Epoch 80, Training Loss 0.25711601208466706\n",
      "Epoch 80, Training Loss 0.25744653900947106\n",
      "Epoch 80, Training Loss 0.2578273764084977\n",
      "Epoch 80, Training Loss 0.2583581092374404\n",
      "Epoch 80, Training Loss 0.25891291018565904\n",
      "Epoch 80, Training Loss 0.2592232527063631\n",
      "Epoch 80, Training Loss 0.2596348772763901\n",
      "Epoch 80, Training Loss 0.2600106501861302\n",
      "Epoch 80, Training Loss 0.2604059183498478\n",
      "Epoch 80, Training Loss 0.26070822304700647\n",
      "Epoch 80, Training Loss 0.2611135780010992\n",
      "Epoch 80, Training Loss 0.261383405236332\n",
      "Epoch 80, Training Loss 0.2616999222494452\n",
      "Epoch 80, Training Loss 0.2621000850063456\n",
      "Epoch 80, Training Loss 0.2624938525735875\n",
      "Epoch 80, Training Loss 0.26297228042122045\n",
      "Epoch 80, Training Loss 0.2633795963833704\n",
      "Epoch 80, Training Loss 0.2635880777293154\n",
      "Epoch 80, Training Loss 0.2641017627152031\n",
      "Epoch 80, Training Loss 0.2647612073918438\n",
      "Epoch 80, Training Loss 0.26524437438039217\n",
      "Epoch 80, Training Loss 0.2656329143458925\n",
      "Epoch 80, Training Loss 0.2659854431210272\n",
      "Epoch 80, Training Loss 0.26650628691439127\n",
      "Epoch 80, Training Loss 0.2669779676229448\n",
      "Epoch 80, Training Loss 0.26729587947621064\n",
      "Epoch 80, Training Loss 0.26774703050056076\n",
      "Epoch 80, Training Loss 0.2682294919134101\n",
      "Epoch 80, Training Loss 0.26860801147682895\n",
      "Epoch 80, Training Loss 0.2692798244983644\n",
      "Epoch 80, Training Loss 0.26986206683051556\n",
      "Epoch 80, Training Loss 0.2703182083337813\n",
      "Epoch 80, Training Loss 0.2707336403219901\n",
      "Epoch 80, Training Loss 0.2711790726541558\n",
      "Epoch 80, Training Loss 0.2716516723946842\n",
      "Epoch 80, Training Loss 0.27204093546666147\n",
      "Epoch 80, Training Loss 0.2725234748152516\n",
      "Epoch 80, Training Loss 0.272942033379584\n",
      "Epoch 80, Training Loss 0.2733640926115958\n",
      "Epoch 80, Training Loss 0.27380682928178013\n",
      "Epoch 80, Training Loss 0.27421339031528025\n",
      "Epoch 80, Training Loss 0.2746569929677812\n",
      "Epoch 80, Training Loss 0.27508291689788594\n",
      "Epoch 80, Training Loss 0.27560764875101007\n",
      "Epoch 80, Training Loss 0.27607453639245094\n",
      "Epoch 80, Training Loss 0.2762863369053587\n",
      "Epoch 80, Training Loss 0.27658634884354405\n",
      "Epoch 80, Training Loss 0.277257220004983\n",
      "Epoch 80, Training Loss 0.2776026650882133\n",
      "Epoch 80, Training Loss 0.2781823812733831\n",
      "Epoch 80, Training Loss 0.278676897306424\n",
      "Epoch 80, Training Loss 0.2789501706162072\n",
      "Epoch 80, Training Loss 0.2793794248224524\n",
      "Epoch 80, Training Loss 0.27994901644985387\n",
      "Epoch 80, Training Loss 0.28046129642011564\n",
      "Epoch 80, Training Loss 0.2810773757450721\n",
      "Epoch 80, Training Loss 0.28147187533662144\n",
      "Epoch 80, Training Loss 0.2819367106956289\n",
      "Epoch 80, Training Loss 0.28227026823460294\n",
      "Epoch 80, Training Loss 0.2826898048638993\n",
      "Epoch 80, Training Loss 0.28307782534671866\n",
      "Epoch 80, Training Loss 0.2835336743527666\n",
      "Epoch 80, Training Loss 0.28400530170677873\n",
      "Epoch 80, Training Loss 0.28437844271321433\n",
      "Epoch 80, Training Loss 0.2847483276063219\n",
      "Epoch 80, Training Loss 0.28523086706924317\n",
      "Epoch 80, Training Loss 0.2854402856448727\n",
      "Epoch 80, Training Loss 0.2858384275222983\n",
      "Epoch 80, Training Loss 0.28628122002419915\n",
      "Epoch 80, Training Loss 0.2866494125112548\n",
      "Epoch 80, Training Loss 0.28691281061952983\n",
      "Epoch 80, Training Loss 0.28767365285807556\n",
      "Epoch 80, Training Loss 0.2881217267354736\n",
      "Epoch 80, Training Loss 0.2884204773913564\n",
      "Epoch 80, Training Loss 0.2887716418527581\n",
      "Epoch 80, Training Loss 0.2890640905560435\n",
      "Epoch 80, Training Loss 0.28948618622158495\n",
      "Epoch 80, Training Loss 0.2898628155098242\n",
      "Epoch 80, Training Loss 0.29037592983078164\n",
      "Epoch 80, Training Loss 0.2910925394014629\n",
      "Epoch 80, Training Loss 0.2917270710134445\n",
      "Epoch 80, Training Loss 0.2920867203713378\n",
      "Epoch 80, Training Loss 0.2923597980223958\n",
      "Epoch 80, Training Loss 0.29261317898702743\n",
      "Epoch 80, Training Loss 0.2929664121183288\n",
      "Epoch 80, Training Loss 0.2933877684423686\n",
      "Epoch 80, Training Loss 0.2937015308557874\n",
      "Epoch 80, Training Loss 0.2944466124676987\n",
      "Epoch 80, Training Loss 0.2948296960548062\n",
      "Epoch 80, Training Loss 0.29520424249608196\n",
      "Epoch 80, Training Loss 0.2955712160224195\n",
      "Epoch 80, Training Loss 0.2959957368043073\n",
      "Epoch 80, Training Loss 0.2963490423834537\n",
      "Epoch 80, Training Loss 0.2965373400112857\n",
      "Epoch 80, Training Loss 0.29689232320965403\n",
      "Epoch 80, Training Loss 0.2974954706895382\n",
      "Epoch 80, Training Loss 0.2980093130332125\n",
      "Epoch 80, Training Loss 0.298676236568357\n",
      "Epoch 80, Training Loss 0.29916774672086893\n",
      "Epoch 80, Training Loss 0.29955149461012665\n",
      "Epoch 80, Training Loss 0.29988944280864027\n",
      "Epoch 80, Training Loss 0.30018570508493486\n",
      "Epoch 80, Training Loss 0.3006010178257437\n",
      "Epoch 80, Training Loss 0.30093341749494945\n",
      "Epoch 80, Training Loss 0.30131892303524116\n",
      "Epoch 80, Training Loss 0.30182614362300814\n",
      "Epoch 80, Training Loss 0.30241060070217113\n",
      "Epoch 80, Training Loss 0.3028538644008929\n",
      "Epoch 80, Training Loss 0.30362111337654424\n",
      "Epoch 80, Training Loss 0.3039979181250038\n",
      "Epoch 80, Training Loss 0.30470049697572316\n",
      "Epoch 80, Training Loss 0.30512170253507315\n",
      "Epoch 80, Training Loss 0.30552722631817886\n",
      "Epoch 80, Training Loss 0.30587322545021084\n",
      "Epoch 80, Training Loss 0.30625086565456733\n",
      "Epoch 80, Training Loss 0.3068476271293962\n",
      "Epoch 80, Training Loss 0.30710833470153687\n",
      "Epoch 80, Training Loss 0.30762765415565435\n",
      "Epoch 80, Training Loss 0.3079269578885239\n",
      "Epoch 80, Training Loss 0.30857816409043337\n",
      "Epoch 80, Training Loss 0.3090981368709098\n",
      "Epoch 80, Training Loss 0.3095481280437516\n",
      "Epoch 80, Training Loss 0.310058872470313\n",
      "Epoch 80, Training Loss 0.31033039992422706\n",
      "Epoch 80, Training Loss 0.3106070558738221\n",
      "Epoch 80, Training Loss 0.3108865451401152\n",
      "Epoch 80, Training Loss 0.31122274803536015\n",
      "Epoch 80, Training Loss 0.3117207513974451\n",
      "Epoch 80, Training Loss 0.312235141577928\n",
      "Epoch 80, Training Loss 0.3125449602332566\n",
      "Epoch 80, Training Loss 0.3130389883771272\n",
      "Epoch 80, Training Loss 0.3134883236702141\n",
      "Epoch 80, Training Loss 0.31382569868851196\n",
      "Epoch 80, Training Loss 0.3142552748131935\n",
      "Epoch 90, Training Loss 0.00018984529063524797\n",
      "Epoch 90, Training Loss 0.0004861965356275554\n",
      "Epoch 90, Training Loss 0.0007188256896670213\n",
      "Epoch 90, Training Loss 0.001061752933980254\n",
      "Epoch 90, Training Loss 0.0015797861244367516\n",
      "Epoch 90, Training Loss 0.0019510031280005375\n",
      "Epoch 90, Training Loss 0.0023634340375890514\n",
      "Epoch 90, Training Loss 0.0028840706628911637\n",
      "Epoch 90, Training Loss 0.0030733753005257043\n",
      "Epoch 90, Training Loss 0.003657273643309503\n",
      "Epoch 90, Training Loss 0.004056431619864901\n",
      "Epoch 90, Training Loss 0.0044550350902940305\n",
      "Epoch 90, Training Loss 0.004754995141187897\n",
      "Epoch 90, Training Loss 0.00510502661890386\n",
      "Epoch 90, Training Loss 0.005383396811802369\n",
      "Epoch 90, Training Loss 0.005685492149551811\n",
      "Epoch 90, Training Loss 0.0060575513164405625\n",
      "Epoch 90, Training Loss 0.0065166601325239975\n",
      "Epoch 90, Training Loss 0.006819759469355464\n",
      "Epoch 90, Training Loss 0.0071781015266542845\n",
      "Epoch 90, Training Loss 0.007494502322143301\n",
      "Epoch 90, Training Loss 0.007742772016988691\n",
      "Epoch 90, Training Loss 0.008260587063591803\n",
      "Epoch 90, Training Loss 0.008436391820840519\n",
      "Epoch 90, Training Loss 0.008650851219206515\n",
      "Epoch 90, Training Loss 0.009076377772309286\n",
      "Epoch 90, Training Loss 0.009415656175759747\n",
      "Epoch 90, Training Loss 0.009752674175955146\n",
      "Epoch 90, Training Loss 0.010023149421147983\n",
      "Epoch 90, Training Loss 0.010263906091527865\n",
      "Epoch 90, Training Loss 0.010710272509271226\n",
      "Epoch 90, Training Loss 0.01089562698627067\n",
      "Epoch 90, Training Loss 0.011204123096850217\n",
      "Epoch 90, Training Loss 0.011466681061650786\n",
      "Epoch 90, Training Loss 0.011915609602580595\n",
      "Epoch 90, Training Loss 0.01229771642047731\n",
      "Epoch 90, Training Loss 0.012600017116045402\n",
      "Epoch 90, Training Loss 0.012921077725679977\n",
      "Epoch 90, Training Loss 0.013234786308177596\n",
      "Epoch 90, Training Loss 0.013681804680305979\n",
      "Epoch 90, Training Loss 0.01413464106028647\n",
      "Epoch 90, Training Loss 0.014444216056857877\n",
      "Epoch 90, Training Loss 0.014749541935865836\n",
      "Epoch 90, Training Loss 0.015035369016630265\n",
      "Epoch 90, Training Loss 0.015307547453114443\n",
      "Epoch 90, Training Loss 0.015632411479340184\n",
      "Epoch 90, Training Loss 0.01615288354399259\n",
      "Epoch 90, Training Loss 0.016495144771188117\n",
      "Epoch 90, Training Loss 0.016811827145269156\n",
      "Epoch 90, Training Loss 0.017070676416844663\n",
      "Epoch 90, Training Loss 0.01755183575022251\n",
      "Epoch 90, Training Loss 0.01777495249457981\n",
      "Epoch 90, Training Loss 0.018091136346692623\n",
      "Epoch 90, Training Loss 0.01857727724115562\n",
      "Epoch 90, Training Loss 0.018894035759789256\n",
      "Epoch 90, Training Loss 0.01916283569143861\n",
      "Epoch 90, Training Loss 0.019455616469578364\n",
      "Epoch 90, Training Loss 0.019799077907181762\n",
      "Epoch 90, Training Loss 0.020029135886817942\n",
      "Epoch 90, Training Loss 0.02028549092886088\n",
      "Epoch 90, Training Loss 0.020723574701934824\n",
      "Epoch 90, Training Loss 0.02106416528410924\n",
      "Epoch 90, Training Loss 0.021428876425451635\n",
      "Epoch 90, Training Loss 0.02188065047840328\n",
      "Epoch 90, Training Loss 0.02245872136195907\n",
      "Epoch 90, Training Loss 0.022697295755376597\n",
      "Epoch 90, Training Loss 0.022999362567501604\n",
      "Epoch 90, Training Loss 0.023308987240962056\n",
      "Epoch 90, Training Loss 0.02369369006217898\n",
      "Epoch 90, Training Loss 0.02399698610577132\n",
      "Epoch 90, Training Loss 0.024297526901792686\n",
      "Epoch 90, Training Loss 0.02477092459759749\n",
      "Epoch 90, Training Loss 0.02496664075519118\n",
      "Epoch 90, Training Loss 0.025409025818947942\n",
      "Epoch 90, Training Loss 0.025706415324259903\n",
      "Epoch 90, Training Loss 0.026167312241576212\n",
      "Epoch 90, Training Loss 0.02666566717197828\n",
      "Epoch 90, Training Loss 0.027053253539382954\n",
      "Epoch 90, Training Loss 0.027276126015216798\n",
      "Epoch 90, Training Loss 0.027539098601969306\n",
      "Epoch 90, Training Loss 0.027854860324384\n",
      "Epoch 90, Training Loss 0.028017511548441086\n",
      "Epoch 90, Training Loss 0.028270265540046156\n",
      "Epoch 90, Training Loss 0.028559994648027297\n",
      "Epoch 90, Training Loss 0.028942666147523523\n",
      "Epoch 90, Training Loss 0.0292620008330211\n",
      "Epoch 90, Training Loss 0.02957632682283821\n",
      "Epoch 90, Training Loss 0.02968332536347077\n",
      "Epoch 90, Training Loss 0.029965895111374843\n",
      "Epoch 90, Training Loss 0.030366834024410417\n",
      "Epoch 90, Training Loss 0.030590611469486485\n",
      "Epoch 90, Training Loss 0.03111954724125545\n",
      "Epoch 90, Training Loss 0.03144802840050224\n",
      "Epoch 90, Training Loss 0.03165138237502264\n",
      "Epoch 90, Training Loss 0.03196124823006523\n",
      "Epoch 90, Training Loss 0.03210196202940038\n",
      "Epoch 90, Training Loss 0.032424493318857134\n",
      "Epoch 90, Training Loss 0.03269277525413067\n",
      "Epoch 90, Training Loss 0.03288325275797063\n",
      "Epoch 90, Training Loss 0.03323664179886394\n",
      "Epoch 90, Training Loss 0.03352328093574785\n",
      "Epoch 90, Training Loss 0.03379258966011464\n",
      "Epoch 90, Training Loss 0.03404863877102847\n",
      "Epoch 90, Training Loss 0.034446583884527614\n",
      "Epoch 90, Training Loss 0.03491520722541968\n",
      "Epoch 90, Training Loss 0.03533686786089712\n",
      "Epoch 90, Training Loss 0.03558453485903228\n",
      "Epoch 90, Training Loss 0.03599603804747772\n",
      "Epoch 90, Training Loss 0.036249304909611604\n",
      "Epoch 90, Training Loss 0.03663038758708693\n",
      "Epoch 90, Training Loss 0.03711333704154814\n",
      "Epoch 90, Training Loss 0.03737709134855234\n",
      "Epoch 90, Training Loss 0.03771972636242047\n",
      "Epoch 90, Training Loss 0.038044933953782174\n",
      "Epoch 90, Training Loss 0.03835592126411855\n",
      "Epoch 90, Training Loss 0.03862984525158887\n",
      "Epoch 90, Training Loss 0.03897321484316035\n",
      "Epoch 90, Training Loss 0.03928263528305856\n",
      "Epoch 90, Training Loss 0.03961648642445159\n",
      "Epoch 90, Training Loss 0.040009490183323546\n",
      "Epoch 90, Training Loss 0.04036346985898969\n",
      "Epoch 90, Training Loss 0.04058103449166278\n",
      "Epoch 90, Training Loss 0.040857056170092214\n",
      "Epoch 90, Training Loss 0.04118914657351001\n",
      "Epoch 90, Training Loss 0.04187644971415515\n",
      "Epoch 90, Training Loss 0.04231999352421907\n",
      "Epoch 90, Training Loss 0.04271483378451499\n",
      "Epoch 90, Training Loss 0.04289499880826991\n",
      "Epoch 90, Training Loss 0.0431625009288111\n",
      "Epoch 90, Training Loss 0.04356847064155142\n",
      "Epoch 90, Training Loss 0.0438512152589648\n",
      "Epoch 90, Training Loss 0.044064255312199485\n",
      "Epoch 90, Training Loss 0.04434495176310125\n",
      "Epoch 90, Training Loss 0.04453735849093598\n",
      "Epoch 90, Training Loss 0.04473147565103553\n",
      "Epoch 90, Training Loss 0.045029395100329536\n",
      "Epoch 90, Training Loss 0.045348459926178995\n",
      "Epoch 90, Training Loss 0.045726634704929485\n",
      "Epoch 90, Training Loss 0.04592497198058821\n",
      "Epoch 90, Training Loss 0.046345375468740076\n",
      "Epoch 90, Training Loss 0.0468557919173137\n",
      "Epoch 90, Training Loss 0.04715835405013445\n",
      "Epoch 90, Training Loss 0.04733951983358854\n",
      "Epoch 90, Training Loss 0.047793008015512506\n",
      "Epoch 90, Training Loss 0.0481485583345451\n",
      "Epoch 90, Training Loss 0.04844437285190653\n",
      "Epoch 90, Training Loss 0.04879996676922149\n",
      "Epoch 90, Training Loss 0.04921078774363488\n",
      "Epoch 90, Training Loss 0.04963343612411443\n",
      "Epoch 90, Training Loss 0.04995218796841324\n",
      "Epoch 90, Training Loss 0.05039636098096133\n",
      "Epoch 90, Training Loss 0.05082619019672084\n",
      "Epoch 90, Training Loss 0.05115240851364782\n",
      "Epoch 90, Training Loss 0.051417712789133686\n",
      "Epoch 90, Training Loss 0.051696854183817156\n",
      "Epoch 90, Training Loss 0.05180314877796966\n",
      "Epoch 90, Training Loss 0.05202293013939467\n",
      "Epoch 90, Training Loss 0.052320032101839095\n",
      "Epoch 90, Training Loss 0.052640786989951685\n",
      "Epoch 90, Training Loss 0.052900425661021794\n",
      "Epoch 90, Training Loss 0.053410607425834214\n",
      "Epoch 90, Training Loss 0.05379319302451885\n",
      "Epoch 90, Training Loss 0.054129537576071134\n",
      "Epoch 90, Training Loss 0.054450089526374625\n",
      "Epoch 90, Training Loss 0.054678424363932034\n",
      "Epoch 90, Training Loss 0.054999936962752696\n",
      "Epoch 90, Training Loss 0.05532461379075904\n",
      "Epoch 90, Training Loss 0.05569176883686839\n",
      "Epoch 90, Training Loss 0.05611353446646115\n",
      "Epoch 90, Training Loss 0.05647959791676468\n",
      "Epoch 90, Training Loss 0.05679267253297979\n",
      "Epoch 90, Training Loss 0.05712457124115256\n",
      "Epoch 90, Training Loss 0.057501222214201834\n",
      "Epoch 90, Training Loss 0.05772327550727388\n",
      "Epoch 90, Training Loss 0.058045575885897704\n",
      "Epoch 90, Training Loss 0.05824684889991875\n",
      "Epoch 90, Training Loss 0.05857428446259645\n",
      "Epoch 90, Training Loss 0.058754920587896386\n",
      "Epoch 90, Training Loss 0.05908539511091874\n",
      "Epoch 90, Training Loss 0.059358448981095456\n",
      "Epoch 90, Training Loss 0.059799751481208044\n",
      "Epoch 90, Training Loss 0.06008179311442863\n",
      "Epoch 90, Training Loss 0.060218979330623854\n",
      "Epoch 90, Training Loss 0.06083820847903981\n",
      "Epoch 90, Training Loss 0.061109424418653066\n",
      "Epoch 90, Training Loss 0.061453760299078945\n",
      "Epoch 90, Training Loss 0.06171375236776479\n",
      "Epoch 90, Training Loss 0.061953451341527806\n",
      "Epoch 90, Training Loss 0.06223607192868772\n",
      "Epoch 90, Training Loss 0.06240030067503605\n",
      "Epoch 90, Training Loss 0.06276071250743573\n",
      "Epoch 90, Training Loss 0.06326610806500516\n",
      "Epoch 90, Training Loss 0.06366388777942608\n",
      "Epoch 90, Training Loss 0.06390111710485595\n",
      "Epoch 90, Training Loss 0.0641659858167324\n",
      "Epoch 90, Training Loss 0.06444345004951862\n",
      "Epoch 90, Training Loss 0.06481307373403589\n",
      "Epoch 90, Training Loss 0.06502319883812419\n",
      "Epoch 90, Training Loss 0.0653692216367063\n",
      "Epoch 90, Training Loss 0.06558674200416525\n",
      "Epoch 90, Training Loss 0.06594387451401147\n",
      "Epoch 90, Training Loss 0.06621624639881846\n",
      "Epoch 90, Training Loss 0.06656674762516071\n",
      "Epoch 90, Training Loss 0.06686892049849186\n",
      "Epoch 90, Training Loss 0.06724551697368816\n",
      "Epoch 90, Training Loss 0.0674502470189958\n",
      "Epoch 90, Training Loss 0.0677219339267677\n",
      "Epoch 90, Training Loss 0.06808246630232048\n",
      "Epoch 90, Training Loss 0.06846715121165566\n",
      "Epoch 90, Training Loss 0.06880367865495364\n",
      "Epoch 90, Training Loss 0.06907258407615335\n",
      "Epoch 90, Training Loss 0.06937322616958252\n",
      "Epoch 90, Training Loss 0.06977723921884967\n",
      "Epoch 90, Training Loss 0.07004615674009713\n",
      "Epoch 90, Training Loss 0.07027931679087832\n",
      "Epoch 90, Training Loss 0.07050668953172386\n",
      "Epoch 90, Training Loss 0.0708855198853461\n",
      "Epoch 90, Training Loss 0.07123436632058809\n",
      "Epoch 90, Training Loss 0.07154774168491973\n",
      "Epoch 90, Training Loss 0.07186349925330228\n",
      "Epoch 90, Training Loss 0.07203485140257784\n",
      "Epoch 90, Training Loss 0.0724523011070993\n",
      "Epoch 90, Training Loss 0.07279278578050911\n",
      "Epoch 90, Training Loss 0.07295923327545986\n",
      "Epoch 90, Training Loss 0.07338556811175383\n",
      "Epoch 90, Training Loss 0.07373562344657186\n",
      "Epoch 90, Training Loss 0.07389110826965793\n",
      "Epoch 90, Training Loss 0.0742848839163018\n",
      "Epoch 90, Training Loss 0.07461215670951797\n",
      "Epoch 90, Training Loss 0.07500521896783348\n",
      "Epoch 90, Training Loss 0.07526656988141177\n",
      "Epoch 90, Training Loss 0.07556667530437565\n",
      "Epoch 90, Training Loss 0.0758986916974225\n",
      "Epoch 90, Training Loss 0.07626497340591057\n",
      "Epoch 90, Training Loss 0.07686564416798485\n",
      "Epoch 90, Training Loss 0.07713964510032588\n",
      "Epoch 90, Training Loss 0.07759725774073845\n",
      "Epoch 90, Training Loss 0.0779119871861642\n",
      "Epoch 90, Training Loss 0.07810237027152116\n",
      "Epoch 90, Training Loss 0.07846669474488024\n",
      "Epoch 90, Training Loss 0.07892016657744833\n",
      "Epoch 90, Training Loss 0.0793075007684243\n",
      "Epoch 90, Training Loss 0.07964935288061877\n",
      "Epoch 90, Training Loss 0.08002987514485789\n",
      "Epoch 90, Training Loss 0.08022266360538086\n",
      "Epoch 90, Training Loss 0.08047444408621325\n",
      "Epoch 90, Training Loss 0.08088821103162777\n",
      "Epoch 90, Training Loss 0.08113213972476742\n",
      "Epoch 90, Training Loss 0.0814190105156368\n",
      "Epoch 90, Training Loss 0.08174382739931421\n",
      "Epoch 90, Training Loss 0.0821569833971198\n",
      "Epoch 90, Training Loss 0.08255774364866258\n",
      "Epoch 90, Training Loss 0.08293115710625258\n",
      "Epoch 90, Training Loss 0.08345720235763303\n",
      "Epoch 90, Training Loss 0.0837701317351645\n",
      "Epoch 90, Training Loss 0.08414617225603985\n",
      "Epoch 90, Training Loss 0.08453545125815874\n",
      "Epoch 90, Training Loss 0.0848484870970554\n",
      "Epoch 90, Training Loss 0.08508777497407725\n",
      "Epoch 90, Training Loss 0.08542472494723242\n",
      "Epoch 90, Training Loss 0.08578045552839404\n",
      "Epoch 90, Training Loss 0.08603138504240214\n",
      "Epoch 90, Training Loss 0.08649066531711527\n",
      "Epoch 90, Training Loss 0.08667483478975113\n",
      "Epoch 90, Training Loss 0.08700460632858069\n",
      "Epoch 90, Training Loss 0.08729288903305597\n",
      "Epoch 90, Training Loss 0.08769294992089272\n",
      "Epoch 90, Training Loss 0.08810433626289257\n",
      "Epoch 90, Training Loss 0.08849070500344267\n",
      "Epoch 90, Training Loss 0.08884103717210957\n",
      "Epoch 90, Training Loss 0.08919695796221114\n",
      "Epoch 90, Training Loss 0.08956762838660909\n",
      "Epoch 90, Training Loss 0.08993825445051694\n",
      "Epoch 90, Training Loss 0.09037111614785535\n",
      "Epoch 90, Training Loss 0.09068305393123566\n",
      "Epoch 90, Training Loss 0.090877527747389\n",
      "Epoch 90, Training Loss 0.09138572617145756\n",
      "Epoch 90, Training Loss 0.0916099441916589\n",
      "Epoch 90, Training Loss 0.09185236005488868\n",
      "Epoch 90, Training Loss 0.09203072064711004\n",
      "Epoch 90, Training Loss 0.09227875472448976\n",
      "Epoch 90, Training Loss 0.09262063000779933\n",
      "Epoch 90, Training Loss 0.09290004227205616\n",
      "Epoch 90, Training Loss 0.09337405788967067\n",
      "Epoch 90, Training Loss 0.09371860760747625\n",
      "Epoch 90, Training Loss 0.09398931725060239\n",
      "Epoch 90, Training Loss 0.09433323875679385\n",
      "Epoch 90, Training Loss 0.09461970536795723\n",
      "Epoch 90, Training Loss 0.09486697367428208\n",
      "Epoch 90, Training Loss 0.09507955497373706\n",
      "Epoch 90, Training Loss 0.09540195219085344\n",
      "Epoch 90, Training Loss 0.09574734163291924\n",
      "Epoch 90, Training Loss 0.09603986486106578\n",
      "Epoch 90, Training Loss 0.0963496710733532\n",
      "Epoch 90, Training Loss 0.09660984285156746\n",
      "Epoch 90, Training Loss 0.09685958545569263\n",
      "Epoch 90, Training Loss 0.09720204828683372\n",
      "Epoch 90, Training Loss 0.0976608799260748\n",
      "Epoch 90, Training Loss 0.09794681417324659\n",
      "Epoch 90, Training Loss 0.09821849159153222\n",
      "Epoch 90, Training Loss 0.0985495061006235\n",
      "Epoch 90, Training Loss 0.0987484465493723\n",
      "Epoch 90, Training Loss 0.09908802381447514\n",
      "Epoch 90, Training Loss 0.09949699750222514\n",
      "Epoch 90, Training Loss 0.09966362083850004\n",
      "Epoch 90, Training Loss 0.10025658556605543\n",
      "Epoch 90, Training Loss 0.10051956280227513\n",
      "Epoch 90, Training Loss 0.10081589415364559\n",
      "Epoch 90, Training Loss 0.1011248217119128\n",
      "Epoch 90, Training Loss 0.10154053758439201\n",
      "Epoch 90, Training Loss 0.10189631446967344\n",
      "Epoch 90, Training Loss 0.10209979628548597\n",
      "Epoch 90, Training Loss 0.10239084386040488\n",
      "Epoch 90, Training Loss 0.10268926880586787\n",
      "Epoch 90, Training Loss 0.10292207654517935\n",
      "Epoch 90, Training Loss 0.10320655615700175\n",
      "Epoch 90, Training Loss 0.10352319689548534\n",
      "Epoch 90, Training Loss 0.103836958603862\n",
      "Epoch 90, Training Loss 0.10425836420463175\n",
      "Epoch 90, Training Loss 0.1047268553405924\n",
      "Epoch 90, Training Loss 0.10504695835054073\n",
      "Epoch 90, Training Loss 0.10530771308428491\n",
      "Epoch 90, Training Loss 0.10563506122173556\n",
      "Epoch 90, Training Loss 0.10600179556728628\n",
      "Epoch 90, Training Loss 0.10657760437073001\n",
      "Epoch 90, Training Loss 0.10702218028628613\n",
      "Epoch 90, Training Loss 0.10724153136239027\n",
      "Epoch 90, Training Loss 0.10746168542434187\n",
      "Epoch 90, Training Loss 0.10771033681376511\n",
      "Epoch 90, Training Loss 0.10787764977654228\n",
      "Epoch 90, Training Loss 0.10821309164547555\n",
      "Epoch 90, Training Loss 0.10858163333800443\n",
      "Epoch 90, Training Loss 0.10889917184286715\n",
      "Epoch 90, Training Loss 0.10929216111979216\n",
      "Epoch 90, Training Loss 0.10974209710879398\n",
      "Epoch 90, Training Loss 0.11008738426258192\n",
      "Epoch 90, Training Loss 0.11037283784250164\n",
      "Epoch 90, Training Loss 0.11061343957510446\n",
      "Epoch 90, Training Loss 0.11096321514157383\n",
      "Epoch 90, Training Loss 0.11152527573735208\n",
      "Epoch 90, Training Loss 0.11174609648335315\n",
      "Epoch 90, Training Loss 0.11209218940504677\n",
      "Epoch 90, Training Loss 0.1124440988482874\n",
      "Epoch 90, Training Loss 0.11273548842581642\n",
      "Epoch 90, Training Loss 0.11312584480856691\n",
      "Epoch 90, Training Loss 0.11348158224959813\n",
      "Epoch 90, Training Loss 0.11385514921582568\n",
      "Epoch 90, Training Loss 0.11433967705006184\n",
      "Epoch 90, Training Loss 0.11482627161056794\n",
      "Epoch 90, Training Loss 0.11520971808477741\n",
      "Epoch 90, Training Loss 0.11548558461582265\n",
      "Epoch 90, Training Loss 0.11579103077121099\n",
      "Epoch 90, Training Loss 0.11610124299250295\n",
      "Epoch 90, Training Loss 0.11636967877940753\n",
      "Epoch 90, Training Loss 0.11660483202246753\n",
      "Epoch 90, Training Loss 0.11684506276951116\n",
      "Epoch 90, Training Loss 0.1171613673362738\n",
      "Epoch 90, Training Loss 0.11751501537535501\n",
      "Epoch 90, Training Loss 0.11774704177551867\n",
      "Epoch 90, Training Loss 0.11803537147010074\n",
      "Epoch 90, Training Loss 0.118279458111738\n",
      "Epoch 90, Training Loss 0.11854278443909971\n",
      "Epoch 90, Training Loss 0.11880220957767323\n",
      "Epoch 90, Training Loss 0.1193089341968679\n",
      "Epoch 90, Training Loss 0.11962107569932023\n",
      "Epoch 90, Training Loss 0.12014609825847399\n",
      "Epoch 90, Training Loss 0.12051788566019529\n",
      "Epoch 90, Training Loss 0.1209035974538997\n",
      "Epoch 90, Training Loss 0.12111212188363685\n",
      "Epoch 90, Training Loss 0.1214678340574817\n",
      "Epoch 90, Training Loss 0.12180175840892755\n",
      "Epoch 90, Training Loss 0.1221977406088501\n",
      "Epoch 90, Training Loss 0.12240794639262702\n",
      "Epoch 90, Training Loss 0.12261473672354922\n",
      "Epoch 90, Training Loss 0.122979872681372\n",
      "Epoch 90, Training Loss 0.12340968444257441\n",
      "Epoch 90, Training Loss 0.12394612594066984\n",
      "Epoch 90, Training Loss 0.12413141526796324\n",
      "Epoch 90, Training Loss 0.12461016209953277\n",
      "Epoch 90, Training Loss 0.12491006300310649\n",
      "Epoch 90, Training Loss 0.12531449137936773\n",
      "Epoch 90, Training Loss 0.12568920478224754\n",
      "Epoch 90, Training Loss 0.1260160310741733\n",
      "Epoch 90, Training Loss 0.12651449968785886\n",
      "Epoch 90, Training Loss 0.12705313348594835\n",
      "Epoch 90, Training Loss 0.12752872008992278\n",
      "Epoch 90, Training Loss 0.1277646889714786\n",
      "Epoch 90, Training Loss 0.12815226047583247\n",
      "Epoch 90, Training Loss 0.12857100395176113\n",
      "Epoch 90, Training Loss 0.12915396161587037\n",
      "Epoch 90, Training Loss 0.12952199270543846\n",
      "Epoch 90, Training Loss 0.12978616573126114\n",
      "Epoch 90, Training Loss 0.1299774400851763\n",
      "Epoch 90, Training Loss 0.13029763928574065\n",
      "Epoch 90, Training Loss 0.13069071842695745\n",
      "Epoch 90, Training Loss 0.13104734818458252\n",
      "Epoch 90, Training Loss 0.13140557480552006\n",
      "Epoch 90, Training Loss 0.1319278016629274\n",
      "Epoch 90, Training Loss 0.13231381266127767\n",
      "Epoch 90, Training Loss 0.13252913306855485\n",
      "Epoch 90, Training Loss 0.1329416523180197\n",
      "Epoch 90, Training Loss 0.1334175727689815\n",
      "Epoch 90, Training Loss 0.13382839317173909\n",
      "Epoch 90, Training Loss 0.1341549324924531\n",
      "Epoch 90, Training Loss 0.1345241951191669\n",
      "Epoch 90, Training Loss 0.13470158258171946\n",
      "Epoch 90, Training Loss 0.13488868470577633\n",
      "Epoch 90, Training Loss 0.13522396986479954\n",
      "Epoch 90, Training Loss 0.1354673824861379\n",
      "Epoch 90, Training Loss 0.1357946302789404\n",
      "Epoch 90, Training Loss 0.13613366976837674\n",
      "Epoch 90, Training Loss 0.1363939390520153\n",
      "Epoch 90, Training Loss 0.13672239646849121\n",
      "Epoch 90, Training Loss 0.1369747976913019\n",
      "Epoch 90, Training Loss 0.13725364212985233\n",
      "Epoch 90, Training Loss 0.13775085344377075\n",
      "Epoch 90, Training Loss 0.13834923685854658\n",
      "Epoch 90, Training Loss 0.13875820753557602\n",
      "Epoch 90, Training Loss 0.13921808522871082\n",
      "Epoch 90, Training Loss 0.13956146240424927\n",
      "Epoch 90, Training Loss 0.1397566775626996\n",
      "Epoch 90, Training Loss 0.14011636259191482\n",
      "Epoch 90, Training Loss 0.14039036586804463\n",
      "Epoch 90, Training Loss 0.14054964811486356\n",
      "Epoch 90, Training Loss 0.14088576053605056\n",
      "Epoch 90, Training Loss 0.14124148360946598\n",
      "Epoch 90, Training Loss 0.14145461110698293\n",
      "Epoch 90, Training Loss 0.1421114535302\n",
      "Epoch 90, Training Loss 0.14230176449165016\n",
      "Epoch 90, Training Loss 0.14266814059956603\n",
      "Epoch 90, Training Loss 0.14292589880888115\n",
      "Epoch 90, Training Loss 0.14312142113704815\n",
      "Epoch 90, Training Loss 0.1434835815502097\n",
      "Epoch 90, Training Loss 0.14378255172191984\n",
      "Epoch 90, Training Loss 0.1442265451202155\n",
      "Epoch 90, Training Loss 0.14486021951526937\n",
      "Epoch 90, Training Loss 0.1451566639592123\n",
      "Epoch 90, Training Loss 0.14571084584230962\n",
      "Epoch 90, Training Loss 0.14604762912063343\n",
      "Epoch 90, Training Loss 0.14646447519474018\n",
      "Epoch 90, Training Loss 0.14673664109290713\n",
      "Epoch 90, Training Loss 0.1469186038026572\n",
      "Epoch 90, Training Loss 0.1472952414751815\n",
      "Epoch 90, Training Loss 0.1475909954160833\n",
      "Epoch 90, Training Loss 0.1478618422375463\n",
      "Epoch 90, Training Loss 0.14804606437873657\n",
      "Epoch 90, Training Loss 0.1484275340671887\n",
      "Epoch 90, Training Loss 0.14875205677679126\n",
      "Epoch 90, Training Loss 0.14906876462766583\n",
      "Epoch 90, Training Loss 0.14934573083391886\n",
      "Epoch 90, Training Loss 0.14950289820199428\n",
      "Epoch 90, Training Loss 0.14976002013934847\n",
      "Epoch 90, Training Loss 0.15007044415911444\n",
      "Epoch 90, Training Loss 0.15029371336407368\n",
      "Epoch 90, Training Loss 0.15053604399342366\n",
      "Epoch 90, Training Loss 0.1508224371849271\n",
      "Epoch 90, Training Loss 0.15110044456694438\n",
      "Epoch 90, Training Loss 0.15147648642168327\n",
      "Epoch 90, Training Loss 0.15192301107375214\n",
      "Epoch 90, Training Loss 0.15229957027699026\n",
      "Epoch 90, Training Loss 0.15275735239429242\n",
      "Epoch 90, Training Loss 0.15314188852067798\n",
      "Epoch 90, Training Loss 0.15345525441457852\n",
      "Epoch 90, Training Loss 0.15380963497340222\n",
      "Epoch 90, Training Loss 0.1542265410332576\n",
      "Epoch 90, Training Loss 0.15446132153768063\n",
      "Epoch 90, Training Loss 0.15478397085500495\n",
      "Epoch 90, Training Loss 0.15513501176253305\n",
      "Epoch 90, Training Loss 0.15550333546365008\n",
      "Epoch 90, Training Loss 0.15582070769289572\n",
      "Epoch 90, Training Loss 0.15608817873441655\n",
      "Epoch 90, Training Loss 0.15664801722787836\n",
      "Epoch 90, Training Loss 0.15689197551373327\n",
      "Epoch 90, Training Loss 0.15721551337472314\n",
      "Epoch 90, Training Loss 0.1575798075884352\n",
      "Epoch 90, Training Loss 0.15791178800527703\n",
      "Epoch 90, Training Loss 0.15851867018872515\n",
      "Epoch 90, Training Loss 0.15876660887580699\n",
      "Epoch 90, Training Loss 0.1591305205755679\n",
      "Epoch 90, Training Loss 0.1595410320936414\n",
      "Epoch 90, Training Loss 0.15984172982823514\n",
      "Epoch 90, Training Loss 0.16002285986414652\n",
      "Epoch 90, Training Loss 0.1605183886044928\n",
      "Epoch 90, Training Loss 0.1607335563796713\n",
      "Epoch 90, Training Loss 0.16105997058398583\n",
      "Epoch 90, Training Loss 0.16127899566384227\n",
      "Epoch 90, Training Loss 0.16154010328071197\n",
      "Epoch 90, Training Loss 0.16182790709006817\n",
      "Epoch 90, Training Loss 0.1619900847354051\n",
      "Epoch 90, Training Loss 0.1626497337599392\n",
      "Epoch 90, Training Loss 0.162972517454487\n",
      "Epoch 90, Training Loss 0.16343663980627\n",
      "Epoch 90, Training Loss 0.16354785667127356\n",
      "Epoch 90, Training Loss 0.16384125834383317\n",
      "Epoch 90, Training Loss 0.16430136206967141\n",
      "Epoch 90, Training Loss 0.16465012313764724\n",
      "Epoch 90, Training Loss 0.16500970770788315\n",
      "Epoch 90, Training Loss 0.16525647009882477\n",
      "Epoch 90, Training Loss 0.16557381298307264\n",
      "Epoch 90, Training Loss 0.16591794356284545\n",
      "Epoch 90, Training Loss 0.166245028052641\n",
      "Epoch 90, Training Loss 0.16650979750601533\n",
      "Epoch 90, Training Loss 0.16675866665818806\n",
      "Epoch 90, Training Loss 0.16703349090826786\n",
      "Epoch 90, Training Loss 0.16734147702565277\n",
      "Epoch 90, Training Loss 0.16769773048131972\n",
      "Epoch 90, Training Loss 0.16799719233418364\n",
      "Epoch 90, Training Loss 0.16841298526586473\n",
      "Epoch 90, Training Loss 0.16871201917719658\n",
      "Epoch 90, Training Loss 0.16921478184059147\n",
      "Epoch 90, Training Loss 0.16965624638606824\n",
      "Epoch 90, Training Loss 0.16991315764920487\n",
      "Epoch 90, Training Loss 0.17021543869886863\n",
      "Epoch 90, Training Loss 0.17072556234534134\n",
      "Epoch 90, Training Loss 0.17123789166855385\n",
      "Epoch 90, Training Loss 0.17167758956894547\n",
      "Epoch 90, Training Loss 0.17194818047916188\n",
      "Epoch 90, Training Loss 0.17255328831922673\n",
      "Epoch 90, Training Loss 0.17282128234958405\n",
      "Epoch 90, Training Loss 0.17306517923960602\n",
      "Epoch 90, Training Loss 0.1733718416308198\n",
      "Epoch 90, Training Loss 0.17358253057807913\n",
      "Epoch 90, Training Loss 0.1740509966755157\n",
      "Epoch 90, Training Loss 0.17440411029264447\n",
      "Epoch 90, Training Loss 0.17469740016838473\n",
      "Epoch 90, Training Loss 0.1749742100077212\n",
      "Epoch 90, Training Loss 0.1754472084591151\n",
      "Epoch 90, Training Loss 0.17568594223016973\n",
      "Epoch 90, Training Loss 0.1759404326834337\n",
      "Epoch 90, Training Loss 0.17632917699682743\n",
      "Epoch 90, Training Loss 0.1767346410609572\n",
      "Epoch 90, Training Loss 0.1771550004172813\n",
      "Epoch 90, Training Loss 0.17742918786185477\n",
      "Epoch 90, Training Loss 0.1778994891268518\n",
      "Epoch 90, Training Loss 0.17821034844345449\n",
      "Epoch 90, Training Loss 0.17866840274513834\n",
      "Epoch 90, Training Loss 0.179083595323898\n",
      "Epoch 90, Training Loss 0.17952708665595946\n",
      "Epoch 90, Training Loss 0.17982367151762213\n",
      "Epoch 90, Training Loss 0.1801813001865926\n",
      "Epoch 90, Training Loss 0.1806505863052195\n",
      "Epoch 90, Training Loss 0.18106898819775227\n",
      "Epoch 90, Training Loss 0.18153568700222714\n",
      "Epoch 90, Training Loss 0.18196759327217135\n",
      "Epoch 90, Training Loss 0.18223585300814465\n",
      "Epoch 90, Training Loss 0.18261563772207026\n",
      "Epoch 90, Training Loss 0.18324460860942027\n",
      "Epoch 90, Training Loss 0.18351723444278892\n",
      "Epoch 90, Training Loss 0.18414705938390455\n",
      "Epoch 90, Training Loss 0.1846709106584339\n",
      "Epoch 90, Training Loss 0.18519008754159483\n",
      "Epoch 90, Training Loss 0.18555743733178015\n",
      "Epoch 90, Training Loss 0.18606058010817184\n",
      "Epoch 90, Training Loss 0.18639636165498163\n",
      "Epoch 90, Training Loss 0.18665576783363777\n",
      "Epoch 90, Training Loss 0.1869445323677319\n",
      "Epoch 90, Training Loss 0.1872088185432927\n",
      "Epoch 90, Training Loss 0.18787752259570314\n",
      "Epoch 90, Training Loss 0.18835018521836958\n",
      "Epoch 90, Training Loss 0.18868436456641272\n",
      "Epoch 90, Training Loss 0.18910082713569826\n",
      "Epoch 90, Training Loss 0.18981179781734486\n",
      "Epoch 90, Training Loss 0.19023888720118481\n",
      "Epoch 90, Training Loss 0.19050440337042066\n",
      "Epoch 90, Training Loss 0.1907621225165894\n",
      "Epoch 90, Training Loss 0.1910172225645436\n",
      "Epoch 90, Training Loss 0.19137033179897786\n",
      "Epoch 90, Training Loss 0.19182761859558428\n",
      "Epoch 90, Training Loss 0.19207215970358277\n",
      "Epoch 90, Training Loss 0.19255959867592662\n",
      "Epoch 90, Training Loss 0.19298876595237982\n",
      "Epoch 90, Training Loss 0.193562623580246\n",
      "Epoch 90, Training Loss 0.19379813296486958\n",
      "Epoch 90, Training Loss 0.19414892447802723\n",
      "Epoch 90, Training Loss 0.19464097385440032\n",
      "Epoch 90, Training Loss 0.1949830714355954\n",
      "Epoch 90, Training Loss 0.19531564393540476\n",
      "Epoch 90, Training Loss 0.19562644433334966\n",
      "Epoch 90, Training Loss 0.19590487805626278\n",
      "Epoch 90, Training Loss 0.1962953058197675\n",
      "Epoch 90, Training Loss 0.1965447776495953\n",
      "Epoch 90, Training Loss 0.19690212662643788\n",
      "Epoch 90, Training Loss 0.19731305046078493\n",
      "Epoch 90, Training Loss 0.19778492484632357\n",
      "Epoch 90, Training Loss 0.1981324985852022\n",
      "Epoch 90, Training Loss 0.19859032125195578\n",
      "Epoch 90, Training Loss 0.1990915751632522\n",
      "Epoch 90, Training Loss 0.19956089127475343\n",
      "Epoch 90, Training Loss 0.19990459871490288\n",
      "Epoch 90, Training Loss 0.20034665625799647\n",
      "Epoch 90, Training Loss 0.20072678369862954\n",
      "Epoch 90, Training Loss 0.20103177134795566\n",
      "Epoch 90, Training Loss 0.20125965150954472\n",
      "Epoch 90, Training Loss 0.20161335038788178\n",
      "Epoch 90, Training Loss 0.20209032565812626\n",
      "Epoch 90, Training Loss 0.20248375975948465\n",
      "Epoch 90, Training Loss 0.2028456886901575\n",
      "Epoch 90, Training Loss 0.20330755441161372\n",
      "Epoch 90, Training Loss 0.20376563051243876\n",
      "Epoch 90, Training Loss 0.20403390694076143\n",
      "Epoch 90, Training Loss 0.20423605363539724\n",
      "Epoch 90, Training Loss 0.2045207351751035\n",
      "Epoch 90, Training Loss 0.20482971033324365\n",
      "Epoch 90, Training Loss 0.2053016974493061\n",
      "Epoch 90, Training Loss 0.20556599307624276\n",
      "Epoch 90, Training Loss 0.20594954629764534\n",
      "Epoch 90, Training Loss 0.206284916557162\n",
      "Epoch 90, Training Loss 0.20666976797077663\n",
      "Epoch 90, Training Loss 0.20712209271881588\n",
      "Epoch 90, Training Loss 0.20734756291293732\n",
      "Epoch 90, Training Loss 0.2077017743378649\n",
      "Epoch 90, Training Loss 0.208176358981663\n",
      "Epoch 90, Training Loss 0.20850469737940128\n",
      "Epoch 90, Training Loss 0.2090661466464667\n",
      "Epoch 90, Training Loss 0.20949035759091073\n",
      "Epoch 90, Training Loss 0.20978266067440857\n",
      "Epoch 90, Training Loss 0.21005837367776106\n",
      "Epoch 90, Training Loss 0.2103008042706553\n",
      "Epoch 90, Training Loss 0.21089678082396002\n",
      "Epoch 90, Training Loss 0.21139878711051038\n",
      "Epoch 90, Training Loss 0.21181606669026568\n",
      "Epoch 90, Training Loss 0.21220273090064373\n",
      "Epoch 90, Training Loss 0.21258920516885454\n",
      "Epoch 90, Training Loss 0.2128374555417339\n",
      "Epoch 90, Training Loss 0.21320742065720547\n",
      "Epoch 90, Training Loss 0.2134425119899423\n",
      "Epoch 90, Training Loss 0.21390083737080665\n",
      "Epoch 90, Training Loss 0.21411590443928832\n",
      "Epoch 90, Training Loss 0.21439298724426942\n",
      "Epoch 90, Training Loss 0.21489762813996172\n",
      "Epoch 90, Training Loss 0.21537374615516808\n",
      "Epoch 90, Training Loss 0.21552972183050706\n",
      "Epoch 90, Training Loss 0.21591333602852833\n",
      "Epoch 90, Training Loss 0.2162870157252797\n",
      "Epoch 90, Training Loss 0.21661914088537015\n",
      "Epoch 90, Training Loss 0.21700713145153602\n",
      "Epoch 90, Training Loss 0.2173086730453669\n",
      "Epoch 90, Training Loss 0.21779582222633045\n",
      "Epoch 90, Training Loss 0.21831983466968513\n",
      "Epoch 90, Training Loss 0.2186727166137732\n",
      "Epoch 90, Training Loss 0.2192694799941214\n",
      "Epoch 90, Training Loss 0.21952345862489223\n",
      "Epoch 90, Training Loss 0.2199878208815594\n",
      "Epoch 90, Training Loss 0.2201666324530416\n",
      "Epoch 90, Training Loss 0.220480888395968\n",
      "Epoch 90, Training Loss 0.22082115439197902\n",
      "Epoch 90, Training Loss 0.221208951700374\n",
      "Epoch 90, Training Loss 0.22158917578894768\n",
      "Epoch 90, Training Loss 0.22199513330636428\n",
      "Epoch 90, Training Loss 0.22245035676852518\n",
      "Epoch 90, Training Loss 0.2226581313192387\n",
      "Epoch 90, Training Loss 0.2230605110716637\n",
      "Epoch 90, Training Loss 0.2235205913596141\n",
      "Epoch 90, Training Loss 0.2238321163693962\n",
      "Epoch 90, Training Loss 0.22406153396114975\n",
      "Epoch 90, Training Loss 0.22438894802956935\n",
      "Epoch 90, Training Loss 0.22479509175433526\n",
      "Epoch 90, Training Loss 0.22525330806327293\n",
      "Epoch 90, Training Loss 0.22560711502266662\n",
      "Epoch 90, Training Loss 0.22599581333682361\n",
      "Epoch 90, Training Loss 0.22642336724816686\n",
      "Epoch 90, Training Loss 0.22709824647897345\n",
      "Epoch 90, Training Loss 0.22769043859465957\n",
      "Epoch 90, Training Loss 0.22805716386993827\n",
      "Epoch 90, Training Loss 0.22832507112294512\n",
      "Epoch 90, Training Loss 0.22867762063013014\n",
      "Epoch 90, Training Loss 0.22907129334061957\n",
      "Epoch 90, Training Loss 0.22931417169244697\n",
      "Epoch 90, Training Loss 0.22965469279938647\n",
      "Epoch 90, Training Loss 0.2299475995895198\n",
      "Epoch 90, Training Loss 0.23009555822100175\n",
      "Epoch 90, Training Loss 0.23047777056655921\n",
      "Epoch 90, Training Loss 0.23065595506973888\n",
      "Epoch 90, Training Loss 0.23097774583627195\n",
      "Epoch 90, Training Loss 0.23139664739408455\n",
      "Epoch 90, Training Loss 0.23168065026402473\n",
      "Epoch 90, Training Loss 0.23202049670278874\n",
      "Epoch 90, Training Loss 0.2324959387540665\n",
      "Epoch 90, Training Loss 0.23305004285386458\n",
      "Epoch 90, Training Loss 0.23347187170858882\n",
      "Epoch 90, Training Loss 0.23369750339547388\n",
      "Epoch 90, Training Loss 0.23406100544668829\n",
      "Epoch 90, Training Loss 0.23439593412115445\n",
      "Epoch 90, Training Loss 0.23469960749568536\n",
      "Epoch 90, Training Loss 0.23497388880614126\n",
      "Epoch 90, Training Loss 0.23521790296182304\n",
      "Epoch 90, Training Loss 0.23558478441346636\n",
      "Epoch 90, Training Loss 0.2358273241354529\n",
      "Epoch 90, Training Loss 0.236097349201703\n",
      "Epoch 90, Training Loss 0.236394927543981\n",
      "Epoch 90, Training Loss 0.23668364261079322\n",
      "Epoch 90, Training Loss 0.23712060875866725\n",
      "Epoch 90, Training Loss 0.2374606416334429\n",
      "Epoch 90, Training Loss 0.23803198877769663\n",
      "Epoch 90, Training Loss 0.2385024072702431\n",
      "Epoch 90, Training Loss 0.23881512059999244\n",
      "Epoch 90, Training Loss 0.23930589015335987\n",
      "Epoch 90, Training Loss 0.239849575140211\n",
      "Epoch 90, Training Loss 0.24034802275507347\n",
      "Epoch 90, Training Loss 0.24072968234758244\n",
      "Epoch 90, Training Loss 0.2409822261985153\n",
      "Epoch 90, Training Loss 0.24136040368310327\n",
      "Epoch 90, Training Loss 0.2415950765066287\n",
      "Epoch 90, Training Loss 0.2419532188273909\n",
      "Epoch 90, Training Loss 0.2423016445620743\n",
      "Epoch 90, Training Loss 0.24261857928408076\n",
      "Epoch 90, Training Loss 0.24292776258209783\n",
      "Epoch 90, Training Loss 0.24327376849777863\n",
      "Epoch 90, Training Loss 0.24386197708718613\n",
      "Epoch 90, Training Loss 0.24413813320000458\n",
      "Epoch 90, Training Loss 0.2443794624979996\n",
      "Epoch 90, Training Loss 0.2447352798946221\n",
      "Epoch 90, Training Loss 0.24498903056811494\n",
      "Epoch 90, Training Loss 0.24541271688497585\n",
      "Epoch 90, Training Loss 0.24582070900160638\n",
      "Epoch 90, Training Loss 0.24606959294060918\n",
      "Epoch 90, Training Loss 0.24650505849200746\n",
      "Epoch 90, Training Loss 0.24697898687494685\n",
      "Epoch 90, Training Loss 0.247285873812559\n",
      "Epoch 90, Training Loss 0.2475695319931068\n",
      "Epoch 90, Training Loss 0.24786953912938342\n",
      "Epoch 90, Training Loss 0.248198942559988\n",
      "Epoch 90, Training Loss 0.24854180734137746\n",
      "Epoch 90, Training Loss 0.24894828129264399\n",
      "Epoch 90, Training Loss 0.2491137454062319\n",
      "Epoch 90, Training Loss 0.24948548097782733\n",
      "Epoch 90, Training Loss 0.24997708065163754\n",
      "Epoch 90, Training Loss 0.2502965454841056\n",
      "Epoch 90, Training Loss 0.25078838090876787\n",
      "Epoch 90, Training Loss 0.2511208814561672\n",
      "Epoch 90, Training Loss 0.25147818565330543\n",
      "Epoch 90, Training Loss 0.2518108567446851\n",
      "Epoch 90, Training Loss 0.2521688460141344\n",
      "Epoch 90, Training Loss 0.2524187454425008\n",
      "Epoch 90, Training Loss 0.25284393269883093\n",
      "Epoch 90, Training Loss 0.25307086600786277\n",
      "Epoch 90, Training Loss 0.25333952020539346\n",
      "Epoch 90, Training Loss 0.2536212329269218\n",
      "Epoch 90, Training Loss 0.25393103889149166\n",
      "Epoch 90, Training Loss 0.2542110073387318\n",
      "Epoch 90, Training Loss 0.2544574978120644\n",
      "Epoch 90, Training Loss 0.25461317675993267\n",
      "Epoch 90, Training Loss 0.25498348353502087\n",
      "Epoch 90, Training Loss 0.2552280843810505\n",
      "Epoch 90, Training Loss 0.25557556962761124\n",
      "Epoch 90, Training Loss 0.25583511900604533\n",
      "Epoch 90, Training Loss 0.25622513441516614\n",
      "Epoch 90, Training Loss 0.25660340782359736\n",
      "Epoch 90, Training Loss 0.2569303269524251\n",
      "Epoch 90, Training Loss 0.2572801788921094\n",
      "Epoch 90, Training Loss 0.25766848352597194\n",
      "Epoch 90, Training Loss 0.2579526043094485\n",
      "Epoch 90, Training Loss 0.2583133281896944\n",
      "Epoch 90, Training Loss 0.2585588223241327\n",
      "Epoch 90, Training Loss 0.2589413267095833\n",
      "Epoch 90, Training Loss 0.2591249324343241\n",
      "Epoch 90, Training Loss 0.2595911347366812\n",
      "Epoch 90, Training Loss 0.25996677868087276\n",
      "Epoch 90, Training Loss 0.2602493951902213\n",
      "Epoch 90, Training Loss 0.26062103060772046\n",
      "Epoch 90, Training Loss 0.2609435894509868\n",
      "Epoch 90, Training Loss 0.2613399155209284\n",
      "Epoch 90, Training Loss 0.26180306975455847\n",
      "Epoch 90, Training Loss 0.2620448660000663\n",
      "Epoch 90, Training Loss 0.2624505288098627\n",
      "Epoch 90, Training Loss 0.26275417335388607\n",
      "Epoch 90, Training Loss 0.2630498935289852\n",
      "Epoch 90, Training Loss 0.2632606250264913\n",
      "Epoch 90, Training Loss 0.26370338818339434\n",
      "Epoch 90, Training Loss 0.26392687755205746\n",
      "Epoch 90, Training Loss 0.26425488858157414\n",
      "Epoch 90, Training Loss 0.26446956812458877\n",
      "Epoch 90, Training Loss 0.2646911213141116\n",
      "Epoch 90, Training Loss 0.2650302112616999\n",
      "Epoch 90, Training Loss 0.26543859763028066\n",
      "Epoch 90, Training Loss 0.26571677109736314\n",
      "Epoch 90, Training Loss 0.26628747874932823\n",
      "Epoch 90, Training Loss 0.2666971595371928\n",
      "Epoch 90, Training Loss 0.26691186920646814\n",
      "Epoch 90, Training Loss 0.26726813737274435\n",
      "Epoch 90, Training Loss 0.26760407086566584\n",
      "Epoch 90, Training Loss 0.26841298364045674\n",
      "Epoch 100, Training Loss 0.0003780570176556287\n",
      "Epoch 100, Training Loss 0.0007968503038596619\n",
      "Epoch 100, Training Loss 0.0010480283738097266\n",
      "Epoch 100, Training Loss 0.001304471355569942\n",
      "Epoch 100, Training Loss 0.0016303057865718442\n",
      "Epoch 100, Training Loss 0.0020366686841715937\n",
      "Epoch 100, Training Loss 0.002218865353585509\n",
      "Epoch 100, Training Loss 0.0025963796412243564\n",
      "Epoch 100, Training Loss 0.00287548641262152\n",
      "Epoch 100, Training Loss 0.0030731785365992496\n",
      "Epoch 100, Training Loss 0.0033739940703982284\n",
      "Epoch 100, Training Loss 0.003708202458556046\n",
      "Epoch 100, Training Loss 0.004025531451567969\n",
      "Epoch 100, Training Loss 0.004389322646286177\n",
      "Epoch 100, Training Loss 0.0047807447860003125\n",
      "Epoch 100, Training Loss 0.004986398245977319\n",
      "Epoch 100, Training Loss 0.005222760331447777\n",
      "Epoch 100, Training Loss 0.00543022687401613\n",
      "Epoch 100, Training Loss 0.005671494147356819\n",
      "Epoch 100, Training Loss 0.00587120301583234\n",
      "Epoch 100, Training Loss 0.0060387076738545355\n",
      "Epoch 100, Training Loss 0.006329527119998736\n",
      "Epoch 100, Training Loss 0.006691999047461069\n",
      "Epoch 100, Training Loss 0.007114696819001756\n",
      "Epoch 100, Training Loss 0.0074949488043785095\n",
      "Epoch 100, Training Loss 0.007831600068322838\n",
      "Epoch 100, Training Loss 0.008164015210345578\n",
      "Epoch 100, Training Loss 0.008681527431816091\n",
      "Epoch 100, Training Loss 0.008967233550213182\n",
      "Epoch 100, Training Loss 0.009240044142736499\n",
      "Epoch 100, Training Loss 0.009460871729551984\n",
      "Epoch 100, Training Loss 0.009660724388516468\n",
      "Epoch 100, Training Loss 0.00981071114044665\n",
      "Epoch 100, Training Loss 0.010346189779622475\n",
      "Epoch 100, Training Loss 0.010677433751351998\n",
      "Epoch 100, Training Loss 0.01092495333851146\n",
      "Epoch 100, Training Loss 0.011380943276769365\n",
      "Epoch 100, Training Loss 0.011672896259199933\n",
      "Epoch 100, Training Loss 0.011867401421146319\n",
      "Epoch 100, Training Loss 0.012063049418313423\n",
      "Epoch 100, Training Loss 0.012290580202932553\n",
      "Epoch 100, Training Loss 0.012693156909835917\n",
      "Epoch 100, Training Loss 0.012967075142637847\n",
      "Epoch 100, Training Loss 0.01320699816736419\n",
      "Epoch 100, Training Loss 0.013638250120079427\n",
      "Epoch 100, Training Loss 0.013872567316531526\n",
      "Epoch 100, Training Loss 0.014085604454321629\n",
      "Epoch 100, Training Loss 0.014316474852125968\n",
      "Epoch 100, Training Loss 0.01441986455827418\n",
      "Epoch 100, Training Loss 0.014658010267007077\n",
      "Epoch 100, Training Loss 0.014863902793439758\n",
      "Epoch 100, Training Loss 0.01508511916336501\n",
      "Epoch 100, Training Loss 0.015360513201836125\n",
      "Epoch 100, Training Loss 0.015541725479961965\n",
      "Epoch 100, Training Loss 0.0159753513667742\n",
      "Epoch 100, Training Loss 0.01638378708830575\n",
      "Epoch 100, Training Loss 0.01657540677948986\n",
      "Epoch 100, Training Loss 0.016728739404236265\n",
      "Epoch 100, Training Loss 0.016897319727922643\n",
      "Epoch 100, Training Loss 0.017239764570961217\n",
      "Epoch 100, Training Loss 0.017600485478596918\n",
      "Epoch 100, Training Loss 0.018128352863785557\n",
      "Epoch 100, Training Loss 0.01836385929485416\n",
      "Epoch 100, Training Loss 0.018607934541485804\n",
      "Epoch 100, Training Loss 0.018931379794236033\n",
      "Epoch 100, Training Loss 0.0192940897591736\n",
      "Epoch 100, Training Loss 0.01959173105981039\n",
      "Epoch 100, Training Loss 0.019850010581104957\n",
      "Epoch 100, Training Loss 0.020164573998631113\n",
      "Epoch 100, Training Loss 0.020365895755836725\n",
      "Epoch 100, Training Loss 0.020596088398524257\n",
      "Epoch 100, Training Loss 0.020791760348069392\n",
      "Epoch 100, Training Loss 0.021222800140261955\n",
      "Epoch 100, Training Loss 0.02156654005046086\n",
      "Epoch 100, Training Loss 0.02181414024108816\n",
      "Epoch 100, Training Loss 0.022092537378030054\n",
      "Epoch 100, Training Loss 0.022370629612823278\n",
      "Epoch 100, Training Loss 0.022585819244308546\n",
      "Epoch 100, Training Loss 0.02287905359321543\n",
      "Epoch 100, Training Loss 0.023177015897639267\n",
      "Epoch 100, Training Loss 0.023444560949531054\n",
      "Epoch 100, Training Loss 0.023621586230023743\n",
      "Epoch 100, Training Loss 0.024009927221195167\n",
      "Epoch 100, Training Loss 0.02410336426647423\n",
      "Epoch 100, Training Loss 0.02439363722872856\n",
      "Epoch 100, Training Loss 0.024625436109883707\n",
      "Epoch 100, Training Loss 0.025037827575222\n",
      "Epoch 100, Training Loss 0.025317761361065422\n",
      "Epoch 100, Training Loss 0.025550266732568935\n",
      "Epoch 100, Training Loss 0.025750145287541173\n",
      "Epoch 100, Training Loss 0.025949186728814678\n",
      "Epoch 100, Training Loss 0.02615939935340601\n",
      "Epoch 100, Training Loss 0.0263569201330852\n",
      "Epoch 100, Training Loss 0.02666878967981814\n",
      "Epoch 100, Training Loss 0.026932197563407367\n",
      "Epoch 100, Training Loss 0.027220062063554363\n",
      "Epoch 100, Training Loss 0.027634192162843615\n",
      "Epoch 100, Training Loss 0.02785632243889677\n",
      "Epoch 100, Training Loss 0.028068991697124204\n",
      "Epoch 100, Training Loss 0.02830696615683453\n",
      "Epoch 100, Training Loss 0.02860013409839262\n",
      "Epoch 100, Training Loss 0.028926213083745878\n",
      "Epoch 100, Training Loss 0.029177254756621997\n",
      "Epoch 100, Training Loss 0.029367379777495515\n",
      "Epoch 100, Training Loss 0.029560539833343853\n",
      "Epoch 100, Training Loss 0.029850913483239806\n",
      "Epoch 100, Training Loss 0.0301323806404915\n",
      "Epoch 100, Training Loss 0.03037333356030762\n",
      "Epoch 100, Training Loss 0.030558848672586937\n",
      "Epoch 100, Training Loss 0.03076655214742931\n",
      "Epoch 100, Training Loss 0.030968492340934857\n",
      "Epoch 100, Training Loss 0.031217349736053314\n",
      "Epoch 100, Training Loss 0.031591857495286584\n",
      "Epoch 100, Training Loss 0.031783600854675485\n",
      "Epoch 100, Training Loss 0.03192575809443393\n",
      "Epoch 100, Training Loss 0.03226593453103624\n",
      "Epoch 100, Training Loss 0.03263171592636791\n",
      "Epoch 100, Training Loss 0.03294175198239743\n",
      "Epoch 100, Training Loss 0.033141662035604265\n",
      "Epoch 100, Training Loss 0.03337913938343068\n",
      "Epoch 100, Training Loss 0.033601051801458344\n",
      "Epoch 100, Training Loss 0.033939400235252916\n",
      "Epoch 100, Training Loss 0.03418740645393996\n",
      "Epoch 100, Training Loss 0.0344329302763695\n",
      "Epoch 100, Training Loss 0.034764542699317495\n",
      "Epoch 100, Training Loss 0.03512308750387348\n",
      "Epoch 100, Training Loss 0.03557190941194134\n",
      "Epoch 100, Training Loss 0.03576182305355511\n",
      "Epoch 100, Training Loss 0.036068935966705115\n",
      "Epoch 100, Training Loss 0.03634740589448558\n",
      "Epoch 100, Training Loss 0.036648419830957644\n",
      "Epoch 100, Training Loss 0.03694631384156854\n",
      "Epoch 100, Training Loss 0.03730613160926058\n",
      "Epoch 100, Training Loss 0.03759169753860025\n",
      "Epoch 100, Training Loss 0.037825345764379674\n",
      "Epoch 100, Training Loss 0.038088559075389675\n",
      "Epoch 100, Training Loss 0.03835535764008227\n",
      "Epoch 100, Training Loss 0.038701347206407194\n",
      "Epoch 100, Training Loss 0.038959253093470696\n",
      "Epoch 100, Training Loss 0.03928908263630879\n",
      "Epoch 100, Training Loss 0.03961188511927719\n",
      "Epoch 100, Training Loss 0.03976422961791763\n",
      "Epoch 100, Training Loss 0.040049295024493774\n",
      "Epoch 100, Training Loss 0.040290623255397964\n",
      "Epoch 100, Training Loss 0.04060327488443126\n",
      "Epoch 100, Training Loss 0.040793159924199816\n",
      "Epoch 100, Training Loss 0.04110356199238306\n",
      "Epoch 100, Training Loss 0.04138481060562231\n",
      "Epoch 100, Training Loss 0.04157982793305536\n",
      "Epoch 100, Training Loss 0.04187722688974322\n",
      "Epoch 100, Training Loss 0.04208931622221647\n",
      "Epoch 100, Training Loss 0.04225224233649271\n",
      "Epoch 100, Training Loss 0.04254208055451093\n",
      "Epoch 100, Training Loss 0.04282389049563567\n",
      "Epoch 100, Training Loss 0.043136965035630005\n",
      "Epoch 100, Training Loss 0.043384217373702835\n",
      "Epoch 100, Training Loss 0.043572943362281145\n",
      "Epoch 100, Training Loss 0.04375825633706949\n",
      "Epoch 100, Training Loss 0.04399758390605907\n",
      "Epoch 100, Training Loss 0.04427839405929951\n",
      "Epoch 100, Training Loss 0.04465936541633533\n",
      "Epoch 100, Training Loss 0.04491500129632633\n",
      "Epoch 100, Training Loss 0.045147613052974274\n",
      "Epoch 100, Training Loss 0.0455665222709746\n",
      "Epoch 100, Training Loss 0.0457739694915769\n",
      "Epoch 100, Training Loss 0.046168373540386824\n",
      "Epoch 100, Training Loss 0.046436494745104515\n",
      "Epoch 100, Training Loss 0.04655267976586471\n",
      "Epoch 100, Training Loss 0.04677020588799206\n",
      "Epoch 100, Training Loss 0.047265246777278384\n",
      "Epoch 100, Training Loss 0.04760585927292514\n",
      "Epoch 100, Training Loss 0.047822535514374216\n",
      "Epoch 100, Training Loss 0.04803376846834827\n",
      "Epoch 100, Training Loss 0.04824331608574713\n",
      "Epoch 100, Training Loss 0.04851050865467247\n",
      "Epoch 100, Training Loss 0.04862795614868479\n",
      "Epoch 100, Training Loss 0.048805945964954085\n",
      "Epoch 100, Training Loss 0.048980219904190436\n",
      "Epoch 100, Training Loss 0.04914745113924336\n",
      "Epoch 100, Training Loss 0.04956893063605289\n",
      "Epoch 100, Training Loss 0.049885974544317216\n",
      "Epoch 100, Training Loss 0.05024775598779359\n",
      "Epoch 100, Training Loss 0.05044604835036161\n",
      "Epoch 100, Training Loss 0.05061646978682874\n",
      "Epoch 100, Training Loss 0.05088274145637022\n",
      "Epoch 100, Training Loss 0.05116076615955824\n",
      "Epoch 100, Training Loss 0.05135338319956189\n",
      "Epoch 100, Training Loss 0.05162647420831044\n",
      "Epoch 100, Training Loss 0.05185652065955464\n",
      "Epoch 100, Training Loss 0.05216289228757324\n",
      "Epoch 100, Training Loss 0.05233593299375166\n",
      "Epoch 100, Training Loss 0.05244879503651043\n",
      "Epoch 100, Training Loss 0.052617906904815104\n",
      "Epoch 100, Training Loss 0.052733143205609165\n",
      "Epoch 100, Training Loss 0.05301310888984624\n",
      "Epoch 100, Training Loss 0.053339124345184896\n",
      "Epoch 100, Training Loss 0.053629545842671336\n",
      "Epoch 100, Training Loss 0.05402894951688969\n",
      "Epoch 100, Training Loss 0.05429248937674801\n",
      "Epoch 100, Training Loss 0.05458215798449029\n",
      "Epoch 100, Training Loss 0.05484488853217696\n",
      "Epoch 100, Training Loss 0.05524593712690541\n",
      "Epoch 100, Training Loss 0.05567042002706881\n",
      "Epoch 100, Training Loss 0.05593062879141334\n",
      "Epoch 100, Training Loss 0.05633931154447139\n",
      "Epoch 100, Training Loss 0.056605102584871184\n",
      "Epoch 100, Training Loss 0.056933109993901096\n",
      "Epoch 100, Training Loss 0.057217625848701235\n",
      "Epoch 100, Training Loss 0.057483592749480394\n",
      "Epoch 100, Training Loss 0.057871054114816745\n",
      "Epoch 100, Training Loss 0.05830789640393404\n",
      "Epoch 100, Training Loss 0.05872418961066114\n",
      "Epoch 100, Training Loss 0.05900973332164538\n",
      "Epoch 100, Training Loss 0.05912686929182933\n",
      "Epoch 100, Training Loss 0.059318475892095615\n",
      "Epoch 100, Training Loss 0.05952230456959256\n",
      "Epoch 100, Training Loss 0.059775868616521816\n",
      "Epoch 100, Training Loss 0.0601834011981097\n",
      "Epoch 100, Training Loss 0.060638323169001536\n",
      "Epoch 100, Training Loss 0.06090027632196541\n",
      "Epoch 100, Training Loss 0.0612169101815242\n",
      "Epoch 100, Training Loss 0.061529601331028486\n",
      "Epoch 100, Training Loss 0.061676675020276434\n",
      "Epoch 100, Training Loss 0.062056457180806136\n",
      "Epoch 100, Training Loss 0.062261080917190105\n",
      "Epoch 100, Training Loss 0.06248819450740619\n",
      "Epoch 100, Training Loss 0.0628967797359847\n",
      "Epoch 100, Training Loss 0.0633638293084586\n",
      "Epoch 100, Training Loss 0.06363015358938891\n",
      "Epoch 100, Training Loss 0.0640325252624119\n",
      "Epoch 100, Training Loss 0.06438250952135877\n",
      "Epoch 100, Training Loss 0.06456147757408869\n",
      "Epoch 100, Training Loss 0.06479603902000905\n",
      "Epoch 100, Training Loss 0.06511603455866694\n",
      "Epoch 100, Training Loss 0.06541528047807992\n",
      "Epoch 100, Training Loss 0.06557538730028036\n",
      "Epoch 100, Training Loss 0.06595766885430002\n",
      "Epoch 100, Training Loss 0.06632648623736619\n",
      "Epoch 100, Training Loss 0.06672206503884567\n",
      "Epoch 100, Training Loss 0.06711114342788906\n",
      "Epoch 100, Training Loss 0.06733424782448108\n",
      "Epoch 100, Training Loss 0.06763659472889302\n",
      "Epoch 100, Training Loss 0.0681220982652491\n",
      "Epoch 100, Training Loss 0.06850409338160245\n",
      "Epoch 100, Training Loss 0.06889272210619334\n",
      "Epoch 100, Training Loss 0.0691222042378867\n",
      "Epoch 100, Training Loss 0.06938406518277\n",
      "Epoch 100, Training Loss 0.06980553248425579\n",
      "Epoch 100, Training Loss 0.07008370234990668\n",
      "Epoch 100, Training Loss 0.0702827303191585\n",
      "Epoch 100, Training Loss 0.0706792564686302\n",
      "Epoch 100, Training Loss 0.0709964392701988\n",
      "Epoch 100, Training Loss 0.0713358392076724\n",
      "Epoch 100, Training Loss 0.07156862282310911\n",
      "Epoch 100, Training Loss 0.07165880935728702\n",
      "Epoch 100, Training Loss 0.07185424399345428\n",
      "Epoch 100, Training Loss 0.07234854256862874\n",
      "Epoch 100, Training Loss 0.07256290207967124\n",
      "Epoch 100, Training Loss 0.07278648842021328\n",
      "Epoch 100, Training Loss 0.07317712003617641\n",
      "Epoch 100, Training Loss 0.0735477585240703\n",
      "Epoch 100, Training Loss 0.07376449507520631\n",
      "Epoch 100, Training Loss 0.07415536655794325\n",
      "Epoch 100, Training Loss 0.07448036667635984\n",
      "Epoch 100, Training Loss 0.07486405435120663\n",
      "Epoch 100, Training Loss 0.07503184437980433\n",
      "Epoch 100, Training Loss 0.07529376073718985\n",
      "Epoch 100, Training Loss 0.075593189891342\n",
      "Epoch 100, Training Loss 0.07597514853605529\n",
      "Epoch 100, Training Loss 0.0761693579423458\n",
      "Epoch 100, Training Loss 0.07654142507430538\n",
      "Epoch 100, Training Loss 0.07704763825211074\n",
      "Epoch 100, Training Loss 0.07740664198194318\n",
      "Epoch 100, Training Loss 0.07783873564065874\n",
      "Epoch 100, Training Loss 0.078099280062234\n",
      "Epoch 100, Training Loss 0.0784001032752759\n",
      "Epoch 100, Training Loss 0.07863714082923996\n",
      "Epoch 100, Training Loss 0.07880648879139014\n",
      "Epoch 100, Training Loss 0.07912902286290513\n",
      "Epoch 100, Training Loss 0.07944418682390467\n",
      "Epoch 100, Training Loss 0.07961125387941174\n",
      "Epoch 100, Training Loss 0.07985673665695484\n",
      "Epoch 100, Training Loss 0.08022483463025154\n",
      "Epoch 100, Training Loss 0.08068517284929905\n",
      "Epoch 100, Training Loss 0.08112139363423028\n",
      "Epoch 100, Training Loss 0.0813730292003173\n",
      "Epoch 100, Training Loss 0.08164888677542168\n",
      "Epoch 100, Training Loss 0.08195436338100896\n",
      "Epoch 100, Training Loss 0.08206124386519117\n",
      "Epoch 100, Training Loss 0.08237086265059688\n",
      "Epoch 100, Training Loss 0.0827763695508013\n",
      "Epoch 100, Training Loss 0.08301711974241545\n",
      "Epoch 100, Training Loss 0.08321394616990443\n",
      "Epoch 100, Training Loss 0.08359860101014452\n",
      "Epoch 100, Training Loss 0.0840555756445736\n",
      "Epoch 100, Training Loss 0.0843105097027386\n",
      "Epoch 100, Training Loss 0.08456298929955954\n",
      "Epoch 100, Training Loss 0.0847817968072184\n",
      "Epoch 100, Training Loss 0.0851317877735933\n",
      "Epoch 100, Training Loss 0.08539453362260023\n",
      "Epoch 100, Training Loss 0.08572268912859281\n",
      "Epoch 100, Training Loss 0.08605326136664661\n",
      "Epoch 100, Training Loss 0.08657029534087461\n",
      "Epoch 100, Training Loss 0.08683374747062278\n",
      "Epoch 100, Training Loss 0.08716692562069735\n",
      "Epoch 100, Training Loss 0.08747618727366943\n",
      "Epoch 100, Training Loss 0.08784982623041743\n",
      "Epoch 100, Training Loss 0.08796906025360918\n",
      "Epoch 100, Training Loss 0.08828954162347652\n",
      "Epoch 100, Training Loss 0.08857277839842355\n",
      "Epoch 100, Training Loss 0.08895557280391683\n",
      "Epoch 100, Training Loss 0.08922585833560476\n",
      "Epoch 100, Training Loss 0.08955645069594273\n",
      "Epoch 100, Training Loss 0.09002067445946471\n",
      "Epoch 100, Training Loss 0.09043262982764817\n",
      "Epoch 100, Training Loss 0.09075511172604378\n",
      "Epoch 100, Training Loss 0.09112790611851246\n",
      "Epoch 100, Training Loss 0.0915463536291781\n",
      "Epoch 100, Training Loss 0.09177589911938933\n",
      "Epoch 100, Training Loss 0.09209637019945227\n",
      "Epoch 100, Training Loss 0.09230235634404985\n",
      "Epoch 100, Training Loss 0.09267778591731625\n",
      "Epoch 100, Training Loss 0.09291284664741258\n",
      "Epoch 100, Training Loss 0.09304760000132539\n",
      "Epoch 100, Training Loss 0.0932933760954596\n",
      "Epoch 100, Training Loss 0.09371918916244946\n",
      "Epoch 100, Training Loss 0.09397453589893667\n",
      "Epoch 100, Training Loss 0.09426650688852496\n",
      "Epoch 100, Training Loss 0.09452114267574857\n",
      "Epoch 100, Training Loss 0.09473531327360427\n",
      "Epoch 100, Training Loss 0.09493290491954749\n",
      "Epoch 100, Training Loss 0.09529737714687576\n",
      "Epoch 100, Training Loss 0.09562645991668677\n",
      "Epoch 100, Training Loss 0.0958660025616436\n",
      "Epoch 100, Training Loss 0.09611948543344923\n",
      "Epoch 100, Training Loss 0.09626366985042381\n",
      "Epoch 100, Training Loss 0.09646371224194841\n",
      "Epoch 100, Training Loss 0.09659019253595406\n",
      "Epoch 100, Training Loss 0.09684680449916884\n",
      "Epoch 100, Training Loss 0.09707778869458782\n",
      "Epoch 100, Training Loss 0.09741295216714635\n",
      "Epoch 100, Training Loss 0.09775922333111849\n",
      "Epoch 100, Training Loss 0.09824993751009407\n",
      "Epoch 100, Training Loss 0.09860033042671736\n",
      "Epoch 100, Training Loss 0.09895766808477509\n",
      "Epoch 100, Training Loss 0.0993676460955454\n",
      "Epoch 100, Training Loss 0.09985949302954442\n",
      "Epoch 100, Training Loss 0.10007362483102647\n",
      "Epoch 100, Training Loss 0.10029266940434571\n",
      "Epoch 100, Training Loss 0.10049839668414172\n",
      "Epoch 100, Training Loss 0.10069673746595602\n",
      "Epoch 100, Training Loss 0.10087510171677451\n",
      "Epoch 100, Training Loss 0.10117818939182764\n",
      "Epoch 100, Training Loss 0.10138948166461857\n",
      "Epoch 100, Training Loss 0.10154258998115655\n",
      "Epoch 100, Training Loss 0.10177214033044207\n",
      "Epoch 100, Training Loss 0.10201745748024463\n",
      "Epoch 100, Training Loss 0.10243284022983383\n",
      "Epoch 100, Training Loss 0.10275261072665834\n",
      "Epoch 100, Training Loss 0.10296820533816772\n",
      "Epoch 100, Training Loss 0.10310906347106485\n",
      "Epoch 100, Training Loss 0.1035047082035133\n",
      "Epoch 100, Training Loss 0.10380840473010412\n",
      "Epoch 100, Training Loss 0.10411691861917906\n",
      "Epoch 100, Training Loss 0.10452370762901234\n",
      "Epoch 100, Training Loss 0.10480893214645289\n",
      "Epoch 100, Training Loss 0.10504597370200755\n",
      "Epoch 100, Training Loss 0.1053149983134416\n",
      "Epoch 100, Training Loss 0.1055391016594894\n",
      "Epoch 100, Training Loss 0.10580967002741211\n",
      "Epoch 100, Training Loss 0.10625334829092026\n",
      "Epoch 100, Training Loss 0.10666974454813297\n",
      "Epoch 100, Training Loss 0.10718784187837026\n",
      "Epoch 100, Training Loss 0.10747082523830101\n",
      "Epoch 100, Training Loss 0.10771380715510424\n",
      "Epoch 100, Training Loss 0.10805361544537118\n",
      "Epoch 100, Training Loss 0.10829589550223802\n",
      "Epoch 100, Training Loss 0.10856656408142251\n",
      "Epoch 100, Training Loss 0.10883051801063216\n",
      "Epoch 100, Training Loss 0.1090974746762639\n",
      "Epoch 100, Training Loss 0.10932579310729985\n",
      "Epoch 100, Training Loss 0.10971492603230659\n",
      "Epoch 100, Training Loss 0.10994809848801865\n",
      "Epoch 100, Training Loss 0.11023245561305824\n",
      "Epoch 100, Training Loss 0.1104359985960414\n",
      "Epoch 100, Training Loss 0.1107540188924126\n",
      "Epoch 100, Training Loss 0.1110343898043913\n",
      "Epoch 100, Training Loss 0.111289665922332\n",
      "Epoch 100, Training Loss 0.11157347252378073\n",
      "Epoch 100, Training Loss 0.11193435654387145\n",
      "Epoch 100, Training Loss 0.1122175949575651\n",
      "Epoch 100, Training Loss 0.11240426342353187\n",
      "Epoch 100, Training Loss 0.11280278324166222\n",
      "Epoch 100, Training Loss 0.11325284819621259\n",
      "Epoch 100, Training Loss 0.11354784783728593\n",
      "Epoch 100, Training Loss 0.11386143581946488\n",
      "Epoch 100, Training Loss 0.11409097326838452\n",
      "Epoch 100, Training Loss 0.11429966946163446\n",
      "Epoch 100, Training Loss 0.11455097384846119\n",
      "Epoch 100, Training Loss 0.11484658681904264\n",
      "Epoch 100, Training Loss 0.11525529845977378\n",
      "Epoch 100, Training Loss 0.11548281161833907\n",
      "Epoch 100, Training Loss 0.11570465019749254\n",
      "Epoch 100, Training Loss 0.11601122565891432\n",
      "Epoch 100, Training Loss 0.11631779262171987\n",
      "Epoch 100, Training Loss 0.11657442480249477\n",
      "Epoch 100, Training Loss 0.11686876420017399\n",
      "Epoch 100, Training Loss 0.1172494256054349\n",
      "Epoch 100, Training Loss 0.11752973348283402\n",
      "Epoch 100, Training Loss 0.11823105575788356\n",
      "Epoch 100, Training Loss 0.11854171994930643\n",
      "Epoch 100, Training Loss 0.11874575253642733\n",
      "Epoch 100, Training Loss 0.11914997698400942\n",
      "Epoch 100, Training Loss 0.1194351796641984\n",
      "Epoch 100, Training Loss 0.11972815935950146\n",
      "Epoch 100, Training Loss 0.12005557636242084\n",
      "Epoch 100, Training Loss 0.12031881076752987\n",
      "Epoch 100, Training Loss 0.12050602661298059\n",
      "Epoch 100, Training Loss 0.1206912060299188\n",
      "Epoch 100, Training Loss 0.12094449248079144\n",
      "Epoch 100, Training Loss 0.12125826051549228\n",
      "Epoch 100, Training Loss 0.12157927314415003\n",
      "Epoch 100, Training Loss 0.12196457338378862\n",
      "Epoch 100, Training Loss 0.122223848900984\n",
      "Epoch 100, Training Loss 0.12263723882034307\n",
      "Epoch 100, Training Loss 0.12286779187295747\n",
      "Epoch 100, Training Loss 0.12320570893528517\n",
      "Epoch 100, Training Loss 0.12355287359727313\n",
      "Epoch 100, Training Loss 0.12386734495916026\n",
      "Epoch 100, Training Loss 0.12420655948960263\n",
      "Epoch 100, Training Loss 0.12453291645211637\n",
      "Epoch 100, Training Loss 0.12482798924607694\n",
      "Epoch 100, Training Loss 0.1251263493657722\n",
      "Epoch 100, Training Loss 0.12524092437513648\n",
      "Epoch 100, Training Loss 0.1255603399499298\n",
      "Epoch 100, Training Loss 0.1260073514240782\n",
      "Epoch 100, Training Loss 0.12633445386386588\n",
      "Epoch 100, Training Loss 0.12652184642717967\n",
      "Epoch 100, Training Loss 0.12684353208526625\n",
      "Epoch 100, Training Loss 0.12717567767252397\n",
      "Epoch 100, Training Loss 0.12737489491701126\n",
      "Epoch 100, Training Loss 0.12749281908621263\n",
      "Epoch 100, Training Loss 0.12784295948341373\n",
      "Epoch 100, Training Loss 0.1281142203932833\n",
      "Epoch 100, Training Loss 0.12854214549979284\n",
      "Epoch 100, Training Loss 0.12871845864959994\n",
      "Epoch 100, Training Loss 0.12896777961946204\n",
      "Epoch 100, Training Loss 0.1292788557269994\n",
      "Epoch 100, Training Loss 0.12961195529345662\n",
      "Epoch 100, Training Loss 0.12992006147761478\n",
      "Epoch 100, Training Loss 0.13025498752246428\n",
      "Epoch 100, Training Loss 0.13063749491863544\n",
      "Epoch 100, Training Loss 0.13106404115324435\n",
      "Epoch 100, Training Loss 0.13134220006215908\n",
      "Epoch 100, Training Loss 0.13153671922967258\n",
      "Epoch 100, Training Loss 0.13196912257339033\n",
      "Epoch 100, Training Loss 0.13243899761182268\n",
      "Epoch 100, Training Loss 0.13280565192556137\n",
      "Epoch 100, Training Loss 0.1331320306872163\n",
      "Epoch 100, Training Loss 0.13336835386198195\n",
      "Epoch 100, Training Loss 0.13357676531347779\n",
      "Epoch 100, Training Loss 0.13373675304071983\n",
      "Epoch 100, Training Loss 0.13414168001517005\n",
      "Epoch 100, Training Loss 0.13435025698007524\n",
      "Epoch 100, Training Loss 0.13473947759708174\n",
      "Epoch 100, Training Loss 0.13497044795843036\n",
      "Epoch 100, Training Loss 0.13530640302183072\n",
      "Epoch 100, Training Loss 0.1355683983058271\n",
      "Epoch 100, Training Loss 0.13577432494105585\n",
      "Epoch 100, Training Loss 0.13604815517697494\n",
      "Epoch 100, Training Loss 0.13622019289399656\n",
      "Epoch 100, Training Loss 0.13654141783561852\n",
      "Epoch 100, Training Loss 0.13692358383894576\n",
      "Epoch 100, Training Loss 0.1372078312632373\n",
      "Epoch 100, Training Loss 0.13742694365398964\n",
      "Epoch 100, Training Loss 0.1376814246558777\n",
      "Epoch 100, Training Loss 0.13803112308692445\n",
      "Epoch 100, Training Loss 0.13830379028911785\n",
      "Epoch 100, Training Loss 0.13860198331382267\n",
      "Epoch 100, Training Loss 0.1388242725482987\n",
      "Epoch 100, Training Loss 0.13932937572298149\n",
      "Epoch 100, Training Loss 0.139873292459094\n",
      "Epoch 100, Training Loss 0.1402170306924359\n",
      "Epoch 100, Training Loss 0.1404715164581223\n",
      "Epoch 100, Training Loss 0.14069464917073166\n",
      "Epoch 100, Training Loss 0.1412005756059876\n",
      "Epoch 100, Training Loss 0.14145749416726325\n",
      "Epoch 100, Training Loss 0.14173630424930006\n",
      "Epoch 100, Training Loss 0.14201316462300928\n",
      "Epoch 100, Training Loss 0.14232342514921636\n",
      "Epoch 100, Training Loss 0.14261975693885628\n",
      "Epoch 100, Training Loss 0.14284621718365823\n",
      "Epoch 100, Training Loss 0.1431455403718802\n",
      "Epoch 100, Training Loss 0.14355100992390568\n",
      "Epoch 100, Training Loss 0.1438154490745586\n",
      "Epoch 100, Training Loss 0.1441635825025761\n",
      "Epoch 100, Training Loss 0.1443526445866546\n",
      "Epoch 100, Training Loss 0.1445989069502677\n",
      "Epoch 100, Training Loss 0.1447850311618022\n",
      "Epoch 100, Training Loss 0.14507765370561643\n",
      "Epoch 100, Training Loss 0.14538616540334415\n",
      "Epoch 100, Training Loss 0.14558276741782114\n",
      "Epoch 100, Training Loss 0.1459761237739907\n",
      "Epoch 100, Training Loss 0.14625273707806302\n",
      "Epoch 100, Training Loss 0.14660536185326173\n",
      "Epoch 100, Training Loss 0.1469966877261391\n",
      "Epoch 100, Training Loss 0.14723401316596418\n",
      "Epoch 100, Training Loss 0.147527484142262\n",
      "Epoch 100, Training Loss 0.1478951942661534\n",
      "Epoch 100, Training Loss 0.14819104197766164\n",
      "Epoch 100, Training Loss 0.14856304150179525\n",
      "Epoch 100, Training Loss 0.1489841147799931\n",
      "Epoch 100, Training Loss 0.14944975362028307\n",
      "Epoch 100, Training Loss 0.14994018528696215\n",
      "Epoch 100, Training Loss 0.15018023270399064\n",
      "Epoch 100, Training Loss 0.1505763748532061\n",
      "Epoch 100, Training Loss 0.15078190565490357\n",
      "Epoch 100, Training Loss 0.15122008169322368\n",
      "Epoch 100, Training Loss 0.15139691348728315\n",
      "Epoch 100, Training Loss 0.1516202716799953\n",
      "Epoch 100, Training Loss 0.15183117032965734\n",
      "Epoch 100, Training Loss 0.15208395606721453\n",
      "Epoch 100, Training Loss 0.15230958007485665\n",
      "Epoch 100, Training Loss 0.15270056672718213\n",
      "Epoch 100, Training Loss 0.15296607904726892\n",
      "Epoch 100, Training Loss 0.15326083434359802\n",
      "Epoch 100, Training Loss 0.1535709823488884\n",
      "Epoch 100, Training Loss 0.1540800494611111\n",
      "Epoch 100, Training Loss 0.15433338332130475\n",
      "Epoch 100, Training Loss 0.15457500145792047\n",
      "Epoch 100, Training Loss 0.1547928338732256\n",
      "Epoch 100, Training Loss 0.15503031560374647\n",
      "Epoch 100, Training Loss 0.15526446661985743\n",
      "Epoch 100, Training Loss 0.15578142220102004\n",
      "Epoch 100, Training Loss 0.15605669026560795\n",
      "Epoch 100, Training Loss 0.1563520874170696\n",
      "Epoch 100, Training Loss 0.15657024058844426\n",
      "Epoch 100, Training Loss 0.1568639565001973\n",
      "Epoch 100, Training Loss 0.15710418579904625\n",
      "Epoch 100, Training Loss 0.15746513719830063\n",
      "Epoch 100, Training Loss 0.1579116421091892\n",
      "Epoch 100, Training Loss 0.15850319949638508\n",
      "Epoch 100, Training Loss 0.1589779521307677\n",
      "Epoch 100, Training Loss 0.15937078759417206\n",
      "Epoch 100, Training Loss 0.15970031194903356\n",
      "Epoch 100, Training Loss 0.16039443844953155\n",
      "Epoch 100, Training Loss 0.16083442130128442\n",
      "Epoch 100, Training Loss 0.16122058184479204\n",
      "Epoch 100, Training Loss 0.16155800613981988\n",
      "Epoch 100, Training Loss 0.1619192338774881\n",
      "Epoch 100, Training Loss 0.1623957042232194\n",
      "Epoch 100, Training Loss 0.16274607854197398\n",
      "Epoch 100, Training Loss 0.1632184981728149\n",
      "Epoch 100, Training Loss 0.16348819777636273\n",
      "Epoch 100, Training Loss 0.16380659927187674\n",
      "Epoch 100, Training Loss 0.1640066906543034\n",
      "Epoch 100, Training Loss 0.16427026567099345\n",
      "Epoch 100, Training Loss 0.16450023792131477\n",
      "Epoch 100, Training Loss 0.1649636232182193\n",
      "Epoch 100, Training Loss 0.1651838170483594\n",
      "Epoch 100, Training Loss 0.16544783643215819\n",
      "Epoch 100, Training Loss 0.16588327717369475\n",
      "Epoch 100, Training Loss 0.1660842459334437\n",
      "Epoch 100, Training Loss 0.16633438991616145\n",
      "Epoch 100, Training Loss 0.16648319561768066\n",
      "Epoch 100, Training Loss 0.16671695266766925\n",
      "Epoch 100, Training Loss 0.16715838771570674\n",
      "Epoch 100, Training Loss 0.16745469940211766\n",
      "Epoch 100, Training Loss 0.1677118788289902\n",
      "Epoch 100, Training Loss 0.16812876949224936\n",
      "Epoch 100, Training Loss 0.1683461093117514\n",
      "Epoch 100, Training Loss 0.16859603172068097\n",
      "Epoch 100, Training Loss 0.16895723327651352\n",
      "Epoch 100, Training Loss 0.16933204908200236\n",
      "Epoch 100, Training Loss 0.16965726425733102\n",
      "Epoch 100, Training Loss 0.16984384186813595\n",
      "Epoch 100, Training Loss 0.17029721087888075\n",
      "Epoch 100, Training Loss 0.17050572334195646\n",
      "Epoch 100, Training Loss 0.17071791484837642\n",
      "Epoch 100, Training Loss 0.17104299812365675\n",
      "Epoch 100, Training Loss 0.17117584907375943\n",
      "Epoch 100, Training Loss 0.17135055897676427\n",
      "Epoch 100, Training Loss 0.17155979563246299\n",
      "Epoch 100, Training Loss 0.17180046617336894\n",
      "Epoch 100, Training Loss 0.17199450652198414\n",
      "Epoch 100, Training Loss 0.17230125824394432\n",
      "Epoch 100, Training Loss 0.17262379174380352\n",
      "Epoch 100, Training Loss 0.17303845999986314\n",
      "Epoch 100, Training Loss 0.173257477038428\n",
      "Epoch 100, Training Loss 0.17366398845220465\n",
      "Epoch 100, Training Loss 0.17382783382711814\n",
      "Epoch 100, Training Loss 0.17404438862982002\n",
      "Epoch 100, Training Loss 0.17443285793866342\n",
      "Epoch 100, Training Loss 0.17471646686153644\n",
      "Epoch 100, Training Loss 0.1748905420741614\n",
      "Epoch 100, Training Loss 0.17519870024088705\n",
      "Epoch 100, Training Loss 0.17550207029485032\n",
      "Epoch 100, Training Loss 0.1757405086341874\n",
      "Epoch 100, Training Loss 0.1760116147396662\n",
      "Epoch 100, Training Loss 0.17621980467453943\n",
      "Epoch 100, Training Loss 0.17635649087293373\n",
      "Epoch 100, Training Loss 0.176578810290836\n",
      "Epoch 100, Training Loss 0.17686710492386232\n",
      "Epoch 100, Training Loss 0.17721394282739486\n",
      "Epoch 100, Training Loss 0.17749413227676736\n",
      "Epoch 100, Training Loss 0.17775895022560873\n",
      "Epoch 100, Training Loss 0.1782651169849631\n",
      "Epoch 100, Training Loss 0.1786439787034336\n",
      "Epoch 100, Training Loss 0.17893958142232103\n",
      "Epoch 100, Training Loss 0.17940097884334566\n",
      "Epoch 100, Training Loss 0.1799578130188043\n",
      "Epoch 100, Training Loss 0.1802100008234496\n",
      "Epoch 100, Training Loss 0.18053439003236763\n",
      "Epoch 100, Training Loss 0.1808456109022088\n",
      "Epoch 100, Training Loss 0.18148896255342248\n",
      "Epoch 100, Training Loss 0.18189795861196945\n",
      "Epoch 100, Training Loss 0.1822671498198186\n",
      "Epoch 100, Training Loss 0.18243692492318275\n",
      "Epoch 100, Training Loss 0.18278524177649136\n",
      "Epoch 100, Training Loss 0.18308461363167713\n",
      "Epoch 100, Training Loss 0.1832936640609713\n",
      "Epoch 100, Training Loss 0.18357427506838614\n",
      "Epoch 100, Training Loss 0.1839365866273413\n",
      "Epoch 100, Training Loss 0.18409436564806783\n",
      "Epoch 100, Training Loss 0.18447210215737142\n",
      "Epoch 100, Training Loss 0.18471341278127698\n",
      "Epoch 100, Training Loss 0.18506391503659966\n",
      "Epoch 100, Training Loss 0.1853041422584325\n",
      "Epoch 100, Training Loss 0.18572060772410745\n",
      "Epoch 100, Training Loss 0.18587238695996497\n",
      "Epoch 100, Training Loss 0.1861335356407763\n",
      "Epoch 100, Training Loss 0.18630426142679152\n",
      "Epoch 100, Training Loss 0.18662228717294801\n",
      "Epoch 100, Training Loss 0.1869302533395455\n",
      "Epoch 100, Training Loss 0.18721065790299565\n",
      "Epoch 100, Training Loss 0.18744846522960518\n",
      "Epoch 100, Training Loss 0.18766429441054458\n",
      "Epoch 100, Training Loss 0.18799148305602695\n",
      "Epoch 100, Training Loss 0.1881452422979695\n",
      "Epoch 100, Training Loss 0.18861592798243704\n",
      "Epoch 100, Training Loss 0.18896122321562694\n",
      "Epoch 100, Training Loss 0.18913541456965535\n",
      "Epoch 100, Training Loss 0.1893346937625762\n",
      "Epoch 100, Training Loss 0.18961033044987932\n",
      "Epoch 100, Training Loss 0.18984867145528878\n",
      "Epoch 100, Training Loss 0.19015510211629635\n",
      "Epoch 100, Training Loss 0.19044150024309486\n",
      "Epoch 100, Training Loss 0.1908222689672047\n",
      "Epoch 100, Training Loss 0.19103294806292906\n",
      "Epoch 100, Training Loss 0.19145304083709827\n",
      "Epoch 100, Training Loss 0.19178145964775245\n",
      "Epoch 100, Training Loss 0.19216144390766274\n",
      "Epoch 100, Training Loss 0.19244715974420842\n",
      "Epoch 100, Training Loss 0.19262163566849422\n",
      "Epoch 100, Training Loss 0.19301861590322325\n",
      "Epoch 100, Training Loss 0.19330714895482867\n",
      "Epoch 100, Training Loss 0.19372581125563368\n",
      "Epoch 100, Training Loss 0.19444734423095003\n",
      "Epoch 100, Training Loss 0.19488702662041424\n",
      "Epoch 100, Training Loss 0.1953742146358618\n",
      "Epoch 100, Training Loss 0.19568674614095627\n",
      "Epoch 100, Training Loss 0.19603242922355146\n",
      "Epoch 100, Training Loss 0.1964604305889448\n",
      "Epoch 100, Training Loss 0.19682341231904982\n",
      "Epoch 100, Training Loss 0.1971563233534241\n",
      "Epoch 100, Training Loss 0.19771722013421375\n",
      "Epoch 100, Training Loss 0.19814284314470523\n",
      "Epoch 100, Training Loss 0.1984654505024938\n",
      "Epoch 100, Training Loss 0.19883966241079523\n",
      "Epoch 100, Training Loss 0.1991893014277491\n",
      "Epoch 100, Training Loss 0.1994323080210277\n",
      "Epoch 100, Training Loss 0.19968772207951302\n",
      "Epoch 100, Training Loss 0.19986790821642217\n",
      "Epoch 100, Training Loss 0.20023505057177277\n",
      "Epoch 100, Training Loss 0.20053398501499534\n",
      "Epoch 100, Training Loss 0.20072045768885052\n",
      "Epoch 100, Training Loss 0.20095169613771427\n",
      "Epoch 100, Training Loss 0.20130444587687094\n",
      "Epoch 100, Training Loss 0.20165998213317082\n",
      "Epoch 100, Training Loss 0.20193642561736008\n",
      "Epoch 100, Training Loss 0.20221435142409466\n",
      "Epoch 100, Training Loss 0.2027819481633051\n",
      "Epoch 100, Training Loss 0.20312712746469871\n",
      "Epoch 100, Training Loss 0.2033929557389463\n",
      "Epoch 100, Training Loss 0.20373001680387864\n",
      "Epoch 100, Training Loss 0.20424442695420417\n",
      "Epoch 100, Training Loss 0.2048522430707884\n",
      "Epoch 100, Training Loss 0.20511928632321869\n",
      "Epoch 100, Training Loss 0.20545393568666084\n",
      "Epoch 100, Training Loss 0.20574695267297727\n",
      "Epoch 100, Training Loss 0.20613278081769223\n",
      "Epoch 100, Training Loss 0.2067555336143507\n",
      "Epoch 100, Training Loss 0.20706047292065133\n",
      "Epoch 100, Training Loss 0.2074958391944923\n",
      "Epoch 100, Training Loss 0.20782604237156146\n",
      "Epoch 100, Training Loss 0.2079704167211757\n",
      "Epoch 100, Training Loss 0.2083158642434708\n",
      "Epoch 100, Training Loss 0.20856426833459482\n",
      "Epoch 100, Training Loss 0.20875567072035406\n",
      "Epoch 100, Training Loss 0.20894394860700574\n",
      "Epoch 100, Training Loss 0.2091403294097432\n",
      "Epoch 100, Training Loss 0.20956607300149815\n",
      "Epoch 100, Training Loss 0.2099768966436386\n",
      "Epoch 100, Training Loss 0.21030357899263386\n",
      "Epoch 100, Training Loss 0.2106658703530841\n",
      "Epoch 100, Training Loss 0.2109969389408141\n",
      "Epoch 100, Training Loss 0.21127372440855827\n",
      "Epoch 100, Training Loss 0.21152913587553726\n",
      "Epoch 100, Training Loss 0.21177890416606307\n",
      "Epoch 100, Training Loss 0.21217521910777176\n",
      "Epoch 100, Training Loss 0.21247012455902442\n",
      "Epoch 100, Training Loss 0.21277541196559702\n",
      "Epoch 100, Training Loss 0.21326604306392963\n",
      "Epoch 100, Training Loss 0.2137376293349449\n",
      "Epoch 100, Training Loss 0.213949388783911\n",
      "Epoch 100, Training Loss 0.2141655265446514\n",
      "Epoch 100, Training Loss 0.2144906640891224\n",
      "Epoch 100, Training Loss 0.21479517291955022\n",
      "Epoch 100, Training Loss 0.21510512162657344\n",
      "Epoch 100, Training Loss 0.21540997909081866\n",
      "Epoch 100, Training Loss 0.2157744239167789\n",
      "Epoch 100, Training Loss 0.2161321066262777\n",
      "Epoch 100, Training Loss 0.21637841727575072\n",
      "Epoch 100, Training Loss 0.21655185615924924\n",
      "Epoch 100, Training Loss 0.21670433094777414\n",
      "Epoch 100, Training Loss 0.21709560697226574\n",
      "Epoch 100, Training Loss 0.2173066477927253\n",
      "Epoch 100, Training Loss 0.21751780467836754\n",
      "Epoch 100, Training Loss 0.2179039301984298\n",
      "Epoch 100, Training Loss 0.21838988774382245\n",
      "Epoch 100, Training Loss 0.2187198634018831\n",
      "Epoch 100, Training Loss 0.21916349589481682\n",
      "Epoch 100, Training Loss 0.21956823833877473\n",
      "Epoch 100, Training Loss 0.21995402056047375\n",
      "Epoch 100, Training Loss 0.220236062555743\n",
      "Epoch 100, Training Loss 0.22047667192947834\n",
      "Epoch 100, Training Loss 0.22081173524794068\n",
      "Epoch 100, Training Loss 0.22118514619024512\n",
      "Epoch 100, Training Loss 0.22150228520297943\n",
      "Epoch 100, Training Loss 0.22175581012959675\n",
      "Epoch 100, Training Loss 0.22200992704390565\n",
      "Epoch 100, Training Loss 0.22250904410582065\n",
      "Epoch 100, Training Loss 0.22279352783356482\n",
      "Epoch 100, Training Loss 0.22302774912522882\n",
      "Epoch 100, Training Loss 0.22330059213063602\n",
      "Epoch 100, Training Loss 0.223637254103599\n",
      "Epoch 100, Training Loss 0.2239404268410352\n",
      "Epoch 100, Training Loss 0.2243839035005978\n",
      "Epoch 100, Training Loss 0.22473150105846812\n",
      "Epoch 100, Training Loss 0.22493951376098806\n",
      "Epoch 100, Training Loss 0.22524815836868933\n",
      "Epoch 100, Training Loss 0.22553471484414453\n",
      "Epoch 100, Training Loss 0.22574823036256347\n",
      "Epoch 100, Training Loss 0.22618202248688243\n",
      "Epoch 100, Training Loss 0.2264557313602751\n",
      "Epoch 100, Training Loss 0.22673945524312955\n",
      "Epoch 100, Training Loss 0.22700484707722884\n",
      "Epoch 100, Training Loss 0.22743662464839723\n",
      "Epoch 100, Training Loss 0.22777213519224732\n",
      "Epoch 100, Training Loss 0.22793082568956458\n",
      "Epoch 100, Training Loss 0.22814500922589656\n",
      "Epoch 100, Training Loss 0.22845887665248588\n",
      "Epoch 100, Training Loss 0.22893140013413052\n",
      "Epoch 100, Training Loss 0.22915216400037947\n",
      "Epoch 100, Training Loss 0.22942445286170907\n",
      "Epoch 100, Training Loss 0.22988470948641868\n",
      "Epoch 100, Training Loss 0.23049160474172944\n",
      "Epoch 100, Training Loss 0.2308360173764741\n",
      "Epoch 100, Training Loss 0.2310473082963463\n",
      "Epoch 100, Training Loss 0.23132122736757674\n",
      "Epoch 100, Training Loss 0.23159692308786886\n",
      "Epoch 100, Training Loss 0.23188665634988215\n",
      "Epoch 100, Training Loss 0.23219786811134088\n",
      "Epoch 100, Training Loss 0.23244331308338037\n",
      "Epoch 100, Training Loss 0.232846581188919\n",
      "Epoch 100, Training Loss 0.23327688647962896\n",
      "Epoch 100, Training Loss 0.23356970016608763\n",
      "Epoch 100, Training Loss 0.2338468068091156\n",
      "Epoch 100, Training Loss 0.23417373828570862\n",
      "Epoch 100, Training Loss 0.23460797900738922\n",
      "Epoch 100, Training Loss 0.23487267624158079\n",
      "Epoch 100, Training Loss 0.23550438463611675\n",
      "Epoch 110, Training Loss 0.00035083438734264327\n",
      "Epoch 110, Training Loss 0.0005958754845592372\n",
      "Epoch 110, Training Loss 0.0007112846064293171\n",
      "Epoch 110, Training Loss 0.0010009178286775604\n",
      "Epoch 110, Training Loss 0.0012555536349563648\n",
      "Epoch 110, Training Loss 0.0014315981637029087\n",
      "Epoch 110, Training Loss 0.0017796514836875984\n",
      "Epoch 110, Training Loss 0.002089473950054944\n",
      "Epoch 110, Training Loss 0.002313879035089327\n",
      "Epoch 110, Training Loss 0.002466907939109046\n",
      "Epoch 110, Training Loss 0.0027401523612191913\n",
      "Epoch 110, Training Loss 0.003030089913960308\n",
      "Epoch 110, Training Loss 0.003214942972602137\n",
      "Epoch 110, Training Loss 0.003444422322237278\n",
      "Epoch 110, Training Loss 0.0036347749573952706\n",
      "Epoch 110, Training Loss 0.003851666674970666\n",
      "Epoch 110, Training Loss 0.003966947412475601\n",
      "Epoch 110, Training Loss 0.004112345864400839\n",
      "Epoch 110, Training Loss 0.004431652996088843\n",
      "Epoch 110, Training Loss 0.004735957249960936\n",
      "Epoch 110, Training Loss 0.004982308944320435\n",
      "Epoch 110, Training Loss 0.005172561311051059\n",
      "Epoch 110, Training Loss 0.005483131293598039\n",
      "Epoch 110, Training Loss 0.005665380383848839\n",
      "Epoch 110, Training Loss 0.005832938644129907\n",
      "Epoch 110, Training Loss 0.006190320319684265\n",
      "Epoch 110, Training Loss 0.006387610817351915\n",
      "Epoch 110, Training Loss 0.006594151811069235\n",
      "Epoch 110, Training Loss 0.006855588987507783\n",
      "Epoch 110, Training Loss 0.007037526582513014\n",
      "Epoch 110, Training Loss 0.007273775003755184\n",
      "Epoch 110, Training Loss 0.007528083384646784\n",
      "Epoch 110, Training Loss 0.00777734072921831\n",
      "Epoch 110, Training Loss 0.007867129133714129\n",
      "Epoch 110, Training Loss 0.008166073619023613\n",
      "Epoch 110, Training Loss 0.008490772970268488\n",
      "Epoch 110, Training Loss 0.008707411291882815\n",
      "Epoch 110, Training Loss 0.009007184125502091\n",
      "Epoch 110, Training Loss 0.009221841888430784\n",
      "Epoch 110, Training Loss 0.009371949374065984\n",
      "Epoch 110, Training Loss 0.009527818721426112\n",
      "Epoch 110, Training Loss 0.009769316086226412\n",
      "Epoch 110, Training Loss 0.009967695397641653\n",
      "Epoch 110, Training Loss 0.010204633159558182\n",
      "Epoch 110, Training Loss 0.010370677370397026\n",
      "Epoch 110, Training Loss 0.010561397065744375\n",
      "Epoch 110, Training Loss 0.010791661298793295\n",
      "Epoch 110, Training Loss 0.010983195260662557\n",
      "Epoch 110, Training Loss 0.011232277221234559\n",
      "Epoch 110, Training Loss 0.011453355536284045\n",
      "Epoch 110, Training Loss 0.011621707254815894\n",
      "Epoch 110, Training Loss 0.011746662199649665\n",
      "Epoch 110, Training Loss 0.011971017969843676\n",
      "Epoch 110, Training Loss 0.012258872732786877\n",
      "Epoch 110, Training Loss 0.012427179862166305\n",
      "Epoch 110, Training Loss 0.0125796827571014\n",
      "Epoch 110, Training Loss 0.012739646970234868\n",
      "Epoch 110, Training Loss 0.012887182014296427\n",
      "Epoch 110, Training Loss 0.0130432495833053\n",
      "Epoch 110, Training Loss 0.013300975684619621\n",
      "Epoch 110, Training Loss 0.013563309922395154\n",
      "Epoch 110, Training Loss 0.013776790093430472\n",
      "Epoch 110, Training Loss 0.014094111140426772\n",
      "Epoch 110, Training Loss 0.014327744236382682\n",
      "Epoch 110, Training Loss 0.014637190134019193\n",
      "Epoch 110, Training Loss 0.014852083495358373\n",
      "Epoch 110, Training Loss 0.01514114016462165\n",
      "Epoch 110, Training Loss 0.015322656030087825\n",
      "Epoch 110, Training Loss 0.015686035365857127\n",
      "Epoch 110, Training Loss 0.015902957876624962\n",
      "Epoch 110, Training Loss 0.01614246210631202\n",
      "Epoch 110, Training Loss 0.016413972239055292\n",
      "Epoch 110, Training Loss 0.01660935049090544\n",
      "Epoch 110, Training Loss 0.016837437668114977\n",
      "Epoch 110, Training Loss 0.016973062234995007\n",
      "Epoch 110, Training Loss 0.017111455740602424\n",
      "Epoch 110, Training Loss 0.017217347174501785\n",
      "Epoch 110, Training Loss 0.01742142275013887\n",
      "Epoch 110, Training Loss 0.017582970202121587\n",
      "Epoch 110, Training Loss 0.01775982663454607\n",
      "Epoch 110, Training Loss 0.01787446546928047\n",
      "Epoch 110, Training Loss 0.0182213403017777\n",
      "Epoch 110, Training Loss 0.018422709392083576\n",
      "Epoch 110, Training Loss 0.018577513620829034\n",
      "Epoch 110, Training Loss 0.018733571347830547\n",
      "Epoch 110, Training Loss 0.01896846134339452\n",
      "Epoch 110, Training Loss 0.019193927421594214\n",
      "Epoch 110, Training Loss 0.019505683913865052\n",
      "Epoch 110, Training Loss 0.019656305946886082\n",
      "Epoch 110, Training Loss 0.01989459719918573\n",
      "Epoch 110, Training Loss 0.020137241781901214\n",
      "Epoch 110, Training Loss 0.020285512861388418\n",
      "Epoch 110, Training Loss 0.020424232915844148\n",
      "Epoch 110, Training Loss 0.020559543205420378\n",
      "Epoch 110, Training Loss 0.02078436285524112\n",
      "Epoch 110, Training Loss 0.020984880045018233\n",
      "Epoch 110, Training Loss 0.021272591227079595\n",
      "Epoch 110, Training Loss 0.02148517776671273\n",
      "Epoch 110, Training Loss 0.021649210797170238\n",
      "Epoch 110, Training Loss 0.021839597788841827\n",
      "Epoch 110, Training Loss 0.02215226626266604\n",
      "Epoch 110, Training Loss 0.02236789361099758\n",
      "Epoch 110, Training Loss 0.022497016469688366\n",
      "Epoch 110, Training Loss 0.02280125715543547\n",
      "Epoch 110, Training Loss 0.023278684224314092\n",
      "Epoch 110, Training Loss 0.02355892577058519\n",
      "Epoch 110, Training Loss 0.023690728189618995\n",
      "Epoch 110, Training Loss 0.02385520381505227\n",
      "Epoch 110, Training Loss 0.024019359275127005\n",
      "Epoch 110, Training Loss 0.0242827022757829\n",
      "Epoch 110, Training Loss 0.024515207323348127\n",
      "Epoch 110, Training Loss 0.02474841943291752\n",
      "Epoch 110, Training Loss 0.02511405345538388\n",
      "Epoch 110, Training Loss 0.025265079306062223\n",
      "Epoch 110, Training Loss 0.02537589741256231\n",
      "Epoch 110, Training Loss 0.02552177574094909\n",
      "Epoch 110, Training Loss 0.025806256438917517\n",
      "Epoch 110, Training Loss 0.026089844572574586\n",
      "Epoch 110, Training Loss 0.026275112215057968\n",
      "Epoch 110, Training Loss 0.0264995085728138\n",
      "Epoch 110, Training Loss 0.026819766272821694\n",
      "Epoch 110, Training Loss 0.027035914856911925\n",
      "Epoch 110, Training Loss 0.027300798565225527\n",
      "Epoch 110, Training Loss 0.027573852073353576\n",
      "Epoch 110, Training Loss 0.027795094529838513\n",
      "Epoch 110, Training Loss 0.028058704093594074\n",
      "Epoch 110, Training Loss 0.028418299944504448\n",
      "Epoch 110, Training Loss 0.028754329513710784\n",
      "Epoch 110, Training Loss 0.028897025262760688\n",
      "Epoch 110, Training Loss 0.029133216956692277\n",
      "Epoch 110, Training Loss 0.02931805595260142\n",
      "Epoch 110, Training Loss 0.02953328241777542\n",
      "Epoch 110, Training Loss 0.029864112586926315\n",
      "Epoch 110, Training Loss 0.030153194218492874\n",
      "Epoch 110, Training Loss 0.030374045662410424\n",
      "Epoch 110, Training Loss 0.030481547946134186\n",
      "Epoch 110, Training Loss 0.030855254243935464\n",
      "Epoch 110, Training Loss 0.031018631451803703\n",
      "Epoch 110, Training Loss 0.03121316569197513\n",
      "Epoch 110, Training Loss 0.03160020791927872\n",
      "Epoch 110, Training Loss 0.031839925738627955\n",
      "Epoch 110, Training Loss 0.032069561531400434\n",
      "Epoch 110, Training Loss 0.03243061392317952\n",
      "Epoch 110, Training Loss 0.032752794787630705\n",
      "Epoch 110, Training Loss 0.03332277597940486\n",
      "Epoch 110, Training Loss 0.033752108121390845\n",
      "Epoch 110, Training Loss 0.03387406748502761\n",
      "Epoch 110, Training Loss 0.034129049486059056\n",
      "Epoch 110, Training Loss 0.0343663590147977\n",
      "Epoch 110, Training Loss 0.034599008348286914\n",
      "Epoch 110, Training Loss 0.0347895172931959\n",
      "Epoch 110, Training Loss 0.03505953355594669\n",
      "Epoch 110, Training Loss 0.03531766366547026\n",
      "Epoch 110, Training Loss 0.03557827182666725\n",
      "Epoch 110, Training Loss 0.03575649928025273\n",
      "Epoch 110, Training Loss 0.03608195508456291\n",
      "Epoch 110, Training Loss 0.03628255173449626\n",
      "Epoch 110, Training Loss 0.03641452671736098\n",
      "Epoch 110, Training Loss 0.03660764799589086\n",
      "Epoch 110, Training Loss 0.03686211663095847\n",
      "Epoch 110, Training Loss 0.0370859497171991\n",
      "Epoch 110, Training Loss 0.037337072450867696\n",
      "Epoch 110, Training Loss 0.03754418599597938\n",
      "Epoch 110, Training Loss 0.03773742377796137\n",
      "Epoch 110, Training Loss 0.038087329300849335\n",
      "Epoch 110, Training Loss 0.03825126899896985\n",
      "Epoch 110, Training Loss 0.038471511000638725\n",
      "Epoch 110, Training Loss 0.038745669729035835\n",
      "Epoch 110, Training Loss 0.03909263865607779\n",
      "Epoch 110, Training Loss 0.0392898602692215\n",
      "Epoch 110, Training Loss 0.0396960933628442\n",
      "Epoch 110, Training Loss 0.03977490559963467\n",
      "Epoch 110, Training Loss 0.04002945903031265\n",
      "Epoch 110, Training Loss 0.0402775510926457\n",
      "Epoch 110, Training Loss 0.04051124939547323\n",
      "Epoch 110, Training Loss 0.04093424785796486\n",
      "Epoch 110, Training Loss 0.04118930684197742\n",
      "Epoch 110, Training Loss 0.04166386816221887\n",
      "Epoch 110, Training Loss 0.04183514224236731\n",
      "Epoch 110, Training Loss 0.04201025598208465\n",
      "Epoch 110, Training Loss 0.04222935028945851\n",
      "Epoch 110, Training Loss 0.04252216780601103\n",
      "Epoch 110, Training Loss 0.042875251620817366\n",
      "Epoch 110, Training Loss 0.043076576846068165\n",
      "Epoch 110, Training Loss 0.04321331499844713\n",
      "Epoch 110, Training Loss 0.04337473108392695\n",
      "Epoch 110, Training Loss 0.0436148132195177\n",
      "Epoch 110, Training Loss 0.044186591163582506\n",
      "Epoch 110, Training Loss 0.044392335612107724\n",
      "Epoch 110, Training Loss 0.04464763570148164\n",
      "Epoch 110, Training Loss 0.04499427844172396\n",
      "Epoch 110, Training Loss 0.04524469554252789\n",
      "Epoch 110, Training Loss 0.04557873757884783\n",
      "Epoch 110, Training Loss 0.04585462550887519\n",
      "Epoch 110, Training Loss 0.04619714790655067\n",
      "Epoch 110, Training Loss 0.04647699278086195\n",
      "Epoch 110, Training Loss 0.04662600890888125\n",
      "Epoch 110, Training Loss 0.04686590885776845\n",
      "Epoch 110, Training Loss 0.04717884963030553\n",
      "Epoch 110, Training Loss 0.04738219600180378\n",
      "Epoch 110, Training Loss 0.04764910937403626\n",
      "Epoch 110, Training Loss 0.047821746605550844\n",
      "Epoch 110, Training Loss 0.04818180385891281\n",
      "Epoch 110, Training Loss 0.0485734431134999\n",
      "Epoch 110, Training Loss 0.04883576839533456\n",
      "Epoch 110, Training Loss 0.04907598976722306\n",
      "Epoch 110, Training Loss 0.04939685224095726\n",
      "Epoch 110, Training Loss 0.049655759154492635\n",
      "Epoch 110, Training Loss 0.049774621391807064\n",
      "Epoch 110, Training Loss 0.04995062859142985\n",
      "Epoch 110, Training Loss 0.05033896977315321\n",
      "Epoch 110, Training Loss 0.05057884068192576\n",
      "Epoch 110, Training Loss 0.050905731799619276\n",
      "Epoch 110, Training Loss 0.051149563711431936\n",
      "Epoch 110, Training Loss 0.05139069043366653\n",
      "Epoch 110, Training Loss 0.05161552915773581\n",
      "Epoch 110, Training Loss 0.05202674019195692\n",
      "Epoch 110, Training Loss 0.05225098147854933\n",
      "Epoch 110, Training Loss 0.052427137480176925\n",
      "Epoch 110, Training Loss 0.05275205185022348\n",
      "Epoch 110, Training Loss 0.05307956412910958\n",
      "Epoch 110, Training Loss 0.05333788751545922\n",
      "Epoch 110, Training Loss 0.05354577351999862\n",
      "Epoch 110, Training Loss 0.05380487803588895\n",
      "Epoch 110, Training Loss 0.05404397195009777\n",
      "Epoch 110, Training Loss 0.05427752146997568\n",
      "Epoch 110, Training Loss 0.05460703785976638\n",
      "Epoch 110, Training Loss 0.054846668349164525\n",
      "Epoch 110, Training Loss 0.055017165968294646\n",
      "Epoch 110, Training Loss 0.05514472562467198\n",
      "Epoch 110, Training Loss 0.05540108657382486\n",
      "Epoch 110, Training Loss 0.05560473346477732\n",
      "Epoch 110, Training Loss 0.05592621885278189\n",
      "Epoch 110, Training Loss 0.056073475538579096\n",
      "Epoch 110, Training Loss 0.056318918986203115\n",
      "Epoch 110, Training Loss 0.056603559080863855\n",
      "Epoch 110, Training Loss 0.05696426447757217\n",
      "Epoch 110, Training Loss 0.05709965585652367\n",
      "Epoch 110, Training Loss 0.057293857126246635\n",
      "Epoch 110, Training Loss 0.057448157821508014\n",
      "Epoch 110, Training Loss 0.05765813166547157\n",
      "Epoch 110, Training Loss 0.058001235017881674\n",
      "Epoch 110, Training Loss 0.058154874307382136\n",
      "Epoch 110, Training Loss 0.058468073735113646\n",
      "Epoch 110, Training Loss 0.05881924002581393\n",
      "Epoch 110, Training Loss 0.05918122314945664\n",
      "Epoch 110, Training Loss 0.05936934442147422\n",
      "Epoch 110, Training Loss 0.059647810633491984\n",
      "Epoch 110, Training Loss 0.059976366450986286\n",
      "Epoch 110, Training Loss 0.060342425556705735\n",
      "Epoch 110, Training Loss 0.06058194762205377\n",
      "Epoch 110, Training Loss 0.060752074884446075\n",
      "Epoch 110, Training Loss 0.06118295632798196\n",
      "Epoch 110, Training Loss 0.06130272924156902\n",
      "Epoch 110, Training Loss 0.06158605511860012\n",
      "Epoch 110, Training Loss 0.061951072705561855\n",
      "Epoch 110, Training Loss 0.06205059977157799\n",
      "Epoch 110, Training Loss 0.06228635592095535\n",
      "Epoch 110, Training Loss 0.06260442206412173\n",
      "Epoch 110, Training Loss 0.0627607612553841\n",
      "Epoch 110, Training Loss 0.0630336648944165\n",
      "Epoch 110, Training Loss 0.0632757502072074\n",
      "Epoch 110, Training Loss 0.06372402340192776\n",
      "Epoch 110, Training Loss 0.06395277543388822\n",
      "Epoch 110, Training Loss 0.0642641356924687\n",
      "Epoch 110, Training Loss 0.06464110614488953\n",
      "Epoch 110, Training Loss 0.06492915096433113\n",
      "Epoch 110, Training Loss 0.06522958409851012\n",
      "Epoch 110, Training Loss 0.06539216963931575\n",
      "Epoch 110, Training Loss 0.06559820669938994\n",
      "Epoch 110, Training Loss 0.06612181639217812\n",
      "Epoch 110, Training Loss 0.06639457079093627\n",
      "Epoch 110, Training Loss 0.06655049571276778\n",
      "Epoch 110, Training Loss 0.066863512782299\n",
      "Epoch 110, Training Loss 0.06709751735924913\n",
      "Epoch 110, Training Loss 0.06738886284306074\n",
      "Epoch 110, Training Loss 0.06769056356681125\n",
      "Epoch 110, Training Loss 0.06787635345497857\n",
      "Epoch 110, Training Loss 0.06809347396826043\n",
      "Epoch 110, Training Loss 0.06839738033064034\n",
      "Epoch 110, Training Loss 0.0687256984536529\n",
      "Epoch 110, Training Loss 0.0689828902282907\n",
      "Epoch 110, Training Loss 0.06922628486152653\n",
      "Epoch 110, Training Loss 0.0694692428831173\n",
      "Epoch 110, Training Loss 0.06981888864560963\n",
      "Epoch 110, Training Loss 0.07025837236562806\n",
      "Epoch 110, Training Loss 0.07052895197611483\n",
      "Epoch 110, Training Loss 0.07083940829443353\n",
      "Epoch 110, Training Loss 0.07107849588231815\n",
      "Epoch 110, Training Loss 0.07130535520003427\n",
      "Epoch 110, Training Loss 0.07145837271857597\n",
      "Epoch 110, Training Loss 0.07176858112882928\n",
      "Epoch 110, Training Loss 0.07197725578971073\n",
      "Epoch 110, Training Loss 0.07221005173385753\n",
      "Epoch 110, Training Loss 0.07250437539194704\n",
      "Epoch 110, Training Loss 0.07267610525326504\n",
      "Epoch 110, Training Loss 0.07281487034467023\n",
      "Epoch 110, Training Loss 0.0729602354881175\n",
      "Epoch 110, Training Loss 0.07329837317147371\n",
      "Epoch 110, Training Loss 0.07361586032259038\n",
      "Epoch 110, Training Loss 0.07390158383837898\n",
      "Epoch 110, Training Loss 0.07405423719312071\n",
      "Epoch 110, Training Loss 0.07423973534627797\n",
      "Epoch 110, Training Loss 0.0746072218384203\n",
      "Epoch 110, Training Loss 0.07478126974495324\n",
      "Epoch 110, Training Loss 0.07494068281520205\n",
      "Epoch 110, Training Loss 0.07517072294012207\n",
      "Epoch 110, Training Loss 0.07540991961898859\n",
      "Epoch 110, Training Loss 0.07568898545983045\n",
      "Epoch 110, Training Loss 0.07586052621740971\n",
      "Epoch 110, Training Loss 0.07597100102554655\n",
      "Epoch 110, Training Loss 0.07627997757471583\n",
      "Epoch 110, Training Loss 0.07646328342787902\n",
      "Epoch 110, Training Loss 0.07656588270555219\n",
      "Epoch 110, Training Loss 0.07687716750556703\n",
      "Epoch 110, Training Loss 0.07718075825677961\n",
      "Epoch 110, Training Loss 0.07737758899074229\n",
      "Epoch 110, Training Loss 0.07776229253605656\n",
      "Epoch 110, Training Loss 0.07806325172695815\n",
      "Epoch 110, Training Loss 0.07847727382141154\n",
      "Epoch 110, Training Loss 0.07880616401943862\n",
      "Epoch 110, Training Loss 0.07904966177938082\n",
      "Epoch 110, Training Loss 0.07918616566721283\n",
      "Epoch 110, Training Loss 0.07950522062247214\n",
      "Epoch 110, Training Loss 0.07981736420194054\n",
      "Epoch 110, Training Loss 0.0801007053612367\n",
      "Epoch 110, Training Loss 0.08040513627497894\n",
      "Epoch 110, Training Loss 0.08073746730737827\n",
      "Epoch 110, Training Loss 0.08092907388858936\n",
      "Epoch 110, Training Loss 0.08127064184974069\n",
      "Epoch 110, Training Loss 0.08151960084715006\n",
      "Epoch 110, Training Loss 0.08199168716454902\n",
      "Epoch 110, Training Loss 0.08226891031102909\n",
      "Epoch 110, Training Loss 0.08242486523526252\n",
      "Epoch 110, Training Loss 0.08253576631760201\n",
      "Epoch 110, Training Loss 0.08287624912360288\n",
      "Epoch 110, Training Loss 0.0831533893609367\n",
      "Epoch 110, Training Loss 0.08349738800255081\n",
      "Epoch 110, Training Loss 0.08372065663585425\n",
      "Epoch 110, Training Loss 0.08388931184645047\n",
      "Epoch 110, Training Loss 0.08414174016574612\n",
      "Epoch 110, Training Loss 0.08457932793213736\n",
      "Epoch 110, Training Loss 0.08482784942230757\n",
      "Epoch 110, Training Loss 0.08506294024055419\n",
      "Epoch 110, Training Loss 0.08534877097991574\n",
      "Epoch 110, Training Loss 0.08542887118104321\n",
      "Epoch 110, Training Loss 0.08563971981082273\n",
      "Epoch 110, Training Loss 0.08596160871160152\n",
      "Epoch 110, Training Loss 0.08643864876950336\n",
      "Epoch 110, Training Loss 0.08673243181270254\n",
      "Epoch 110, Training Loss 0.08699733983544285\n",
      "Epoch 110, Training Loss 0.08717358964578727\n",
      "Epoch 110, Training Loss 0.08761940247205365\n",
      "Epoch 110, Training Loss 0.0878596520265731\n",
      "Epoch 110, Training Loss 0.08818097289203836\n",
      "Epoch 110, Training Loss 0.08840477173609654\n",
      "Epoch 110, Training Loss 0.08872328283708267\n",
      "Epoch 110, Training Loss 0.08889779963475816\n",
      "Epoch 110, Training Loss 0.08917467412836563\n",
      "Epoch 110, Training Loss 0.08937050625586601\n",
      "Epoch 110, Training Loss 0.08961791629471895\n",
      "Epoch 110, Training Loss 0.08978025400367996\n",
      "Epoch 110, Training Loss 0.08998659966261033\n",
      "Epoch 110, Training Loss 0.09034515456641878\n",
      "Epoch 110, Training Loss 0.09065029446673972\n",
      "Epoch 110, Training Loss 0.09105544122378997\n",
      "Epoch 110, Training Loss 0.09125188770977889\n",
      "Epoch 110, Training Loss 0.09150312831887351\n",
      "Epoch 110, Training Loss 0.09172010709962729\n",
      "Epoch 110, Training Loss 0.09194121102485663\n",
      "Epoch 110, Training Loss 0.09228012955192562\n",
      "Epoch 110, Training Loss 0.09259725252018712\n",
      "Epoch 110, Training Loss 0.09292751251984283\n",
      "Epoch 110, Training Loss 0.09333950737038689\n",
      "Epoch 110, Training Loss 0.09371900888602905\n",
      "Epoch 110, Training Loss 0.09410801356958459\n",
      "Epoch 110, Training Loss 0.09425998971228251\n",
      "Epoch 110, Training Loss 0.09453002983213538\n",
      "Epoch 110, Training Loss 0.09480310550145328\n",
      "Epoch 110, Training Loss 0.09505502175053825\n",
      "Epoch 110, Training Loss 0.09534515768689725\n",
      "Epoch 110, Training Loss 0.09588965381521855\n",
      "Epoch 110, Training Loss 0.09621346839572616\n",
      "Epoch 110, Training Loss 0.09639036539665725\n",
      "Epoch 110, Training Loss 0.09654823651113321\n",
      "Epoch 110, Training Loss 0.0968378573093954\n",
      "Epoch 110, Training Loss 0.09704054711515184\n",
      "Epoch 110, Training Loss 0.09719828418587022\n",
      "Epoch 110, Training Loss 0.09744324022070373\n",
      "Epoch 110, Training Loss 0.09763473088917372\n",
      "Epoch 110, Training Loss 0.09782794296569988\n",
      "Epoch 110, Training Loss 0.09812160731409975\n",
      "Epoch 110, Training Loss 0.09837014603016474\n",
      "Epoch 110, Training Loss 0.09872391583193141\n",
      "Epoch 110, Training Loss 0.09905449489174444\n",
      "Epoch 110, Training Loss 0.09943117258017478\n",
      "Epoch 110, Training Loss 0.09963039035820748\n",
      "Epoch 110, Training Loss 0.0998555294873998\n",
      "Epoch 110, Training Loss 0.10002853715187296\n",
      "Epoch 110, Training Loss 0.10023368780246324\n",
      "Epoch 110, Training Loss 0.10051709475934201\n",
      "Epoch 110, Training Loss 0.10071685717291082\n",
      "Epoch 110, Training Loss 0.10102687487878916\n",
      "Epoch 110, Training Loss 0.10128332513963323\n",
      "Epoch 110, Training Loss 0.10179832912600406\n",
      "Epoch 110, Training Loss 0.10207585014326645\n",
      "Epoch 110, Training Loss 0.10237641026125387\n",
      "Epoch 110, Training Loss 0.1026258808143837\n",
      "Epoch 110, Training Loss 0.10290381672037074\n",
      "Epoch 110, Training Loss 0.1031575293406425\n",
      "Epoch 110, Training Loss 0.10342590634227561\n",
      "Epoch 110, Training Loss 0.10374633104199796\n",
      "Epoch 110, Training Loss 0.1040180084412284\n",
      "Epoch 110, Training Loss 0.10427559846940705\n",
      "Epoch 110, Training Loss 0.10474704994398462\n",
      "Epoch 110, Training Loss 0.1050458538972432\n",
      "Epoch 110, Training Loss 0.10534579618869687\n",
      "Epoch 110, Training Loss 0.1057375145985571\n",
      "Epoch 110, Training Loss 0.1059015548318777\n",
      "Epoch 110, Training Loss 0.10604403943033017\n",
      "Epoch 110, Training Loss 0.10634486018525213\n",
      "Epoch 110, Training Loss 0.10651403282056837\n",
      "Epoch 110, Training Loss 0.10676212101469716\n",
      "Epoch 110, Training Loss 0.10715443028799256\n",
      "Epoch 110, Training Loss 0.1073783154139662\n",
      "Epoch 110, Training Loss 0.10768977969484714\n",
      "Epoch 110, Training Loss 0.10808315217171026\n",
      "Epoch 110, Training Loss 0.10832676902661086\n",
      "Epoch 110, Training Loss 0.10855605296523826\n",
      "Epoch 110, Training Loss 0.10879300696217953\n",
      "Epoch 110, Training Loss 0.10911130531193197\n",
      "Epoch 110, Training Loss 0.10928837983104427\n",
      "Epoch 110, Training Loss 0.10951584721903515\n",
      "Epoch 110, Training Loss 0.10966099320870379\n",
      "Epoch 110, Training Loss 0.11012927050728474\n",
      "Epoch 110, Training Loss 0.11027885256501872\n",
      "Epoch 110, Training Loss 0.11053425115068703\n",
      "Epoch 110, Training Loss 0.11075618106138219\n",
      "Epoch 110, Training Loss 0.11093376707904937\n",
      "Epoch 110, Training Loss 0.11112236995202349\n",
      "Epoch 110, Training Loss 0.11128031582002292\n",
      "Epoch 110, Training Loss 0.11152329200121296\n",
      "Epoch 110, Training Loss 0.1117181513610932\n",
      "Epoch 110, Training Loss 0.11187343594268002\n",
      "Epoch 110, Training Loss 0.11226907244328495\n",
      "Epoch 110, Training Loss 0.11249330869930632\n",
      "Epoch 110, Training Loss 0.11279833752690527\n",
      "Epoch 110, Training Loss 0.11292413210072329\n",
      "Epoch 110, Training Loss 0.11328601834776304\n",
      "Epoch 110, Training Loss 0.11349684992314452\n",
      "Epoch 110, Training Loss 0.11399203700387417\n",
      "Epoch 110, Training Loss 0.1145072791420514\n",
      "Epoch 110, Training Loss 0.11477682453191951\n",
      "Epoch 110, Training Loss 0.11491614152841709\n",
      "Epoch 110, Training Loss 0.11525128068654891\n",
      "Epoch 110, Training Loss 0.11544241709991947\n",
      "Epoch 110, Training Loss 0.11579260890803221\n",
      "Epoch 110, Training Loss 0.11618026284515248\n",
      "Epoch 110, Training Loss 0.11662888669354074\n",
      "Epoch 110, Training Loss 0.11681992560148696\n",
      "Epoch 110, Training Loss 0.11708656265436078\n",
      "Epoch 110, Training Loss 0.11759108647494518\n",
      "Epoch 110, Training Loss 0.11791143087608277\n",
      "Epoch 110, Training Loss 0.11819882696146702\n",
      "Epoch 110, Training Loss 0.11844609728764238\n",
      "Epoch 110, Training Loss 0.11870572174353825\n",
      "Epoch 110, Training Loss 0.11892315272308523\n",
      "Epoch 110, Training Loss 0.11928612191487303\n",
      "Epoch 110, Training Loss 0.11958055742694747\n",
      "Epoch 110, Training Loss 0.11989040843799444\n",
      "Epoch 110, Training Loss 0.1201910225202894\n",
      "Epoch 110, Training Loss 0.12041388729782514\n",
      "Epoch 110, Training Loss 0.12070404066968604\n",
      "Epoch 110, Training Loss 0.1209157327013781\n",
      "Epoch 110, Training Loss 0.12149565513996059\n",
      "Epoch 110, Training Loss 0.12182607451725341\n",
      "Epoch 110, Training Loss 0.12216382507530167\n",
      "Epoch 110, Training Loss 0.1225667347526535\n",
      "Epoch 110, Training Loss 0.12284186510535915\n",
      "Epoch 110, Training Loss 0.12305297557255039\n",
      "Epoch 110, Training Loss 0.1232643846536765\n",
      "Epoch 110, Training Loss 0.1233761573015043\n",
      "Epoch 110, Training Loss 0.12370539380861517\n",
      "Epoch 110, Training Loss 0.12398129827855035\n",
      "Epoch 110, Training Loss 0.12419421372987578\n",
      "Epoch 110, Training Loss 0.12447906813829604\n",
      "Epoch 110, Training Loss 0.12477669408044699\n",
      "Epoch 110, Training Loss 0.1251830207731794\n",
      "Epoch 110, Training Loss 0.1254382556556817\n",
      "Epoch 110, Training Loss 0.125806562626339\n",
      "Epoch 110, Training Loss 0.1260292757054805\n",
      "Epoch 110, Training Loss 0.12620737766156265\n",
      "Epoch 110, Training Loss 0.12649886342970765\n",
      "Epoch 110, Training Loss 0.126835540913598\n",
      "Epoch 110, Training Loss 0.12698673886125503\n",
      "Epoch 110, Training Loss 0.12716504289766253\n",
      "Epoch 110, Training Loss 0.12750114971662269\n",
      "Epoch 110, Training Loss 0.12783338002327\n",
      "Epoch 110, Training Loss 0.12816579966231836\n",
      "Epoch 110, Training Loss 0.12858441104764676\n",
      "Epoch 110, Training Loss 0.12894397195133253\n",
      "Epoch 110, Training Loss 0.1291023033440037\n",
      "Epoch 110, Training Loss 0.12931546474070957\n",
      "Epoch 110, Training Loss 0.1297677007677686\n",
      "Epoch 110, Training Loss 0.13012476970948983\n",
      "Epoch 110, Training Loss 0.1303702291253659\n",
      "Epoch 110, Training Loss 0.13064703784044593\n",
      "Epoch 110, Training Loss 0.1310216850074737\n",
      "Epoch 110, Training Loss 0.13122650146808312\n",
      "Epoch 110, Training Loss 0.13161984143678643\n",
      "Epoch 110, Training Loss 0.13195876259347208\n",
      "Epoch 110, Training Loss 0.132290364459843\n",
      "Epoch 110, Training Loss 0.1325182000894452\n",
      "Epoch 110, Training Loss 0.13269546571309151\n",
      "Epoch 110, Training Loss 0.13305393159103668\n",
      "Epoch 110, Training Loss 0.13329144819256136\n",
      "Epoch 110, Training Loss 0.13346742795270575\n",
      "Epoch 110, Training Loss 0.1337558358498013\n",
      "Epoch 110, Training Loss 0.1339510181189879\n",
      "Epoch 110, Training Loss 0.1342234662055131\n",
      "Epoch 110, Training Loss 0.13446292281627198\n",
      "Epoch 110, Training Loss 0.13466119724790307\n",
      "Epoch 110, Training Loss 0.1350095832715635\n",
      "Epoch 110, Training Loss 0.13536592750617152\n",
      "Epoch 110, Training Loss 0.13568655754465733\n",
      "Epoch 110, Training Loss 0.13591530401250132\n",
      "Epoch 110, Training Loss 0.1361140596639851\n",
      "Epoch 110, Training Loss 0.13631782532595765\n",
      "Epoch 110, Training Loss 0.1366799213706761\n",
      "Epoch 110, Training Loss 0.13693988920115602\n",
      "Epoch 110, Training Loss 0.13715160065009008\n",
      "Epoch 110, Training Loss 0.1373546203202985\n",
      "Epoch 110, Training Loss 0.1376578513856815\n",
      "Epoch 110, Training Loss 0.13798114381578114\n",
      "Epoch 110, Training Loss 0.13822580373291013\n",
      "Epoch 110, Training Loss 0.1384125383203978\n",
      "Epoch 110, Training Loss 0.1390108426942316\n",
      "Epoch 110, Training Loss 0.13938078391449074\n",
      "Epoch 110, Training Loss 0.13967536632305064\n",
      "Epoch 110, Training Loss 0.14007099684032484\n",
      "Epoch 110, Training Loss 0.14034652093049052\n",
      "Epoch 110, Training Loss 0.14068630429656456\n",
      "Epoch 110, Training Loss 0.14090925931949597\n",
      "Epoch 110, Training Loss 0.14110026892055483\n",
      "Epoch 110, Training Loss 0.1414101465135965\n",
      "Epoch 110, Training Loss 0.14173384943067113\n",
      "Epoch 110, Training Loss 0.1419375385860424\n",
      "Epoch 110, Training Loss 0.14215291867418514\n",
      "Epoch 110, Training Loss 0.14231589267416225\n",
      "Epoch 110, Training Loss 0.14256691830256557\n",
      "Epoch 110, Training Loss 0.14292135903768985\n",
      "Epoch 110, Training Loss 0.143051624055142\n",
      "Epoch 110, Training Loss 0.14333062926235865\n",
      "Epoch 110, Training Loss 0.1435789497202391\n",
      "Epoch 110, Training Loss 0.1438747643138213\n",
      "Epoch 110, Training Loss 0.14404573030960377\n",
      "Epoch 110, Training Loss 0.14428758579770776\n",
      "Epoch 110, Training Loss 0.1444554133840915\n",
      "Epoch 110, Training Loss 0.14473199974412046\n",
      "Epoch 110, Training Loss 0.14486654901691257\n",
      "Epoch 110, Training Loss 0.14511136179480255\n",
      "Epoch 110, Training Loss 0.14535152941199062\n",
      "Epoch 110, Training Loss 0.14556169744742953\n",
      "Epoch 110, Training Loss 0.1458046286011977\n",
      "Epoch 110, Training Loss 0.1460915342630709\n",
      "Epoch 110, Training Loss 0.14630674363573645\n",
      "Epoch 110, Training Loss 0.14652618331848966\n",
      "Epoch 110, Training Loss 0.14677671556982697\n",
      "Epoch 110, Training Loss 0.14702642156892573\n",
      "Epoch 110, Training Loss 0.14736725133188697\n",
      "Epoch 110, Training Loss 0.1476476556285644\n",
      "Epoch 110, Training Loss 0.14795914540529403\n",
      "Epoch 110, Training Loss 0.14821120235792665\n",
      "Epoch 110, Training Loss 0.14840758986809216\n",
      "Epoch 110, Training Loss 0.14882353024885936\n",
      "Epoch 110, Training Loss 0.14911879022198413\n",
      "Epoch 110, Training Loss 0.1493181377849387\n",
      "Epoch 110, Training Loss 0.149597697536392\n",
      "Epoch 110, Training Loss 0.15005480231779159\n",
      "Epoch 110, Training Loss 0.15037694953077133\n",
      "Epoch 110, Training Loss 0.15087001855054016\n",
      "Epoch 110, Training Loss 0.15101582904243865\n",
      "Epoch 110, Training Loss 0.15117926415903946\n",
      "Epoch 110, Training Loss 0.15144190556653167\n",
      "Epoch 110, Training Loss 0.1517392516641132\n",
      "Epoch 110, Training Loss 0.1520282223753993\n",
      "Epoch 110, Training Loss 0.15225813450658565\n",
      "Epoch 110, Training Loss 0.1524964841439977\n",
      "Epoch 110, Training Loss 0.15281645010899553\n",
      "Epoch 110, Training Loss 0.15318553296901533\n",
      "Epoch 110, Training Loss 0.15361260703247984\n",
      "Epoch 110, Training Loss 0.154015212398394\n",
      "Epoch 110, Training Loss 0.1542810309354378\n",
      "Epoch 110, Training Loss 0.15470756615614495\n",
      "Epoch 110, Training Loss 0.15489251170869525\n",
      "Epoch 110, Training Loss 0.15519397845372673\n",
      "Epoch 110, Training Loss 0.1555822559911043\n",
      "Epoch 110, Training Loss 0.1560934121622835\n",
      "Epoch 110, Training Loss 0.15627291181679728\n",
      "Epoch 110, Training Loss 0.1564905840541472\n",
      "Epoch 110, Training Loss 0.15686967458737933\n",
      "Epoch 110, Training Loss 0.15719442988467186\n",
      "Epoch 110, Training Loss 0.15749012452581196\n",
      "Epoch 110, Training Loss 0.1578742074053687\n",
      "Epoch 110, Training Loss 0.15813204587515814\n",
      "Epoch 110, Training Loss 0.1583507870135786\n",
      "Epoch 110, Training Loss 0.1586847973801672\n",
      "Epoch 110, Training Loss 0.15911059707041134\n",
      "Epoch 110, Training Loss 0.159360165471959\n",
      "Epoch 110, Training Loss 0.1596355354818313\n",
      "Epoch 110, Training Loss 0.15992287173390846\n",
      "Epoch 110, Training Loss 0.16032152535760646\n",
      "Epoch 110, Training Loss 0.16064987377837645\n",
      "Epoch 110, Training Loss 0.16092733431445516\n",
      "Epoch 110, Training Loss 0.1611861359099369\n",
      "Epoch 110, Training Loss 0.16132668846422601\n",
      "Epoch 110, Training Loss 0.16145976850539065\n",
      "Epoch 110, Training Loss 0.16186652378753172\n",
      "Epoch 110, Training Loss 0.16223452953369263\n",
      "Epoch 110, Training Loss 0.16250222933280956\n",
      "Epoch 110, Training Loss 0.16284404443981854\n",
      "Epoch 110, Training Loss 0.163287263854271\n",
      "Epoch 110, Training Loss 0.1634869335833794\n",
      "Epoch 110, Training Loss 0.16365346030982406\n",
      "Epoch 110, Training Loss 0.16401042067505361\n",
      "Epoch 110, Training Loss 0.164237700559942\n",
      "Epoch 110, Training Loss 0.16437990712406841\n",
      "Epoch 110, Training Loss 0.16457164493839607\n",
      "Epoch 110, Training Loss 0.1647240745184748\n",
      "Epoch 110, Training Loss 0.1649426127190861\n",
      "Epoch 110, Training Loss 0.16512924816240282\n",
      "Epoch 110, Training Loss 0.16537715460809752\n",
      "Epoch 110, Training Loss 0.16559702451900601\n",
      "Epoch 110, Training Loss 0.1658818158547363\n",
      "Epoch 110, Training Loss 0.16619243850583768\n",
      "Epoch 110, Training Loss 0.166407491740249\n",
      "Epoch 110, Training Loss 0.16661807218723743\n",
      "Epoch 110, Training Loss 0.16681072028720623\n",
      "Epoch 110, Training Loss 0.16712550996609812\n",
      "Epoch 110, Training Loss 0.167497507546602\n",
      "Epoch 110, Training Loss 0.16781773250978774\n",
      "Epoch 110, Training Loss 0.16793367794960204\n",
      "Epoch 110, Training Loss 0.1681027923713026\n",
      "Epoch 110, Training Loss 0.16849935613572598\n",
      "Epoch 110, Training Loss 0.16864792246590643\n",
      "Epoch 110, Training Loss 0.16886926793476656\n",
      "Epoch 110, Training Loss 0.16911160525248942\n",
      "Epoch 110, Training Loss 0.16942732411977426\n",
      "Epoch 110, Training Loss 0.1696240628743187\n",
      "Epoch 110, Training Loss 0.16987056899196504\n",
      "Epoch 110, Training Loss 0.16996104378358026\n",
      "Epoch 110, Training Loss 0.17019841874785283\n",
      "Epoch 110, Training Loss 0.17042529500563583\n",
      "Epoch 110, Training Loss 0.17057765552016627\n",
      "Epoch 110, Training Loss 0.1707649065796143\n",
      "Epoch 110, Training Loss 0.17092024100005931\n",
      "Epoch 110, Training Loss 0.17106861210025637\n",
      "Epoch 110, Training Loss 0.17128663562009555\n",
      "Epoch 110, Training Loss 0.17164049692490063\n",
      "Epoch 110, Training Loss 0.17202114007528632\n",
      "Epoch 110, Training Loss 0.17231389579584683\n",
      "Epoch 110, Training Loss 0.17255406615698277\n",
      "Epoch 110, Training Loss 0.17293200396534886\n",
      "Epoch 110, Training Loss 0.1734500690923094\n",
      "Epoch 110, Training Loss 0.17367748388796664\n",
      "Epoch 110, Training Loss 0.17393454519169563\n",
      "Epoch 110, Training Loss 0.17418331733387907\n",
      "Epoch 110, Training Loss 0.17443797638749375\n",
      "Epoch 110, Training Loss 0.17466085151199948\n",
      "Epoch 110, Training Loss 0.17500277908275957\n",
      "Epoch 110, Training Loss 0.17520716634419414\n",
      "Epoch 110, Training Loss 0.17555213306585085\n",
      "Epoch 110, Training Loss 0.17588722523387587\n",
      "Epoch 110, Training Loss 0.17624937097453858\n",
      "Epoch 110, Training Loss 0.1764465049337929\n",
      "Epoch 110, Training Loss 0.1767345200461881\n",
      "Epoch 110, Training Loss 0.17701869391743333\n",
      "Epoch 110, Training Loss 0.1774090469083594\n",
      "Epoch 110, Training Loss 0.17759768541454507\n",
      "Epoch 110, Training Loss 0.17794017465618414\n",
      "Epoch 110, Training Loss 0.17816451489639556\n",
      "Epoch 110, Training Loss 0.17833180699373602\n",
      "Epoch 110, Training Loss 0.17856779531158906\n",
      "Epoch 110, Training Loss 0.17889993979364557\n",
      "Epoch 110, Training Loss 0.17918161545282282\n",
      "Epoch 110, Training Loss 0.1794397844349408\n",
      "Epoch 110, Training Loss 0.17985572473948722\n",
      "Epoch 110, Training Loss 0.18013835491140937\n",
      "Epoch 110, Training Loss 0.18025200627744198\n",
      "Epoch 110, Training Loss 0.18041935635973577\n",
      "Epoch 110, Training Loss 0.18064536432952374\n",
      "Epoch 110, Training Loss 0.18087387174996725\n",
      "Epoch 110, Training Loss 0.1812224716872282\n",
      "Epoch 110, Training Loss 0.18159794196238754\n",
      "Epoch 110, Training Loss 0.18188831500251731\n",
      "Epoch 110, Training Loss 0.18234650528206087\n",
      "Epoch 110, Training Loss 0.1825149770292556\n",
      "Epoch 110, Training Loss 0.1827626784982355\n",
      "Epoch 110, Training Loss 0.18302694721923918\n",
      "Epoch 110, Training Loss 0.1831657353864835\n",
      "Epoch 110, Training Loss 0.18341950715407537\n",
      "Epoch 110, Training Loss 0.18355322185703707\n",
      "Epoch 110, Training Loss 0.18378580262994065\n",
      "Epoch 110, Training Loss 0.18409791269132394\n",
      "Epoch 110, Training Loss 0.18415778944902408\n",
      "Epoch 110, Training Loss 0.18480135977763654\n",
      "Epoch 110, Training Loss 0.18520429506516822\n",
      "Epoch 110, Training Loss 0.18583184710282194\n",
      "Epoch 110, Training Loss 0.18619380796999882\n",
      "Epoch 110, Training Loss 0.1863687819589282\n",
      "Epoch 110, Training Loss 0.18668232866755838\n",
      "Epoch 110, Training Loss 0.18694057036429415\n",
      "Epoch 110, Training Loss 0.18723843640188123\n",
      "Epoch 110, Training Loss 0.18756988929475055\n",
      "Epoch 110, Training Loss 0.18790525974481917\n",
      "Epoch 110, Training Loss 0.18812404007977232\n",
      "Epoch 110, Training Loss 0.1883704490349878\n",
      "Epoch 110, Training Loss 0.1884605623972233\n",
      "Epoch 110, Training Loss 0.1887415478201321\n",
      "Epoch 110, Training Loss 0.1889876765306191\n",
      "Epoch 110, Training Loss 0.1893432595102531\n",
      "Epoch 110, Training Loss 0.1896646076627552\n",
      "Epoch 110, Training Loss 0.1898906410521711\n",
      "Epoch 110, Training Loss 0.19010622988995687\n",
      "Epoch 110, Training Loss 0.19027580668592392\n",
      "Epoch 110, Training Loss 0.1905563058965194\n",
      "Epoch 110, Training Loss 0.19073224779399459\n",
      "Epoch 110, Training Loss 0.19111650312305106\n",
      "Epoch 110, Training Loss 0.19153940360373853\n",
      "Epoch 110, Training Loss 0.1920788674079396\n",
      "Epoch 110, Training Loss 0.19220436923682233\n",
      "Epoch 110, Training Loss 0.19253758008560867\n",
      "Epoch 110, Training Loss 0.19292591612242982\n",
      "Epoch 110, Training Loss 0.1932485712706433\n",
      "Epoch 110, Training Loss 0.19345408412234863\n",
      "Epoch 110, Training Loss 0.19374275393307666\n",
      "Epoch 110, Training Loss 0.1939762269272981\n",
      "Epoch 110, Training Loss 0.1941481611174543\n",
      "Epoch 110, Training Loss 0.19452399172631982\n",
      "Epoch 110, Training Loss 0.19474194127389843\n",
      "Epoch 110, Training Loss 0.1950389514093661\n",
      "Epoch 110, Training Loss 0.19526533892049508\n",
      "Epoch 110, Training Loss 0.19553454930100905\n",
      "Epoch 110, Training Loss 0.19573668141842193\n",
      "Epoch 110, Training Loss 0.19591306204266865\n",
      "Epoch 110, Training Loss 0.19630904584322745\n",
      "Epoch 110, Training Loss 0.19656245377095763\n",
      "Epoch 110, Training Loss 0.19684078074667766\n",
      "Epoch 110, Training Loss 0.19712530893018787\n",
      "Epoch 110, Training Loss 0.19741156700131535\n",
      "Epoch 110, Training Loss 0.19777146895485156\n",
      "Epoch 110, Training Loss 0.19823048752553932\n",
      "Epoch 110, Training Loss 0.19856400074213362\n",
      "Epoch 110, Training Loss 0.19892599686141818\n",
      "Epoch 110, Training Loss 0.19926315491728466\n",
      "Epoch 110, Training Loss 0.19950898996818706\n",
      "Epoch 110, Training Loss 0.19974128725697926\n",
      "Epoch 110, Training Loss 0.19992157204262437\n",
      "Epoch 110, Training Loss 0.20020946400130496\n",
      "Epoch 110, Training Loss 0.20050728017145106\n",
      "Epoch 110, Training Loss 0.20067306313558916\n",
      "Epoch 110, Training Loss 0.20087596319634896\n",
      "Epoch 110, Training Loss 0.20110989223851267\n",
      "Epoch 110, Training Loss 0.20125013365007727\n",
      "Epoch 110, Training Loss 0.20153095163500218\n",
      "Epoch 110, Training Loss 0.20175770553938874\n",
      "Epoch 110, Training Loss 0.20208071355167254\n",
      "Epoch 110, Training Loss 0.20231511091332302\n",
      "Epoch 110, Training Loss 0.20260292659406468\n",
      "Epoch 110, Training Loss 0.20299771597699437\n",
      "Epoch 110, Training Loss 0.20329995924020972\n",
      "Epoch 110, Training Loss 0.20354034248596567\n",
      "Epoch 110, Training Loss 0.2037449128296979\n",
      "Epoch 110, Training Loss 0.20409519252036235\n",
      "Epoch 110, Training Loss 0.20429714401359753\n",
      "Epoch 110, Training Loss 0.2045949668340061\n",
      "Epoch 110, Training Loss 0.2048446438311006\n",
      "Epoch 110, Training Loss 0.20511424869222714\n",
      "Epoch 110, Training Loss 0.2052776862669479\n",
      "Epoch 110, Training Loss 0.2053910156383234\n",
      "Epoch 110, Training Loss 0.20556210791287216\n",
      "Epoch 110, Training Loss 0.2055888889914812\n",
      "Epoch 120, Training Loss 0.00038223696486724306\n",
      "Epoch 120, Training Loss 0.0007050954701040712\n",
      "Epoch 120, Training Loss 0.0009084335525932214\n",
      "Epoch 120, Training Loss 0.0011315644549591767\n",
      "Epoch 120, Training Loss 0.00136714908854126\n",
      "Epoch 120, Training Loss 0.0017225900688744567\n",
      "Epoch 120, Training Loss 0.0019241194895771154\n",
      "Epoch 120, Training Loss 0.002037173770653927\n",
      "Epoch 120, Training Loss 0.0021765306115607776\n",
      "Epoch 120, Training Loss 0.0023671822798678943\n",
      "Epoch 120, Training Loss 0.002531348372740514\n",
      "Epoch 120, Training Loss 0.0027892930942880526\n",
      "Epoch 120, Training Loss 0.003122534317052578\n",
      "Epoch 120, Training Loss 0.003432107674877357\n",
      "Epoch 120, Training Loss 0.003723619510526852\n",
      "Epoch 120, Training Loss 0.003902483655287482\n",
      "Epoch 120, Training Loss 0.0040662060860935075\n",
      "Epoch 120, Training Loss 0.004431559806665801\n",
      "Epoch 120, Training Loss 0.004832379129307959\n",
      "Epoch 120, Training Loss 0.005042437159115701\n",
      "Epoch 120, Training Loss 0.005254980691177461\n",
      "Epoch 120, Training Loss 0.005427757330486537\n",
      "Epoch 120, Training Loss 0.005774480597975919\n",
      "Epoch 120, Training Loss 0.005935376228959969\n",
      "Epoch 120, Training Loss 0.0060954141856916725\n",
      "Epoch 120, Training Loss 0.00641295185212589\n",
      "Epoch 120, Training Loss 0.006580878506459848\n",
      "Epoch 120, Training Loss 0.006647775616601605\n",
      "Epoch 120, Training Loss 0.0069874063886873556\n",
      "Epoch 120, Training Loss 0.007153924768957336\n",
      "Epoch 120, Training Loss 0.007336889095890247\n",
      "Epoch 120, Training Loss 0.007502059645169532\n",
      "Epoch 120, Training Loss 0.007707916652721822\n",
      "Epoch 120, Training Loss 0.007800851288773215\n",
      "Epoch 120, Training Loss 0.007964776028566958\n",
      "Epoch 120, Training Loss 0.008233189997275162\n",
      "Epoch 120, Training Loss 0.008409395409019096\n",
      "Epoch 120, Training Loss 0.008672548886722007\n",
      "Epoch 120, Training Loss 0.009011398776984581\n",
      "Epoch 120, Training Loss 0.009218337700304472\n",
      "Epoch 120, Training Loss 0.009456563174076702\n",
      "Epoch 120, Training Loss 0.009597552787807896\n",
      "Epoch 120, Training Loss 0.009805943831191648\n",
      "Epoch 120, Training Loss 0.01001130555615858\n",
      "Epoch 120, Training Loss 0.010166949490109062\n",
      "Epoch 120, Training Loss 0.010379413219021104\n",
      "Epoch 120, Training Loss 0.010691920257247318\n",
      "Epoch 120, Training Loss 0.011041643993590798\n",
      "Epoch 120, Training Loss 0.01127062588834854\n",
      "Epoch 120, Training Loss 0.011640975880615242\n",
      "Epoch 120, Training Loss 0.011824968940271137\n",
      "Epoch 120, Training Loss 0.012024324982786728\n",
      "Epoch 120, Training Loss 0.01231357732983044\n",
      "Epoch 120, Training Loss 0.012563104648381243\n",
      "Epoch 120, Training Loss 0.012772593899722904\n",
      "Epoch 120, Training Loss 0.013011083630916407\n",
      "Epoch 120, Training Loss 0.013182790244898528\n",
      "Epoch 120, Training Loss 0.013305930830442996\n",
      "Epoch 120, Training Loss 0.013445691755779868\n",
      "Epoch 120, Training Loss 0.013681614733374942\n",
      "Epoch 120, Training Loss 0.013814662056772606\n",
      "Epoch 120, Training Loss 0.014031659054291218\n",
      "Epoch 120, Training Loss 0.014161442070627762\n",
      "Epoch 120, Training Loss 0.014316278264460051\n",
      "Epoch 120, Training Loss 0.014499177556018086\n",
      "Epoch 120, Training Loss 0.014660193063223453\n",
      "Epoch 120, Training Loss 0.014920078937316795\n",
      "Epoch 120, Training Loss 0.015057983886822106\n",
      "Epoch 120, Training Loss 0.015175601095913926\n",
      "Epoch 120, Training Loss 0.015398771004260653\n",
      "Epoch 120, Training Loss 0.015558081824342004\n",
      "Epoch 120, Training Loss 0.015758662982403165\n",
      "Epoch 120, Training Loss 0.015916344335736216\n",
      "Epoch 120, Training Loss 0.01618276949486007\n",
      "Epoch 120, Training Loss 0.01642847182634084\n",
      "Epoch 120, Training Loss 0.016619357206594305\n",
      "Epoch 120, Training Loss 0.016901476051458312\n",
      "Epoch 120, Training Loss 0.017034153284890878\n",
      "Epoch 120, Training Loss 0.01713674734144107\n",
      "Epoch 120, Training Loss 0.017230761844826782\n",
      "Epoch 120, Training Loss 0.017572924099348085\n",
      "Epoch 120, Training Loss 0.017751801125419416\n",
      "Epoch 120, Training Loss 0.01812230157749275\n",
      "Epoch 120, Training Loss 0.018421480022466092\n",
      "Epoch 120, Training Loss 0.018634137275921718\n",
      "Epoch 120, Training Loss 0.0187963603107292\n",
      "Epoch 120, Training Loss 0.019029513692192714\n",
      "Epoch 120, Training Loss 0.019312118101493476\n",
      "Epoch 120, Training Loss 0.01974729603380346\n",
      "Epoch 120, Training Loss 0.020270830077474076\n",
      "Epoch 120, Training Loss 0.020526470683153022\n",
      "Epoch 120, Training Loss 0.020695842426184496\n",
      "Epoch 120, Training Loss 0.020991316293854544\n",
      "Epoch 120, Training Loss 0.02123557422262476\n",
      "Epoch 120, Training Loss 0.021437047925942084\n",
      "Epoch 120, Training Loss 0.021707486356501385\n",
      "Epoch 120, Training Loss 0.022037438005018416\n",
      "Epoch 120, Training Loss 0.02237479095740239\n",
      "Epoch 120, Training Loss 0.022541900810873722\n",
      "Epoch 120, Training Loss 0.02274757212556689\n",
      "Epoch 120, Training Loss 0.022928334510577915\n",
      "Epoch 120, Training Loss 0.023118639183814262\n",
      "Epoch 120, Training Loss 0.023358683799729323\n",
      "Epoch 120, Training Loss 0.023546651544054147\n",
      "Epoch 120, Training Loss 0.023746923893652\n",
      "Epoch 120, Training Loss 0.02387998562277583\n",
      "Epoch 120, Training Loss 0.02399784033579747\n",
      "Epoch 120, Training Loss 0.0241897147186958\n",
      "Epoch 120, Training Loss 0.024361309688300123\n",
      "Epoch 120, Training Loss 0.024608254475552406\n",
      "Epoch 120, Training Loss 0.02482687449916397\n",
      "Epoch 120, Training Loss 0.025125127833555725\n",
      "Epoch 120, Training Loss 0.025297482457497845\n",
      "Epoch 120, Training Loss 0.025565086256550705\n",
      "Epoch 120, Training Loss 0.025678745397102194\n",
      "Epoch 120, Training Loss 0.025817607050699652\n",
      "Epoch 120, Training Loss 0.02608425988127356\n",
      "Epoch 120, Training Loss 0.026326394013469787\n",
      "Epoch 120, Training Loss 0.026559450403999183\n",
      "Epoch 120, Training Loss 0.026792125347668252\n",
      "Epoch 120, Training Loss 0.026967095468393373\n",
      "Epoch 120, Training Loss 0.027152641030871654\n",
      "Epoch 120, Training Loss 0.027351899148748658\n",
      "Epoch 120, Training Loss 0.027584863178755924\n",
      "Epoch 120, Training Loss 0.027799853168980544\n",
      "Epoch 120, Training Loss 0.027991513142843382\n",
      "Epoch 120, Training Loss 0.028312837819347295\n",
      "Epoch 120, Training Loss 0.028570003983805246\n",
      "Epoch 120, Training Loss 0.028768574552196065\n",
      "Epoch 120, Training Loss 0.02896254677849505\n",
      "Epoch 120, Training Loss 0.029136856274722178\n",
      "Epoch 120, Training Loss 0.029325498134621878\n",
      "Epoch 120, Training Loss 0.029550710035597578\n",
      "Epoch 120, Training Loss 0.029725730766916213\n",
      "Epoch 120, Training Loss 0.029874585316423567\n",
      "Epoch 120, Training Loss 0.030056639853150338\n",
      "Epoch 120, Training Loss 0.030230563519822666\n",
      "Epoch 120, Training Loss 0.03058350407768546\n",
      "Epoch 120, Training Loss 0.03073872503398172\n",
      "Epoch 120, Training Loss 0.030923498880184825\n",
      "Epoch 120, Training Loss 0.031095270262769117\n",
      "Epoch 120, Training Loss 0.03125519167793834\n",
      "Epoch 120, Training Loss 0.03160903125982303\n",
      "Epoch 120, Training Loss 0.03183424015007818\n",
      "Epoch 120, Training Loss 0.03201711087790139\n",
      "Epoch 120, Training Loss 0.03220212087511559\n",
      "Epoch 120, Training Loss 0.03240877997292125\n",
      "Epoch 120, Training Loss 0.03252291962828325\n",
      "Epoch 120, Training Loss 0.03271660669361386\n",
      "Epoch 120, Training Loss 0.03284614808057123\n",
      "Epoch 120, Training Loss 0.03301258134129255\n",
      "Epoch 120, Training Loss 0.033226016256243676\n",
      "Epoch 120, Training Loss 0.03341493750338816\n",
      "Epoch 120, Training Loss 0.03355692425633178\n",
      "Epoch 120, Training Loss 0.03379001405061511\n",
      "Epoch 120, Training Loss 0.03391680571600757\n",
      "Epoch 120, Training Loss 0.034179702748918475\n",
      "Epoch 120, Training Loss 0.03437063597676242\n",
      "Epoch 120, Training Loss 0.034600181829022324\n",
      "Epoch 120, Training Loss 0.034767232325566395\n",
      "Epoch 120, Training Loss 0.03506596855666784\n",
      "Epoch 120, Training Loss 0.03531098385315264\n",
      "Epoch 120, Training Loss 0.03555482094797789\n",
      "Epoch 120, Training Loss 0.03582308028379212\n",
      "Epoch 120, Training Loss 0.03618180954261967\n",
      "Epoch 120, Training Loss 0.0363044484811442\n",
      "Epoch 120, Training Loss 0.036511794061345214\n",
      "Epoch 120, Training Loss 0.0368260909157717\n",
      "Epoch 120, Training Loss 0.03698027854704339\n",
      "Epoch 120, Training Loss 0.03718949176485429\n",
      "Epoch 120, Training Loss 0.03738349821428051\n",
      "Epoch 120, Training Loss 0.03755569279365375\n",
      "Epoch 120, Training Loss 0.037694683384216957\n",
      "Epoch 120, Training Loss 0.037902179919659634\n",
      "Epoch 120, Training Loss 0.038213271309462046\n",
      "Epoch 120, Training Loss 0.038399515606825\n",
      "Epoch 120, Training Loss 0.03863851613153125\n",
      "Epoch 120, Training Loss 0.038860037079666884\n",
      "Epoch 120, Training Loss 0.03907235715147632\n",
      "Epoch 120, Training Loss 0.03937632205141017\n",
      "Epoch 120, Training Loss 0.039537167567235736\n",
      "Epoch 120, Training Loss 0.039780822780240525\n",
      "Epoch 120, Training Loss 0.03996476980254931\n",
      "Epoch 120, Training Loss 0.0401355042015119\n",
      "Epoch 120, Training Loss 0.04037869120936107\n",
      "Epoch 120, Training Loss 0.040514258877433776\n",
      "Epoch 120, Training Loss 0.040832794654895276\n",
      "Epoch 120, Training Loss 0.04105560241929253\n",
      "Epoch 120, Training Loss 0.0411527949668791\n",
      "Epoch 120, Training Loss 0.04142329084884633\n",
      "Epoch 120, Training Loss 0.041639066351306105\n",
      "Epoch 120, Training Loss 0.04198888970820038\n",
      "Epoch 120, Training Loss 0.042271655362547206\n",
      "Epoch 120, Training Loss 0.04261622493586424\n",
      "Epoch 120, Training Loss 0.04282544695717447\n",
      "Epoch 120, Training Loss 0.04310297294784232\n",
      "Epoch 120, Training Loss 0.043385320855185504\n",
      "Epoch 120, Training Loss 0.04366645993917342\n",
      "Epoch 120, Training Loss 0.04388881399941719\n",
      "Epoch 120, Training Loss 0.044140130314794954\n",
      "Epoch 120, Training Loss 0.044429250571238414\n",
      "Epoch 120, Training Loss 0.044611768492156895\n",
      "Epoch 120, Training Loss 0.044765798051072204\n",
      "Epoch 120, Training Loss 0.04495834626371751\n",
      "Epoch 120, Training Loss 0.04531652406524972\n",
      "Epoch 120, Training Loss 0.04549090046426067\n",
      "Epoch 120, Training Loss 0.04579711856458651\n",
      "Epoch 120, Training Loss 0.046095477617190926\n",
      "Epoch 120, Training Loss 0.04623986701088031\n",
      "Epoch 120, Training Loss 0.04634347106889843\n",
      "Epoch 120, Training Loss 0.046525538486935906\n",
      "Epoch 120, Training Loss 0.04674349371296213\n",
      "Epoch 120, Training Loss 0.04692001713682776\n",
      "Epoch 120, Training Loss 0.047011465810791915\n",
      "Epoch 120, Training Loss 0.04721066148480034\n",
      "Epoch 120, Training Loss 0.0473841934910287\n",
      "Epoch 120, Training Loss 0.04766896272273472\n",
      "Epoch 120, Training Loss 0.04781131483995549\n",
      "Epoch 120, Training Loss 0.04815726339950434\n",
      "Epoch 120, Training Loss 0.04868692201097755\n",
      "Epoch 120, Training Loss 0.04882309289977831\n",
      "Epoch 120, Training Loss 0.04904987123292273\n",
      "Epoch 120, Training Loss 0.049198725229829474\n",
      "Epoch 120, Training Loss 0.0494072162367575\n",
      "Epoch 120, Training Loss 0.049589189312532735\n",
      "Epoch 120, Training Loss 0.04973677631057894\n",
      "Epoch 120, Training Loss 0.049882012059735824\n",
      "Epoch 120, Training Loss 0.050111740651299884\n",
      "Epoch 120, Training Loss 0.05028795206542972\n",
      "Epoch 120, Training Loss 0.05052476909364124\n",
      "Epoch 120, Training Loss 0.050704876437325914\n",
      "Epoch 120, Training Loss 0.050989984679976695\n",
      "Epoch 120, Training Loss 0.05124017409027538\n",
      "Epoch 120, Training Loss 0.05146030915419921\n",
      "Epoch 120, Training Loss 0.05173497831406038\n",
      "Epoch 120, Training Loss 0.05194048127612037\n",
      "Epoch 120, Training Loss 0.052253587809784334\n",
      "Epoch 120, Training Loss 0.052509263610405386\n",
      "Epoch 120, Training Loss 0.0526526545691292\n",
      "Epoch 120, Training Loss 0.052915611740230295\n",
      "Epoch 120, Training Loss 0.05327334338346558\n",
      "Epoch 120, Training Loss 0.05344097546356566\n",
      "Epoch 120, Training Loss 0.05364066083698779\n",
      "Epoch 120, Training Loss 0.05399101084131566\n",
      "Epoch 120, Training Loss 0.05419054609792464\n",
      "Epoch 120, Training Loss 0.054622289331634634\n",
      "Epoch 120, Training Loss 0.05472752579090083\n",
      "Epoch 120, Training Loss 0.05512226845049645\n",
      "Epoch 120, Training Loss 0.0553308475495833\n",
      "Epoch 120, Training Loss 0.055577655501492185\n",
      "Epoch 120, Training Loss 0.05576763438808796\n",
      "Epoch 120, Training Loss 0.05600872559144216\n",
      "Epoch 120, Training Loss 0.05618870189256223\n",
      "Epoch 120, Training Loss 0.056423243959350965\n",
      "Epoch 120, Training Loss 0.056695553876669205\n",
      "Epoch 120, Training Loss 0.05689915760760875\n",
      "Epoch 120, Training Loss 0.057055336678081464\n",
      "Epoch 120, Training Loss 0.05735290173888969\n",
      "Epoch 120, Training Loss 0.05768977232810939\n",
      "Epoch 120, Training Loss 0.05791108791842638\n",
      "Epoch 120, Training Loss 0.058163261164903945\n",
      "Epoch 120, Training Loss 0.05849307325318494\n",
      "Epoch 120, Training Loss 0.058703588512356936\n",
      "Epoch 120, Training Loss 0.058912471027287375\n",
      "Epoch 120, Training Loss 0.05905710066885442\n",
      "Epoch 120, Training Loss 0.05922238065210907\n",
      "Epoch 120, Training Loss 0.05936071358125686\n",
      "Epoch 120, Training Loss 0.05950021080177306\n",
      "Epoch 120, Training Loss 0.05959921475509396\n",
      "Epoch 120, Training Loss 0.05973962960702836\n",
      "Epoch 120, Training Loss 0.05988991247666309\n",
      "Epoch 120, Training Loss 0.060026874013073606\n",
      "Epoch 120, Training Loss 0.06019800100618464\n",
      "Epoch 120, Training Loss 0.06039712476589338\n",
      "Epoch 120, Training Loss 0.06050635086815528\n",
      "Epoch 120, Training Loss 0.06072904292941856\n",
      "Epoch 120, Training Loss 0.06080347423434562\n",
      "Epoch 120, Training Loss 0.06100970462841146\n",
      "Epoch 120, Training Loss 0.061238038646595556\n",
      "Epoch 120, Training Loss 0.06134246098225379\n",
      "Epoch 120, Training Loss 0.06148476493747338\n",
      "Epoch 120, Training Loss 0.06172944105151669\n",
      "Epoch 120, Training Loss 0.061936000166722883\n",
      "Epoch 120, Training Loss 0.062237861601974045\n",
      "Epoch 120, Training Loss 0.062438506652098484\n",
      "Epoch 120, Training Loss 0.06279734597372277\n",
      "Epoch 120, Training Loss 0.0629778828214654\n",
      "Epoch 120, Training Loss 0.06325646528921773\n",
      "Epoch 120, Training Loss 0.06339265239398803\n",
      "Epoch 120, Training Loss 0.06361240686853524\n",
      "Epoch 120, Training Loss 0.06400704758284646\n",
      "Epoch 120, Training Loss 0.06433087283426234\n",
      "Epoch 120, Training Loss 0.06457689125329028\n",
      "Epoch 120, Training Loss 0.06486392912009488\n",
      "Epoch 120, Training Loss 0.06538748157108226\n",
      "Epoch 120, Training Loss 0.06556743289084385\n",
      "Epoch 120, Training Loss 0.06592613208057631\n",
      "Epoch 120, Training Loss 0.06613287488784632\n",
      "Epoch 120, Training Loss 0.06631165639023341\n",
      "Epoch 120, Training Loss 0.06642805855444935\n",
      "Epoch 120, Training Loss 0.06662159757998289\n",
      "Epoch 120, Training Loss 0.0668820980221719\n",
      "Epoch 120, Training Loss 0.06705412922231742\n",
      "Epoch 120, Training Loss 0.06726303946255419\n",
      "Epoch 120, Training Loss 0.06752094807451034\n",
      "Epoch 120, Training Loss 0.06773288012541773\n",
      "Epoch 120, Training Loss 0.06795820610983597\n",
      "Epoch 120, Training Loss 0.06813999305448264\n",
      "Epoch 120, Training Loss 0.068344296225349\n",
      "Epoch 120, Training Loss 0.06854152738514459\n",
      "Epoch 120, Training Loss 0.06885719550844958\n",
      "Epoch 120, Training Loss 0.06904083768577526\n",
      "Epoch 120, Training Loss 0.06936795495050338\n",
      "Epoch 120, Training Loss 0.06965606190893046\n",
      "Epoch 120, Training Loss 0.06987513578913705\n",
      "Epoch 120, Training Loss 0.07012065753455052\n",
      "Epoch 120, Training Loss 0.07027302233649947\n",
      "Epoch 120, Training Loss 0.07036211925661168\n",
      "Epoch 120, Training Loss 0.0705450059907973\n",
      "Epoch 120, Training Loss 0.07084290514631039\n",
      "Epoch 120, Training Loss 0.07111564946487127\n",
      "Epoch 120, Training Loss 0.07137986624141789\n",
      "Epoch 120, Training Loss 0.07153285291912916\n",
      "Epoch 120, Training Loss 0.07181510008166513\n",
      "Epoch 120, Training Loss 0.07207311896602515\n",
      "Epoch 120, Training Loss 0.0722339816886903\n",
      "Epoch 120, Training Loss 0.07252106319188767\n",
      "Epoch 120, Training Loss 0.07270953986231628\n",
      "Epoch 120, Training Loss 0.07301028292921498\n",
      "Epoch 120, Training Loss 0.07320075549775987\n",
      "Epoch 120, Training Loss 0.07331773529157919\n",
      "Epoch 120, Training Loss 0.07358392689119825\n",
      "Epoch 120, Training Loss 0.07374292214775025\n",
      "Epoch 120, Training Loss 0.07391218960170856\n",
      "Epoch 120, Training Loss 0.07427458687092336\n",
      "Epoch 120, Training Loss 0.07444326336617055\n",
      "Epoch 120, Training Loss 0.07457420552897331\n",
      "Epoch 120, Training Loss 0.07485494352019656\n",
      "Epoch 120, Training Loss 0.07499341364673641\n",
      "Epoch 120, Training Loss 0.07518929192591506\n",
      "Epoch 120, Training Loss 0.0754320116432579\n",
      "Epoch 120, Training Loss 0.07569794982786068\n",
      "Epoch 120, Training Loss 0.07588684033897831\n",
      "Epoch 120, Training Loss 0.07616295294879037\n",
      "Epoch 120, Training Loss 0.07644729128540934\n",
      "Epoch 120, Training Loss 0.07670052180928952\n",
      "Epoch 120, Training Loss 0.07708115996721455\n",
      "Epoch 120, Training Loss 0.07747077464561938\n",
      "Epoch 120, Training Loss 0.07780823727969623\n",
      "Epoch 120, Training Loss 0.07797236223240643\n",
      "Epoch 120, Training Loss 0.07810156481802616\n",
      "Epoch 120, Training Loss 0.0783789305735732\n",
      "Epoch 120, Training Loss 0.07857128560466839\n",
      "Epoch 120, Training Loss 0.07880962605747725\n",
      "Epoch 120, Training Loss 0.07900006015358678\n",
      "Epoch 120, Training Loss 0.07910947941834359\n",
      "Epoch 120, Training Loss 0.07923894067821295\n",
      "Epoch 120, Training Loss 0.07941586031671376\n",
      "Epoch 120, Training Loss 0.07959648077864476\n",
      "Epoch 120, Training Loss 0.08001245439166912\n",
      "Epoch 120, Training Loss 0.08019435526731679\n",
      "Epoch 120, Training Loss 0.0804736095640208\n",
      "Epoch 120, Training Loss 0.08083505567420474\n",
      "Epoch 120, Training Loss 0.08095205743866199\n",
      "Epoch 120, Training Loss 0.08110180985935204\n",
      "Epoch 120, Training Loss 0.08130736733832018\n",
      "Epoch 120, Training Loss 0.08186449000940603\n",
      "Epoch 120, Training Loss 0.08199612098886534\n",
      "Epoch 120, Training Loss 0.08220806651179442\n",
      "Epoch 120, Training Loss 0.08239864291208784\n",
      "Epoch 120, Training Loss 0.08249443228287465\n",
      "Epoch 120, Training Loss 0.08269109293018155\n",
      "Epoch 120, Training Loss 0.0828923132947034\n",
      "Epoch 120, Training Loss 0.08301974974972817\n",
      "Epoch 120, Training Loss 0.08313454715225398\n",
      "Epoch 120, Training Loss 0.08339101986011581\n",
      "Epoch 120, Training Loss 0.08357224961185394\n",
      "Epoch 120, Training Loss 0.08378631092817582\n",
      "Epoch 120, Training Loss 0.08390809372639108\n",
      "Epoch 120, Training Loss 0.08414455815730497\n",
      "Epoch 120, Training Loss 0.08428035747936315\n",
      "Epoch 120, Training Loss 0.08445054396529637\n",
      "Epoch 120, Training Loss 0.08465730691390574\n",
      "Epoch 120, Training Loss 0.08488568870345954\n",
      "Epoch 120, Training Loss 0.08520235724347022\n",
      "Epoch 120, Training Loss 0.0855853462787083\n",
      "Epoch 120, Training Loss 0.08590398154333424\n",
      "Epoch 120, Training Loss 0.08605666749198418\n",
      "Epoch 120, Training Loss 0.08633835435561511\n",
      "Epoch 120, Training Loss 0.08666399480474879\n",
      "Epoch 120, Training Loss 0.08680011921793299\n",
      "Epoch 120, Training Loss 0.08729588706284533\n",
      "Epoch 120, Training Loss 0.0877349253105538\n",
      "Epoch 120, Training Loss 0.08792362305933557\n",
      "Epoch 120, Training Loss 0.08821309366456383\n",
      "Epoch 120, Training Loss 0.08836276428130886\n",
      "Epoch 120, Training Loss 0.08867933779307034\n",
      "Epoch 120, Training Loss 0.0888429261896464\n",
      "Epoch 120, Training Loss 0.08903751559460255\n",
      "Epoch 120, Training Loss 0.08921727417108348\n",
      "Epoch 120, Training Loss 0.0893541813334998\n",
      "Epoch 120, Training Loss 0.08963765536465913\n",
      "Epoch 120, Training Loss 0.08988433027320811\n",
      "Epoch 120, Training Loss 0.09013322608359635\n",
      "Epoch 120, Training Loss 0.09030235826473712\n",
      "Epoch 120, Training Loss 0.09049245350234345\n",
      "Epoch 120, Training Loss 0.09085099834500981\n",
      "Epoch 120, Training Loss 0.09108874957313014\n",
      "Epoch 120, Training Loss 0.09137575853320644\n",
      "Epoch 120, Training Loss 0.09177470732184932\n",
      "Epoch 120, Training Loss 0.0919158080178301\n",
      "Epoch 120, Training Loss 0.09199405785011666\n",
      "Epoch 120, Training Loss 0.09218820498403534\n",
      "Epoch 120, Training Loss 0.0923652758064401\n",
      "Epoch 120, Training Loss 0.09263133645877052\n",
      "Epoch 120, Training Loss 0.09290599416646049\n",
      "Epoch 120, Training Loss 0.09327346274672109\n",
      "Epoch 120, Training Loss 0.09344102447504735\n",
      "Epoch 120, Training Loss 0.09371443098063206\n",
      "Epoch 120, Training Loss 0.09396001513180373\n",
      "Epoch 120, Training Loss 0.09434502451297115\n",
      "Epoch 120, Training Loss 0.09478945054037644\n",
      "Epoch 120, Training Loss 0.09505281878916351\n",
      "Epoch 120, Training Loss 0.09516362896870317\n",
      "Epoch 120, Training Loss 0.09529578721488985\n",
      "Epoch 120, Training Loss 0.09546753763676147\n",
      "Epoch 120, Training Loss 0.09572628050413735\n",
      "Epoch 120, Training Loss 0.09599526870109694\n",
      "Epoch 120, Training Loss 0.09621593898729137\n",
      "Epoch 120, Training Loss 0.09635956418197936\n",
      "Epoch 120, Training Loss 0.09653394082870782\n",
      "Epoch 120, Training Loss 0.09672556219674895\n",
      "Epoch 120, Training Loss 0.09689672303188335\n",
      "Epoch 120, Training Loss 0.09717217741815178\n",
      "Epoch 120, Training Loss 0.0973119396392418\n",
      "Epoch 120, Training Loss 0.09744343140145854\n",
      "Epoch 120, Training Loss 0.09760145846363681\n",
      "Epoch 120, Training Loss 0.09809240854590598\n",
      "Epoch 120, Training Loss 0.09837937762346262\n",
      "Epoch 120, Training Loss 0.09868190450417569\n",
      "Epoch 120, Training Loss 0.09898274803005369\n",
      "Epoch 120, Training Loss 0.09919730228517214\n",
      "Epoch 120, Training Loss 0.09930523509240669\n",
      "Epoch 120, Training Loss 0.09960841234592373\n",
      "Epoch 120, Training Loss 0.0998880262903469\n",
      "Epoch 120, Training Loss 0.10009808226219376\n",
      "Epoch 120, Training Loss 0.10017898052340121\n",
      "Epoch 120, Training Loss 0.10041327445346224\n",
      "Epoch 120, Training Loss 0.10055951597383413\n",
      "Epoch 120, Training Loss 0.1007245220005741\n",
      "Epoch 120, Training Loss 0.10095286766148132\n",
      "Epoch 120, Training Loss 0.10109169962708756\n",
      "Epoch 120, Training Loss 0.10129645955093834\n",
      "Epoch 120, Training Loss 0.10152712598195314\n",
      "Epoch 120, Training Loss 0.10174138078470822\n",
      "Epoch 120, Training Loss 0.10196746836709397\n",
      "Epoch 120, Training Loss 0.10222492277946162\n",
      "Epoch 120, Training Loss 0.10249645520678109\n",
      "Epoch 120, Training Loss 0.10278807741011042\n",
      "Epoch 120, Training Loss 0.1031816692884697\n",
      "Epoch 120, Training Loss 0.10351601559811693\n",
      "Epoch 120, Training Loss 0.10363205962950159\n",
      "Epoch 120, Training Loss 0.10391318878573377\n",
      "Epoch 120, Training Loss 0.10423214936062046\n",
      "Epoch 120, Training Loss 0.10448805252780848\n",
      "Epoch 120, Training Loss 0.10467837029196264\n",
      "Epoch 120, Training Loss 0.10487415333328497\n",
      "Epoch 120, Training Loss 0.10520208196814561\n",
      "Epoch 120, Training Loss 0.10540184320029243\n",
      "Epoch 120, Training Loss 0.1055618969728346\n",
      "Epoch 120, Training Loss 0.10573983225314056\n",
      "Epoch 120, Training Loss 0.10585691534516299\n",
      "Epoch 120, Training Loss 0.10607734921357363\n",
      "Epoch 120, Training Loss 0.10629025033539366\n",
      "Epoch 120, Training Loss 0.1066568919202632\n",
      "Epoch 120, Training Loss 0.10676710785407087\n",
      "Epoch 120, Training Loss 0.10696341798109624\n",
      "Epoch 120, Training Loss 0.10719602687946518\n",
      "Epoch 120, Training Loss 0.10734497244610354\n",
      "Epoch 120, Training Loss 0.10754200880465757\n",
      "Epoch 120, Training Loss 0.10787849024871883\n",
      "Epoch 120, Training Loss 0.10813263041036361\n",
      "Epoch 120, Training Loss 0.10846643334211749\n",
      "Epoch 120, Training Loss 0.10864074472480875\n",
      "Epoch 120, Training Loss 0.10878509133006148\n",
      "Epoch 120, Training Loss 0.10909655404841656\n",
      "Epoch 120, Training Loss 0.10917750598810365\n",
      "Epoch 120, Training Loss 0.10930235710118891\n",
      "Epoch 120, Training Loss 0.10954630258195389\n",
      "Epoch 120, Training Loss 0.10965843882192583\n",
      "Epoch 120, Training Loss 0.10997349637872575\n",
      "Epoch 120, Training Loss 0.11020197445417151\n",
      "Epoch 120, Training Loss 0.11043913272278541\n",
      "Epoch 120, Training Loss 0.11066846403262347\n",
      "Epoch 120, Training Loss 0.11104899098444015\n",
      "Epoch 120, Training Loss 0.1113036532964929\n",
      "Epoch 120, Training Loss 0.11149627386170732\n",
      "Epoch 120, Training Loss 0.1115977748933122\n",
      "Epoch 120, Training Loss 0.11180762817506748\n",
      "Epoch 120, Training Loss 0.1120131061176586\n",
      "Epoch 120, Training Loss 0.11220313965454888\n",
      "Epoch 120, Training Loss 0.11244390470921384\n",
      "Epoch 120, Training Loss 0.11267899148775946\n",
      "Epoch 120, Training Loss 0.11291633638293694\n",
      "Epoch 120, Training Loss 0.1130813109304975\n",
      "Epoch 120, Training Loss 0.1131930683627534\n",
      "Epoch 120, Training Loss 0.11333842189206034\n",
      "Epoch 120, Training Loss 0.11386058556244653\n",
      "Epoch 120, Training Loss 0.1144295993220547\n",
      "Epoch 120, Training Loss 0.11474760370733945\n",
      "Epoch 120, Training Loss 0.11501052903721247\n",
      "Epoch 120, Training Loss 0.11525404089799775\n",
      "Epoch 120, Training Loss 0.11562349567728122\n",
      "Epoch 120, Training Loss 0.11574398146470642\n",
      "Epoch 120, Training Loss 0.11606672567689358\n",
      "Epoch 120, Training Loss 0.1163057566136884\n",
      "Epoch 120, Training Loss 0.11654215408465289\n",
      "Epoch 120, Training Loss 0.11676588448245659\n",
      "Epoch 120, Training Loss 0.1169657030135698\n",
      "Epoch 120, Training Loss 0.1171588374401831\n",
      "Epoch 120, Training Loss 0.11741558608629972\n",
      "Epoch 120, Training Loss 0.1177317992071895\n",
      "Epoch 120, Training Loss 0.11807292573573187\n",
      "Epoch 120, Training Loss 0.11823763863643265\n",
      "Epoch 120, Training Loss 0.11857057901580467\n",
      "Epoch 120, Training Loss 0.11893938199314467\n",
      "Epoch 120, Training Loss 0.11918897941575178\n",
      "Epoch 120, Training Loss 0.1192965187808818\n",
      "Epoch 120, Training Loss 0.11949023690617755\n",
      "Epoch 120, Training Loss 0.11963463909542926\n",
      "Epoch 120, Training Loss 0.12001559459854422\n",
      "Epoch 120, Training Loss 0.12033866259657666\n",
      "Epoch 120, Training Loss 0.12085716187706232\n",
      "Epoch 120, Training Loss 0.12115013920952139\n",
      "Epoch 120, Training Loss 0.12137052063803996\n",
      "Epoch 120, Training Loss 0.12174074330350475\n",
      "Epoch 120, Training Loss 0.12201904594574285\n",
      "Epoch 120, Training Loss 0.12226067048490352\n",
      "Epoch 120, Training Loss 0.12248967579849389\n",
      "Epoch 120, Training Loss 0.12267525213510941\n",
      "Epoch 120, Training Loss 0.12295656957570701\n",
      "Epoch 120, Training Loss 0.12321665604382068\n",
      "Epoch 120, Training Loss 0.1234725955302072\n",
      "Epoch 120, Training Loss 0.12365996766635372\n",
      "Epoch 120, Training Loss 0.12401147302516434\n",
      "Epoch 120, Training Loss 0.12425148831513684\n",
      "Epoch 120, Training Loss 0.12454956066330224\n",
      "Epoch 120, Training Loss 0.12468931239450831\n",
      "Epoch 120, Training Loss 0.12497340951143476\n",
      "Epoch 120, Training Loss 0.1250978267472952\n",
      "Epoch 120, Training Loss 0.12527598314406468\n",
      "Epoch 120, Training Loss 0.12543322265510212\n",
      "Epoch 120, Training Loss 0.12557097776409457\n",
      "Epoch 120, Training Loss 0.1258744352813953\n",
      "Epoch 120, Training Loss 0.12609579963395207\n",
      "Epoch 120, Training Loss 0.12637987430386075\n",
      "Epoch 120, Training Loss 0.12665673563032961\n",
      "Epoch 120, Training Loss 0.1268691384969541\n",
      "Epoch 120, Training Loss 0.12697352852453203\n",
      "Epoch 120, Training Loss 0.1271147706862682\n",
      "Epoch 120, Training Loss 0.12747077360425307\n",
      "Epoch 120, Training Loss 0.12762382253528098\n",
      "Epoch 120, Training Loss 0.12780954613043066\n",
      "Epoch 120, Training Loss 0.12790384233626714\n",
      "Epoch 120, Training Loss 0.12818697348827748\n",
      "Epoch 120, Training Loss 0.12839959651860586\n",
      "Epoch 120, Training Loss 0.12863430497534287\n",
      "Epoch 120, Training Loss 0.12875598668575744\n",
      "Epoch 120, Training Loss 0.12902710654908586\n",
      "Epoch 120, Training Loss 0.12930297786298464\n",
      "Epoch 120, Training Loss 0.12944314822726086\n",
      "Epoch 120, Training Loss 0.12965142840752975\n",
      "Epoch 120, Training Loss 0.13003316204852003\n",
      "Epoch 120, Training Loss 0.13026845386094602\n",
      "Epoch 120, Training Loss 0.13041446517552713\n",
      "Epoch 120, Training Loss 0.13067315407859548\n",
      "Epoch 120, Training Loss 0.13081881947472423\n",
      "Epoch 120, Training Loss 0.13116458243668994\n",
      "Epoch 120, Training Loss 0.13135174477515776\n",
      "Epoch 120, Training Loss 0.13157058795413856\n",
      "Epoch 120, Training Loss 0.13167801214968\n",
      "Epoch 120, Training Loss 0.1320029283490251\n",
      "Epoch 120, Training Loss 0.13217899284761428\n",
      "Epoch 120, Training Loss 0.1324044371266728\n",
      "Epoch 120, Training Loss 0.13262867046724958\n",
      "Epoch 120, Training Loss 0.1329073526127183\n",
      "Epoch 120, Training Loss 0.13317586999396078\n",
      "Epoch 120, Training Loss 0.1333193038459248\n",
      "Epoch 120, Training Loss 0.13357042391186633\n",
      "Epoch 120, Training Loss 0.13377776981600562\n",
      "Epoch 120, Training Loss 0.13402021830172642\n",
      "Epoch 120, Training Loss 0.1343100746221783\n",
      "Epoch 120, Training Loss 0.13446062104419218\n",
      "Epoch 120, Training Loss 0.13461667433133362\n",
      "Epoch 120, Training Loss 0.13482568743150405\n",
      "Epoch 120, Training Loss 0.13502897015865653\n",
      "Epoch 120, Training Loss 0.13522852372730632\n",
      "Epoch 120, Training Loss 0.13546907311529302\n",
      "Epoch 120, Training Loss 0.1357229955141883\n",
      "Epoch 120, Training Loss 0.13595430978366632\n",
      "Epoch 120, Training Loss 0.13613147148028817\n",
      "Epoch 120, Training Loss 0.13630033260130364\n",
      "Epoch 120, Training Loss 0.1365544115386122\n",
      "Epoch 120, Training Loss 0.1368169702465653\n",
      "Epoch 120, Training Loss 0.13697322041196439\n",
      "Epoch 120, Training Loss 0.137319202851647\n",
      "Epoch 120, Training Loss 0.13753925007112952\n",
      "Epoch 120, Training Loss 0.13780564213138255\n",
      "Epoch 120, Training Loss 0.13803780130813342\n",
      "Epoch 120, Training Loss 0.1381507050912932\n",
      "Epoch 120, Training Loss 0.1383304137241124\n",
      "Epoch 120, Training Loss 0.1385658863703232\n",
      "Epoch 120, Training Loss 0.13881010904698574\n",
      "Epoch 120, Training Loss 0.13899031684964971\n",
      "Epoch 120, Training Loss 0.13924634309433154\n",
      "Epoch 120, Training Loss 0.1396511883982231\n",
      "Epoch 120, Training Loss 0.13991721606601382\n",
      "Epoch 120, Training Loss 0.1402870002714798\n",
      "Epoch 120, Training Loss 0.14050667534303635\n",
      "Epoch 120, Training Loss 0.14073888943685442\n",
      "Epoch 120, Training Loss 0.14098195466296295\n",
      "Epoch 120, Training Loss 0.14115922524095953\n",
      "Epoch 120, Training Loss 0.1414484539405083\n",
      "Epoch 120, Training Loss 0.14177282115973322\n",
      "Epoch 120, Training Loss 0.14206745650362024\n",
      "Epoch 120, Training Loss 0.14220666324676912\n",
      "Epoch 120, Training Loss 0.14241667901215804\n",
      "Epoch 120, Training Loss 0.14278270903965243\n",
      "Epoch 120, Training Loss 0.14295889299524867\n",
      "Epoch 120, Training Loss 0.14309569274354011\n",
      "Epoch 120, Training Loss 0.14335275216080495\n",
      "Epoch 120, Training Loss 0.1434623316416274\n",
      "Epoch 120, Training Loss 0.14370634164327703\n",
      "Epoch 120, Training Loss 0.14385424554824372\n",
      "Epoch 120, Training Loss 0.14413004990219308\n",
      "Epoch 120, Training Loss 0.14454217120300017\n",
      "Epoch 120, Training Loss 0.1446977608963428\n",
      "Epoch 120, Training Loss 0.14490650911027056\n",
      "Epoch 120, Training Loss 0.1450452111099306\n",
      "Epoch 120, Training Loss 0.14520057007346465\n",
      "Epoch 120, Training Loss 0.14528097005089377\n",
      "Epoch 120, Training Loss 0.1455163285374413\n",
      "Epoch 120, Training Loss 0.14571841315025716\n",
      "Epoch 120, Training Loss 0.14596493982845712\n",
      "Epoch 120, Training Loss 0.14619977244883395\n",
      "Epoch 120, Training Loss 0.14657580447109306\n",
      "Epoch 120, Training Loss 0.14683665371859622\n",
      "Epoch 120, Training Loss 0.14708202852941382\n",
      "Epoch 120, Training Loss 0.1471618084937258\n",
      "Epoch 120, Training Loss 0.14753816155788232\n",
      "Epoch 120, Training Loss 0.14774127334089535\n",
      "Epoch 120, Training Loss 0.14803132981709813\n",
      "Epoch 120, Training Loss 0.14821501014292088\n",
      "Epoch 120, Training Loss 0.14847633390284865\n",
      "Epoch 120, Training Loss 0.1487050928137339\n",
      "Epoch 120, Training Loss 0.14890764143956287\n",
      "Epoch 120, Training Loss 0.14916170945824564\n",
      "Epoch 120, Training Loss 0.14929333035750766\n",
      "Epoch 120, Training Loss 0.14949160109243126\n",
      "Epoch 120, Training Loss 0.14962727122980615\n",
      "Epoch 120, Training Loss 0.14985300499536192\n",
      "Epoch 120, Training Loss 0.14998761267232164\n",
      "Epoch 120, Training Loss 0.15017553597993558\n",
      "Epoch 120, Training Loss 0.15041303998597746\n",
      "Epoch 120, Training Loss 0.15061929543762256\n",
      "Epoch 120, Training Loss 0.15098011958629579\n",
      "Epoch 120, Training Loss 0.151195600743184\n",
      "Epoch 120, Training Loss 0.15149559465515644\n",
      "Epoch 120, Training Loss 0.15178073258579844\n",
      "Epoch 120, Training Loss 0.15192077246011065\n",
      "Epoch 120, Training Loss 0.15224112208237123\n",
      "Epoch 120, Training Loss 0.15238885654855872\n",
      "Epoch 120, Training Loss 0.15259672723272266\n",
      "Epoch 120, Training Loss 0.15284261998275053\n",
      "Epoch 120, Training Loss 0.15304764793695086\n",
      "Epoch 120, Training Loss 0.15337446481561112\n",
      "Epoch 120, Training Loss 0.15360420017176882\n",
      "Epoch 120, Training Loss 0.1538355066095624\n",
      "Epoch 120, Training Loss 0.15405218675732613\n",
      "Epoch 120, Training Loss 0.1543754177915928\n",
      "Epoch 120, Training Loss 0.15468580817894254\n",
      "Epoch 120, Training Loss 0.1549118204170938\n",
      "Epoch 120, Training Loss 0.1552750998655396\n",
      "Epoch 120, Training Loss 0.1556106103331689\n",
      "Epoch 120, Training Loss 0.15589378889450026\n",
      "Epoch 120, Training Loss 0.15618373033450084\n",
      "Epoch 120, Training Loss 0.15661378245791205\n",
      "Epoch 120, Training Loss 0.156930077001643\n",
      "Epoch 120, Training Loss 0.15715102263538125\n",
      "Epoch 120, Training Loss 0.15733784386683303\n",
      "Epoch 120, Training Loss 0.15780019843021928\n",
      "Epoch 120, Training Loss 0.1580144333202973\n",
      "Epoch 120, Training Loss 0.1582128356694413\n",
      "Epoch 120, Training Loss 0.15842381347437648\n",
      "Epoch 120, Training Loss 0.1585944980630637\n",
      "Epoch 120, Training Loss 0.15883490764309682\n",
      "Epoch 120, Training Loss 0.15912899823711657\n",
      "Epoch 120, Training Loss 0.15927213227466855\n",
      "Epoch 120, Training Loss 0.15976039828051386\n",
      "Epoch 120, Training Loss 0.15999394879126183\n",
      "Epoch 120, Training Loss 0.16031837298551485\n",
      "Epoch 120, Training Loss 0.16058054382500747\n",
      "Epoch 120, Training Loss 0.16089672203678304\n",
      "Epoch 120, Training Loss 0.16109602238096848\n",
      "Epoch 120, Training Loss 0.16144514233445573\n",
      "Epoch 120, Training Loss 0.16201573775133207\n",
      "Epoch 120, Training Loss 0.16228203384963144\n",
      "Epoch 120, Training Loss 0.16264398526543242\n",
      "Epoch 120, Training Loss 0.16293064867863266\n",
      "Epoch 120, Training Loss 0.16329240401649414\n",
      "Epoch 120, Training Loss 0.16363560298786445\n",
      "Epoch 120, Training Loss 0.16379274856632628\n",
      "Epoch 120, Training Loss 0.16394952287340103\n",
      "Epoch 120, Training Loss 0.16456529083649826\n",
      "Epoch 120, Training Loss 0.16472923318329064\n",
      "Epoch 120, Training Loss 0.16506846173835532\n",
      "Epoch 120, Training Loss 0.16528568911316144\n",
      "Epoch 120, Training Loss 0.16563896077406376\n",
      "Epoch 120, Training Loss 0.1658249318199542\n",
      "Epoch 120, Training Loss 0.16615190216914164\n",
      "Epoch 120, Training Loss 0.16629352418662946\n",
      "Epoch 120, Training Loss 0.16656503588189858\n",
      "Epoch 120, Training Loss 0.16674778204592292\n",
      "Epoch 120, Training Loss 0.16688636448377234\n",
      "Epoch 120, Training Loss 0.16717877473367754\n",
      "Epoch 120, Training Loss 0.16737653423681895\n",
      "Epoch 120, Training Loss 0.16753913504083442\n",
      "Epoch 120, Training Loss 0.16765300561781124\n",
      "Epoch 120, Training Loss 0.1679410445301429\n",
      "Epoch 120, Training Loss 0.16829918052458093\n",
      "Epoch 120, Training Loss 0.1685556894182549\n",
      "Epoch 120, Training Loss 0.16877465977159609\n",
      "Epoch 120, Training Loss 0.16911871554067984\n",
      "Epoch 120, Training Loss 0.16949679111809376\n",
      "Epoch 120, Training Loss 0.1696988344192505\n",
      "Epoch 120, Training Loss 0.169972030879439\n",
      "Epoch 120, Training Loss 0.17023665890516831\n",
      "Epoch 120, Training Loss 0.17062229139115803\n",
      "Epoch 120, Training Loss 0.1707869265276148\n",
      "Epoch 120, Training Loss 0.17116279699994474\n",
      "Epoch 120, Training Loss 0.1713476176457027\n",
      "Epoch 120, Training Loss 0.1716715166407168\n",
      "Epoch 120, Training Loss 0.17184357402269798\n",
      "Epoch 120, Training Loss 0.17202999452343376\n",
      "Epoch 120, Training Loss 0.17231115513979017\n",
      "Epoch 120, Training Loss 0.17261902639246962\n",
      "Epoch 120, Training Loss 0.17295630762110586\n",
      "Epoch 120, Training Loss 0.17324573224615258\n",
      "Epoch 120, Training Loss 0.17338092564164526\n",
      "Epoch 120, Training Loss 0.17363663368365345\n",
      "Epoch 120, Training Loss 0.17382684765416948\n",
      "Epoch 120, Training Loss 0.17403217380309044\n",
      "Epoch 120, Training Loss 0.17426143133121988\n",
      "Epoch 120, Training Loss 0.17458890485184272\n",
      "Epoch 120, Training Loss 0.17475819585802\n",
      "Epoch 120, Training Loss 0.1749672875036974\n",
      "Epoch 120, Training Loss 0.1751374248081766\n",
      "Epoch 120, Training Loss 0.17549592121254148\n",
      "Epoch 120, Training Loss 0.1756759319845063\n",
      "Epoch 120, Training Loss 0.1758853355636987\n",
      "Epoch 120, Training Loss 0.1761098751021773\n",
      "Epoch 120, Training Loss 0.176336990921851\n",
      "Epoch 120, Training Loss 0.17664692501354096\n",
      "Epoch 120, Training Loss 0.17682999987965045\n",
      "Epoch 120, Training Loss 0.17716484741710337\n",
      "Epoch 120, Training Loss 0.1774284210046539\n",
      "Epoch 120, Training Loss 0.17771916442057667\n",
      "Epoch 120, Training Loss 0.17799749827522146\n",
      "Epoch 120, Training Loss 0.1781835951997191\n",
      "Epoch 120, Training Loss 0.17848535876749727\n",
      "Epoch 120, Training Loss 0.1786385856549758\n",
      "Epoch 120, Training Loss 0.1788767179679078\n",
      "Epoch 120, Training Loss 0.17918647282644914\n",
      "Epoch 120, Training Loss 0.17949417730807649\n",
      "Epoch 120, Training Loss 0.17969761219094782\n",
      "Epoch 120, Training Loss 0.17996544993060934\n",
      "Epoch 120, Training Loss 0.1803015924971122\n",
      "Epoch 120, Training Loss 0.18056369279427906\n",
      "Epoch 120, Training Loss 0.18076546574035263\n",
      "Epoch 120, Training Loss 0.18095710324814251\n",
      "Epoch 120, Training Loss 0.18114719659928472\n",
      "Epoch 120, Training Loss 0.18140829545076545\n",
      "Epoch 130, Training Loss 0.00017300998920674824\n",
      "Epoch 130, Training Loss 0.0002955550713764737\n",
      "Epoch 130, Training Loss 0.00041195450117216087\n",
      "Epoch 130, Training Loss 0.0006429892690742717\n",
      "Epoch 130, Training Loss 0.000778091618853152\n",
      "Epoch 130, Training Loss 0.0008707313662599725\n",
      "Epoch 130, Training Loss 0.0010488941274640505\n",
      "Epoch 130, Training Loss 0.0013274259465124905\n",
      "Epoch 130, Training Loss 0.0015132291542599573\n",
      "Epoch 130, Training Loss 0.0017823012512358254\n",
      "Epoch 130, Training Loss 0.001965356395220208\n",
      "Epoch 130, Training Loss 0.00209351813854159\n",
      "Epoch 130, Training Loss 0.0022563471284973652\n",
      "Epoch 130, Training Loss 0.0024111243369786635\n",
      "Epoch 130, Training Loss 0.0025726336595195028\n",
      "Epoch 130, Training Loss 0.002760383488653261\n",
      "Epoch 130, Training Loss 0.002869431034225942\n",
      "Epoch 130, Training Loss 0.0031052716933857756\n",
      "Epoch 130, Training Loss 0.0032677942377222166\n",
      "Epoch 130, Training Loss 0.003592624726807675\n",
      "Epoch 130, Training Loss 0.0037481024994722107\n",
      "Epoch 130, Training Loss 0.003916227137264998\n",
      "Epoch 130, Training Loss 0.004077863557945432\n",
      "Epoch 130, Training Loss 0.004182035079621293\n",
      "Epoch 130, Training Loss 0.004302517189394178\n",
      "Epoch 130, Training Loss 0.004507535882770558\n",
      "Epoch 130, Training Loss 0.004786902738501654\n",
      "Epoch 130, Training Loss 0.004889260301047274\n",
      "Epoch 130, Training Loss 0.00497639872838774\n",
      "Epoch 130, Training Loss 0.00516808722787501\n",
      "Epoch 130, Training Loss 0.005367011555930233\n",
      "Epoch 130, Training Loss 0.005691801990999285\n",
      "Epoch 130, Training Loss 0.005903629943385454\n",
      "Epoch 130, Training Loss 0.006218022988427935\n",
      "Epoch 130, Training Loss 0.0062839030752630184\n",
      "Epoch 130, Training Loss 0.0064755701376577775\n",
      "Epoch 130, Training Loss 0.0068188930058951875\n",
      "Epoch 130, Training Loss 0.006979047885293241\n",
      "Epoch 130, Training Loss 0.007184230777270654\n",
      "Epoch 130, Training Loss 0.007275272032145954\n",
      "Epoch 130, Training Loss 0.007388574154594975\n",
      "Epoch 130, Training Loss 0.007552450132148955\n",
      "Epoch 130, Training Loss 0.007923833220777913\n",
      "Epoch 130, Training Loss 0.008182978781554705\n",
      "Epoch 130, Training Loss 0.008604776728755373\n",
      "Epoch 130, Training Loss 0.008847622724864489\n",
      "Epoch 130, Training Loss 0.009027688814055584\n",
      "Epoch 130, Training Loss 0.009216765361025815\n",
      "Epoch 130, Training Loss 0.009426318904589814\n",
      "Epoch 130, Training Loss 0.009632063448390998\n",
      "Epoch 130, Training Loss 0.009804868441827767\n",
      "Epoch 130, Training Loss 0.009976645274197353\n",
      "Epoch 130, Training Loss 0.010155185937995801\n",
      "Epoch 130, Training Loss 0.010326932453552781\n",
      "Epoch 130, Training Loss 0.010402835943662297\n",
      "Epoch 130, Training Loss 0.01050267548626646\n",
      "Epoch 130, Training Loss 0.010649814934986632\n",
      "Epoch 130, Training Loss 0.010782777891515771\n",
      "Epoch 130, Training Loss 0.010967277914590543\n",
      "Epoch 130, Training Loss 0.011087157880253804\n",
      "Epoch 130, Training Loss 0.011256952496135937\n",
      "Epoch 130, Training Loss 0.011409564558273692\n",
      "Epoch 130, Training Loss 0.011621048545364833\n",
      "Epoch 130, Training Loss 0.011897535570670881\n",
      "Epoch 130, Training Loss 0.012074272507977912\n",
      "Epoch 130, Training Loss 0.012243106170459782\n",
      "Epoch 130, Training Loss 0.012362230626289801\n",
      "Epoch 130, Training Loss 0.012509708333274593\n",
      "Epoch 130, Training Loss 0.012730981983111032\n",
      "Epoch 130, Training Loss 0.012904873808555286\n",
      "Epoch 130, Training Loss 0.013096012794376943\n",
      "Epoch 130, Training Loss 0.01335144655593216\n",
      "Epoch 130, Training Loss 0.01352980651933214\n",
      "Epoch 130, Training Loss 0.013769064155762152\n",
      "Epoch 130, Training Loss 0.014150680874086097\n",
      "Epoch 130, Training Loss 0.014421831930766021\n",
      "Epoch 130, Training Loss 0.014545157570820635\n",
      "Epoch 130, Training Loss 0.014747740819935908\n",
      "Epoch 130, Training Loss 0.014845431575080012\n",
      "Epoch 130, Training Loss 0.015117199524589207\n",
      "Epoch 130, Training Loss 0.015313234704229838\n",
      "Epoch 130, Training Loss 0.015432053190820357\n",
      "Epoch 130, Training Loss 0.015606176174815049\n",
      "Epoch 130, Training Loss 0.015805046710059466\n",
      "Epoch 130, Training Loss 0.015938391265890483\n",
      "Epoch 130, Training Loss 0.01607845931330605\n",
      "Epoch 130, Training Loss 0.01620593606053716\n",
      "Epoch 130, Training Loss 0.016300861249723093\n",
      "Epoch 130, Training Loss 0.016469172342582737\n",
      "Epoch 130, Training Loss 0.01670051184113678\n",
      "Epoch 130, Training Loss 0.016827592678615808\n",
      "Epoch 130, Training Loss 0.017039008676777104\n",
      "Epoch 130, Training Loss 0.01720387451445965\n",
      "Epoch 130, Training Loss 0.017324106019857288\n",
      "Epoch 130, Training Loss 0.017555446718888514\n",
      "Epoch 130, Training Loss 0.017809064215635096\n",
      "Epoch 130, Training Loss 0.01808938785167911\n",
      "Epoch 130, Training Loss 0.01839356472158371\n",
      "Epoch 130, Training Loss 0.01852406245058455\n",
      "Epoch 130, Training Loss 0.01860571989927755\n",
      "Epoch 130, Training Loss 0.018746285001411463\n",
      "Epoch 130, Training Loss 0.018879856647509137\n",
      "Epoch 130, Training Loss 0.019083319617735454\n",
      "Epoch 130, Training Loss 0.019173926071209067\n",
      "Epoch 130, Training Loss 0.01936292138589008\n",
      "Epoch 130, Training Loss 0.019459039373013673\n",
      "Epoch 130, Training Loss 0.01966690361652228\n",
      "Epoch 130, Training Loss 0.019927193555990447\n",
      "Epoch 130, Training Loss 0.020080736797788868\n",
      "Epoch 130, Training Loss 0.020216296862839434\n",
      "Epoch 130, Training Loss 0.02045896231099163\n",
      "Epoch 130, Training Loss 0.020758740213292333\n",
      "Epoch 130, Training Loss 0.020877517388223688\n",
      "Epoch 130, Training Loss 0.021036095631396984\n",
      "Epoch 130, Training Loss 0.021224001903668084\n",
      "Epoch 130, Training Loss 0.021394769820715764\n",
      "Epoch 130, Training Loss 0.021612362921847712\n",
      "Epoch 130, Training Loss 0.021938750410781187\n",
      "Epoch 130, Training Loss 0.02221808788340415\n",
      "Epoch 130, Training Loss 0.022521609426154506\n",
      "Epoch 130, Training Loss 0.02263323230016262\n",
      "Epoch 130, Training Loss 0.022854319895091263\n",
      "Epoch 130, Training Loss 0.023190880189542577\n",
      "Epoch 130, Training Loss 0.02346103736544814\n",
      "Epoch 130, Training Loss 0.02361038129995851\n",
      "Epoch 130, Training Loss 0.023803253622387378\n",
      "Epoch 130, Training Loss 0.02400705514623381\n",
      "Epoch 130, Training Loss 0.024164116472158284\n",
      "Epoch 130, Training Loss 0.02440354051759176\n",
      "Epoch 130, Training Loss 0.024520862590321494\n",
      "Epoch 130, Training Loss 0.024756877337727707\n",
      "Epoch 130, Training Loss 0.02486539736885549\n",
      "Epoch 130, Training Loss 0.025037547835456135\n",
      "Epoch 130, Training Loss 0.025415029996039007\n",
      "Epoch 130, Training Loss 0.02558689739774255\n",
      "Epoch 130, Training Loss 0.025694303709985045\n",
      "Epoch 130, Training Loss 0.025822381505652157\n",
      "Epoch 130, Training Loss 0.026161857778230286\n",
      "Epoch 130, Training Loss 0.026413960596713262\n",
      "Epoch 130, Training Loss 0.026528750072278635\n",
      "Epoch 130, Training Loss 0.02675262020181512\n",
      "Epoch 130, Training Loss 0.026805684146711892\n",
      "Epoch 130, Training Loss 0.027012343835228544\n",
      "Epoch 130, Training Loss 0.027214268155758035\n",
      "Epoch 130, Training Loss 0.027492883779546794\n",
      "Epoch 130, Training Loss 0.02774884742791848\n",
      "Epoch 130, Training Loss 0.027889314595886203\n",
      "Epoch 130, Training Loss 0.028076036073400847\n",
      "Epoch 130, Training Loss 0.02821954346888358\n",
      "Epoch 130, Training Loss 0.028384958705900576\n",
      "Epoch 130, Training Loss 0.028510883464799512\n",
      "Epoch 130, Training Loss 0.028657270976535195\n",
      "Epoch 130, Training Loss 0.02887642626052775\n",
      "Epoch 130, Training Loss 0.02920779256659853\n",
      "Epoch 130, Training Loss 0.02937230214838634\n",
      "Epoch 130, Training Loss 0.029659204094496835\n",
      "Epoch 130, Training Loss 0.0298458067247706\n",
      "Epoch 130, Training Loss 0.030110680086114217\n",
      "Epoch 130, Training Loss 0.03024599182388514\n",
      "Epoch 130, Training Loss 0.030468516999765126\n",
      "Epoch 130, Training Loss 0.03058591976171107\n",
      "Epoch 130, Training Loss 0.030740319200984353\n",
      "Epoch 130, Training Loss 0.03089688775960899\n",
      "Epoch 130, Training Loss 0.031021675733311098\n",
      "Epoch 130, Training Loss 0.031221138503964598\n",
      "Epoch 130, Training Loss 0.03138953746032075\n",
      "Epoch 130, Training Loss 0.03160652704536915\n",
      "Epoch 130, Training Loss 0.03181455310557962\n",
      "Epoch 130, Training Loss 0.032149201783034806\n",
      "Epoch 130, Training Loss 0.0324274695542691\n",
      "Epoch 130, Training Loss 0.0326172379953096\n",
      "Epoch 130, Training Loss 0.03280135196016725\n",
      "Epoch 130, Training Loss 0.033038517603140964\n",
      "Epoch 130, Training Loss 0.033274775628200574\n",
      "Epoch 130, Training Loss 0.0334461828469849\n",
      "Epoch 130, Training Loss 0.03381458086335598\n",
      "Epoch 130, Training Loss 0.03385653372501473\n",
      "Epoch 130, Training Loss 0.03401249065957106\n",
      "Epoch 130, Training Loss 0.034106431524162095\n",
      "Epoch 130, Training Loss 0.034254846984849256\n",
      "Epoch 130, Training Loss 0.034410037814884845\n",
      "Epoch 130, Training Loss 0.0346087699903704\n",
      "Epoch 130, Training Loss 0.034815545362965834\n",
      "Epoch 130, Training Loss 0.035072521299428644\n",
      "Epoch 130, Training Loss 0.03524817700695504\n",
      "Epoch 130, Training Loss 0.03533047300470454\n",
      "Epoch 130, Training Loss 0.03555624379450098\n",
      "Epoch 130, Training Loss 0.035680195492932866\n",
      "Epoch 130, Training Loss 0.035786723589424585\n",
      "Epoch 130, Training Loss 0.03593459098464083\n",
      "Epoch 130, Training Loss 0.03614412459647259\n",
      "Epoch 130, Training Loss 0.03620414011409063\n",
      "Epoch 130, Training Loss 0.03640607691577176\n",
      "Epoch 130, Training Loss 0.036549161495569416\n",
      "Epoch 130, Training Loss 0.03682216664161676\n",
      "Epoch 130, Training Loss 0.03694608347857242\n",
      "Epoch 130, Training Loss 0.037088289137577155\n",
      "Epoch 130, Training Loss 0.03724069781410877\n",
      "Epoch 130, Training Loss 0.03739168385848822\n",
      "Epoch 130, Training Loss 0.03761123680531064\n",
      "Epoch 130, Training Loss 0.03773776319382898\n",
      "Epoch 130, Training Loss 0.03798562642234518\n",
      "Epoch 130, Training Loss 0.03812243606980957\n",
      "Epoch 130, Training Loss 0.03820634116430569\n",
      "Epoch 130, Training Loss 0.03829425749133158\n",
      "Epoch 130, Training Loss 0.038444184610034196\n",
      "Epoch 130, Training Loss 0.03868863356711767\n",
      "Epoch 130, Training Loss 0.03892977096026053\n",
      "Epoch 130, Training Loss 0.03939044355984082\n",
      "Epoch 130, Training Loss 0.03949504066973239\n",
      "Epoch 130, Training Loss 0.0397236771700556\n",
      "Epoch 130, Training Loss 0.039947097959077874\n",
      "Epoch 130, Training Loss 0.040071688949718805\n",
      "Epoch 130, Training Loss 0.04029798027023178\n",
      "Epoch 130, Training Loss 0.04042544217346727\n",
      "Epoch 130, Training Loss 0.04056450783196465\n",
      "Epoch 130, Training Loss 0.040903746143288316\n",
      "Epoch 130, Training Loss 0.0411331893071113\n",
      "Epoch 130, Training Loss 0.04122956419635154\n",
      "Epoch 130, Training Loss 0.04149167276347232\n",
      "Epoch 130, Training Loss 0.04162388493585617\n",
      "Epoch 130, Training Loss 0.042046112806329035\n",
      "Epoch 130, Training Loss 0.042258086149840404\n",
      "Epoch 130, Training Loss 0.042430298812592124\n",
      "Epoch 130, Training Loss 0.04256183302977963\n",
      "Epoch 130, Training Loss 0.04279534520147859\n",
      "Epoch 130, Training Loss 0.042995849643331356\n",
      "Epoch 130, Training Loss 0.04309688505175931\n",
      "Epoch 130, Training Loss 0.0432991768707476\n",
      "Epoch 130, Training Loss 0.04344369563967218\n",
      "Epoch 130, Training Loss 0.043642747618467606\n",
      "Epoch 130, Training Loss 0.04390889725855092\n",
      "Epoch 130, Training Loss 0.04401731307682631\n",
      "Epoch 130, Training Loss 0.04416640496829434\n",
      "Epoch 130, Training Loss 0.04441878408708078\n",
      "Epoch 130, Training Loss 0.04467958274304562\n",
      "Epoch 130, Training Loss 0.044872731156170825\n",
      "Epoch 130, Training Loss 0.04516194451628896\n",
      "Epoch 130, Training Loss 0.045364671555893195\n",
      "Epoch 130, Training Loss 0.0454896172494306\n",
      "Epoch 130, Training Loss 0.04566502099013542\n",
      "Epoch 130, Training Loss 0.045801603242450056\n",
      "Epoch 130, Training Loss 0.045936760909455206\n",
      "Epoch 130, Training Loss 0.04610740565373312\n",
      "Epoch 130, Training Loss 0.04630553919126463\n",
      "Epoch 130, Training Loss 0.046412712086916275\n",
      "Epoch 130, Training Loss 0.04661565959510748\n",
      "Epoch 130, Training Loss 0.04673637277768244\n",
      "Epoch 130, Training Loss 0.04695569990140855\n",
      "Epoch 130, Training Loss 0.04731507288277759\n",
      "Epoch 130, Training Loss 0.04763658742522797\n",
      "Epoch 130, Training Loss 0.04786917157090076\n",
      "Epoch 130, Training Loss 0.04802337352215024\n",
      "Epoch 130, Training Loss 0.048197977859383956\n",
      "Epoch 130, Training Loss 0.048391578528467954\n",
      "Epoch 130, Training Loss 0.04857125295721509\n",
      "Epoch 130, Training Loss 0.04878178742402198\n",
      "Epoch 130, Training Loss 0.04904781861702347\n",
      "Epoch 130, Training Loss 0.04930515269584516\n",
      "Epoch 130, Training Loss 0.049538455584355635\n",
      "Epoch 130, Training Loss 0.049927013880951936\n",
      "Epoch 130, Training Loss 0.05007511090077555\n",
      "Epoch 130, Training Loss 0.0502624099412004\n",
      "Epoch 130, Training Loss 0.05041951359823689\n",
      "Epoch 130, Training Loss 0.05073000701701702\n",
      "Epoch 130, Training Loss 0.050894776854635504\n",
      "Epoch 130, Training Loss 0.051103002270278725\n",
      "Epoch 130, Training Loss 0.05131432312585966\n",
      "Epoch 130, Training Loss 0.05150352154984651\n",
      "Epoch 130, Training Loss 0.051702249647520696\n",
      "Epoch 130, Training Loss 0.052027099877786455\n",
      "Epoch 130, Training Loss 0.05235353416627478\n",
      "Epoch 130, Training Loss 0.05256754715862634\n",
      "Epoch 130, Training Loss 0.05272934472907687\n",
      "Epoch 130, Training Loss 0.05285140380854039\n",
      "Epoch 130, Training Loss 0.05305369694233703\n",
      "Epoch 130, Training Loss 0.05350493436293376\n",
      "Epoch 130, Training Loss 0.053836328832580306\n",
      "Epoch 130, Training Loss 0.05409740420806286\n",
      "Epoch 130, Training Loss 0.054283642846033396\n",
      "Epoch 130, Training Loss 0.054391881965500925\n",
      "Epoch 130, Training Loss 0.05458668839005406\n",
      "Epoch 130, Training Loss 0.05473166241613038\n",
      "Epoch 130, Training Loss 0.054944885913711376\n",
      "Epoch 130, Training Loss 0.055196830269206514\n",
      "Epoch 130, Training Loss 0.05552881758402833\n",
      "Epoch 130, Training Loss 0.05571748648324738\n",
      "Epoch 130, Training Loss 0.05597232961002976\n",
      "Epoch 130, Training Loss 0.056255723056776445\n",
      "Epoch 130, Training Loss 0.056475957950858205\n",
      "Epoch 130, Training Loss 0.05668099017342186\n",
      "Epoch 130, Training Loss 0.05695350954542532\n",
      "Epoch 130, Training Loss 0.05710052729815321\n",
      "Epoch 130, Training Loss 0.05728005782208022\n",
      "Epoch 130, Training Loss 0.05739939455276408\n",
      "Epoch 130, Training Loss 0.057577575035297965\n",
      "Epoch 130, Training Loss 0.05776830288626806\n",
      "Epoch 130, Training Loss 0.05799320289183913\n",
      "Epoch 130, Training Loss 0.05814970343295113\n",
      "Epoch 130, Training Loss 0.05831135408786099\n",
      "Epoch 130, Training Loss 0.0584044183468651\n",
      "Epoch 130, Training Loss 0.058630189708317335\n",
      "Epoch 130, Training Loss 0.05876455927158103\n",
      "Epoch 130, Training Loss 0.059025184849224734\n",
      "Epoch 130, Training Loss 0.05916411399631701\n",
      "Epoch 130, Training Loss 0.05931221868585595\n",
      "Epoch 130, Training Loss 0.05954951401847555\n",
      "Epoch 130, Training Loss 0.05967249178692051\n",
      "Epoch 130, Training Loss 0.05979508234430914\n",
      "Epoch 130, Training Loss 0.05999469757556458\n",
      "Epoch 130, Training Loss 0.060147781968307314\n",
      "Epoch 130, Training Loss 0.06023845978347999\n",
      "Epoch 130, Training Loss 0.060346072025196935\n",
      "Epoch 130, Training Loss 0.06059713601170446\n",
      "Epoch 130, Training Loss 0.06069527808910288\n",
      "Epoch 130, Training Loss 0.060845636293444486\n",
      "Epoch 130, Training Loss 0.06099087645387863\n",
      "Epoch 130, Training Loss 0.061167942417209104\n",
      "Epoch 130, Training Loss 0.061421211546911\n",
      "Epoch 130, Training Loss 0.06168049070364831\n",
      "Epoch 130, Training Loss 0.06189893078906914\n",
      "Epoch 130, Training Loss 0.06209880343693144\n",
      "Epoch 130, Training Loss 0.06230461404508794\n",
      "Epoch 130, Training Loss 0.0624238897991531\n",
      "Epoch 130, Training Loss 0.06283819074730587\n",
      "Epoch 130, Training Loss 0.0631062208110338\n",
      "Epoch 130, Training Loss 0.06345218305221147\n",
      "Epoch 130, Training Loss 0.06367957383832511\n",
      "Epoch 130, Training Loss 0.06384428195617235\n",
      "Epoch 130, Training Loss 0.06408317556218876\n",
      "Epoch 130, Training Loss 0.06427440012488371\n",
      "Epoch 130, Training Loss 0.06454476296349103\n",
      "Epoch 130, Training Loss 0.06472620821041067\n",
      "Epoch 130, Training Loss 0.064804371117669\n",
      "Epoch 130, Training Loss 0.06504344997827506\n",
      "Epoch 130, Training Loss 0.06535193991497197\n",
      "Epoch 130, Training Loss 0.06556311558903483\n",
      "Epoch 130, Training Loss 0.06563980568228933\n",
      "Epoch 130, Training Loss 0.06584241103542887\n",
      "Epoch 130, Training Loss 0.06602584631146526\n",
      "Epoch 130, Training Loss 0.06616498811451522\n",
      "Epoch 130, Training Loss 0.06630062720145258\n",
      "Epoch 130, Training Loss 0.06652582260063085\n",
      "Epoch 130, Training Loss 0.06679615733282798\n",
      "Epoch 130, Training Loss 0.06698093668598196\n",
      "Epoch 130, Training Loss 0.0671475439015633\n",
      "Epoch 130, Training Loss 0.06730700265663817\n",
      "Epoch 130, Training Loss 0.06766071453061707\n",
      "Epoch 130, Training Loss 0.06794641478001462\n",
      "Epoch 130, Training Loss 0.06818166225100569\n",
      "Epoch 130, Training Loss 0.06831317775599334\n",
      "Epoch 130, Training Loss 0.0685615569038693\n",
      "Epoch 130, Training Loss 0.06876281051970351\n",
      "Epoch 130, Training Loss 0.0689672968300331\n",
      "Epoch 130, Training Loss 0.0690845251226288\n",
      "Epoch 130, Training Loss 0.06920330294543672\n",
      "Epoch 130, Training Loss 0.06932317411712825\n",
      "Epoch 130, Training Loss 0.069466012856349\n",
      "Epoch 130, Training Loss 0.06966626323530893\n",
      "Epoch 130, Training Loss 0.06979534698321539\n",
      "Epoch 130, Training Loss 0.07006051916810101\n",
      "Epoch 130, Training Loss 0.07022194388558341\n",
      "Epoch 130, Training Loss 0.07048800072687514\n",
      "Epoch 130, Training Loss 0.07066096561243924\n",
      "Epoch 130, Training Loss 0.0708892977060488\n",
      "Epoch 130, Training Loss 0.071011355470704\n",
      "Epoch 130, Training Loss 0.07131811946420871\n",
      "Epoch 130, Training Loss 0.07143735012415882\n",
      "Epoch 130, Training Loss 0.07156547913065804\n",
      "Epoch 130, Training Loss 0.07176760297811702\n",
      "Epoch 130, Training Loss 0.07197858886245419\n",
      "Epoch 130, Training Loss 0.07226287212956439\n",
      "Epoch 130, Training Loss 0.07260317943723457\n",
      "Epoch 130, Training Loss 0.07288097881276132\n",
      "Epoch 130, Training Loss 0.07309118147148654\n",
      "Epoch 130, Training Loss 0.07321287195681764\n",
      "Epoch 130, Training Loss 0.07336480396768778\n",
      "Epoch 130, Training Loss 0.07357424836310432\n",
      "Epoch 130, Training Loss 0.0738033120236967\n",
      "Epoch 130, Training Loss 0.07397962492578627\n",
      "Epoch 130, Training Loss 0.07415688100278073\n",
      "Epoch 130, Training Loss 0.07436984674076137\n",
      "Epoch 130, Training Loss 0.07461877425899134\n",
      "Epoch 130, Training Loss 0.07491090550275563\n",
      "Epoch 130, Training Loss 0.07512851487702268\n",
      "Epoch 130, Training Loss 0.07530617017937286\n",
      "Epoch 130, Training Loss 0.07541801610394665\n",
      "Epoch 130, Training Loss 0.07558990698164839\n",
      "Epoch 130, Training Loss 0.07593556411107026\n",
      "Epoch 130, Training Loss 0.0761513133745288\n",
      "Epoch 130, Training Loss 0.07632169290862577\n",
      "Epoch 130, Training Loss 0.07651447471888627\n",
      "Epoch 130, Training Loss 0.07665001332778912\n",
      "Epoch 130, Training Loss 0.0767761494945306\n",
      "Epoch 130, Training Loss 0.07718433357317887\n",
      "Epoch 130, Training Loss 0.077617968911367\n",
      "Epoch 130, Training Loss 0.07774718811311533\n",
      "Epoch 130, Training Loss 0.07789855085008437\n",
      "Epoch 130, Training Loss 0.0780800384948092\n",
      "Epoch 130, Training Loss 0.07823643912001492\n",
      "Epoch 130, Training Loss 0.07836898012787027\n",
      "Epoch 130, Training Loss 0.07856568246794021\n",
      "Epoch 130, Training Loss 0.07867496576436493\n",
      "Epoch 130, Training Loss 0.0789451260653222\n",
      "Epoch 130, Training Loss 0.07911597483355523\n",
      "Epoch 130, Training Loss 0.07931529629089491\n",
      "Epoch 130, Training Loss 0.07948648629953033\n",
      "Epoch 130, Training Loss 0.07973301534057425\n",
      "Epoch 130, Training Loss 0.07994442106798634\n",
      "Epoch 130, Training Loss 0.08035352013890854\n",
      "Epoch 130, Training Loss 0.08061069672655724\n",
      "Epoch 130, Training Loss 0.0808081964311926\n",
      "Epoch 130, Training Loss 0.08104793987977688\n",
      "Epoch 130, Training Loss 0.08129847984846748\n",
      "Epoch 130, Training Loss 0.0814585375082691\n",
      "Epoch 130, Training Loss 0.08165788097435708\n",
      "Epoch 130, Training Loss 0.08179181206809438\n",
      "Epoch 130, Training Loss 0.08215775120231654\n",
      "Epoch 130, Training Loss 0.08231457216603219\n",
      "Epoch 130, Training Loss 0.08268785376168425\n",
      "Epoch 130, Training Loss 0.08282645286330025\n",
      "Epoch 130, Training Loss 0.08300160509450814\n",
      "Epoch 130, Training Loss 0.08316922758984596\n",
      "Epoch 130, Training Loss 0.08348832543834549\n",
      "Epoch 130, Training Loss 0.08365459823528366\n",
      "Epoch 130, Training Loss 0.0838567313626218\n",
      "Epoch 130, Training Loss 0.08410601467942186\n",
      "Epoch 130, Training Loss 0.08436832422166682\n",
      "Epoch 130, Training Loss 0.08454658475506793\n",
      "Epoch 130, Training Loss 0.08479432648767138\n",
      "Epoch 130, Training Loss 0.08490375557538037\n",
      "Epoch 130, Training Loss 0.08505444152904745\n",
      "Epoch 130, Training Loss 0.08533161610383969\n",
      "Epoch 130, Training Loss 0.08554379722517927\n",
      "Epoch 130, Training Loss 0.08592135766449639\n",
      "Epoch 130, Training Loss 0.08608262382371498\n",
      "Epoch 130, Training Loss 0.08631181917474855\n",
      "Epoch 130, Training Loss 0.08658504496087961\n",
      "Epoch 130, Training Loss 0.08678848887591259\n",
      "Epoch 130, Training Loss 0.08693372410105164\n",
      "Epoch 130, Training Loss 0.08726273449923834\n",
      "Epoch 130, Training Loss 0.08753493000917575\n",
      "Epoch 130, Training Loss 0.08779021837960577\n",
      "Epoch 130, Training Loss 0.08796921528189841\n",
      "Epoch 130, Training Loss 0.08812390172096622\n",
      "Epoch 130, Training Loss 0.08844118393347848\n",
      "Epoch 130, Training Loss 0.08866853352226413\n",
      "Epoch 130, Training Loss 0.08888976420263958\n",
      "Epoch 130, Training Loss 0.0891506018264748\n",
      "Epoch 130, Training Loss 0.0893641741865355\n",
      "Epoch 130, Training Loss 0.08958463457977528\n",
      "Epoch 130, Training Loss 0.08979389006200501\n",
      "Epoch 130, Training Loss 0.09027551129326948\n",
      "Epoch 130, Training Loss 0.09042416766876607\n",
      "Epoch 130, Training Loss 0.09067409857631187\n",
      "Epoch 130, Training Loss 0.09088941736389762\n",
      "Epoch 130, Training Loss 0.09105265927989312\n",
      "Epoch 130, Training Loss 0.09128750986931726\n",
      "Epoch 130, Training Loss 0.09155536999883097\n",
      "Epoch 130, Training Loss 0.09174403600166063\n",
      "Epoch 130, Training Loss 0.09183209318467571\n",
      "Epoch 130, Training Loss 0.09203146633875492\n",
      "Epoch 130, Training Loss 0.09224039643450314\n",
      "Epoch 130, Training Loss 0.09238761742039563\n",
      "Epoch 130, Training Loss 0.0925961892594538\n",
      "Epoch 130, Training Loss 0.09279453916870573\n",
      "Epoch 130, Training Loss 0.09302362834420198\n",
      "Epoch 130, Training Loss 0.09322255227209815\n",
      "Epoch 130, Training Loss 0.0933923545672232\n",
      "Epoch 130, Training Loss 0.09367375264939902\n",
      "Epoch 130, Training Loss 0.09382143307505819\n",
      "Epoch 130, Training Loss 0.09400218871453077\n",
      "Epoch 130, Training Loss 0.09425285614340964\n",
      "Epoch 130, Training Loss 0.09451296341979443\n",
      "Epoch 130, Training Loss 0.09486176846715648\n",
      "Epoch 130, Training Loss 0.09510754185545323\n",
      "Epoch 130, Training Loss 0.09533168866639705\n",
      "Epoch 130, Training Loss 0.09548747212247317\n",
      "Epoch 130, Training Loss 0.09573230898612753\n",
      "Epoch 130, Training Loss 0.09590039564691999\n",
      "Epoch 130, Training Loss 0.0959399254311381\n",
      "Epoch 130, Training Loss 0.0960877540490359\n",
      "Epoch 130, Training Loss 0.09631497661590271\n",
      "Epoch 130, Training Loss 0.09641545287826482\n",
      "Epoch 130, Training Loss 0.09652139326495587\n",
      "Epoch 130, Training Loss 0.09669157795970092\n",
      "Epoch 130, Training Loss 0.09681123638015879\n",
      "Epoch 130, Training Loss 0.09693945018226838\n",
      "Epoch 130, Training Loss 0.09716310260622092\n",
      "Epoch 130, Training Loss 0.09757067024936457\n",
      "Epoch 130, Training Loss 0.09782772101557163\n",
      "Epoch 130, Training Loss 0.09797960387357056\n",
      "Epoch 130, Training Loss 0.09814835672297746\n",
      "Epoch 130, Training Loss 0.09832203825530798\n",
      "Epoch 130, Training Loss 0.09851495007915265\n",
      "Epoch 130, Training Loss 0.09866890535139672\n",
      "Epoch 130, Training Loss 0.09879493638110892\n",
      "Epoch 130, Training Loss 0.09890988503423188\n",
      "Epoch 130, Training Loss 0.09906806116518767\n",
      "Epoch 130, Training Loss 0.09921156330143704\n",
      "Epoch 130, Training Loss 0.09947352868783504\n",
      "Epoch 130, Training Loss 0.09975274733227232\n",
      "Epoch 130, Training Loss 0.09991729233766455\n",
      "Epoch 130, Training Loss 0.10005653553339831\n",
      "Epoch 130, Training Loss 0.10037951195217154\n",
      "Epoch 130, Training Loss 0.10070317355758698\n",
      "Epoch 130, Training Loss 0.10090300735191006\n",
      "Epoch 130, Training Loss 0.10109719345369912\n",
      "Epoch 130, Training Loss 0.10124397636069667\n",
      "Epoch 130, Training Loss 0.1014171964524652\n",
      "Epoch 130, Training Loss 0.10160045095188233\n",
      "Epoch 130, Training Loss 0.10187646638020835\n",
      "Epoch 130, Training Loss 0.10218249450025656\n",
      "Epoch 130, Training Loss 0.10242434711102634\n",
      "Epoch 130, Training Loss 0.10261248288404606\n",
      "Epoch 130, Training Loss 0.10283845097131436\n",
      "Epoch 130, Training Loss 0.10307593228262099\n",
      "Epoch 130, Training Loss 0.10340082146170194\n",
      "Epoch 130, Training Loss 0.10375282480893537\n",
      "Epoch 130, Training Loss 0.10405497910345302\n",
      "Epoch 130, Training Loss 0.10431092962279649\n",
      "Epoch 130, Training Loss 0.10445048479015565\n",
      "Epoch 130, Training Loss 0.1046720692492507\n",
      "Epoch 130, Training Loss 0.10493918241518538\n",
      "Epoch 130, Training Loss 0.10514587640305005\n",
      "Epoch 130, Training Loss 0.10526391794271481\n",
      "Epoch 130, Training Loss 0.10542267499982244\n",
      "Epoch 130, Training Loss 0.10569024798662766\n",
      "Epoch 130, Training Loss 0.10585488975429169\n",
      "Epoch 130, Training Loss 0.10611092226813211\n",
      "Epoch 130, Training Loss 0.10631176610203351\n",
      "Epoch 130, Training Loss 0.10642927937457324\n",
      "Epoch 130, Training Loss 0.10657672978499357\n",
      "Epoch 130, Training Loss 0.10691273408701353\n",
      "Epoch 130, Training Loss 0.10702056178580159\n",
      "Epoch 130, Training Loss 0.10712902732860402\n",
      "Epoch 130, Training Loss 0.10739596088028625\n",
      "Epoch 130, Training Loss 0.1074898988770707\n",
      "Epoch 130, Training Loss 0.10767216805149527\n",
      "Epoch 130, Training Loss 0.1079797789340129\n",
      "Epoch 130, Training Loss 0.10825799893387748\n",
      "Epoch 130, Training Loss 0.10845606865556649\n",
      "Epoch 130, Training Loss 0.10876289610286503\n",
      "Epoch 130, Training Loss 0.10895044642412449\n",
      "Epoch 130, Training Loss 0.1092810836403876\n",
      "Epoch 130, Training Loss 0.10945649469828667\n",
      "Epoch 130, Training Loss 0.10959404332520407\n",
      "Epoch 130, Training Loss 0.10978312471219341\n",
      "Epoch 130, Training Loss 0.10991769742287333\n",
      "Epoch 130, Training Loss 0.11024370704732282\n",
      "Epoch 130, Training Loss 0.11044215288041803\n",
      "Epoch 130, Training Loss 0.11077135771779759\n",
      "Epoch 130, Training Loss 0.11095274593252355\n",
      "Epoch 130, Training Loss 0.11124812827809998\n",
      "Epoch 130, Training Loss 0.11145691258256393\n",
      "Epoch 130, Training Loss 0.11165352814528338\n",
      "Epoch 130, Training Loss 0.11183880461032128\n",
      "Epoch 130, Training Loss 0.11212438860398424\n",
      "Epoch 130, Training Loss 0.11235068021032511\n",
      "Epoch 130, Training Loss 0.11251133995707077\n",
      "Epoch 130, Training Loss 0.11273809778682715\n",
      "Epoch 130, Training Loss 0.1128800255067818\n",
      "Epoch 130, Training Loss 0.11319705192237864\n",
      "Epoch 130, Training Loss 0.11354260531532795\n",
      "Epoch 130, Training Loss 0.11386778359980229\n",
      "Epoch 130, Training Loss 0.11433474841477621\n",
      "Epoch 130, Training Loss 0.11458234241246568\n",
      "Epoch 130, Training Loss 0.11481521413911638\n",
      "Epoch 130, Training Loss 0.11494208873271028\n",
      "Epoch 130, Training Loss 0.11520101513970843\n",
      "Epoch 130, Training Loss 0.11535424602877759\n",
      "Epoch 130, Training Loss 0.11572034568394847\n",
      "Epoch 130, Training Loss 0.1158523781086935\n",
      "Epoch 130, Training Loss 0.11607136303925758\n",
      "Epoch 130, Training Loss 0.11629399888770049\n",
      "Epoch 130, Training Loss 0.11658181691223093\n",
      "Epoch 130, Training Loss 0.11676132840001979\n",
      "Epoch 130, Training Loss 0.11693805218924341\n",
      "Epoch 130, Training Loss 0.11710661258119756\n",
      "Epoch 130, Training Loss 0.1172766367359387\n",
      "Epoch 130, Training Loss 0.11747047959653008\n",
      "Epoch 130, Training Loss 0.11767399125277539\n",
      "Epoch 130, Training Loss 0.11806178295894353\n",
      "Epoch 130, Training Loss 0.11826465912449086\n",
      "Epoch 130, Training Loss 0.11847483512500058\n",
      "Epoch 130, Training Loss 0.11869132500665877\n",
      "Epoch 130, Training Loss 0.11884320474913358\n",
      "Epoch 130, Training Loss 0.11898303638829295\n",
      "Epoch 130, Training Loss 0.119236464209645\n",
      "Epoch 130, Training Loss 0.11940189522436208\n",
      "Epoch 130, Training Loss 0.11956980427169739\n",
      "Epoch 130, Training Loss 0.11973987248204553\n",
      "Epoch 130, Training Loss 0.12009016795041007\n",
      "Epoch 130, Training Loss 0.12036088788334061\n",
      "Epoch 130, Training Loss 0.12051954215673534\n",
      "Epoch 130, Training Loss 0.12057934269842589\n",
      "Epoch 130, Training Loss 0.12075956229625455\n",
      "Epoch 130, Training Loss 0.12106079605343702\n",
      "Epoch 130, Training Loss 0.1211973981803183\n",
      "Epoch 130, Training Loss 0.12140360902375577\n",
      "Epoch 130, Training Loss 0.12168124945991485\n",
      "Epoch 130, Training Loss 0.12178224533834421\n",
      "Epoch 130, Training Loss 0.12192686331813293\n",
      "Epoch 130, Training Loss 0.12210702447368361\n",
      "Epoch 130, Training Loss 0.12224564310687278\n",
      "Epoch 130, Training Loss 0.12252183658692538\n",
      "Epoch 130, Training Loss 0.12271260847444729\n",
      "Epoch 130, Training Loss 0.1228119198261472\n",
      "Epoch 130, Training Loss 0.12288024710477008\n",
      "Epoch 130, Training Loss 0.12300995563435585\n",
      "Epoch 130, Training Loss 0.12318630657061135\n",
      "Epoch 130, Training Loss 0.12330125126978168\n",
      "Epoch 130, Training Loss 0.12361937450230731\n",
      "Epoch 130, Training Loss 0.12377502305714218\n",
      "Epoch 130, Training Loss 0.12411072213307518\n",
      "Epoch 130, Training Loss 0.12423559313978227\n",
      "Epoch 130, Training Loss 0.12433728748632361\n",
      "Epoch 130, Training Loss 0.12448830391420886\n",
      "Epoch 130, Training Loss 0.12465740301553398\n",
      "Epoch 130, Training Loss 0.12482537359208859\n",
      "Epoch 130, Training Loss 0.12504755806111162\n",
      "Epoch 130, Training Loss 0.12524842373226458\n",
      "Epoch 130, Training Loss 0.1253378099483221\n",
      "Epoch 130, Training Loss 0.12543725829733454\n",
      "Epoch 130, Training Loss 0.12553695011455232\n",
      "Epoch 130, Training Loss 0.12568526261050225\n",
      "Epoch 130, Training Loss 0.1259348364618352\n",
      "Epoch 130, Training Loss 0.12603901710142107\n",
      "Epoch 130, Training Loss 0.12618199954538242\n",
      "Epoch 130, Training Loss 0.1263445786645879\n",
      "Epoch 130, Training Loss 0.12651454580619054\n",
      "Epoch 130, Training Loss 0.12672892968882532\n",
      "Epoch 130, Training Loss 0.12695245417620976\n",
      "Epoch 130, Training Loss 0.12715435909855244\n",
      "Epoch 130, Training Loss 0.12725428662851185\n",
      "Epoch 130, Training Loss 0.1276004266927538\n",
      "Epoch 130, Training Loss 0.1277278078877179\n",
      "Epoch 130, Training Loss 0.12793494286992207\n",
      "Epoch 130, Training Loss 0.1280672927446606\n",
      "Epoch 130, Training Loss 0.1282461263630015\n",
      "Epoch 130, Training Loss 0.1285775940427009\n",
      "Epoch 130, Training Loss 0.12882485830932475\n",
      "Epoch 130, Training Loss 0.1289924158073981\n",
      "Epoch 130, Training Loss 0.12938268613689544\n",
      "Epoch 130, Training Loss 0.12969477276039093\n",
      "Epoch 130, Training Loss 0.12991507223252294\n",
      "Epoch 130, Training Loss 0.13009563329941629\n",
      "Epoch 130, Training Loss 0.1304758801780012\n",
      "Epoch 130, Training Loss 0.1305988453414358\n",
      "Epoch 130, Training Loss 0.13100710392589002\n",
      "Epoch 130, Training Loss 0.13134109535161645\n",
      "Epoch 130, Training Loss 0.13148451698920155\n",
      "Epoch 130, Training Loss 0.13163290483891354\n",
      "Epoch 130, Training Loss 0.1318182755890481\n",
      "Epoch 130, Training Loss 0.13197759216379784\n",
      "Epoch 130, Training Loss 0.13213563778573442\n",
      "Epoch 130, Training Loss 0.13235443166893004\n",
      "Epoch 130, Training Loss 0.13258671472349282\n",
      "Epoch 130, Training Loss 0.13272102714975928\n",
      "Epoch 130, Training Loss 0.13291008875745794\n",
      "Epoch 130, Training Loss 0.13315207976609697\n",
      "Epoch 130, Training Loss 0.13339639766156064\n",
      "Epoch 130, Training Loss 0.13363159669425023\n",
      "Epoch 130, Training Loss 0.1339068873011319\n",
      "Epoch 130, Training Loss 0.13402163595094552\n",
      "Epoch 130, Training Loss 0.13424172516807417\n",
      "Epoch 130, Training Loss 0.13450980814807403\n",
      "Epoch 130, Training Loss 0.1346660386008756\n",
      "Epoch 130, Training Loss 0.13483980660567352\n",
      "Epoch 130, Training Loss 0.13498938534780383\n",
      "Epoch 130, Training Loss 0.1351824145706947\n",
      "Epoch 130, Training Loss 0.1353281977946115\n",
      "Epoch 130, Training Loss 0.13552915010972857\n",
      "Epoch 130, Training Loss 0.13569801212633814\n",
      "Epoch 130, Training Loss 0.13577461509448488\n",
      "Epoch 130, Training Loss 0.13593220369666434\n",
      "Epoch 130, Training Loss 0.13604045170537957\n",
      "Epoch 130, Training Loss 0.13614361042447407\n",
      "Epoch 130, Training Loss 0.13635946231920396\n",
      "Epoch 130, Training Loss 0.13653224503711972\n",
      "Epoch 130, Training Loss 0.13676589008068185\n",
      "Epoch 130, Training Loss 0.1369765004824342\n",
      "Epoch 130, Training Loss 0.13730764742511922\n",
      "Epoch 130, Training Loss 0.13740170716553393\n",
      "Epoch 130, Training Loss 0.13757124247834507\n",
      "Epoch 130, Training Loss 0.13772771161649844\n",
      "Epoch 130, Training Loss 0.13815271761982947\n",
      "Epoch 130, Training Loss 0.1383918414888022\n",
      "Epoch 130, Training Loss 0.1385593041110679\n",
      "Epoch 130, Training Loss 0.1388189058531733\n",
      "Epoch 130, Training Loss 0.13920844427269438\n",
      "Epoch 130, Training Loss 0.13944348284160085\n",
      "Epoch 130, Training Loss 0.13978511196992283\n",
      "Epoch 130, Training Loss 0.1399089422581903\n",
      "Epoch 130, Training Loss 0.14017029157113237\n",
      "Epoch 130, Training Loss 0.14035268572857007\n",
      "Epoch 130, Training Loss 0.1405078781878247\n",
      "Epoch 130, Training Loss 0.14095037320004705\n",
      "Epoch 130, Training Loss 0.14124885146194102\n",
      "Epoch 130, Training Loss 0.14144894635052327\n",
      "Epoch 130, Training Loss 0.1416309712373692\n",
      "Epoch 130, Training Loss 0.14186242093210635\n",
      "Epoch 130, Training Loss 0.14211176196708702\n",
      "Epoch 130, Training Loss 0.14235068794787692\n",
      "Epoch 130, Training Loss 0.14245947664770323\n",
      "Epoch 130, Training Loss 0.14270003683998456\n",
      "Epoch 130, Training Loss 0.1427954091402271\n",
      "Epoch 130, Training Loss 0.14292591567272725\n",
      "Epoch 130, Training Loss 0.14310749180976998\n",
      "Epoch 130, Training Loss 0.14338590525795736\n",
      "Epoch 130, Training Loss 0.14361866937993129\n",
      "Epoch 130, Training Loss 0.14384743581761789\n",
      "Epoch 130, Training Loss 0.14396443387584004\n",
      "Epoch 130, Training Loss 0.14410011085402935\n",
      "Epoch 130, Training Loss 0.14433131944340513\n",
      "Epoch 130, Training Loss 0.14452646794678914\n",
      "Epoch 130, Training Loss 0.14469989312960363\n",
      "Epoch 130, Training Loss 0.14501114671721177\n",
      "Epoch 130, Training Loss 0.14523765355196144\n",
      "Epoch 130, Training Loss 0.1453605286224419\n",
      "Epoch 130, Training Loss 0.1454538960019341\n",
      "Epoch 130, Training Loss 0.14565987500083416\n",
      "Epoch 130, Training Loss 0.14585788380307005\n",
      "Epoch 130, Training Loss 0.14609848864166938\n",
      "Epoch 130, Training Loss 0.14625249857373554\n",
      "Epoch 130, Training Loss 0.1465476075153979\n",
      "Epoch 130, Training Loss 0.1467378948293531\n",
      "Epoch 130, Training Loss 0.14691941275278017\n",
      "Epoch 130, Training Loss 0.14709270372986794\n",
      "Epoch 130, Training Loss 0.1472953440302321\n",
      "Epoch 130, Training Loss 0.14750520752557097\n",
      "Epoch 130, Training Loss 0.14778192630013848\n",
      "Epoch 130, Training Loss 0.1481100933916886\n",
      "Epoch 130, Training Loss 0.14829842396594983\n",
      "Epoch 130, Training Loss 0.14864487485850558\n",
      "Epoch 130, Training Loss 0.14889239980017438\n",
      "Epoch 130, Training Loss 0.14921282274681893\n",
      "Epoch 130, Training Loss 0.14932495860568704\n",
      "Epoch 130, Training Loss 0.14963419333367092\n",
      "Epoch 130, Training Loss 0.14983072747354922\n",
      "Epoch 130, Training Loss 0.15020629714059708\n",
      "Epoch 130, Training Loss 0.15038315829870952\n",
      "Epoch 130, Training Loss 0.15065554542767118\n",
      "Epoch 130, Training Loss 0.1510877542940857\n",
      "Epoch 130, Training Loss 0.15137381737341965\n",
      "Epoch 130, Training Loss 0.15175071674996934\n",
      "Epoch 130, Training Loss 0.15186707668787683\n",
      "Epoch 130, Training Loss 0.1521285596158346\n",
      "Epoch 130, Training Loss 0.15240963983833028\n",
      "Epoch 130, Training Loss 0.1526720656744202\n",
      "Epoch 130, Training Loss 0.15286965329971763\n",
      "Epoch 130, Training Loss 0.1532975844657787\n",
      "Epoch 130, Training Loss 0.15351692929177943\n",
      "Epoch 130, Training Loss 0.15364412314561018\n",
      "Epoch 130, Training Loss 0.15382569994958464\n",
      "Epoch 130, Training Loss 0.15409681240044287\n",
      "Epoch 130, Training Loss 0.15442110073116735\n",
      "Epoch 130, Training Loss 0.15459996586679803\n",
      "Epoch 130, Training Loss 0.15495110639487691\n",
      "Epoch 130, Training Loss 0.15512936757615461\n",
      "Epoch 130, Training Loss 0.15530576959938344\n",
      "Epoch 130, Training Loss 0.15556904556386916\n",
      "Epoch 130, Training Loss 0.15589770302176476\n",
      "Epoch 130, Training Loss 0.15608191081439443\n",
      "Epoch 130, Training Loss 0.15637859047564398\n",
      "Epoch 130, Training Loss 0.1565182946831979\n",
      "Epoch 130, Training Loss 0.15678493095480878\n",
      "Epoch 130, Training Loss 0.15711751721246772\n",
      "Epoch 130, Training Loss 0.15753932872696605\n",
      "Epoch 130, Training Loss 0.15767587101101266\n",
      "Epoch 130, Training Loss 0.1578184699025148\n",
      "Epoch 130, Training Loss 0.15800645085208861\n",
      "Epoch 130, Training Loss 0.15815899081890236\n",
      "Epoch 130, Training Loss 0.1582595035910149\n",
      "Epoch 130, Training Loss 0.15850501342693255\n",
      "Epoch 130, Training Loss 0.15864121191718084\n",
      "Epoch 130, Training Loss 0.15883500209016263\n",
      "Epoch 130, Training Loss 0.15893844075863015\n",
      "Epoch 130, Training Loss 0.15916732209913262\n",
      "Epoch 130, Training Loss 0.1594321743092116\n",
      "Epoch 130, Training Loss 0.15971807528601584\n",
      "Epoch 130, Training Loss 0.16013835403887208\n",
      "Epoch 140, Training Loss 0.00014461385433936057\n",
      "Epoch 140, Training Loss 0.00030308194896754096\n",
      "Epoch 140, Training Loss 0.00042979854642582674\n",
      "Epoch 140, Training Loss 0.000608510916571483\n",
      "Epoch 140, Training Loss 0.000671764325988872\n",
      "Epoch 140, Training Loss 0.0008219074734184138\n",
      "Epoch 140, Training Loss 0.001044873357810023\n",
      "Epoch 140, Training Loss 0.0012851583740442914\n",
      "Epoch 140, Training Loss 0.0013935103288391971\n",
      "Epoch 140, Training Loss 0.0015534215494799797\n",
      "Epoch 140, Training Loss 0.0016744643678445646\n",
      "Epoch 140, Training Loss 0.001883613769812962\n",
      "Epoch 140, Training Loss 0.0020163999513134628\n",
      "Epoch 140, Training Loss 0.0020784198323174205\n",
      "Epoch 140, Training Loss 0.0021799548012216377\n",
      "Epoch 140, Training Loss 0.0024447378028384253\n",
      "Epoch 140, Training Loss 0.002700818850256293\n",
      "Epoch 140, Training Loss 0.0028565716274711484\n",
      "Epoch 140, Training Loss 0.003152381133316728\n",
      "Epoch 140, Training Loss 0.003308959028986104\n",
      "Epoch 140, Training Loss 0.003433224518814355\n",
      "Epoch 140, Training Loss 0.0036007764337160396\n",
      "Epoch 140, Training Loss 0.0037695673935096282\n",
      "Epoch 140, Training Loss 0.003935417832087373\n",
      "Epoch 140, Training Loss 0.004048076270105284\n",
      "Epoch 140, Training Loss 0.00418942963794979\n",
      "Epoch 140, Training Loss 0.004442158362368488\n",
      "Epoch 140, Training Loss 0.004583672251161712\n",
      "Epoch 140, Training Loss 0.004733330729748587\n",
      "Epoch 140, Training Loss 0.004826617820183639\n",
      "Epoch 140, Training Loss 0.005034464551969563\n",
      "Epoch 140, Training Loss 0.005218982810864363\n",
      "Epoch 140, Training Loss 0.005328681929718198\n",
      "Epoch 140, Training Loss 0.005495173708938272\n",
      "Epoch 140, Training Loss 0.005603361207887035\n",
      "Epoch 140, Training Loss 0.005734430392608618\n",
      "Epoch 140, Training Loss 0.005836217290224017\n",
      "Epoch 140, Training Loss 0.005986048875714812\n",
      "Epoch 140, Training Loss 0.006198554449831434\n",
      "Epoch 140, Training Loss 0.0063982442631136125\n",
      "Epoch 140, Training Loss 0.006510486440432956\n",
      "Epoch 140, Training Loss 0.006717708219042824\n",
      "Epoch 140, Training Loss 0.00693453791196389\n",
      "Epoch 140, Training Loss 0.007093009769992755\n",
      "Epoch 140, Training Loss 0.007188701902127937\n",
      "Epoch 140, Training Loss 0.007401982012688352\n",
      "Epoch 140, Training Loss 0.007607100326615527\n",
      "Epoch 140, Training Loss 0.00772908161325223\n",
      "Epoch 140, Training Loss 0.00789087769739768\n",
      "Epoch 140, Training Loss 0.008044467586309403\n",
      "Epoch 140, Training Loss 0.008166233701702883\n",
      "Epoch 140, Training Loss 0.008253062101047667\n",
      "Epoch 140, Training Loss 0.008345588879740757\n",
      "Epoch 140, Training Loss 0.00852128195450129\n",
      "Epoch 140, Training Loss 0.008713094941566668\n",
      "Epoch 140, Training Loss 0.008886173720021382\n",
      "Epoch 140, Training Loss 0.009072403478271821\n",
      "Epoch 140, Training Loss 0.009307886833501288\n",
      "Epoch 140, Training Loss 0.009461254872324522\n",
      "Epoch 140, Training Loss 0.009672190431896073\n",
      "Epoch 140, Training Loss 0.009759866539627085\n",
      "Epoch 140, Training Loss 0.009855369510857956\n",
      "Epoch 140, Training Loss 0.009964777675964643\n",
      "Epoch 140, Training Loss 0.010218580960846314\n",
      "Epoch 140, Training Loss 0.01033807808862013\n",
      "Epoch 140, Training Loss 0.010452292297426087\n",
      "Epoch 140, Training Loss 0.01053937417848031\n",
      "Epoch 140, Training Loss 0.01060918112621283\n",
      "Epoch 140, Training Loss 0.010779597303446601\n",
      "Epoch 140, Training Loss 0.011070642858515006\n",
      "Epoch 140, Training Loss 0.011455291029437424\n",
      "Epoch 140, Training Loss 0.011570310463076052\n",
      "Epoch 140, Training Loss 0.011694824642232617\n",
      "Epoch 140, Training Loss 0.011862230213249432\n",
      "Epoch 140, Training Loss 0.012041112860602795\n",
      "Epoch 140, Training Loss 0.0122714152040384\n",
      "Epoch 140, Training Loss 0.012480384744036838\n",
      "Epoch 140, Training Loss 0.012695621175076955\n",
      "Epoch 140, Training Loss 0.012811963953782835\n",
      "Epoch 140, Training Loss 0.013040425927590226\n",
      "Epoch 140, Training Loss 0.013147150511708101\n",
      "Epoch 140, Training Loss 0.01339402098370635\n",
      "Epoch 140, Training Loss 0.013678108553980927\n",
      "Epoch 140, Training Loss 0.013860664813948409\n",
      "Epoch 140, Training Loss 0.014126945972976172\n",
      "Epoch 140, Training Loss 0.014237549222643723\n",
      "Epoch 140, Training Loss 0.014448812036105738\n",
      "Epoch 140, Training Loss 0.014557086426736143\n",
      "Epoch 140, Training Loss 0.014701424333293115\n",
      "Epoch 140, Training Loss 0.015069538655945711\n",
      "Epoch 140, Training Loss 0.015324277341213373\n",
      "Epoch 140, Training Loss 0.015452951478683735\n",
      "Epoch 140, Training Loss 0.015803219621901014\n",
      "Epoch 140, Training Loss 0.015968759860986333\n",
      "Epoch 140, Training Loss 0.016126809675065454\n",
      "Epoch 140, Training Loss 0.016294062290045305\n",
      "Epoch 140, Training Loss 0.01646078934373758\n",
      "Epoch 140, Training Loss 0.016544349482068624\n",
      "Epoch 140, Training Loss 0.01674256103156168\n",
      "Epoch 140, Training Loss 0.01685322011294572\n",
      "Epoch 140, Training Loss 0.016941364568746304\n",
      "Epoch 140, Training Loss 0.01709446386265023\n",
      "Epoch 140, Training Loss 0.017215721785564863\n",
      "Epoch 140, Training Loss 0.017318660483869444\n",
      "Epoch 140, Training Loss 0.017547532411106407\n",
      "Epoch 140, Training Loss 0.017690880428952026\n",
      "Epoch 140, Training Loss 0.017833163413931343\n",
      "Epoch 140, Training Loss 0.018136979397529226\n",
      "Epoch 140, Training Loss 0.018419243176193797\n",
      "Epoch 140, Training Loss 0.018765922121303465\n",
      "Epoch 140, Training Loss 0.01889754666010742\n",
      "Epoch 140, Training Loss 0.01905819350648719\n",
      "Epoch 140, Training Loss 0.019209017724637182\n",
      "Epoch 140, Training Loss 0.0193833395495744\n",
      "Epoch 140, Training Loss 0.01958669440063369\n",
      "Epoch 140, Training Loss 0.019820010094234095\n",
      "Epoch 140, Training Loss 0.019946573487937908\n",
      "Epoch 140, Training Loss 0.020080522198201445\n",
      "Epoch 140, Training Loss 0.020229992354312516\n",
      "Epoch 140, Training Loss 0.020342625553726845\n",
      "Epoch 140, Training Loss 0.02045470959199664\n",
      "Epoch 140, Training Loss 0.020598020480798028\n",
      "Epoch 140, Training Loss 0.020834178142154307\n",
      "Epoch 140, Training Loss 0.021063370673019256\n",
      "Epoch 140, Training Loss 0.02129239058288772\n",
      "Epoch 140, Training Loss 0.02137500364952685\n",
      "Epoch 140, Training Loss 0.021486696808615608\n",
      "Epoch 140, Training Loss 0.021583002527504017\n",
      "Epoch 140, Training Loss 0.021705510070942857\n",
      "Epoch 140, Training Loss 0.021837548336104663\n",
      "Epoch 140, Training Loss 0.021969518926747315\n",
      "Epoch 140, Training Loss 0.022050251026668816\n",
      "Epoch 140, Training Loss 0.022157080509626042\n",
      "Epoch 140, Training Loss 0.022347407191610703\n",
      "Epoch 140, Training Loss 0.0224470571540963\n",
      "Epoch 140, Training Loss 0.022579531926099603\n",
      "Epoch 140, Training Loss 0.022622474900368228\n",
      "Epoch 140, Training Loss 0.022827097683992534\n",
      "Epoch 140, Training Loss 0.022998108116485883\n",
      "Epoch 140, Training Loss 0.023154279726850406\n",
      "Epoch 140, Training Loss 0.023395964118373365\n",
      "Epoch 140, Training Loss 0.023485789070730016\n",
      "Epoch 140, Training Loss 0.023613368010963014\n",
      "Epoch 140, Training Loss 0.023775125146293274\n",
      "Epoch 140, Training Loss 0.023956826380680283\n",
      "Epoch 140, Training Loss 0.02403369549747623\n",
      "Epoch 140, Training Loss 0.0241401662187808\n",
      "Epoch 140, Training Loss 0.02434068967771652\n",
      "Epoch 140, Training Loss 0.024528169551926195\n",
      "Epoch 140, Training Loss 0.024762899121817422\n",
      "Epoch 140, Training Loss 0.024898252247468285\n",
      "Epoch 140, Training Loss 0.02514871098386967\n",
      "Epoch 140, Training Loss 0.025498171796655412\n",
      "Epoch 140, Training Loss 0.025623333347422998\n",
      "Epoch 140, Training Loss 0.025690354125769546\n",
      "Epoch 140, Training Loss 0.025896625297949137\n",
      "Epoch 140, Training Loss 0.026078595705997305\n",
      "Epoch 140, Training Loss 0.026270742501939653\n",
      "Epoch 140, Training Loss 0.026344582910084968\n",
      "Epoch 140, Training Loss 0.026604584868416152\n",
      "Epoch 140, Training Loss 0.02680824018176407\n",
      "Epoch 140, Training Loss 0.027102274372411507\n",
      "Epoch 140, Training Loss 0.02723898357042419\n",
      "Epoch 140, Training Loss 0.027477226155760037\n",
      "Epoch 140, Training Loss 0.027693015473234987\n",
      "Epoch 140, Training Loss 0.02782215835893398\n",
      "Epoch 140, Training Loss 0.027909438776047638\n",
      "Epoch 140, Training Loss 0.028038204807187894\n",
      "Epoch 140, Training Loss 0.02819246745875577\n",
      "Epoch 140, Training Loss 0.02850171857420593\n",
      "Epoch 140, Training Loss 0.028784746408957958\n",
      "Epoch 140, Training Loss 0.028922026786391084\n",
      "Epoch 140, Training Loss 0.02915084669295022\n",
      "Epoch 140, Training Loss 0.029301460322630986\n",
      "Epoch 140, Training Loss 0.029429654335922293\n",
      "Epoch 140, Training Loss 0.029508382829901814\n",
      "Epoch 140, Training Loss 0.02961021695581391\n",
      "Epoch 140, Training Loss 0.02972090078036651\n",
      "Epoch 140, Training Loss 0.02994659344387024\n",
      "Epoch 140, Training Loss 0.030074917327831772\n",
      "Epoch 140, Training Loss 0.03027817210101563\n",
      "Epoch 140, Training Loss 0.03040905705060038\n",
      "Epoch 140, Training Loss 0.0305770307522067\n",
      "Epoch 140, Training Loss 0.030714023901182975\n",
      "Epoch 140, Training Loss 0.030859218195766744\n",
      "Epoch 140, Training Loss 0.031072995494431852\n",
      "Epoch 140, Training Loss 0.03117825074211868\n",
      "Epoch 140, Training Loss 0.03130005082339429\n",
      "Epoch 140, Training Loss 0.03155263838694071\n",
      "Epoch 140, Training Loss 0.03180486435913826\n",
      "Epoch 140, Training Loss 0.03202620432104753\n",
      "Epoch 140, Training Loss 0.032248439800823134\n",
      "Epoch 140, Training Loss 0.032447354848999195\n",
      "Epoch 140, Training Loss 0.03258039352133908\n",
      "Epoch 140, Training Loss 0.032833318685745946\n",
      "Epoch 140, Training Loss 0.03293727906635198\n",
      "Epoch 140, Training Loss 0.03300116905737716\n",
      "Epoch 140, Training Loss 0.03313998104361317\n",
      "Epoch 140, Training Loss 0.03327265822940775\n",
      "Epoch 140, Training Loss 0.03339265124953311\n",
      "Epoch 140, Training Loss 0.03352805431884573\n",
      "Epoch 140, Training Loss 0.033736968160514025\n",
      "Epoch 140, Training Loss 0.03394093834187673\n",
      "Epoch 140, Training Loss 0.03409840443821819\n",
      "Epoch 140, Training Loss 0.034241558407502405\n",
      "Epoch 140, Training Loss 0.03456804856581761\n",
      "Epoch 140, Training Loss 0.03468691469991908\n",
      "Epoch 140, Training Loss 0.034922818488934464\n",
      "Epoch 140, Training Loss 0.035221879325254495\n",
      "Epoch 140, Training Loss 0.03531927229536464\n",
      "Epoch 140, Training Loss 0.03547266103765544\n",
      "Epoch 140, Training Loss 0.03565932826503464\n",
      "Epoch 140, Training Loss 0.035799607034305785\n",
      "Epoch 140, Training Loss 0.0360886326817143\n",
      "Epoch 140, Training Loss 0.03618166046907835\n",
      "Epoch 140, Training Loss 0.03627836376504825\n",
      "Epoch 140, Training Loss 0.0364131942829665\n",
      "Epoch 140, Training Loss 0.0365177057302364\n",
      "Epoch 140, Training Loss 0.03664196036813204\n",
      "Epoch 140, Training Loss 0.03673866005314281\n",
      "Epoch 140, Training Loss 0.036936294840043765\n",
      "Epoch 140, Training Loss 0.03718133311709175\n",
      "Epoch 140, Training Loss 0.037298578988103306\n",
      "Epoch 140, Training Loss 0.037504308745074454\n",
      "Epoch 140, Training Loss 0.0375758592953996\n",
      "Epoch 140, Training Loss 0.03785187988768301\n",
      "Epoch 140, Training Loss 0.03797641651385733\n",
      "Epoch 140, Training Loss 0.038176715693167405\n",
      "Epoch 140, Training Loss 0.038378854689505096\n",
      "Epoch 140, Training Loss 0.03857829019217692\n",
      "Epoch 140, Training Loss 0.03872005001682302\n",
      "Epoch 140, Training Loss 0.03883931267997036\n",
      "Epoch 140, Training Loss 0.03889141400890125\n",
      "Epoch 140, Training Loss 0.03894913849204093\n",
      "Epoch 140, Training Loss 0.03911136960625039\n",
      "Epoch 140, Training Loss 0.03934248457746128\n",
      "Epoch 140, Training Loss 0.03951669874055611\n",
      "Epoch 140, Training Loss 0.03973547017673397\n",
      "Epoch 140, Training Loss 0.03996749215609277\n",
      "Epoch 140, Training Loss 0.04009750746476376\n",
      "Epoch 140, Training Loss 0.04021608932396335\n",
      "Epoch 140, Training Loss 0.04050469773885844\n",
      "Epoch 140, Training Loss 0.0406709954980999\n",
      "Epoch 140, Training Loss 0.04090896799512531\n",
      "Epoch 140, Training Loss 0.04117783385774364\n",
      "Epoch 140, Training Loss 0.04146365337359631\n",
      "Epoch 140, Training Loss 0.04161415065226652\n",
      "Epoch 140, Training Loss 0.04185687399009609\n",
      "Epoch 140, Training Loss 0.04195437503173528\n",
      "Epoch 140, Training Loss 0.04201589886318235\n",
      "Epoch 140, Training Loss 0.04221323791348264\n",
      "Epoch 140, Training Loss 0.04239098332784212\n",
      "Epoch 140, Training Loss 0.04247465330983519\n",
      "Epoch 140, Training Loss 0.0425507113804369\n",
      "Epoch 140, Training Loss 0.042816735370575315\n",
      "Epoch 140, Training Loss 0.04290966478550373\n",
      "Epoch 140, Training Loss 0.043113240333812315\n",
      "Epoch 140, Training Loss 0.0432623065865177\n",
      "Epoch 140, Training Loss 0.0434233260190929\n",
      "Epoch 140, Training Loss 0.04356440561144705\n",
      "Epoch 140, Training Loss 0.0436449982082981\n",
      "Epoch 140, Training Loss 0.04382582713404427\n",
      "Epoch 140, Training Loss 0.04393078812190791\n",
      "Epoch 140, Training Loss 0.04425456375360032\n",
      "Epoch 140, Training Loss 0.044545916802323685\n",
      "Epoch 140, Training Loss 0.0446735701340315\n",
      "Epoch 140, Training Loss 0.044895443393636845\n",
      "Epoch 140, Training Loss 0.04509396384686917\n",
      "Epoch 140, Training Loss 0.04517575888358571\n",
      "Epoch 140, Training Loss 0.045267649638035416\n",
      "Epoch 140, Training Loss 0.04534172513962859\n",
      "Epoch 140, Training Loss 0.045509570771280455\n",
      "Epoch 140, Training Loss 0.04566674102621768\n",
      "Epoch 140, Training Loss 0.04579472876704105\n",
      "Epoch 140, Training Loss 0.04593490056998437\n",
      "Epoch 140, Training Loss 0.04611681667073151\n",
      "Epoch 140, Training Loss 0.04627648190788143\n",
      "Epoch 140, Training Loss 0.04646455274080224\n",
      "Epoch 140, Training Loss 0.0465327052070814\n",
      "Epoch 140, Training Loss 0.04666680738787212\n",
      "Epoch 140, Training Loss 0.04680690052144972\n",
      "Epoch 140, Training Loss 0.046958749613646045\n",
      "Epoch 140, Training Loss 0.047126265533287505\n",
      "Epoch 140, Training Loss 0.047266652297866925\n",
      "Epoch 140, Training Loss 0.04747371069724907\n",
      "Epoch 140, Training Loss 0.04758413337037691\n",
      "Epoch 140, Training Loss 0.04782421100894203\n",
      "Epoch 140, Training Loss 0.0479438073189972\n",
      "Epoch 140, Training Loss 0.04811918605928835\n",
      "Epoch 140, Training Loss 0.048191487460451966\n",
      "Epoch 140, Training Loss 0.04825616118681553\n",
      "Epoch 140, Training Loss 0.04843290814715426\n",
      "Epoch 140, Training Loss 0.04854934415339356\n",
      "Epoch 140, Training Loss 0.04884223789071945\n",
      "Epoch 140, Training Loss 0.049081560105199706\n",
      "Epoch 140, Training Loss 0.04942703624839521\n",
      "Epoch 140, Training Loss 0.04960960283627748\n",
      "Epoch 140, Training Loss 0.049694026108173764\n",
      "Epoch 140, Training Loss 0.04985650894148728\n",
      "Epoch 140, Training Loss 0.050017789582652815\n",
      "Epoch 140, Training Loss 0.05012262770262978\n",
      "Epoch 140, Training Loss 0.05040453427263995\n",
      "Epoch 140, Training Loss 0.050510054644759356\n",
      "Epoch 140, Training Loss 0.05073035194459931\n",
      "Epoch 140, Training Loss 0.05090725599595196\n",
      "Epoch 140, Training Loss 0.051123395052445515\n",
      "Epoch 140, Training Loss 0.05130080541934046\n",
      "Epoch 140, Training Loss 0.051432645424743136\n",
      "Epoch 140, Training Loss 0.051578120354686854\n",
      "Epoch 140, Training Loss 0.05183491553358562\n",
      "Epoch 140, Training Loss 0.05195269912309811\n",
      "Epoch 140, Training Loss 0.05204937936228407\n",
      "Epoch 140, Training Loss 0.052111719761167644\n",
      "Epoch 140, Training Loss 0.052342832483865724\n",
      "Epoch 140, Training Loss 0.05253289814781197\n",
      "Epoch 140, Training Loss 0.05269160676661812\n",
      "Epoch 140, Training Loss 0.05289170460875534\n",
      "Epoch 140, Training Loss 0.05314615072058442\n",
      "Epoch 140, Training Loss 0.05332079139607184\n",
      "Epoch 140, Training Loss 0.053447356142694386\n",
      "Epoch 140, Training Loss 0.053558209453664164\n",
      "Epoch 140, Training Loss 0.053733860568888965\n",
      "Epoch 140, Training Loss 0.05382565939155838\n",
      "Epoch 140, Training Loss 0.05397923980051142\n",
      "Epoch 140, Training Loss 0.05408256364238384\n",
      "Epoch 140, Training Loss 0.054317317850640055\n",
      "Epoch 140, Training Loss 0.05441059675686957\n",
      "Epoch 140, Training Loss 0.054568643836528444\n",
      "Epoch 140, Training Loss 0.05468162961418519\n",
      "Epoch 140, Training Loss 0.054976397906156146\n",
      "Epoch 140, Training Loss 0.05517307025335177\n",
      "Epoch 140, Training Loss 0.055325339541144075\n",
      "Epoch 140, Training Loss 0.055527123424898635\n",
      "Epoch 140, Training Loss 0.05574139667308087\n",
      "Epoch 140, Training Loss 0.056019250584570954\n",
      "Epoch 140, Training Loss 0.056185699375274845\n",
      "Epoch 140, Training Loss 0.056406731995970696\n",
      "Epoch 140, Training Loss 0.05654894029411971\n",
      "Epoch 140, Training Loss 0.05673370724234282\n",
      "Epoch 140, Training Loss 0.05705293398017011\n",
      "Epoch 140, Training Loss 0.05725801959538551\n",
      "Epoch 140, Training Loss 0.057388547955609645\n",
      "Epoch 140, Training Loss 0.057552267242308774\n",
      "Epoch 140, Training Loss 0.05776101719025913\n",
      "Epoch 140, Training Loss 0.05796247120956173\n",
      "Epoch 140, Training Loss 0.0581483937790403\n",
      "Epoch 140, Training Loss 0.058288135058472836\n",
      "Epoch 140, Training Loss 0.058361191774153956\n",
      "Epoch 140, Training Loss 0.05848675425571706\n",
      "Epoch 140, Training Loss 0.058610090757231885\n",
      "Epoch 140, Training Loss 0.05877587556972376\n",
      "Epoch 140, Training Loss 0.0589564346929874\n",
      "Epoch 140, Training Loss 0.05915837141844775\n",
      "Epoch 140, Training Loss 0.05935128676750318\n",
      "Epoch 140, Training Loss 0.05955543604862812\n",
      "Epoch 140, Training Loss 0.059759270576069425\n",
      "Epoch 140, Training Loss 0.05992253129954076\n",
      "Epoch 140, Training Loss 0.06000914300322685\n",
      "Epoch 140, Training Loss 0.06026805641458315\n",
      "Epoch 140, Training Loss 0.060440672209005224\n",
      "Epoch 140, Training Loss 0.060600443183423\n",
      "Epoch 140, Training Loss 0.06078810263377474\n",
      "Epoch 140, Training Loss 0.06095787266845746\n",
      "Epoch 140, Training Loss 0.06112005978422549\n",
      "Epoch 140, Training Loss 0.061249023970321316\n",
      "Epoch 140, Training Loss 0.06137988869281833\n",
      "Epoch 140, Training Loss 0.0614515805707487\n",
      "Epoch 140, Training Loss 0.06166056230607088\n",
      "Epoch 140, Training Loss 0.06185732830954177\n",
      "Epoch 140, Training Loss 0.06213438788624218\n",
      "Epoch 140, Training Loss 0.0624118055100255\n",
      "Epoch 140, Training Loss 0.06251530567912952\n",
      "Epoch 140, Training Loss 0.06273411480647981\n",
      "Epoch 140, Training Loss 0.0630354400667007\n",
      "Epoch 140, Training Loss 0.0631889335839721\n",
      "Epoch 140, Training Loss 0.06335896406503742\n",
      "Epoch 140, Training Loss 0.06363063224155427\n",
      "Epoch 140, Training Loss 0.06382259576464705\n",
      "Epoch 140, Training Loss 0.06405326888403472\n",
      "Epoch 140, Training Loss 0.06419947256079263\n",
      "Epoch 140, Training Loss 0.06432198199069561\n",
      "Epoch 140, Training Loss 0.0644354164419348\n",
      "Epoch 140, Training Loss 0.0646957961504188\n",
      "Epoch 140, Training Loss 0.06492664798846483\n",
      "Epoch 140, Training Loss 0.06522218125117252\n",
      "Epoch 140, Training Loss 0.06548534076937172\n",
      "Epoch 140, Training Loss 0.06569477464632152\n",
      "Epoch 140, Training Loss 0.06579841865355249\n",
      "Epoch 140, Training Loss 0.06599566408568788\n",
      "Epoch 140, Training Loss 0.06612395951071816\n",
      "Epoch 140, Training Loss 0.06630696049984307\n",
      "Epoch 140, Training Loss 0.06642792582073632\n",
      "Epoch 140, Training Loss 0.0665498785626934\n",
      "Epoch 140, Training Loss 0.06667846543690585\n",
      "Epoch 140, Training Loss 0.06687486439924259\n",
      "Epoch 140, Training Loss 0.06699500744090513\n",
      "Epoch 140, Training Loss 0.06716917175561418\n",
      "Epoch 140, Training Loss 0.06731995455730144\n",
      "Epoch 140, Training Loss 0.06745208577369638\n",
      "Epoch 140, Training Loss 0.06769566907240149\n",
      "Epoch 140, Training Loss 0.06790529635956373\n",
      "Epoch 140, Training Loss 0.06808583145422856\n",
      "Epoch 140, Training Loss 0.0683046075398736\n",
      "Epoch 140, Training Loss 0.06847747089936758\n",
      "Epoch 140, Training Loss 0.06859247841398276\n",
      "Epoch 140, Training Loss 0.06876723858458764\n",
      "Epoch 140, Training Loss 0.06894377065951104\n",
      "Epoch 140, Training Loss 0.0690898325179925\n",
      "Epoch 140, Training Loss 0.06926177318691445\n",
      "Epoch 140, Training Loss 0.06953675826282604\n",
      "Epoch 140, Training Loss 0.06978183445970879\n",
      "Epoch 140, Training Loss 0.0699876658951916\n",
      "Epoch 140, Training Loss 0.07015516253097741\n",
      "Epoch 140, Training Loss 0.07029627376448011\n",
      "Epoch 140, Training Loss 0.07048382492416808\n",
      "Epoch 140, Training Loss 0.07067299030644966\n",
      "Epoch 140, Training Loss 0.07090956712489391\n",
      "Epoch 140, Training Loss 0.07113888789721005\n",
      "Epoch 140, Training Loss 0.0712914753305104\n",
      "Epoch 140, Training Loss 0.07153782167036057\n",
      "Epoch 140, Training Loss 0.07167305598688095\n",
      "Epoch 140, Training Loss 0.07197246759120003\n",
      "Epoch 140, Training Loss 0.07215660627064345\n",
      "Epoch 140, Training Loss 0.07237910802769082\n",
      "Epoch 140, Training Loss 0.07250223823768251\n",
      "Epoch 140, Training Loss 0.07264039159068823\n",
      "Epoch 140, Training Loss 0.0729492411179387\n",
      "Epoch 140, Training Loss 0.07321574577532919\n",
      "Epoch 140, Training Loss 0.07340958169983018\n",
      "Epoch 140, Training Loss 0.07352592172506063\n",
      "Epoch 140, Training Loss 0.07379756754983569\n",
      "Epoch 140, Training Loss 0.07403967525247875\n",
      "Epoch 140, Training Loss 0.07416690283400171\n",
      "Epoch 140, Training Loss 0.07440004234328447\n",
      "Epoch 140, Training Loss 0.07455903940055224\n",
      "Epoch 140, Training Loss 0.07472766286519635\n",
      "Epoch 140, Training Loss 0.07489180639672005\n",
      "Epoch 140, Training Loss 0.0751310897004955\n",
      "Epoch 140, Training Loss 0.07529622227277445\n",
      "Epoch 140, Training Loss 0.07543444036579955\n",
      "Epoch 140, Training Loss 0.0759096027630598\n",
      "Epoch 140, Training Loss 0.07607427025523485\n",
      "Epoch 140, Training Loss 0.07624902164139559\n",
      "Epoch 140, Training Loss 0.07656817669358552\n",
      "Epoch 140, Training Loss 0.07675013031400836\n",
      "Epoch 140, Training Loss 0.0768861715608012\n",
      "Epoch 140, Training Loss 0.07706249191347138\n",
      "Epoch 140, Training Loss 0.07720197663378076\n",
      "Epoch 140, Training Loss 0.07741308266111195\n",
      "Epoch 140, Training Loss 0.07765953639603179\n",
      "Epoch 140, Training Loss 0.07782449697613564\n",
      "Epoch 140, Training Loss 0.07798033908409688\n",
      "Epoch 140, Training Loss 0.07822040568970506\n",
      "Epoch 140, Training Loss 0.07853579936106034\n",
      "Epoch 140, Training Loss 0.07875673154183209\n",
      "Epoch 140, Training Loss 0.078995592182364\n",
      "Epoch 140, Training Loss 0.07927610597018238\n",
      "Epoch 140, Training Loss 0.07947201851059867\n",
      "Epoch 140, Training Loss 0.07968905112227363\n",
      "Epoch 140, Training Loss 0.07984874054046391\n",
      "Epoch 140, Training Loss 0.08007401698137946\n",
      "Epoch 140, Training Loss 0.0802427010224832\n",
      "Epoch 140, Training Loss 0.08044447970416997\n",
      "Epoch 140, Training Loss 0.0805495366158769\n",
      "Epoch 140, Training Loss 0.08062992994304356\n",
      "Epoch 140, Training Loss 0.08094509554278972\n",
      "Epoch 140, Training Loss 0.08118920943335346\n",
      "Epoch 140, Training Loss 0.08145248556933592\n",
      "Epoch 140, Training Loss 0.0815723211268711\n",
      "Epoch 140, Training Loss 0.08180345585831748\n",
      "Epoch 140, Training Loss 0.08200811844824067\n",
      "Epoch 140, Training Loss 0.08229986630151491\n",
      "Epoch 140, Training Loss 0.0825189917260195\n",
      "Epoch 140, Training Loss 0.0826844132131399\n",
      "Epoch 140, Training Loss 0.08280638440052414\n",
      "Epoch 140, Training Loss 0.08301220125878406\n",
      "Epoch 140, Training Loss 0.08316169748239963\n",
      "Epoch 140, Training Loss 0.08326834860874717\n",
      "Epoch 140, Training Loss 0.08333942984395168\n",
      "Epoch 140, Training Loss 0.08349473880665839\n",
      "Epoch 140, Training Loss 0.08362321825721837\n",
      "Epoch 140, Training Loss 0.08372975200357492\n",
      "Epoch 140, Training Loss 0.08385979757189294\n",
      "Epoch 140, Training Loss 0.08405735865330605\n",
      "Epoch 140, Training Loss 0.08416006513549697\n",
      "Epoch 140, Training Loss 0.0842876731730102\n",
      "Epoch 140, Training Loss 0.08444804482428772\n",
      "Epoch 140, Training Loss 0.08471608952240413\n",
      "Epoch 140, Training Loss 0.08488475862423629\n",
      "Epoch 140, Training Loss 0.08509877331250007\n",
      "Epoch 140, Training Loss 0.0852942274611853\n",
      "Epoch 140, Training Loss 0.08536264149810346\n",
      "Epoch 140, Training Loss 0.085738307127105\n",
      "Epoch 140, Training Loss 0.08590470904203328\n",
      "Epoch 140, Training Loss 0.08603352821810777\n",
      "Epoch 140, Training Loss 0.08616454694467737\n",
      "Epoch 140, Training Loss 0.086574309936646\n",
      "Epoch 140, Training Loss 0.08680443641970224\n",
      "Epoch 140, Training Loss 0.08695019396674603\n",
      "Epoch 140, Training Loss 0.08727539751840674\n",
      "Epoch 140, Training Loss 0.08752527465219692\n",
      "Epoch 140, Training Loss 0.08772691013410573\n",
      "Epoch 140, Training Loss 0.08785029922795418\n",
      "Epoch 140, Training Loss 0.08824771258722791\n",
      "Epoch 140, Training Loss 0.08850053056617223\n",
      "Epoch 140, Training Loss 0.08869949066082534\n",
      "Epoch 140, Training Loss 0.08885986048280431\n",
      "Epoch 140, Training Loss 0.08902176020814635\n",
      "Epoch 140, Training Loss 0.08915254336488826\n",
      "Epoch 140, Training Loss 0.08947361662717121\n",
      "Epoch 140, Training Loss 0.08955660441418743\n",
      "Epoch 140, Training Loss 0.0897977821471746\n",
      "Epoch 140, Training Loss 0.08997186881197078\n",
      "Epoch 140, Training Loss 0.09007999087538561\n",
      "Epoch 140, Training Loss 0.09027903565131795\n",
      "Epoch 140, Training Loss 0.09040140952257548\n",
      "Epoch 140, Training Loss 0.09055595853558891\n",
      "Epoch 140, Training Loss 0.09077342019399719\n",
      "Epoch 140, Training Loss 0.09097975824990541\n",
      "Epoch 140, Training Loss 0.09110203901748828\n",
      "Epoch 140, Training Loss 0.091275634134517\n",
      "Epoch 140, Training Loss 0.09140039122927829\n",
      "Epoch 140, Training Loss 0.0915249917284607\n",
      "Epoch 140, Training Loss 0.09164841159644639\n",
      "Epoch 140, Training Loss 0.09178051246744592\n",
      "Epoch 140, Training Loss 0.09196152417060664\n",
      "Epoch 140, Training Loss 0.09225160812439821\n",
      "Epoch 140, Training Loss 0.09252182200855916\n",
      "Epoch 140, Training Loss 0.09274755607899803\n",
      "Epoch 140, Training Loss 0.0930364836683816\n",
      "Epoch 140, Training Loss 0.09321700132754453\n",
      "Epoch 140, Training Loss 0.09330085192418769\n",
      "Epoch 140, Training Loss 0.09348628039250288\n",
      "Epoch 140, Training Loss 0.09359737339874973\n",
      "Epoch 140, Training Loss 0.09373937294725566\n",
      "Epoch 140, Training Loss 0.09385782753681893\n",
      "Epoch 140, Training Loss 0.09397798344073698\n",
      "Epoch 140, Training Loss 0.09410320308126147\n",
      "Epoch 140, Training Loss 0.09422698102491286\n",
      "Epoch 140, Training Loss 0.09447620700463615\n",
      "Epoch 140, Training Loss 0.09461982212865444\n",
      "Epoch 140, Training Loss 0.09480337588988302\n",
      "Epoch 140, Training Loss 0.09488495940442586\n",
      "Epoch 140, Training Loss 0.09511777815763908\n",
      "Epoch 140, Training Loss 0.09554243457439306\n",
      "Epoch 140, Training Loss 0.09569646571488942\n",
      "Epoch 140, Training Loss 0.09587974060336342\n",
      "Epoch 140, Training Loss 0.09624084474904762\n",
      "Epoch 140, Training Loss 0.09654877587314457\n",
      "Epoch 140, Training Loss 0.09675279500729897\n",
      "Epoch 140, Training Loss 0.09694999352554835\n",
      "Epoch 140, Training Loss 0.09720908836140048\n",
      "Epoch 140, Training Loss 0.09738990267181336\n",
      "Epoch 140, Training Loss 0.0976648452355886\n",
      "Epoch 140, Training Loss 0.09782965688030128\n",
      "Epoch 140, Training Loss 0.09801487036792518\n",
      "Epoch 140, Training Loss 0.09812876537365986\n",
      "Epoch 140, Training Loss 0.09826994900736967\n",
      "Epoch 140, Training Loss 0.09843470150476222\n",
      "Epoch 140, Training Loss 0.09858095624943829\n",
      "Epoch 140, Training Loss 0.09869856313061531\n",
      "Epoch 140, Training Loss 0.09890912494162464\n",
      "Epoch 140, Training Loss 0.09914606649552465\n",
      "Epoch 140, Training Loss 0.09925270622686656\n",
      "Epoch 140, Training Loss 0.09938660235432407\n",
      "Epoch 140, Training Loss 0.09968979163166812\n",
      "Epoch 140, Training Loss 0.09992242593060979\n",
      "Epoch 140, Training Loss 0.10006696220172946\n",
      "Epoch 140, Training Loss 0.10013295713897861\n",
      "Epoch 140, Training Loss 0.10021222911565505\n",
      "Epoch 140, Training Loss 0.10032814471503658\n",
      "Epoch 140, Training Loss 0.10056668418028471\n",
      "Epoch 140, Training Loss 0.10068516724783441\n",
      "Epoch 140, Training Loss 0.1010904515452702\n",
      "Epoch 140, Training Loss 0.10116744402062405\n",
      "Epoch 140, Training Loss 0.10134030383585206\n",
      "Epoch 140, Training Loss 0.10158951685804388\n",
      "Epoch 140, Training Loss 0.10185057919977418\n",
      "Epoch 140, Training Loss 0.10206103089082119\n",
      "Epoch 140, Training Loss 0.10222462134535813\n",
      "Epoch 140, Training Loss 0.10231318157594985\n",
      "Epoch 140, Training Loss 0.10250799785203793\n",
      "Epoch 140, Training Loss 0.10280766492933416\n",
      "Epoch 140, Training Loss 0.10287651217178158\n",
      "Epoch 140, Training Loss 0.10310363839559085\n",
      "Epoch 140, Training Loss 0.10330709805021353\n",
      "Epoch 140, Training Loss 0.10345186475578629\n",
      "Epoch 140, Training Loss 0.10373478470003361\n",
      "Epoch 140, Training Loss 0.10393009618248629\n",
      "Epoch 140, Training Loss 0.10419503436959765\n",
      "Epoch 140, Training Loss 0.10436029840365547\n",
      "Epoch 140, Training Loss 0.10450147122354306\n",
      "Epoch 140, Training Loss 0.1046698246760975\n",
      "Epoch 140, Training Loss 0.10482492226907207\n",
      "Epoch 140, Training Loss 0.10493459577774605\n",
      "Epoch 140, Training Loss 0.10521919412247817\n",
      "Epoch 140, Training Loss 0.10536097924289343\n",
      "Epoch 140, Training Loss 0.10566297728482567\n",
      "Epoch 140, Training Loss 0.10587973451084645\n",
      "Epoch 140, Training Loss 0.1059753497838593\n",
      "Epoch 140, Training Loss 0.10616057622901466\n",
      "Epoch 140, Training Loss 0.10642256478652777\n",
      "Epoch 140, Training Loss 0.10674375851574304\n",
      "Epoch 140, Training Loss 0.10688200239044474\n",
      "Epoch 140, Training Loss 0.10706483958112767\n",
      "Epoch 140, Training Loss 0.10724045249068981\n",
      "Epoch 140, Training Loss 0.10750328755611196\n",
      "Epoch 140, Training Loss 0.1076664257737453\n",
      "Epoch 140, Training Loss 0.10791090449504077\n",
      "Epoch 140, Training Loss 0.10808498203239934\n",
      "Epoch 140, Training Loss 0.10837270379009302\n",
      "Epoch 140, Training Loss 0.10848595963700501\n",
      "Epoch 140, Training Loss 0.10859468188184454\n",
      "Epoch 140, Training Loss 0.10884888172435486\n",
      "Epoch 140, Training Loss 0.10902522366655909\n",
      "Epoch 140, Training Loss 0.10924147916933917\n",
      "Epoch 140, Training Loss 0.10941208458369803\n",
      "Epoch 140, Training Loss 0.10960022548256475\n",
      "Epoch 140, Training Loss 0.10969925058715026\n",
      "Epoch 140, Training Loss 0.10982862238288688\n",
      "Epoch 140, Training Loss 0.10995811390240326\n",
      "Epoch 140, Training Loss 0.11018653898535634\n",
      "Epoch 140, Training Loss 0.11033338699442194\n",
      "Epoch 140, Training Loss 0.11042973522544669\n",
      "Epoch 140, Training Loss 0.1107233046980389\n",
      "Epoch 140, Training Loss 0.11086704501467745\n",
      "Epoch 140, Training Loss 0.11114188513773329\n",
      "Epoch 140, Training Loss 0.11127146739808037\n",
      "Epoch 140, Training Loss 0.11141074077724039\n",
      "Epoch 140, Training Loss 0.11157579373692156\n",
      "Epoch 140, Training Loss 0.11173467526255208\n",
      "Epoch 140, Training Loss 0.11189532041301965\n",
      "Epoch 140, Training Loss 0.11207043287603904\n",
      "Epoch 140, Training Loss 0.1122369616223342\n",
      "Epoch 140, Training Loss 0.11242841964925798\n",
      "Epoch 140, Training Loss 0.11255411470732878\n",
      "Epoch 140, Training Loss 0.11290484798781554\n",
      "Epoch 140, Training Loss 0.11322799754207549\n",
      "Epoch 140, Training Loss 0.11340807119617834\n",
      "Epoch 140, Training Loss 0.11359279971007648\n",
      "Epoch 140, Training Loss 0.11386154171393807\n",
      "Epoch 140, Training Loss 0.11403902298520746\n",
      "Epoch 140, Training Loss 0.11415885189248015\n",
      "Epoch 140, Training Loss 0.11427604902983474\n",
      "Epoch 140, Training Loss 0.11444246253032056\n",
      "Epoch 140, Training Loss 0.11478019471916244\n",
      "Epoch 140, Training Loss 0.11499901120181737\n",
      "Epoch 140, Training Loss 0.11514495738574763\n",
      "Epoch 140, Training Loss 0.11523838328373859\n",
      "Epoch 140, Training Loss 0.1153976801840012\n",
      "Epoch 140, Training Loss 0.11549178761956484\n",
      "Epoch 140, Training Loss 0.11568136407953242\n",
      "Epoch 140, Training Loss 0.11584623450951656\n",
      "Epoch 140, Training Loss 0.11602759393184539\n",
      "Epoch 140, Training Loss 0.11625129692828107\n",
      "Epoch 140, Training Loss 0.1165185178322789\n",
      "Epoch 140, Training Loss 0.11677585562686328\n",
      "Epoch 140, Training Loss 0.11698901317442012\n",
      "Epoch 140, Training Loss 0.11714258730468695\n",
      "Epoch 140, Training Loss 0.11732163860955659\n",
      "Epoch 140, Training Loss 0.11751064886350918\n",
      "Epoch 140, Training Loss 0.11765270065182767\n",
      "Epoch 140, Training Loss 0.11795971266772894\n",
      "Epoch 140, Training Loss 0.11815489712826278\n",
      "Epoch 140, Training Loss 0.11832337960353136\n",
      "Epoch 140, Training Loss 0.11850326846036917\n",
      "Epoch 140, Training Loss 0.1187506341768424\n",
      "Epoch 140, Training Loss 0.11886438901257484\n",
      "Epoch 140, Training Loss 0.11904026217796766\n",
      "Epoch 140, Training Loss 0.11917187678901588\n",
      "Epoch 140, Training Loss 0.11939536009812751\n",
      "Epoch 140, Training Loss 0.11957899676850232\n",
      "Epoch 140, Training Loss 0.1197582517733888\n",
      "Epoch 140, Training Loss 0.11982774853592029\n",
      "Epoch 140, Training Loss 0.11999457561032241\n",
      "Epoch 140, Training Loss 0.12020070099121774\n",
      "Epoch 140, Training Loss 0.12033194874215614\n",
      "Epoch 140, Training Loss 0.12058027726038338\n",
      "Epoch 140, Training Loss 0.12078966670062231\n",
      "Epoch 140, Training Loss 0.12091323115941509\n",
      "Epoch 140, Training Loss 0.12115359843691902\n",
      "Epoch 140, Training Loss 0.12140282742736284\n",
      "Epoch 140, Training Loss 0.1215960956023782\n",
      "Epoch 140, Training Loss 0.12171477039375574\n",
      "Epoch 140, Training Loss 0.12188521380086079\n",
      "Epoch 140, Training Loss 0.12200809824649635\n",
      "Epoch 140, Training Loss 0.12218171611542591\n",
      "Epoch 140, Training Loss 0.12237885037956335\n",
      "Epoch 140, Training Loss 0.12248282022107287\n",
      "Epoch 140, Training Loss 0.12274705294681632\n",
      "Epoch 140, Training Loss 0.12306046672641774\n",
      "Epoch 140, Training Loss 0.1231237794594158\n",
      "Epoch 140, Training Loss 0.12333950961051542\n",
      "Epoch 140, Training Loss 0.12351279557608735\n",
      "Epoch 140, Training Loss 0.12374287992811112\n",
      "Epoch 140, Training Loss 0.12387015789156527\n",
      "Epoch 140, Training Loss 0.123982291349479\n",
      "Epoch 140, Training Loss 0.12412604436640391\n",
      "Epoch 140, Training Loss 0.1242865391904512\n",
      "Epoch 140, Training Loss 0.12439896818488608\n",
      "Epoch 140, Training Loss 0.12459767806579543\n",
      "Epoch 140, Training Loss 0.1248764224455256\n",
      "Epoch 140, Training Loss 0.1251377735877662\n",
      "Epoch 140, Training Loss 0.1252718265966305\n",
      "Epoch 140, Training Loss 0.12538514783143845\n",
      "Epoch 140, Training Loss 0.12556047548475624\n",
      "Epoch 140, Training Loss 0.12582207905114307\n",
      "Epoch 140, Training Loss 0.12608567885863964\n",
      "Epoch 140, Training Loss 0.1262921164588779\n",
      "Epoch 140, Training Loss 0.12644558732905198\n",
      "Epoch 140, Training Loss 0.12667727447055338\n",
      "Epoch 140, Training Loss 0.1267697032579147\n",
      "Epoch 140, Training Loss 0.12694871450876793\n",
      "Epoch 140, Training Loss 0.12704893372609943\n",
      "Epoch 140, Training Loss 0.12719539535300964\n",
      "Epoch 140, Training Loss 0.12734236175199146\n",
      "Epoch 140, Training Loss 0.12752942115430485\n",
      "Epoch 140, Training Loss 0.1277179274746143\n",
      "Epoch 140, Training Loss 0.12787352088371964\n",
      "Epoch 140, Training Loss 0.12803979236584948\n",
      "Epoch 140, Training Loss 0.12822918402855202\n",
      "Epoch 140, Training Loss 0.12835437459085147\n",
      "Epoch 140, Training Loss 0.12856427646334975\n",
      "Epoch 140, Training Loss 0.1287730802803317\n",
      "Epoch 140, Training Loss 0.12898641264023225\n",
      "Epoch 140, Training Loss 0.12916077893999073\n",
      "Epoch 140, Training Loss 0.12932587702236975\n",
      "Epoch 140, Training Loss 0.12945812345599123\n",
      "Epoch 140, Training Loss 0.1295703904050619\n",
      "Epoch 140, Training Loss 0.12989327928427694\n",
      "Epoch 140, Training Loss 0.13008325409306132\n",
      "Epoch 140, Training Loss 0.13019498939271015\n",
      "Epoch 140, Training Loss 0.1303138768114626\n",
      "Epoch 140, Training Loss 0.1304289277814462\n",
      "Epoch 140, Training Loss 0.1306661204256289\n",
      "Epoch 140, Training Loss 0.1307455315719099\n",
      "Epoch 140, Training Loss 0.13092025972978996\n",
      "Epoch 140, Training Loss 0.1312947062837422\n",
      "Epoch 140, Training Loss 0.13147506256447272\n",
      "Epoch 140, Training Loss 0.1316286457269012\n",
      "Epoch 140, Training Loss 0.13181390530903778\n",
      "Epoch 140, Training Loss 0.13202760427180307\n",
      "Epoch 140, Training Loss 0.1322464247179382\n",
      "Epoch 140, Training Loss 0.13241682372644276\n",
      "Epoch 140, Training Loss 0.13271927415772014\n",
      "Epoch 140, Training Loss 0.13280618460872745\n",
      "Epoch 140, Training Loss 0.13302890978791676\n",
      "Epoch 140, Training Loss 0.13313414678072838\n",
      "Epoch 140, Training Loss 0.1332368844562708\n",
      "Epoch 140, Training Loss 0.13337909773735288\n",
      "Epoch 140, Training Loss 0.13358647637831433\n",
      "Epoch 140, Training Loss 0.13397412414155196\n",
      "Epoch 140, Training Loss 0.13418041873256417\n",
      "Epoch 140, Training Loss 0.13436138382672197\n",
      "Epoch 140, Training Loss 0.13467372803832106\n",
      "Epoch 140, Training Loss 0.13480270125657853\n",
      "Epoch 140, Training Loss 0.13489174425049358\n",
      "Epoch 140, Training Loss 0.13507598268387416\n",
      "Epoch 140, Training Loss 0.1353644228985776\n",
      "Epoch 140, Training Loss 0.13563631577393437\n",
      "Epoch 140, Training Loss 0.13576574457328186\n",
      "Epoch 140, Training Loss 0.13588012440982836\n",
      "Epoch 140, Training Loss 0.13600585465331363\n",
      "Epoch 140, Training Loss 0.13605398166438806\n",
      "Epoch 140, Training Loss 0.136261566234824\n",
      "Epoch 140, Training Loss 0.13640816498290548\n",
      "Epoch 140, Training Loss 0.13675843720393413\n",
      "Epoch 140, Training Loss 0.13695733350179995\n",
      "Epoch 140, Training Loss 0.13718803939612015\n",
      "Epoch 140, Training Loss 0.1375457224867228\n",
      "Epoch 140, Training Loss 0.13777326154129585\n",
      "Epoch 140, Training Loss 0.13801629326837447\n",
      "Epoch 140, Training Loss 0.13823451883042864\n",
      "Epoch 140, Training Loss 0.13850876854737396\n",
      "Epoch 140, Training Loss 0.1386672270096019\n",
      "Epoch 140, Training Loss 0.13885552162671333\n",
      "Epoch 140, Training Loss 0.13902742405186225\n",
      "Epoch 140, Training Loss 0.1391343894174032\n",
      "Epoch 140, Training Loss 0.13922529815293638\n",
      "Epoch 140, Training Loss 0.1394240998036569\n",
      "Epoch 140, Training Loss 0.1395416586371639\n",
      "Epoch 140, Training Loss 0.13959135816854132\n",
      "Epoch 150, Training Loss 0.00016386281041538015\n",
      "Epoch 150, Training Loss 0.00041945187179633726\n",
      "Epoch 150, Training Loss 0.0005663197554285874\n",
      "Epoch 150, Training Loss 0.0006223191671511706\n",
      "Epoch 150, Training Loss 0.0008152206630810448\n",
      "Epoch 150, Training Loss 0.0009547849959882019\n",
      "Epoch 150, Training Loss 0.0011254813036193018\n",
      "Epoch 150, Training Loss 0.0012939955438951703\n",
      "Epoch 150, Training Loss 0.0014434569346173036\n",
      "Epoch 150, Training Loss 0.0015607942038637294\n",
      "Epoch 150, Training Loss 0.0016950610691629103\n",
      "Epoch 150, Training Loss 0.001970388528788486\n",
      "Epoch 150, Training Loss 0.002105655887014116\n",
      "Epoch 150, Training Loss 0.002237570445860743\n",
      "Epoch 150, Training Loss 0.0023858319120029048\n",
      "Epoch 150, Training Loss 0.002467231086605345\n",
      "Epoch 150, Training Loss 0.002614112661394012\n",
      "Epoch 150, Training Loss 0.002696307742839579\n",
      "Epoch 150, Training Loss 0.002802538587842756\n",
      "Epoch 150, Training Loss 0.002978578181294224\n",
      "Epoch 150, Training Loss 0.003027387091990017\n",
      "Epoch 150, Training Loss 0.0030907795090428397\n",
      "Epoch 150, Training Loss 0.003309151777030562\n",
      "Epoch 150, Training Loss 0.0034400453872006874\n",
      "Epoch 150, Training Loss 0.0036054465043194152\n",
      "Epoch 150, Training Loss 0.003783309489221829\n",
      "Epoch 150, Training Loss 0.003943862755547094\n",
      "Epoch 150, Training Loss 0.004120232061008968\n",
      "Epoch 150, Training Loss 0.004313299794445562\n",
      "Epoch 150, Training Loss 0.004378448881189842\n",
      "Epoch 150, Training Loss 0.004429848097703036\n",
      "Epoch 150, Training Loss 0.004533794844318229\n",
      "Epoch 150, Training Loss 0.0046234197933655566\n",
      "Epoch 150, Training Loss 0.004726293151412169\n",
      "Epoch 150, Training Loss 0.005103302400206666\n",
      "Epoch 150, Training Loss 0.005227004932930403\n",
      "Epoch 150, Training Loss 0.005370048248706876\n",
      "Epoch 150, Training Loss 0.005532049252401533\n",
      "Epoch 150, Training Loss 0.005633347784466756\n",
      "Epoch 150, Training Loss 0.005761056747811529\n",
      "Epoch 150, Training Loss 0.00591069929625677\n",
      "Epoch 150, Training Loss 0.006008860219241408\n",
      "Epoch 150, Training Loss 0.006137594742619473\n",
      "Epoch 150, Training Loss 0.0062448076160667495\n",
      "Epoch 150, Training Loss 0.006324612833273686\n",
      "Epoch 150, Training Loss 0.006445474274780439\n",
      "Epoch 150, Training Loss 0.006517336091688831\n",
      "Epoch 150, Training Loss 0.006698330016354161\n",
      "Epoch 150, Training Loss 0.006770466208991492\n",
      "Epoch 150, Training Loss 0.006864987685323676\n",
      "Epoch 150, Training Loss 0.0071463072791581265\n",
      "Epoch 150, Training Loss 0.007432368891242215\n",
      "Epoch 150, Training Loss 0.0076044322374989\n",
      "Epoch 150, Training Loss 0.007724337022551491\n",
      "Epoch 150, Training Loss 0.007800975218986916\n",
      "Epoch 150, Training Loss 0.007908694455614481\n",
      "Epoch 150, Training Loss 0.008033513927551183\n",
      "Epoch 150, Training Loss 0.008131742067730336\n",
      "Epoch 150, Training Loss 0.008326656039794692\n",
      "Epoch 150, Training Loss 0.008405273522981597\n",
      "Epoch 150, Training Loss 0.008490803234679314\n",
      "Epoch 150, Training Loss 0.008617038402677801\n",
      "Epoch 150, Training Loss 0.0086846947384155\n",
      "Epoch 150, Training Loss 0.00886912530531054\n",
      "Epoch 150, Training Loss 0.008984064573750776\n",
      "Epoch 150, Training Loss 0.009027964995264093\n",
      "Epoch 150, Training Loss 0.009265111906982749\n",
      "Epoch 150, Training Loss 0.009525629251128268\n",
      "Epoch 150, Training Loss 0.009658271251508341\n",
      "Epoch 150, Training Loss 0.00984133759994641\n",
      "Epoch 150, Training Loss 0.009914899216321727\n",
      "Epoch 150, Training Loss 0.0100384413614831\n",
      "Epoch 150, Training Loss 0.010159928823256736\n",
      "Epoch 150, Training Loss 0.010260504782390411\n",
      "Epoch 150, Training Loss 0.010358886431207132\n",
      "Epoch 150, Training Loss 0.010424493414247433\n",
      "Epoch 150, Training Loss 0.010587789732819933\n",
      "Epoch 150, Training Loss 0.010859140411705312\n",
      "Epoch 150, Training Loss 0.010958856333742667\n",
      "Epoch 150, Training Loss 0.011289933724971989\n",
      "Epoch 150, Training Loss 0.01136857156863298\n",
      "Epoch 150, Training Loss 0.01149937265631183\n",
      "Epoch 150, Training Loss 0.011691857034059438\n",
      "Epoch 150, Training Loss 0.011756323163619126\n",
      "Epoch 150, Training Loss 0.011893339693317634\n",
      "Epoch 150, Training Loss 0.012018813149017446\n",
      "Epoch 150, Training Loss 0.012156949076048858\n",
      "Epoch 150, Training Loss 0.012348160261998092\n",
      "Epoch 150, Training Loss 0.012540711047094496\n",
      "Epoch 150, Training Loss 0.01260400577293485\n",
      "Epoch 150, Training Loss 0.012780228867898208\n",
      "Epoch 150, Training Loss 0.01291515712943071\n",
      "Epoch 150, Training Loss 0.013102162243498255\n",
      "Epoch 150, Training Loss 0.013292355424798358\n",
      "Epoch 150, Training Loss 0.013359468111106197\n",
      "Epoch 150, Training Loss 0.013646310605013461\n",
      "Epoch 150, Training Loss 0.013783252324022905\n",
      "Epoch 150, Training Loss 0.013859634766417087\n",
      "Epoch 150, Training Loss 0.014096164861527245\n",
      "Epoch 150, Training Loss 0.014212086777705366\n",
      "Epoch 150, Training Loss 0.014308981444982006\n",
      "Epoch 150, Training Loss 0.014460257690428469\n",
      "Epoch 150, Training Loss 0.014567240929740774\n",
      "Epoch 150, Training Loss 0.014666382070926144\n",
      "Epoch 150, Training Loss 0.014849974285534886\n",
      "Epoch 150, Training Loss 0.014950513449090217\n",
      "Epoch 150, Training Loss 0.015060064042239543\n",
      "Epoch 150, Training Loss 0.015144909081785271\n",
      "Epoch 150, Training Loss 0.015319157067848288\n",
      "Epoch 150, Training Loss 0.015383746031948062\n",
      "Epoch 150, Training Loss 0.015589659500038227\n",
      "Epoch 150, Training Loss 0.015762964844741784\n",
      "Epoch 150, Training Loss 0.015922830390084124\n",
      "Epoch 150, Training Loss 0.016108202569358184\n",
      "Epoch 150, Training Loss 0.016268284010041094\n",
      "Epoch 150, Training Loss 0.016483935882406466\n",
      "Epoch 150, Training Loss 0.016568546790791595\n",
      "Epoch 150, Training Loss 0.016671111407068076\n",
      "Epoch 150, Training Loss 0.016802452366484706\n",
      "Epoch 150, Training Loss 0.016889853012340757\n",
      "Epoch 150, Training Loss 0.01697583728567566\n",
      "Epoch 150, Training Loss 0.017145248377681388\n",
      "Epoch 150, Training Loss 0.017244601250647584\n",
      "Epoch 150, Training Loss 0.0174373008377488\n",
      "Epoch 150, Training Loss 0.01750074698568305\n",
      "Epoch 150, Training Loss 0.017669336551138202\n",
      "Epoch 150, Training Loss 0.017787303661217776\n",
      "Epoch 150, Training Loss 0.017915748993454078\n",
      "Epoch 150, Training Loss 0.01815105867965142\n",
      "Epoch 150, Training Loss 0.0183131643725783\n",
      "Epoch 150, Training Loss 0.018547566879130995\n",
      "Epoch 150, Training Loss 0.01872784326143582\n",
      "Epoch 150, Training Loss 0.018814066239177724\n",
      "Epoch 150, Training Loss 0.01893261711463294\n",
      "Epoch 150, Training Loss 0.019108794153193987\n",
      "Epoch 150, Training Loss 0.019199234674043972\n",
      "Epoch 150, Training Loss 0.01933030270592636\n",
      "Epoch 150, Training Loss 0.019446833883328817\n",
      "Epoch 150, Training Loss 0.019561374231296426\n",
      "Epoch 150, Training Loss 0.01970463567187109\n",
      "Epoch 150, Training Loss 0.01984782246372584\n",
      "Epoch 150, Training Loss 0.01993297257691698\n",
      "Epoch 150, Training Loss 0.01998054556300878\n",
      "Epoch 150, Training Loss 0.02017588690494942\n",
      "Epoch 150, Training Loss 0.020455673718086594\n",
      "Epoch 150, Training Loss 0.020528484936184286\n",
      "Epoch 150, Training Loss 0.020643713395766285\n",
      "Epoch 150, Training Loss 0.020780327536946978\n",
      "Epoch 150, Training Loss 0.020935256386656897\n",
      "Epoch 150, Training Loss 0.02100600555653462\n",
      "Epoch 150, Training Loss 0.02118678932147258\n",
      "Epoch 150, Training Loss 0.02128308249251617\n",
      "Epoch 150, Training Loss 0.021414063747048073\n",
      "Epoch 150, Training Loss 0.02154113202715469\n",
      "Epoch 150, Training Loss 0.021641649563065576\n",
      "Epoch 150, Training Loss 0.021767349937535308\n",
      "Epoch 150, Training Loss 0.022024088503454654\n",
      "Epoch 150, Training Loss 0.02219036271047714\n",
      "Epoch 150, Training Loss 0.02232296098871609\n",
      "Epoch 150, Training Loss 0.022528966779218\n",
      "Epoch 150, Training Loss 0.0226132795214653\n",
      "Epoch 150, Training Loss 0.022900989903208546\n",
      "Epoch 150, Training Loss 0.023174343911735604\n",
      "Epoch 150, Training Loss 0.023405927183378077\n",
      "Epoch 150, Training Loss 0.02348063637018966\n",
      "Epoch 150, Training Loss 0.023593720329730103\n",
      "Epoch 150, Training Loss 0.023732721610256777\n",
      "Epoch 150, Training Loss 0.02386799640000781\n",
      "Epoch 150, Training Loss 0.02396714393421055\n",
      "Epoch 150, Training Loss 0.02410036058205625\n",
      "Epoch 150, Training Loss 0.02444128786949703\n",
      "Epoch 150, Training Loss 0.02457158220336413\n",
      "Epoch 150, Training Loss 0.02473701138878265\n",
      "Epoch 150, Training Loss 0.02481374010215025\n",
      "Epoch 150, Training Loss 0.02492186338509745\n",
      "Epoch 150, Training Loss 0.025030912007288554\n",
      "Epoch 150, Training Loss 0.02516603872865972\n",
      "Epoch 150, Training Loss 0.025236129027117245\n",
      "Epoch 150, Training Loss 0.02533075711725618\n",
      "Epoch 150, Training Loss 0.025405322180112915\n",
      "Epoch 150, Training Loss 0.025626783352107038\n",
      "Epoch 150, Training Loss 0.025729527039562956\n",
      "Epoch 150, Training Loss 0.025845104714145747\n",
      "Epoch 150, Training Loss 0.026038208271345824\n",
      "Epoch 150, Training Loss 0.026160213300753434\n",
      "Epoch 150, Training Loss 0.026287690200426084\n",
      "Epoch 150, Training Loss 0.02654249716045149\n",
      "Epoch 150, Training Loss 0.026772005931305153\n",
      "Epoch 150, Training Loss 0.026885876422533598\n",
      "Epoch 150, Training Loss 0.0269838677161871\n",
      "Epoch 150, Training Loss 0.027076567315956212\n",
      "Epoch 150, Training Loss 0.027194958632749976\n",
      "Epoch 150, Training Loss 0.027340246621719407\n",
      "Epoch 150, Training Loss 0.027463780620785624\n",
      "Epoch 150, Training Loss 0.02768870877087726\n",
      "Epoch 150, Training Loss 0.027816623473144553\n",
      "Epoch 150, Training Loss 0.027972190662303848\n",
      "Epoch 150, Training Loss 0.028103215981970357\n",
      "Epoch 150, Training Loss 0.02817726396310055\n",
      "Epoch 150, Training Loss 0.028312345009173272\n",
      "Epoch 150, Training Loss 0.028414382892267782\n",
      "Epoch 150, Training Loss 0.028470863990695275\n",
      "Epoch 150, Training Loss 0.028665635620465365\n",
      "Epoch 150, Training Loss 0.028870782364741006\n",
      "Epoch 150, Training Loss 0.029003862320157267\n",
      "Epoch 150, Training Loss 0.029217862354977357\n",
      "Epoch 150, Training Loss 0.02932952200550862\n",
      "Epoch 150, Training Loss 0.029558282307422985\n",
      "Epoch 150, Training Loss 0.02966171098144158\n",
      "Epoch 150, Training Loss 0.029717326878815356\n",
      "Epoch 150, Training Loss 0.029865014154816526\n",
      "Epoch 150, Training Loss 0.03002253528255636\n",
      "Epoch 150, Training Loss 0.030227124376598833\n",
      "Epoch 150, Training Loss 0.030458713860234337\n",
      "Epoch 150, Training Loss 0.030526213681377718\n",
      "Epoch 150, Training Loss 0.030597995718955384\n",
      "Epoch 150, Training Loss 0.030781491190347524\n",
      "Epoch 150, Training Loss 0.031031039602997357\n",
      "Epoch 150, Training Loss 0.03117414637256766\n",
      "Epoch 150, Training Loss 0.031399433255729164\n",
      "Epoch 150, Training Loss 0.0315724910925264\n",
      "Epoch 150, Training Loss 0.03165416259442449\n",
      "Epoch 150, Training Loss 0.03180254636632512\n",
      "Epoch 150, Training Loss 0.03190349341582154\n",
      "Epoch 150, Training Loss 0.03198397469223308\n",
      "Epoch 150, Training Loss 0.03208527685431263\n",
      "Epoch 150, Training Loss 0.032194091440619105\n",
      "Epoch 150, Training Loss 0.03235618644358252\n",
      "Epoch 150, Training Loss 0.032471101321375276\n",
      "Epoch 150, Training Loss 0.032634955175849786\n",
      "Epoch 150, Training Loss 0.032808505627505306\n",
      "Epoch 150, Training Loss 0.03293759612095021\n",
      "Epoch 150, Training Loss 0.033043154422431956\n",
      "Epoch 150, Training Loss 0.03312737012610716\n",
      "Epoch 150, Training Loss 0.03332798609800656\n",
      "Epoch 150, Training Loss 0.03348407856262554\n",
      "Epoch 150, Training Loss 0.033629506683486805\n",
      "Epoch 150, Training Loss 0.03372862949357618\n",
      "Epoch 150, Training Loss 0.033821176242111894\n",
      "Epoch 150, Training Loss 0.034091193895891804\n",
      "Epoch 150, Training Loss 0.03417366187628883\n",
      "Epoch 150, Training Loss 0.034229687122089784\n",
      "Epoch 150, Training Loss 0.03436017407537879\n",
      "Epoch 150, Training Loss 0.03459722847413377\n",
      "Epoch 150, Training Loss 0.03474082782045197\n",
      "Epoch 150, Training Loss 0.03489820115611224\n",
      "Epoch 150, Training Loss 0.03500442502691465\n",
      "Epoch 150, Training Loss 0.03520387262963425\n",
      "Epoch 150, Training Loss 0.03525227939952975\n",
      "Epoch 150, Training Loss 0.03536506876578111\n",
      "Epoch 150, Training Loss 0.03581450280287991\n",
      "Epoch 150, Training Loss 0.03600795960525417\n",
      "Epoch 150, Training Loss 0.036302075466460276\n",
      "Epoch 150, Training Loss 0.03658525917269385\n",
      "Epoch 150, Training Loss 0.036814865696689354\n",
      "Epoch 150, Training Loss 0.03697857579878529\n",
      "Epoch 150, Training Loss 0.03711049364465277\n",
      "Epoch 150, Training Loss 0.037301101409794424\n",
      "Epoch 150, Training Loss 0.037396165882916096\n",
      "Epoch 150, Training Loss 0.037522355680499234\n",
      "Epoch 150, Training Loss 0.037596841554736235\n",
      "Epoch 150, Training Loss 0.03784528989201921\n",
      "Epoch 150, Training Loss 0.038094372538578176\n",
      "Epoch 150, Training Loss 0.03824753582934894\n",
      "Epoch 150, Training Loss 0.0383106244589819\n",
      "Epoch 150, Training Loss 0.03855476445516052\n",
      "Epoch 150, Training Loss 0.038609980422136425\n",
      "Epoch 150, Training Loss 0.038823218858989\n",
      "Epoch 150, Training Loss 0.039000572022193536\n",
      "Epoch 150, Training Loss 0.03913376732822269\n",
      "Epoch 150, Training Loss 0.039296446087034156\n",
      "Epoch 150, Training Loss 0.039353363778051514\n",
      "Epoch 150, Training Loss 0.03944541563463333\n",
      "Epoch 150, Training Loss 0.03948939298196217\n",
      "Epoch 150, Training Loss 0.039597188296449155\n",
      "Epoch 150, Training Loss 0.03966058857376923\n",
      "Epoch 150, Training Loss 0.03976290071826152\n",
      "Epoch 150, Training Loss 0.03988411479517627\n",
      "Epoch 150, Training Loss 0.040110078918964356\n",
      "Epoch 150, Training Loss 0.040282580382226374\n",
      "Epoch 150, Training Loss 0.040513754138708724\n",
      "Epoch 150, Training Loss 0.040658011336994294\n",
      "Epoch 150, Training Loss 0.04080617767961129\n",
      "Epoch 150, Training Loss 0.0409284534642611\n",
      "Epoch 150, Training Loss 0.0410367762360274\n",
      "Epoch 150, Training Loss 0.04116017041761247\n",
      "Epoch 150, Training Loss 0.041361419879414545\n",
      "Epoch 150, Training Loss 0.04147878914233059\n",
      "Epoch 150, Training Loss 0.04157517713201625\n",
      "Epoch 150, Training Loss 0.041706137463945865\n",
      "Epoch 150, Training Loss 0.041758727401380646\n",
      "Epoch 150, Training Loss 0.04194356775020852\n",
      "Epoch 150, Training Loss 0.042015092790393574\n",
      "Epoch 150, Training Loss 0.042111366315532826\n",
      "Epoch 150, Training Loss 0.042253949114924194\n",
      "Epoch 150, Training Loss 0.042364329171112124\n",
      "Epoch 150, Training Loss 0.04267024840978558\n",
      "Epoch 150, Training Loss 0.042828433391878674\n",
      "Epoch 150, Training Loss 0.04294040634789888\n",
      "Epoch 150, Training Loss 0.04309419383916556\n",
      "Epoch 150, Training Loss 0.04318768311472957\n",
      "Epoch 150, Training Loss 0.043351820472370635\n",
      "Epoch 150, Training Loss 0.04355395047465706\n",
      "Epoch 150, Training Loss 0.043790923393405304\n",
      "Epoch 150, Training Loss 0.043935629141414564\n",
      "Epoch 150, Training Loss 0.04414330689174592\n",
      "Epoch 150, Training Loss 0.044248537891699226\n",
      "Epoch 150, Training Loss 0.044425725007948974\n",
      "Epoch 150, Training Loss 0.044625077753916116\n",
      "Epoch 150, Training Loss 0.04481509050159046\n",
      "Epoch 150, Training Loss 0.04492629629552669\n",
      "Epoch 150, Training Loss 0.045119100419418585\n",
      "Epoch 150, Training Loss 0.04529118718450789\n",
      "Epoch 150, Training Loss 0.04551939317084792\n",
      "Epoch 150, Training Loss 0.04567488961760193\n",
      "Epoch 150, Training Loss 0.046022470102019014\n",
      "Epoch 150, Training Loss 0.04638581578631688\n",
      "Epoch 150, Training Loss 0.0464861022494257\n",
      "Epoch 150, Training Loss 0.046750294406662514\n",
      "Epoch 150, Training Loss 0.046927760300390864\n",
      "Epoch 150, Training Loss 0.04721431205015811\n",
      "Epoch 150, Training Loss 0.047459340456615935\n",
      "Epoch 150, Training Loss 0.04765343257819143\n",
      "Epoch 150, Training Loss 0.04779468761171069\n",
      "Epoch 150, Training Loss 0.048091800721443215\n",
      "Epoch 150, Training Loss 0.04826842340381097\n",
      "Epoch 150, Training Loss 0.048467919654439175\n",
      "Epoch 150, Training Loss 0.04860596252543389\n",
      "Epoch 150, Training Loss 0.048765856177186415\n",
      "Epoch 150, Training Loss 0.048841464426130285\n",
      "Epoch 150, Training Loss 0.04906804978733172\n",
      "Epoch 150, Training Loss 0.049245951378055854\n",
      "Epoch 150, Training Loss 0.04953699453216989\n",
      "Epoch 150, Training Loss 0.049619989798349494\n",
      "Epoch 150, Training Loss 0.04976059873695569\n",
      "Epoch 150, Training Loss 0.049915329288796086\n",
      "Epoch 150, Training Loss 0.05000294282879976\n",
      "Epoch 150, Training Loss 0.05020186075431002\n",
      "Epoch 150, Training Loss 0.05029076394979911\n",
      "Epoch 150, Training Loss 0.050556680582978235\n",
      "Epoch 150, Training Loss 0.05065991275984308\n",
      "Epoch 150, Training Loss 0.05076176589331054\n",
      "Epoch 150, Training Loss 0.05091185207523958\n",
      "Epoch 150, Training Loss 0.05101096459552455\n",
      "Epoch 150, Training Loss 0.051239138226146286\n",
      "Epoch 150, Training Loss 0.05133064246505423\n",
      "Epoch 150, Training Loss 0.05152426977329852\n",
      "Epoch 150, Training Loss 0.05162488849228605\n",
      "Epoch 150, Training Loss 0.05179399723553901\n",
      "Epoch 150, Training Loss 0.05205938769766437\n",
      "Epoch 150, Training Loss 0.052375658022244566\n",
      "Epoch 150, Training Loss 0.05262056243655932\n",
      "Epoch 150, Training Loss 0.05282700784942683\n",
      "Epoch 150, Training Loss 0.05308379268135561\n",
      "Epoch 150, Training Loss 0.053201010484067376\n",
      "Epoch 150, Training Loss 0.053401076694583646\n",
      "Epoch 150, Training Loss 0.053698818930579574\n",
      "Epoch 150, Training Loss 0.053788409122954246\n",
      "Epoch 150, Training Loss 0.05401156054776343\n",
      "Epoch 150, Training Loss 0.054137337533637996\n",
      "Epoch 150, Training Loss 0.05422619934124715\n",
      "Epoch 150, Training Loss 0.05447900188548486\n",
      "Epoch 150, Training Loss 0.05458233077698352\n",
      "Epoch 150, Training Loss 0.05480571665689159\n",
      "Epoch 150, Training Loss 0.05490911948254042\n",
      "Epoch 150, Training Loss 0.055066271091970945\n",
      "Epoch 150, Training Loss 0.05517662322277303\n",
      "Epoch 150, Training Loss 0.05523484161176035\n",
      "Epoch 150, Training Loss 0.055415419275727115\n",
      "Epoch 150, Training Loss 0.05548270663146473\n",
      "Epoch 150, Training Loss 0.05561444012786421\n",
      "Epoch 150, Training Loss 0.05576914341172294\n",
      "Epoch 150, Training Loss 0.055894724243437235\n",
      "Epoch 150, Training Loss 0.056034319864971864\n",
      "Epoch 150, Training Loss 0.056117899191882606\n",
      "Epoch 150, Training Loss 0.0561782050534816\n",
      "Epoch 150, Training Loss 0.05621565804790581\n",
      "Epoch 150, Training Loss 0.05633288882720425\n",
      "Epoch 150, Training Loss 0.056449788512990755\n",
      "Epoch 150, Training Loss 0.056531748996423485\n",
      "Epoch 150, Training Loss 0.056622132313583054\n",
      "Epoch 150, Training Loss 0.05673942356453756\n",
      "Epoch 150, Training Loss 0.0569501562244104\n",
      "Epoch 150, Training Loss 0.0571168788954082\n",
      "Epoch 150, Training Loss 0.05724820299097873\n",
      "Epoch 150, Training Loss 0.05732016644828841\n",
      "Epoch 150, Training Loss 0.057477056868183794\n",
      "Epoch 150, Training Loss 0.05753699897090569\n",
      "Epoch 150, Training Loss 0.0577215194783133\n",
      "Epoch 150, Training Loss 0.057861845313912\n",
      "Epoch 150, Training Loss 0.0579806309493492\n",
      "Epoch 150, Training Loss 0.05809364905414145\n",
      "Epoch 150, Training Loss 0.05818782657470621\n",
      "Epoch 150, Training Loss 0.05839976824848624\n",
      "Epoch 150, Training Loss 0.05861405548318039\n",
      "Epoch 150, Training Loss 0.058766069364688735\n",
      "Epoch 150, Training Loss 0.059007780833641436\n",
      "Epoch 150, Training Loss 0.05926336629359085\n",
      "Epoch 150, Training Loss 0.059420491101891945\n",
      "Epoch 150, Training Loss 0.059564125100079246\n",
      "Epoch 150, Training Loss 0.05964113843134221\n",
      "Epoch 150, Training Loss 0.059859680080943556\n",
      "Epoch 150, Training Loss 0.06001436035565632\n",
      "Epoch 150, Training Loss 0.060156135872254136\n",
      "Epoch 150, Training Loss 0.060273457344745276\n",
      "Epoch 150, Training Loss 0.06039857608802102\n",
      "Epoch 150, Training Loss 0.06064797935726316\n",
      "Epoch 150, Training Loss 0.060923915172991394\n",
      "Epoch 150, Training Loss 0.06103924647345186\n",
      "Epoch 150, Training Loss 0.06125965118741669\n",
      "Epoch 150, Training Loss 0.06156798248962902\n",
      "Epoch 150, Training Loss 0.061775353908672205\n",
      "Epoch 150, Training Loss 0.06214458616140782\n",
      "Epoch 150, Training Loss 0.062437650061610256\n",
      "Epoch 150, Training Loss 0.06279129851990572\n",
      "Epoch 150, Training Loss 0.06305061523442912\n",
      "Epoch 150, Training Loss 0.06314301317143242\n",
      "Epoch 150, Training Loss 0.06325710605582237\n",
      "Epoch 150, Training Loss 0.06337196686450401\n",
      "Epoch 150, Training Loss 0.06351917390199498\n",
      "Epoch 150, Training Loss 0.06363114389493262\n",
      "Epoch 150, Training Loss 0.06395137724240342\n",
      "Epoch 150, Training Loss 0.06424954379944468\n",
      "Epoch 150, Training Loss 0.06436388843152148\n",
      "Epoch 150, Training Loss 0.06459799381044438\n",
      "Epoch 150, Training Loss 0.06478439683756788\n",
      "Epoch 150, Training Loss 0.0650269065285697\n",
      "Epoch 150, Training Loss 0.06513484395668863\n",
      "Epoch 150, Training Loss 0.06535397791791983\n",
      "Epoch 150, Training Loss 0.06547861938576793\n",
      "Epoch 150, Training Loss 0.06559547647366971\n",
      "Epoch 150, Training Loss 0.06573980681531494\n",
      "Epoch 150, Training Loss 0.06591134766817017\n",
      "Epoch 150, Training Loss 0.06605993537827755\n",
      "Epoch 150, Training Loss 0.06618496862090076\n",
      "Epoch 150, Training Loss 0.06632115640213042\n",
      "Epoch 150, Training Loss 0.06645338013148902\n",
      "Epoch 150, Training Loss 0.06669672699335515\n",
      "Epoch 150, Training Loss 0.06678712651695666\n",
      "Epoch 150, Training Loss 0.06691611263558002\n",
      "Epoch 150, Training Loss 0.06710999143426605\n",
      "Epoch 150, Training Loss 0.06724652600095933\n",
      "Epoch 150, Training Loss 0.06734051190249984\n",
      "Epoch 150, Training Loss 0.06739254291300349\n",
      "Epoch 150, Training Loss 0.06753894079076436\n",
      "Epoch 150, Training Loss 0.06762357763326762\n",
      "Epoch 150, Training Loss 0.06787029023298903\n",
      "Epoch 150, Training Loss 0.0679758942669348\n",
      "Epoch 150, Training Loss 0.0682345918591713\n",
      "Epoch 150, Training Loss 0.06840324282045941\n",
      "Epoch 150, Training Loss 0.06850378022140935\n",
      "Epoch 150, Training Loss 0.06874574819231963\n",
      "Epoch 150, Training Loss 0.06887320878074678\n",
      "Epoch 150, Training Loss 0.0691224063491768\n",
      "Epoch 150, Training Loss 0.06932017442715518\n",
      "Epoch 150, Training Loss 0.06952529100705977\n",
      "Epoch 150, Training Loss 0.0696951314125715\n",
      "Epoch 150, Training Loss 0.06991328946207567\n",
      "Epoch 150, Training Loss 0.07019887377967692\n",
      "Epoch 150, Training Loss 0.07040927413603305\n",
      "Epoch 150, Training Loss 0.07049633861016816\n",
      "Epoch 150, Training Loss 0.07067881994511542\n",
      "Epoch 150, Training Loss 0.07083092274768349\n",
      "Epoch 150, Training Loss 0.07101945162934187\n",
      "Epoch 150, Training Loss 0.07120423551763186\n",
      "Epoch 150, Training Loss 0.0713770658163654\n",
      "Epoch 150, Training Loss 0.07151236009005162\n",
      "Epoch 150, Training Loss 0.07172940365846275\n",
      "Epoch 150, Training Loss 0.07182278874022957\n",
      "Epoch 150, Training Loss 0.07204205406796368\n",
      "Epoch 150, Training Loss 0.07214509986598244\n",
      "Epoch 150, Training Loss 0.07247716869892137\n",
      "Epoch 150, Training Loss 0.07268842899709788\n",
      "Epoch 150, Training Loss 0.07282241902854818\n",
      "Epoch 150, Training Loss 0.07290655307119231\n",
      "Epoch 150, Training Loss 0.07308035853850034\n",
      "Epoch 150, Training Loss 0.07315740368717238\n",
      "Epoch 150, Training Loss 0.07332496461518051\n",
      "Epoch 150, Training Loss 0.07348216270022762\n",
      "Epoch 150, Training Loss 0.07359999821514196\n",
      "Epoch 150, Training Loss 0.07370013025496393\n",
      "Epoch 150, Training Loss 0.07382037707120942\n",
      "Epoch 150, Training Loss 0.07397642841472117\n",
      "Epoch 150, Training Loss 0.0741498271298721\n",
      "Epoch 150, Training Loss 0.07436592611329407\n",
      "Epoch 150, Training Loss 0.074590560298918\n",
      "Epoch 150, Training Loss 0.07477567910128619\n",
      "Epoch 150, Training Loss 0.07488403527680641\n",
      "Epoch 150, Training Loss 0.07503437602659092\n",
      "Epoch 150, Training Loss 0.07508113233091505\n",
      "Epoch 150, Training Loss 0.07523567366468556\n",
      "Epoch 150, Training Loss 0.07554586591613491\n",
      "Epoch 150, Training Loss 0.0756791993723158\n",
      "Epoch 150, Training Loss 0.07582716414909763\n",
      "Epoch 150, Training Loss 0.07589731569571034\n",
      "Epoch 150, Training Loss 0.07622568206647244\n",
      "Epoch 150, Training Loss 0.07638226462113659\n",
      "Epoch 150, Training Loss 0.07643994647304496\n",
      "Epoch 150, Training Loss 0.07666803001071257\n",
      "Epoch 150, Training Loss 0.07673644110836718\n",
      "Epoch 150, Training Loss 0.07691469680174919\n",
      "Epoch 150, Training Loss 0.07702596203836105\n",
      "Epoch 150, Training Loss 0.07716619581832072\n",
      "Epoch 150, Training Loss 0.07744221703942551\n",
      "Epoch 150, Training Loss 0.07762546666071314\n",
      "Epoch 150, Training Loss 0.07776962576286338\n",
      "Epoch 150, Training Loss 0.07799094966124467\n",
      "Epoch 150, Training Loss 0.07812320190670012\n",
      "Epoch 150, Training Loss 0.07823595684974471\n",
      "Epoch 150, Training Loss 0.07839898136503937\n",
      "Epoch 150, Training Loss 0.07853454504104908\n",
      "Epoch 150, Training Loss 0.07876561877682157\n",
      "Epoch 150, Training Loss 0.0789477507157437\n",
      "Epoch 150, Training Loss 0.07905518027532207\n",
      "Epoch 150, Training Loss 0.07932600402094596\n",
      "Epoch 150, Training Loss 0.07951554974488666\n",
      "Epoch 150, Training Loss 0.0796540703105233\n",
      "Epoch 150, Training Loss 0.07974067322976525\n",
      "Epoch 150, Training Loss 0.0800302260200424\n",
      "Epoch 150, Training Loss 0.0803644086836892\n",
      "Epoch 150, Training Loss 0.08057820276997964\n",
      "Epoch 150, Training Loss 0.08076129528119817\n",
      "Epoch 150, Training Loss 0.08092711986187855\n",
      "Epoch 150, Training Loss 0.08099636420502764\n",
      "Epoch 150, Training Loss 0.08112104650815506\n",
      "Epoch 150, Training Loss 0.08128574972881762\n",
      "Epoch 150, Training Loss 0.08137650207837906\n",
      "Epoch 150, Training Loss 0.08154204075493852\n",
      "Epoch 150, Training Loss 0.08174095799446182\n",
      "Epoch 150, Training Loss 0.08185062280444004\n",
      "Epoch 150, Training Loss 0.08207280738779422\n",
      "Epoch 150, Training Loss 0.08240645867831947\n",
      "Epoch 150, Training Loss 0.08255553130498704\n",
      "Epoch 150, Training Loss 0.08271697338175057\n",
      "Epoch 150, Training Loss 0.0828103484309581\n",
      "Epoch 150, Training Loss 0.08303162913121608\n",
      "Epoch 150, Training Loss 0.08324220248967257\n",
      "Epoch 150, Training Loss 0.08339227758147909\n",
      "Epoch 150, Training Loss 0.08363787985413962\n",
      "Epoch 150, Training Loss 0.08393745754352387\n",
      "Epoch 150, Training Loss 0.08401485481787749\n",
      "Epoch 150, Training Loss 0.08410297317759079\n",
      "Epoch 150, Training Loss 0.08420690307941507\n",
      "Epoch 150, Training Loss 0.08434113470213417\n",
      "Epoch 150, Training Loss 0.08459599021479221\n",
      "Epoch 150, Training Loss 0.08471015031637667\n",
      "Epoch 150, Training Loss 0.08487698531773923\n",
      "Epoch 150, Training Loss 0.08505362339670319\n",
      "Epoch 150, Training Loss 0.08521981467552425\n",
      "Epoch 150, Training Loss 0.08530547414356104\n",
      "Epoch 150, Training Loss 0.08548443874730097\n",
      "Epoch 150, Training Loss 0.08569904248875654\n",
      "Epoch 150, Training Loss 0.08588390071135577\n",
      "Epoch 150, Training Loss 0.08601127418896655\n",
      "Epoch 150, Training Loss 0.08612308679553478\n",
      "Epoch 150, Training Loss 0.0862454595520635\n",
      "Epoch 150, Training Loss 0.08641515912064125\n",
      "Epoch 150, Training Loss 0.08659288344090171\n",
      "Epoch 150, Training Loss 0.08670463223162743\n",
      "Epoch 150, Training Loss 0.08678771715249171\n",
      "Epoch 150, Training Loss 0.0868968110212394\n",
      "Epoch 150, Training Loss 0.08701627208705982\n",
      "Epoch 150, Training Loss 0.08710356081223777\n",
      "Epoch 150, Training Loss 0.08729180552379782\n",
      "Epoch 150, Training Loss 0.08737980942849231\n",
      "Epoch 150, Training Loss 0.0876146175629457\n",
      "Epoch 150, Training Loss 0.08787301509305263\n",
      "Epoch 150, Training Loss 0.08803050118781951\n",
      "Epoch 150, Training Loss 0.08815847392267812\n",
      "Epoch 150, Training Loss 0.08834868320561659\n",
      "Epoch 150, Training Loss 0.08845851818799896\n",
      "Epoch 150, Training Loss 0.08858689674131019\n",
      "Epoch 150, Training Loss 0.08872257351941998\n",
      "Epoch 150, Training Loss 0.08890247283755895\n",
      "Epoch 150, Training Loss 0.08913199702406402\n",
      "Epoch 150, Training Loss 0.08930471630724114\n",
      "Epoch 150, Training Loss 0.08949828486832435\n",
      "Epoch 150, Training Loss 0.08965637693729471\n",
      "Epoch 150, Training Loss 0.08976087853302965\n",
      "Epoch 150, Training Loss 0.08983200308545243\n",
      "Epoch 150, Training Loss 0.0899827844294174\n",
      "Epoch 150, Training Loss 0.09008798325467673\n",
      "Epoch 150, Training Loss 0.09025789963324433\n",
      "Epoch 150, Training Loss 0.09036054280455537\n",
      "Epoch 150, Training Loss 0.09046565713908743\n",
      "Epoch 150, Training Loss 0.09075643321561158\n",
      "Epoch 150, Training Loss 0.09095463163607642\n",
      "Epoch 150, Training Loss 0.09100578605528455\n",
      "Epoch 150, Training Loss 0.09121482173585907\n",
      "Epoch 150, Training Loss 0.09145351512895902\n",
      "Epoch 150, Training Loss 0.09155429613150065\n",
      "Epoch 150, Training Loss 0.09170881943905826\n",
      "Epoch 150, Training Loss 0.09193621952410626\n",
      "Epoch 150, Training Loss 0.09199765229554814\n",
      "Epoch 150, Training Loss 0.09217995293009693\n",
      "Epoch 150, Training Loss 0.09225314995869423\n",
      "Epoch 150, Training Loss 0.09237703566899157\n",
      "Epoch 150, Training Loss 0.09246196977251098\n",
      "Epoch 150, Training Loss 0.0926324536528467\n",
      "Epoch 150, Training Loss 0.09273149609765814\n",
      "Epoch 150, Training Loss 0.09278932810925386\n",
      "Epoch 150, Training Loss 0.09296359311160453\n",
      "Epoch 150, Training Loss 0.09311359263051425\n",
      "Epoch 150, Training Loss 0.09328726433036501\n",
      "Epoch 150, Training Loss 0.09352359076356873\n",
      "Epoch 150, Training Loss 0.09368306818320547\n",
      "Epoch 150, Training Loss 0.0938503409204695\n",
      "Epoch 150, Training Loss 0.09404656341266068\n",
      "Epoch 150, Training Loss 0.0942102655496858\n",
      "Epoch 150, Training Loss 0.09432159338260779\n",
      "Epoch 150, Training Loss 0.09451628002621558\n",
      "Epoch 150, Training Loss 0.09463928789233841\n",
      "Epoch 150, Training Loss 0.09469632532142694\n",
      "Epoch 150, Training Loss 0.09481413763664338\n",
      "Epoch 150, Training Loss 0.0949900583256884\n",
      "Epoch 150, Training Loss 0.09516291259585516\n",
      "Epoch 150, Training Loss 0.09526101266607985\n",
      "Epoch 150, Training Loss 0.09534624497384747\n",
      "Epoch 150, Training Loss 0.09545295761988672\n",
      "Epoch 150, Training Loss 0.09576549402216587\n",
      "Epoch 150, Training Loss 0.09589451597408032\n",
      "Epoch 150, Training Loss 0.09617181332148325\n",
      "Epoch 150, Training Loss 0.09630657272541995\n",
      "Epoch 150, Training Loss 0.09643414287168123\n",
      "Epoch 150, Training Loss 0.09659509410571945\n",
      "Epoch 150, Training Loss 0.09666399497900853\n",
      "Epoch 150, Training Loss 0.09685882653736169\n",
      "Epoch 150, Training Loss 0.09711515995176018\n",
      "Epoch 150, Training Loss 0.09736688248574962\n",
      "Epoch 150, Training Loss 0.09755219241170704\n",
      "Epoch 150, Training Loss 0.09775674789238845\n",
      "Epoch 150, Training Loss 0.09800438264913647\n",
      "Epoch 150, Training Loss 0.09818795784745755\n",
      "Epoch 150, Training Loss 0.09839213834098919\n",
      "Epoch 150, Training Loss 0.0986033814323261\n",
      "Epoch 150, Training Loss 0.09876925923416148\n",
      "Epoch 150, Training Loss 0.09902635211234584\n",
      "Epoch 150, Training Loss 0.09920096704426705\n",
      "Epoch 150, Training Loss 0.09923622952273968\n",
      "Epoch 150, Training Loss 0.09944885457291856\n",
      "Epoch 150, Training Loss 0.09959022170814026\n",
      "Epoch 150, Training Loss 0.09975652280204056\n",
      "Epoch 150, Training Loss 0.0999080260710605\n",
      "Epoch 150, Training Loss 0.10020124178398829\n",
      "Epoch 150, Training Loss 0.10031171298116598\n",
      "Epoch 150, Training Loss 0.10043261825914501\n",
      "Epoch 150, Training Loss 0.10061630879855141\n",
      "Epoch 150, Training Loss 0.10099079456809155\n",
      "Epoch 150, Training Loss 0.10119403640279913\n",
      "Epoch 150, Training Loss 0.10139197138521601\n",
      "Epoch 150, Training Loss 0.10155096043930258\n",
      "Epoch 150, Training Loss 0.10175666641539244\n",
      "Epoch 150, Training Loss 0.10183755646381155\n",
      "Epoch 150, Training Loss 0.10194532207125212\n",
      "Epoch 150, Training Loss 0.10206946109176215\n",
      "Epoch 150, Training Loss 0.10246280159401086\n",
      "Epoch 150, Training Loss 0.10262490271369133\n",
      "Epoch 150, Training Loss 0.10281337850758106\n",
      "Epoch 150, Training Loss 0.10300277001784204\n",
      "Epoch 150, Training Loss 0.10308885073903805\n",
      "Epoch 150, Training Loss 0.10316078495615355\n",
      "Epoch 150, Training Loss 0.10336650762459278\n",
      "Epoch 150, Training Loss 0.10339690038405569\n",
      "Epoch 150, Training Loss 0.1035721286835954\n",
      "Epoch 150, Training Loss 0.10386564525897088\n",
      "Epoch 150, Training Loss 0.1040578602725054\n",
      "Epoch 150, Training Loss 0.10426596697429409\n",
      "Epoch 150, Training Loss 0.1044000002133953\n",
      "Epoch 150, Training Loss 0.10447641868439629\n",
      "Epoch 150, Training Loss 0.10466118176441516\n",
      "Epoch 150, Training Loss 0.10478899512163665\n",
      "Epoch 150, Training Loss 0.10507253336879756\n",
      "Epoch 150, Training Loss 0.10530313329718759\n",
      "Epoch 150, Training Loss 0.1054366029408353\n",
      "Epoch 150, Training Loss 0.10551973495660993\n",
      "Epoch 150, Training Loss 0.10571455826406437\n",
      "Epoch 150, Training Loss 0.10588162438586697\n",
      "Epoch 150, Training Loss 0.10612301559418516\n",
      "Epoch 150, Training Loss 0.10623479731228498\n",
      "Epoch 150, Training Loss 0.10643736339267106\n",
      "Epoch 150, Training Loss 0.10659271341931942\n",
      "Epoch 150, Training Loss 0.10671780605698028\n",
      "Epoch 150, Training Loss 0.1069117329700295\n",
      "Epoch 150, Training Loss 0.10711670046210137\n",
      "Epoch 150, Training Loss 0.10737412360489673\n",
      "Epoch 150, Training Loss 0.10746122639902565\n",
      "Epoch 150, Training Loss 0.10759729037866415\n",
      "Epoch 150, Training Loss 0.10773085100133248\n",
      "Epoch 150, Training Loss 0.10794208673736476\n",
      "Epoch 150, Training Loss 0.10818213950890257\n",
      "Epoch 150, Training Loss 0.10835937887449246\n",
      "Epoch 150, Training Loss 0.10849647378772878\n",
      "Epoch 150, Training Loss 0.10863757297834929\n",
      "Epoch 150, Training Loss 0.10893368790083377\n",
      "Epoch 150, Training Loss 0.10904210382391272\n",
      "Epoch 150, Training Loss 0.10916738596547138\n",
      "Epoch 150, Training Loss 0.10925328341381782\n",
      "Epoch 150, Training Loss 0.10947421212654437\n",
      "Epoch 150, Training Loss 0.10965999835611456\n",
      "Epoch 150, Training Loss 0.10989293104032878\n",
      "Epoch 150, Training Loss 0.1100325292914801\n",
      "Epoch 150, Training Loss 0.1102240568317492\n",
      "Epoch 150, Training Loss 0.11036475427220087\n",
      "Epoch 150, Training Loss 0.11044131902039356\n",
      "Epoch 150, Training Loss 0.11053022080579834\n",
      "Epoch 150, Training Loss 0.11065734466990394\n",
      "Epoch 150, Training Loss 0.11076311144472846\n",
      "Epoch 150, Training Loss 0.11109617271024705\n",
      "Epoch 150, Training Loss 0.11139747868661228\n",
      "Epoch 150, Training Loss 0.11148569302733445\n",
      "Epoch 150, Training Loss 0.11176459255444882\n",
      "Epoch 150, Training Loss 0.1119076049150637\n",
      "Epoch 150, Training Loss 0.11205618705152703\n",
      "Epoch 150, Training Loss 0.11219229134242706\n",
      "Epoch 150, Training Loss 0.1123696092725791\n",
      "Epoch 150, Training Loss 0.11260605445774773\n",
      "Epoch 150, Training Loss 0.11286606514335745\n",
      "Epoch 150, Training Loss 0.11303622929660408\n",
      "Epoch 150, Training Loss 0.11315441663708072\n",
      "Epoch 150, Training Loss 0.11341231492112207\n",
      "Epoch 150, Training Loss 0.11360831021824304\n",
      "Epoch 150, Training Loss 0.11380576167512885\n",
      "Epoch 150, Training Loss 0.11393272436088156\n",
      "Epoch 150, Training Loss 0.11405074827449249\n",
      "Epoch 150, Training Loss 0.11418058971405182\n",
      "Epoch 150, Training Loss 0.11442022555795929\n",
      "Epoch 150, Training Loss 0.11449887172397598\n",
      "Epoch 150, Training Loss 0.11467491497602457\n",
      "Epoch 150, Training Loss 0.11471449412272104\n",
      "Epoch 150, Training Loss 0.11486396445985646\n",
      "Epoch 150, Training Loss 0.11499324282798011\n",
      "Epoch 150, Training Loss 0.11520610530586804\n",
      "Epoch 150, Training Loss 0.11537591078321037\n",
      "Epoch 150, Training Loss 0.1154509902410114\n",
      "Epoch 150, Training Loss 0.11565152778173499\n",
      "Epoch 150, Training Loss 0.11581268795596822\n",
      "Epoch 150, Training Loss 0.11599768516714767\n",
      "Epoch 150, Training Loss 0.11619281718778945\n",
      "Epoch 150, Training Loss 0.116436796529633\n",
      "Epoch 150, Training Loss 0.11652448051668646\n",
      "Epoch 150, Training Loss 0.11665680454305523\n",
      "Epoch 150, Training Loss 0.11696557123738024\n",
      "Epoch 150, Training Loss 0.11716890600426576\n",
      "Epoch 150, Training Loss 0.11737031116605262\n",
      "Epoch 150, Training Loss 0.11769343484336\n",
      "Epoch 150, Training Loss 0.11802893582150302\n",
      "Epoch 150, Training Loss 0.11822045775954529\n",
      "Epoch 150, Training Loss 0.11843571576106426\n",
      "Epoch 150, Training Loss 0.11869769120860435\n",
      "Epoch 150, Training Loss 0.11909728448199647\n",
      "Epoch 150, Training Loss 0.11921571205129557\n",
      "Epoch 150, Training Loss 0.11944870407338185\n",
      "Epoch 150, Training Loss 0.11962479223852115\n",
      "Epoch 150, Training Loss 0.11991055173051479\n",
      "Epoch 150, Training Loss 0.12001005069012075\n",
      "Epoch 150, Training Loss 0.12011257355646862\n",
      "Epoch 150, Training Loss 0.12023916112168519\n",
      "Epoch 150, Training Loss 0.12039145159885249\n",
      "Epoch 150, Training Loss 0.12048301914864032\n",
      "Epoch 150, Training Loss 0.12068603820908252\n",
      "Epoch 150, Training Loss 0.12086044730680526\n",
      "Epoch 150, Training Loss 0.12097044538258744\n",
      "Epoch 150, Training Loss 0.1210929935231157\n",
      "Epoch 150, Training Loss 0.12122950751972777\n",
      "Epoch 150, Training Loss 0.12149836105363601\n",
      "Epoch 150, Training Loss 0.12163022431590216\n",
      "Epoch 150, Training Loss 0.1218953520831321\n",
      "Epoch 150, Training Loss 0.12211439364573078\n",
      "Epoch 150, Training Loss 0.12238602959991568\n",
      "Epoch 150, Training Loss 0.12253475394528693\n",
      "Epoch 150, Training Loss 0.12268256186924474\n",
      "Epoch 150, Training Loss 0.12300012497436208\n",
      "Epoch 150, Training Loss 0.12315554786806979\n",
      "Epoch 150, Training Loss 0.12326427299977111\n",
      "Epoch 150, Training Loss 0.1233577743563277\n",
      "Epoch 150, Training Loss 0.12349568589892991\n",
      "Epoch 150, Training Loss 0.12369723065906321\n",
      "Epoch 150, Training Loss 0.12392858688807701\n",
      "Epoch 150, Training Loss 0.12400110095948971\n",
      "Epoch 150, Training Loss 0.12422279824438455\n",
      "Epoch 150, Training Loss 0.12432960431803675\n",
      "Epoch 150, Training Loss 0.12434929915849129\n",
      "Epoch 160, Training Loss 8.429891770453099e-05\n",
      "Epoch 160, Training Loss 0.0002767684895669103\n",
      "Epoch 160, Training Loss 0.00039406654322543717\n",
      "Epoch 160, Training Loss 0.000453646111366389\n",
      "Epoch 160, Training Loss 0.0005970845746872066\n",
      "Epoch 160, Training Loss 0.0006628427559228809\n",
      "Epoch 160, Training Loss 0.0007093382899261191\n",
      "Epoch 160, Training Loss 0.0007772778334748714\n",
      "Epoch 160, Training Loss 0.0009439045996845835\n",
      "Epoch 160, Training Loss 0.0010236458676626614\n",
      "Epoch 160, Training Loss 0.0011151816238603933\n",
      "Epoch 160, Training Loss 0.00123812357806946\n",
      "Epoch 160, Training Loss 0.0012897180531488356\n",
      "Epoch 160, Training Loss 0.0014278495784305855\n",
      "Epoch 160, Training Loss 0.0015051938412363266\n",
      "Epoch 160, Training Loss 0.0016554670403604312\n",
      "Epoch 160, Training Loss 0.001803244190181003\n",
      "Epoch 160, Training Loss 0.0020792486236604585\n",
      "Epoch 160, Training Loss 0.0021514505138406363\n",
      "Epoch 160, Training Loss 0.0023427063270526774\n",
      "Epoch 160, Training Loss 0.0025293236869908964\n",
      "Epoch 160, Training Loss 0.002666617317310989\n",
      "Epoch 160, Training Loss 0.0028089628414348566\n",
      "Epoch 160, Training Loss 0.0029081852149094463\n",
      "Epoch 160, Training Loss 0.0029870756303943943\n",
      "Epoch 160, Training Loss 0.003117902099590777\n",
      "Epoch 160, Training Loss 0.0032328626526819775\n",
      "Epoch 160, Training Loss 0.0033281734713431817\n",
      "Epoch 160, Training Loss 0.0034433512155281005\n",
      "Epoch 160, Training Loss 0.003612097338451754\n",
      "Epoch 160, Training Loss 0.003685434565634069\n",
      "Epoch 160, Training Loss 0.0037917273304880123\n",
      "Epoch 160, Training Loss 0.0038961408245365334\n",
      "Epoch 160, Training Loss 0.003990532449253684\n",
      "Epoch 160, Training Loss 0.004063331214782527\n",
      "Epoch 160, Training Loss 0.004204176075737495\n",
      "Epoch 160, Training Loss 0.004298697714038822\n",
      "Epoch 160, Training Loss 0.004336955656995402\n",
      "Epoch 160, Training Loss 0.004424140121682983\n",
      "Epoch 160, Training Loss 0.004574379231066197\n",
      "Epoch 160, Training Loss 0.004692672775662921\n",
      "Epoch 160, Training Loss 0.004767433101849635\n",
      "Epoch 160, Training Loss 0.004828345776080628\n",
      "Epoch 160, Training Loss 0.0049552330223228925\n",
      "Epoch 160, Training Loss 0.005059866670547696\n",
      "Epoch 160, Training Loss 0.005206024071768574\n",
      "Epoch 160, Training Loss 0.005357683631484313\n",
      "Epoch 160, Training Loss 0.005496261076873068\n",
      "Epoch 160, Training Loss 0.005593925269077654\n",
      "Epoch 160, Training Loss 0.0059081438447698914\n",
      "Epoch 160, Training Loss 0.006112975054098974\n",
      "Epoch 160, Training Loss 0.0061803254777627525\n",
      "Epoch 160, Training Loss 0.0062798905017240276\n",
      "Epoch 160, Training Loss 0.006379394634814976\n",
      "Epoch 160, Training Loss 0.006477132684948957\n",
      "Epoch 160, Training Loss 0.006625995380551461\n",
      "Epoch 160, Training Loss 0.006724689029576376\n",
      "Epoch 160, Training Loss 0.006788611166712725\n",
      "Epoch 160, Training Loss 0.006927331926210609\n",
      "Epoch 160, Training Loss 0.00709079345330939\n",
      "Epoch 160, Training Loss 0.0071717031857432305\n",
      "Epoch 160, Training Loss 0.007276128065269774\n",
      "Epoch 160, Training Loss 0.007405517725250148\n",
      "Epoch 160, Training Loss 0.007508250960932516\n",
      "Epoch 160, Training Loss 0.0075913312513848094\n",
      "Epoch 160, Training Loss 0.007657751357635421\n",
      "Epoch 160, Training Loss 0.007787638262886068\n",
      "Epoch 160, Training Loss 0.007896267642240848\n",
      "Epoch 160, Training Loss 0.007994422543784388\n",
      "Epoch 160, Training Loss 0.008054828669046958\n",
      "Epoch 160, Training Loss 0.008146594974028943\n",
      "Epoch 160, Training Loss 0.008294766766431235\n",
      "Epoch 160, Training Loss 0.008414203003334725\n",
      "Epoch 160, Training Loss 0.008516625086765003\n",
      "Epoch 160, Training Loss 0.008559094344163338\n",
      "Epoch 160, Training Loss 0.00872991106156117\n",
      "Epoch 160, Training Loss 0.008877499774574776\n",
      "Epoch 160, Training Loss 0.008992298415688145\n",
      "Epoch 160, Training Loss 0.009070671301649506\n",
      "Epoch 160, Training Loss 0.00918761648171012\n",
      "Epoch 160, Training Loss 0.00925667805697226\n",
      "Epoch 160, Training Loss 0.009291932870493368\n",
      "Epoch 160, Training Loss 0.009343117444544954\n",
      "Epoch 160, Training Loss 0.009432227817623665\n",
      "Epoch 160, Training Loss 0.009585942784824487\n",
      "Epoch 160, Training Loss 0.009694810887641462\n",
      "Epoch 160, Training Loss 0.009752694405424778\n",
      "Epoch 160, Training Loss 0.00981482241035956\n",
      "Epoch 160, Training Loss 0.009994201159671597\n",
      "Epoch 160, Training Loss 0.010068184519877366\n",
      "Epoch 160, Training Loss 0.010190973108362816\n",
      "Epoch 160, Training Loss 0.010289204735642352\n",
      "Epoch 160, Training Loss 0.010525121863293069\n",
      "Epoch 160, Training Loss 0.010590350919920008\n",
      "Epoch 160, Training Loss 0.010721091078618145\n",
      "Epoch 160, Training Loss 0.010824743746434483\n",
      "Epoch 160, Training Loss 0.0108651854550404\n",
      "Epoch 160, Training Loss 0.010959857743700294\n",
      "Epoch 160, Training Loss 0.011018674723003679\n",
      "Epoch 160, Training Loss 0.011077440548640535\n",
      "Epoch 160, Training Loss 0.011220213690358203\n",
      "Epoch 160, Training Loss 0.011360515849879179\n",
      "Epoch 160, Training Loss 0.011586840879029173\n",
      "Epoch 160, Training Loss 0.011790932327642313\n",
      "Epoch 160, Training Loss 0.012048239710138124\n",
      "Epoch 160, Training Loss 0.012176160824477977\n",
      "Epoch 160, Training Loss 0.012219406666753389\n",
      "Epoch 160, Training Loss 0.012456164471423992\n",
      "Epoch 160, Training Loss 0.012710791417037891\n",
      "Epoch 160, Training Loss 0.013007677160203457\n",
      "Epoch 160, Training Loss 0.013151156744631508\n",
      "Epoch 160, Training Loss 0.013201717575511817\n",
      "Epoch 160, Training Loss 0.013404971148218493\n",
      "Epoch 160, Training Loss 0.013577426269250301\n",
      "Epoch 160, Training Loss 0.013772314211920551\n",
      "Epoch 160, Training Loss 0.01382656278484084\n",
      "Epoch 160, Training Loss 0.013952433331714718\n",
      "Epoch 160, Training Loss 0.014065063787181207\n",
      "Epoch 160, Training Loss 0.014188564077610403\n",
      "Epoch 160, Training Loss 0.01443779828917721\n",
      "Epoch 160, Training Loss 0.014623566454424121\n",
      "Epoch 160, Training Loss 0.014712557017974689\n",
      "Epoch 160, Training Loss 0.014885804758828771\n",
      "Epoch 160, Training Loss 0.015030084794763561\n",
      "Epoch 160, Training Loss 0.015228712757396729\n",
      "Epoch 160, Training Loss 0.015366051996322086\n",
      "Epoch 160, Training Loss 0.015536903565668541\n",
      "Epoch 160, Training Loss 0.01572569483733924\n",
      "Epoch 160, Training Loss 0.01587131803574236\n",
      "Epoch 160, Training Loss 0.016016269433776587\n",
      "Epoch 160, Training Loss 0.01619966478918291\n",
      "Epoch 160, Training Loss 0.016363442632491173\n",
      "Epoch 160, Training Loss 0.016465861038269136\n",
      "Epoch 160, Training Loss 0.01666577549322563\n",
      "Epoch 160, Training Loss 0.01674428826336132\n",
      "Epoch 160, Training Loss 0.01682152746059477\n",
      "Epoch 160, Training Loss 0.017195235435253062\n",
      "Epoch 160, Training Loss 0.017302823896088716\n",
      "Epoch 160, Training Loss 0.01736610763184631\n",
      "Epoch 160, Training Loss 0.01744209588898341\n",
      "Epoch 160, Training Loss 0.01767004647856707\n",
      "Epoch 160, Training Loss 0.01785569834996901\n",
      "Epoch 160, Training Loss 0.018015610961638905\n",
      "Epoch 160, Training Loss 0.01827017426290704\n",
      "Epoch 160, Training Loss 0.01840845016938875\n",
      "Epoch 160, Training Loss 0.01850158352252391\n",
      "Epoch 160, Training Loss 0.01867821091152442\n",
      "Epoch 160, Training Loss 0.018838531018503944\n",
      "Epoch 160, Training Loss 0.019013067462083783\n",
      "Epoch 160, Training Loss 0.01914860828138907\n",
      "Epoch 160, Training Loss 0.01937242770505607\n",
      "Epoch 160, Training Loss 0.019467976084812676\n",
      "Epoch 160, Training Loss 0.019572103760488655\n",
      "Epoch 160, Training Loss 0.01971317406343606\n",
      "Epoch 160, Training Loss 0.01979022088420132\n",
      "Epoch 160, Training Loss 0.020004439443978658\n",
      "Epoch 160, Training Loss 0.02009781404538914\n",
      "Epoch 160, Training Loss 0.020216570764570438\n",
      "Epoch 160, Training Loss 0.020330606902594608\n",
      "Epoch 160, Training Loss 0.020410000870618825\n",
      "Epoch 160, Training Loss 0.020588847751374286\n",
      "Epoch 160, Training Loss 0.02062654668879707\n",
      "Epoch 160, Training Loss 0.020769883244943894\n",
      "Epoch 160, Training Loss 0.020843076200493613\n",
      "Epoch 160, Training Loss 0.020997081797503297\n",
      "Epoch 160, Training Loss 0.021193020900859096\n",
      "Epoch 160, Training Loss 0.021319846341581752\n",
      "Epoch 160, Training Loss 0.021514900845697012\n",
      "Epoch 160, Training Loss 0.021725063736233716\n",
      "Epoch 160, Training Loss 0.0218044489293414\n",
      "Epoch 160, Training Loss 0.021936913039964027\n",
      "Epoch 160, Training Loss 0.02212714825702064\n",
      "Epoch 160, Training Loss 0.022314964912717453\n",
      "Epoch 160, Training Loss 0.022399212857779793\n",
      "Epoch 160, Training Loss 0.022601109281506226\n",
      "Epoch 160, Training Loss 0.022813054309000293\n",
      "Epoch 160, Training Loss 0.022918354412612252\n",
      "Epoch 160, Training Loss 0.023035979586774887\n",
      "Epoch 160, Training Loss 0.023201042969646812\n",
      "Epoch 160, Training Loss 0.023284108635238217\n",
      "Epoch 160, Training Loss 0.023511915739216006\n",
      "Epoch 160, Training Loss 0.023661760415624625\n",
      "Epoch 160, Training Loss 0.02374579998262017\n",
      "Epoch 160, Training Loss 0.023878789768861534\n",
      "Epoch 160, Training Loss 0.024031403888960173\n",
      "Epoch 160, Training Loss 0.024264170488109218\n",
      "Epoch 160, Training Loss 0.024433144339648508\n",
      "Epoch 160, Training Loss 0.024484264797738294\n",
      "Epoch 160, Training Loss 0.02456587074977129\n",
      "Epoch 160, Training Loss 0.024615619600752887\n",
      "Epoch 160, Training Loss 0.024688245654773074\n",
      "Epoch 160, Training Loss 0.02474962671518402\n",
      "Epoch 160, Training Loss 0.024988925872880326\n",
      "Epoch 160, Training Loss 0.025104939854225083\n",
      "Epoch 160, Training Loss 0.025245026728171673\n",
      "Epoch 160, Training Loss 0.025420745432167255\n",
      "Epoch 160, Training Loss 0.025536067690938483\n",
      "Epoch 160, Training Loss 0.025649219763267526\n",
      "Epoch 160, Training Loss 0.025810430700535816\n",
      "Epoch 160, Training Loss 0.025923546501304334\n",
      "Epoch 160, Training Loss 0.026030887730533966\n",
      "Epoch 160, Training Loss 0.026210971102785426\n",
      "Epoch 160, Training Loss 0.026374196898087364\n",
      "Epoch 160, Training Loss 0.026525742136170646\n",
      "Epoch 160, Training Loss 0.02670661677532565\n",
      "Epoch 160, Training Loss 0.026832500946662768\n",
      "Epoch 160, Training Loss 0.0269874449261962\n",
      "Epoch 160, Training Loss 0.02711295972213797\n",
      "Epoch 160, Training Loss 0.02725598593587842\n",
      "Epoch 160, Training Loss 0.027316155166977354\n",
      "Epoch 160, Training Loss 0.02744785222984717\n",
      "Epoch 160, Training Loss 0.02765777769267483\n",
      "Epoch 160, Training Loss 0.027772851138258985\n",
      "Epoch 160, Training Loss 0.027959496528386613\n",
      "Epoch 160, Training Loss 0.028150847770006912\n",
      "Epoch 160, Training Loss 0.028257637189420134\n",
      "Epoch 160, Training Loss 0.028456828480734088\n",
      "Epoch 160, Training Loss 0.028575963054871773\n",
      "Epoch 160, Training Loss 0.028705946255541976\n",
      "Epoch 160, Training Loss 0.028817880615268066\n",
      "Epoch 160, Training Loss 0.02886004461323285\n",
      "Epoch 160, Training Loss 0.029016218757938087\n",
      "Epoch 160, Training Loss 0.029094465003084498\n",
      "Epoch 160, Training Loss 0.029211945958016322\n",
      "Epoch 160, Training Loss 0.029442401600482365\n",
      "Epoch 160, Training Loss 0.02962707561652755\n",
      "Epoch 160, Training Loss 0.029720760541289207\n",
      "Epoch 160, Training Loss 0.029846335570697134\n",
      "Epoch 160, Training Loss 0.02994340805151045\n",
      "Epoch 160, Training Loss 0.030132768520861483\n",
      "Epoch 160, Training Loss 0.03018085504917766\n",
      "Epoch 160, Training Loss 0.030314554317432747\n",
      "Epoch 160, Training Loss 0.030482726783760825\n",
      "Epoch 160, Training Loss 0.03055976806899242\n",
      "Epoch 160, Training Loss 0.030714894854523184\n",
      "Epoch 160, Training Loss 0.030934314243495464\n",
      "Epoch 160, Training Loss 0.031069496758472738\n",
      "Epoch 160, Training Loss 0.031203166043381098\n",
      "Epoch 160, Training Loss 0.031427182802630356\n",
      "Epoch 160, Training Loss 0.0315462568193636\n",
      "Epoch 160, Training Loss 0.03165552650085267\n",
      "Epoch 160, Training Loss 0.03184133914092084\n",
      "Epoch 160, Training Loss 0.03201131500980205\n",
      "Epoch 160, Training Loss 0.0320838381038488\n",
      "Epoch 160, Training Loss 0.03218978213960939\n",
      "Epoch 160, Training Loss 0.03234459654382809\n",
      "Epoch 160, Training Loss 0.03240519335441044\n",
      "Epoch 160, Training Loss 0.032506423373528\n",
      "Epoch 160, Training Loss 0.03267875530273485\n",
      "Epoch 160, Training Loss 0.03279807395004022\n",
      "Epoch 160, Training Loss 0.032977527452875736\n",
      "Epoch 160, Training Loss 0.03311180887634263\n",
      "Epoch 160, Training Loss 0.03317419879014611\n",
      "Epoch 160, Training Loss 0.033293262498019754\n",
      "Epoch 160, Training Loss 0.03336349076084088\n",
      "Epoch 160, Training Loss 0.03354941374953369\n",
      "Epoch 160, Training Loss 0.03371072745145968\n",
      "Epoch 160, Training Loss 0.033981125103905226\n",
      "Epoch 160, Training Loss 0.034120448026567926\n",
      "Epoch 160, Training Loss 0.03438262782200142\n",
      "Epoch 160, Training Loss 0.03466830585304352\n",
      "Epoch 160, Training Loss 0.0348563733322503\n",
      "Epoch 160, Training Loss 0.034944083596415376\n",
      "Epoch 160, Training Loss 0.03517254865716409\n",
      "Epoch 160, Training Loss 0.03524150691993173\n",
      "Epoch 160, Training Loss 0.03533892629101225\n",
      "Epoch 160, Training Loss 0.0355046398942466\n",
      "Epoch 160, Training Loss 0.03555149022165848\n",
      "Epoch 160, Training Loss 0.035722465725982436\n",
      "Epoch 160, Training Loss 0.03598754353049542\n",
      "Epoch 160, Training Loss 0.0361144310149276\n",
      "Epoch 160, Training Loss 0.036453127167890294\n",
      "Epoch 160, Training Loss 0.03654993875452396\n",
      "Epoch 160, Training Loss 0.03676943015307188\n",
      "Epoch 160, Training Loss 0.036931707609511547\n",
      "Epoch 160, Training Loss 0.03698286772622248\n",
      "Epoch 160, Training Loss 0.03716534942321841\n",
      "Epoch 160, Training Loss 0.03726226846208734\n",
      "Epoch 160, Training Loss 0.03735601754568498\n",
      "Epoch 160, Training Loss 0.03748376093223653\n",
      "Epoch 160, Training Loss 0.03763881390747588\n",
      "Epoch 160, Training Loss 0.03769474997973579\n",
      "Epoch 160, Training Loss 0.0378372147512596\n",
      "Epoch 160, Training Loss 0.03797467620066746\n",
      "Epoch 160, Training Loss 0.03812972896629968\n",
      "Epoch 160, Training Loss 0.03827895009008896\n",
      "Epoch 160, Training Loss 0.038485458042101024\n",
      "Epoch 160, Training Loss 0.03856960873898414\n",
      "Epoch 160, Training Loss 0.03862676406731767\n",
      "Epoch 160, Training Loss 0.03877091828181082\n",
      "Epoch 160, Training Loss 0.039001730637496235\n",
      "Epoch 160, Training Loss 0.03913177056547702\n",
      "Epoch 160, Training Loss 0.039253951645815924\n",
      "Epoch 160, Training Loss 0.03939327271059727\n",
      "Epoch 160, Training Loss 0.039445853506303044\n",
      "Epoch 160, Training Loss 0.039619885501749524\n",
      "Epoch 160, Training Loss 0.03981024648308221\n",
      "Epoch 160, Training Loss 0.04012099162096639\n",
      "Epoch 160, Training Loss 0.0401840305927655\n",
      "Epoch 160, Training Loss 0.04022799582099137\n",
      "Epoch 160, Training Loss 0.040327156207922016\n",
      "Epoch 160, Training Loss 0.04040619567789309\n",
      "Epoch 160, Training Loss 0.040589873736147836\n",
      "Epoch 160, Training Loss 0.04087558385613553\n",
      "Epoch 160, Training Loss 0.04093249541138063\n",
      "Epoch 160, Training Loss 0.04104946347672845\n",
      "Epoch 160, Training Loss 0.041254132450141416\n",
      "Epoch 160, Training Loss 0.0417240591662581\n",
      "Epoch 160, Training Loss 0.0419508598892547\n",
      "Epoch 160, Training Loss 0.04225129942597864\n",
      "Epoch 160, Training Loss 0.04242792946365102\n",
      "Epoch 160, Training Loss 0.04265894692943758\n",
      "Epoch 160, Training Loss 0.04272449189258735\n",
      "Epoch 160, Training Loss 0.042956261847959\n",
      "Epoch 160, Training Loss 0.043080166613449676\n",
      "Epoch 160, Training Loss 0.04326386664234235\n",
      "Epoch 160, Training Loss 0.043488296365741724\n",
      "Epoch 160, Training Loss 0.043659500056005954\n",
      "Epoch 160, Training Loss 0.04384992047048667\n",
      "Epoch 160, Training Loss 0.04395588975437843\n",
      "Epoch 160, Training Loss 0.0441449968087132\n",
      "Epoch 160, Training Loss 0.04427861781728923\n",
      "Epoch 160, Training Loss 0.04435250638743572\n",
      "Epoch 160, Training Loss 0.04446701030187366\n",
      "Epoch 160, Training Loss 0.04469084866640284\n",
      "Epoch 160, Training Loss 0.044874634729493455\n",
      "Epoch 160, Training Loss 0.04504533225665693\n",
      "Epoch 160, Training Loss 0.04520264050930434\n",
      "Epoch 160, Training Loss 0.0453171059989449\n",
      "Epoch 160, Training Loss 0.04549554012515737\n",
      "Epoch 160, Training Loss 0.04566801571146683\n",
      "Epoch 160, Training Loss 0.04588193459021847\n",
      "Epoch 160, Training Loss 0.0459762249556382\n",
      "Epoch 160, Training Loss 0.046146662131695035\n",
      "Epoch 160, Training Loss 0.046257725859637305\n",
      "Epoch 160, Training Loss 0.04646852031550215\n",
      "Epoch 160, Training Loss 0.04675213674612134\n",
      "Epoch 160, Training Loss 0.04702710526306992\n",
      "Epoch 160, Training Loss 0.04714744069549205\n",
      "Epoch 160, Training Loss 0.04724920900476634\n",
      "Epoch 160, Training Loss 0.04735822275595363\n",
      "Epoch 160, Training Loss 0.047591727820065474\n",
      "Epoch 160, Training Loss 0.04769367267570608\n",
      "Epoch 160, Training Loss 0.0477997070426107\n",
      "Epoch 160, Training Loss 0.047997208347882306\n",
      "Epoch 160, Training Loss 0.04817305235882931\n",
      "Epoch 160, Training Loss 0.04833021200002383\n",
      "Epoch 160, Training Loss 0.04853096353056867\n",
      "Epoch 160, Training Loss 0.04869372449348421\n",
      "Epoch 160, Training Loss 0.048785789774449737\n",
      "Epoch 160, Training Loss 0.04888783375520612\n",
      "Epoch 160, Training Loss 0.04900737009856783\n",
      "Epoch 160, Training Loss 0.049073853853928005\n",
      "Epoch 160, Training Loss 0.04918947438364062\n",
      "Epoch 160, Training Loss 0.04941341335959066\n",
      "Epoch 160, Training Loss 0.04953799417475834\n",
      "Epoch 160, Training Loss 0.04967255233679815\n",
      "Epoch 160, Training Loss 0.04980921674557888\n",
      "Epoch 160, Training Loss 0.04989976973017997\n",
      "Epoch 160, Training Loss 0.050009053884088384\n",
      "Epoch 160, Training Loss 0.05021165214869601\n",
      "Epoch 160, Training Loss 0.05035139437136062\n",
      "Epoch 160, Training Loss 0.050535803577383916\n",
      "Epoch 160, Training Loss 0.05067289398406701\n",
      "Epoch 160, Training Loss 0.05080906984627323\n",
      "Epoch 160, Training Loss 0.05096445770700798\n",
      "Epoch 160, Training Loss 0.05115753153810644\n",
      "Epoch 160, Training Loss 0.05130934874143671\n",
      "Epoch 160, Training Loss 0.05141915880915378\n",
      "Epoch 160, Training Loss 0.051608815034160684\n",
      "Epoch 160, Training Loss 0.051721505399155036\n",
      "Epoch 160, Training Loss 0.05180120686559802\n",
      "Epoch 160, Training Loss 0.05190316105828337\n",
      "Epoch 160, Training Loss 0.05198358190114922\n",
      "Epoch 160, Training Loss 0.052249184904901116\n",
      "Epoch 160, Training Loss 0.05238530141970767\n",
      "Epoch 160, Training Loss 0.05254491572232579\n",
      "Epoch 160, Training Loss 0.05262287589661834\n",
      "Epoch 160, Training Loss 0.05275821991388679\n",
      "Epoch 160, Training Loss 0.05294728876255891\n",
      "Epoch 160, Training Loss 0.05307541939827716\n",
      "Epoch 160, Training Loss 0.05322648940936608\n",
      "Epoch 160, Training Loss 0.05330634382708222\n",
      "Epoch 160, Training Loss 0.05347432254020439\n",
      "Epoch 160, Training Loss 0.05358845848933129\n",
      "Epoch 160, Training Loss 0.053654266066391905\n",
      "Epoch 160, Training Loss 0.053758418818702323\n",
      "Epoch 160, Training Loss 0.0538311384956512\n",
      "Epoch 160, Training Loss 0.0539378518467326\n",
      "Epoch 160, Training Loss 0.05409710372910094\n",
      "Epoch 160, Training Loss 0.054204896690271544\n",
      "Epoch 160, Training Loss 0.054248044860389685\n",
      "Epoch 160, Training Loss 0.054347495743742834\n",
      "Epoch 160, Training Loss 0.05441064911339518\n",
      "Epoch 160, Training Loss 0.05450623831890352\n",
      "Epoch 160, Training Loss 0.054730513504684886\n",
      "Epoch 160, Training Loss 0.054822242080384054\n",
      "Epoch 160, Training Loss 0.05493687141610457\n",
      "Epoch 160, Training Loss 0.05504099734823036\n",
      "Epoch 160, Training Loss 0.05519484573989497\n",
      "Epoch 160, Training Loss 0.05535347559405943\n",
      "Epoch 160, Training Loss 0.0554958142487861\n",
      "Epoch 160, Training Loss 0.05564810012412422\n",
      "Epoch 160, Training Loss 0.05589298752215131\n",
      "Epoch 160, Training Loss 0.05596888533381321\n",
      "Epoch 160, Training Loss 0.056085006595896485\n",
      "Epoch 160, Training Loss 0.05616882057798564\n",
      "Epoch 160, Training Loss 0.05624907923257336\n",
      "Epoch 160, Training Loss 0.05635220649154366\n",
      "Epoch 160, Training Loss 0.056531491794187545\n",
      "Epoch 160, Training Loss 0.05672202552990307\n",
      "Epoch 160, Training Loss 0.0568288621490302\n",
      "Epoch 160, Training Loss 0.057018561248222126\n",
      "Epoch 160, Training Loss 0.05710812363906971\n",
      "Epoch 160, Training Loss 0.05726880425125208\n",
      "Epoch 160, Training Loss 0.05740313581846026\n",
      "Epoch 160, Training Loss 0.057627082549873976\n",
      "Epoch 160, Training Loss 0.05795178757956647\n",
      "Epoch 160, Training Loss 0.058051325545153194\n",
      "Epoch 160, Training Loss 0.05816542164034322\n",
      "Epoch 160, Training Loss 0.05837396196211162\n",
      "Epoch 160, Training Loss 0.05850343158721085\n",
      "Epoch 160, Training Loss 0.058648599690431374\n",
      "Epoch 160, Training Loss 0.0589121967539801\n",
      "Epoch 160, Training Loss 0.05910607210367613\n",
      "Epoch 160, Training Loss 0.05925303786430899\n",
      "Epoch 160, Training Loss 0.0593191422545887\n",
      "Epoch 160, Training Loss 0.05936947820560478\n",
      "Epoch 160, Training Loss 0.05942718632986097\n",
      "Epoch 160, Training Loss 0.05956671477231147\n",
      "Epoch 160, Training Loss 0.059706065410752886\n",
      "Epoch 160, Training Loss 0.05979385021883432\n",
      "Epoch 160, Training Loss 0.05990282000016297\n",
      "Epoch 160, Training Loss 0.060092935417219044\n",
      "Epoch 160, Training Loss 0.06022021093208085\n",
      "Epoch 160, Training Loss 0.060435826885406774\n",
      "Epoch 160, Training Loss 0.06052554152724917\n",
      "Epoch 160, Training Loss 0.06064030655500148\n",
      "Epoch 160, Training Loss 0.06077739719987449\n",
      "Epoch 160, Training Loss 0.06085111909900861\n",
      "Epoch 160, Training Loss 0.060993558165076595\n",
      "Epoch 160, Training Loss 0.061113124491785034\n",
      "Epoch 160, Training Loss 0.061215583132603744\n",
      "Epoch 160, Training Loss 0.06147023845140053\n",
      "Epoch 160, Training Loss 0.06156271159925195\n",
      "Epoch 160, Training Loss 0.06171233807464161\n",
      "Epoch 160, Training Loss 0.06177850974880902\n",
      "Epoch 160, Training Loss 0.061986307165754574\n",
      "Epoch 160, Training Loss 0.06211511289839016\n",
      "Epoch 160, Training Loss 0.06224225364539705\n",
      "Epoch 160, Training Loss 0.06247419948020326\n",
      "Epoch 160, Training Loss 0.06275226955380662\n",
      "Epoch 160, Training Loss 0.06290801482327531\n",
      "Epoch 160, Training Loss 0.06310005679659908\n",
      "Epoch 160, Training Loss 0.06320890913124355\n",
      "Epoch 160, Training Loss 0.06329547230135221\n",
      "Epoch 160, Training Loss 0.06336508073684428\n",
      "Epoch 160, Training Loss 0.06349712024059366\n",
      "Epoch 160, Training Loss 0.06363454981066306\n",
      "Epoch 160, Training Loss 0.06378300405819626\n",
      "Epoch 160, Training Loss 0.06397634248970949\n",
      "Epoch 160, Training Loss 0.06405054167379885\n",
      "Epoch 160, Training Loss 0.06411802133931147\n",
      "Epoch 160, Training Loss 0.06432684253462974\n",
      "Epoch 160, Training Loss 0.06453535450227044\n",
      "Epoch 160, Training Loss 0.06477451049829916\n",
      "Epoch 160, Training Loss 0.06493970067442759\n",
      "Epoch 160, Training Loss 0.06532933837150598\n",
      "Epoch 160, Training Loss 0.06557669701850247\n",
      "Epoch 160, Training Loss 0.06565821048615457\n",
      "Epoch 160, Training Loss 0.06577554306782343\n",
      "Epoch 160, Training Loss 0.06583262026033666\n",
      "Epoch 160, Training Loss 0.06600015180523666\n",
      "Epoch 160, Training Loss 0.06627103869977129\n",
      "Epoch 160, Training Loss 0.0663387656778745\n",
      "Epoch 160, Training Loss 0.06644040526931777\n",
      "Epoch 160, Training Loss 0.06652360953285795\n",
      "Epoch 160, Training Loss 0.06682256792845857\n",
      "Epoch 160, Training Loss 0.06698868340572052\n",
      "Epoch 160, Training Loss 0.06706855809816238\n",
      "Epoch 160, Training Loss 0.06713513566700317\n",
      "Epoch 160, Training Loss 0.06724184243451528\n",
      "Epoch 160, Training Loss 0.06736779545226594\n",
      "Epoch 160, Training Loss 0.06746252699304953\n",
      "Epoch 160, Training Loss 0.06754841386576367\n",
      "Epoch 160, Training Loss 0.06767732188667712\n",
      "Epoch 160, Training Loss 0.06781587717087602\n",
      "Epoch 160, Training Loss 0.06785078139742241\n",
      "Epoch 160, Training Loss 0.06793970404826391\n",
      "Epoch 160, Training Loss 0.06823450378130388\n",
      "Epoch 160, Training Loss 0.06841525898250701\n",
      "Epoch 160, Training Loss 0.06849040097468878\n",
      "Epoch 160, Training Loss 0.06859629546694668\n",
      "Epoch 160, Training Loss 0.06885602078198091\n",
      "Epoch 160, Training Loss 0.06910892024093196\n",
      "Epoch 160, Training Loss 0.06929850969654139\n",
      "Epoch 160, Training Loss 0.0694104580237242\n",
      "Epoch 160, Training Loss 0.06956595695718094\n",
      "Epoch 160, Training Loss 0.06972334303123795\n",
      "Epoch 160, Training Loss 0.069854082885053\n",
      "Epoch 160, Training Loss 0.07004372346574617\n",
      "Epoch 160, Training Loss 0.0701968152900143\n",
      "Epoch 160, Training Loss 0.07024533318026978\n",
      "Epoch 160, Training Loss 0.07051728844232953\n",
      "Epoch 160, Training Loss 0.07070933285471805\n",
      "Epoch 160, Training Loss 0.07091173887148004\n",
      "Epoch 160, Training Loss 0.07109904008419693\n",
      "Epoch 160, Training Loss 0.07119124239105779\n",
      "Epoch 160, Training Loss 0.07128049170507876\n",
      "Epoch 160, Training Loss 0.07141049114077369\n",
      "Epoch 160, Training Loss 0.0715357696618456\n",
      "Epoch 160, Training Loss 0.07170402168360589\n",
      "Epoch 160, Training Loss 0.0719233502555267\n",
      "Epoch 160, Training Loss 0.07197859134677502\n",
      "Epoch 160, Training Loss 0.07209467495579625\n",
      "Epoch 160, Training Loss 0.07224396159134024\n",
      "Epoch 160, Training Loss 0.07243364565598462\n",
      "Epoch 160, Training Loss 0.07249630456718871\n",
      "Epoch 160, Training Loss 0.0725990008262684\n",
      "Epoch 160, Training Loss 0.0727032621074325\n",
      "Epoch 160, Training Loss 0.07283946710502934\n",
      "Epoch 160, Training Loss 0.0729890454470006\n",
      "Epoch 160, Training Loss 0.0730488032002545\n",
      "Epoch 160, Training Loss 0.07318054121273482\n",
      "Epoch 160, Training Loss 0.07325095722637594\n",
      "Epoch 160, Training Loss 0.07346452512752141\n",
      "Epoch 160, Training Loss 0.07356834720553415\n",
      "Epoch 160, Training Loss 0.07364221370500296\n",
      "Epoch 160, Training Loss 0.07378116093547371\n",
      "Epoch 160, Training Loss 0.07401983186250072\n",
      "Epoch 160, Training Loss 0.07415365770487758\n",
      "Epoch 160, Training Loss 0.07427466159348217\n",
      "Epoch 160, Training Loss 0.07448084772233386\n",
      "Epoch 160, Training Loss 0.07486558549077538\n",
      "Epoch 160, Training Loss 0.07501570018880127\n",
      "Epoch 160, Training Loss 0.07509441531556266\n",
      "Epoch 160, Training Loss 0.07516812100349103\n",
      "Epoch 160, Training Loss 0.07536051976625495\n",
      "Epoch 160, Training Loss 0.07556217220728584\n",
      "Epoch 160, Training Loss 0.07576189546243232\n",
      "Epoch 160, Training Loss 0.07592747527915422\n",
      "Epoch 160, Training Loss 0.07609352303425902\n",
      "Epoch 160, Training Loss 0.07634623941091244\n",
      "Epoch 160, Training Loss 0.07645156257130835\n",
      "Epoch 160, Training Loss 0.07659457160679199\n",
      "Epoch 160, Training Loss 0.07675919763486633\n",
      "Epoch 160, Training Loss 0.0768524635073436\n",
      "Epoch 160, Training Loss 0.07700953830528975\n",
      "Epoch 160, Training Loss 0.07712881189659047\n",
      "Epoch 160, Training Loss 0.07745742876692426\n",
      "Epoch 160, Training Loss 0.07767913666556177\n",
      "Epoch 160, Training Loss 0.07781656456830176\n",
      "Epoch 160, Training Loss 0.07796434570065773\n",
      "Epoch 160, Training Loss 0.07807585540587259\n",
      "Epoch 160, Training Loss 0.07814684035042134\n",
      "Epoch 160, Training Loss 0.07834315354533283\n",
      "Epoch 160, Training Loss 0.07849510743156494\n",
      "Epoch 160, Training Loss 0.07867301088969803\n",
      "Epoch 160, Training Loss 0.07881801158351742\n",
      "Epoch 160, Training Loss 0.0789946983294452\n",
      "Epoch 160, Training Loss 0.07910739905093714\n",
      "Epoch 160, Training Loss 0.07929903749243149\n",
      "Epoch 160, Training Loss 0.07945936659346227\n",
      "Epoch 160, Training Loss 0.079652395606746\n",
      "Epoch 160, Training Loss 0.07977120334620746\n",
      "Epoch 160, Training Loss 0.07993703080422204\n",
      "Epoch 160, Training Loss 0.07998422743595393\n",
      "Epoch 160, Training Loss 0.08012418382946411\n",
      "Epoch 160, Training Loss 0.0803204097897005\n",
      "Epoch 160, Training Loss 0.08036791733073076\n",
      "Epoch 160, Training Loss 0.08054670494983492\n",
      "Epoch 160, Training Loss 0.08061969209500515\n",
      "Epoch 160, Training Loss 0.08080903924477603\n",
      "Epoch 160, Training Loss 0.08099377019773893\n",
      "Epoch 160, Training Loss 0.08113552598268403\n",
      "Epoch 160, Training Loss 0.08127772937412076\n",
      "Epoch 160, Training Loss 0.0815000566046523\n",
      "Epoch 160, Training Loss 0.08171160393120612\n",
      "Epoch 160, Training Loss 0.08185899457501253\n",
      "Epoch 160, Training Loss 0.08198738190324029\n",
      "Epoch 160, Training Loss 0.08206877639026516\n",
      "Epoch 160, Training Loss 0.0821897827179345\n",
      "Epoch 160, Training Loss 0.08227268427067325\n",
      "Epoch 160, Training Loss 0.08233483889571312\n",
      "Epoch 160, Training Loss 0.08264626920118433\n",
      "Epoch 160, Training Loss 0.08288464694500655\n",
      "Epoch 160, Training Loss 0.08320242156276998\n",
      "Epoch 160, Training Loss 0.08329716549895684\n",
      "Epoch 160, Training Loss 0.083443588882094\n",
      "Epoch 160, Training Loss 0.08360362708654322\n",
      "Epoch 160, Training Loss 0.08374084742106211\n",
      "Epoch 160, Training Loss 0.0839028234433983\n",
      "Epoch 160, Training Loss 0.08412691270765822\n",
      "Epoch 160, Training Loss 0.08423724971816439\n",
      "Epoch 160, Training Loss 0.0843802608973573\n",
      "Epoch 160, Training Loss 0.08450217358529796\n",
      "Epoch 160, Training Loss 0.08466623064435427\n",
      "Epoch 160, Training Loss 0.08478852032976764\n",
      "Epoch 160, Training Loss 0.08494805796381534\n",
      "Epoch 160, Training Loss 0.08503645276674605\n",
      "Epoch 160, Training Loss 0.08515211847632209\n",
      "Epoch 160, Training Loss 0.0853305756402633\n",
      "Epoch 160, Training Loss 0.0854908574844146\n",
      "Epoch 160, Training Loss 0.08557488245513205\n",
      "Epoch 160, Training Loss 0.08571917282850922\n",
      "Epoch 160, Training Loss 0.08586462489222094\n",
      "Epoch 160, Training Loss 0.08593371162033828\n",
      "Epoch 160, Training Loss 0.0860750695519016\n",
      "Epoch 160, Training Loss 0.08626269961914519\n",
      "Epoch 160, Training Loss 0.08646878765186157\n",
      "Epoch 160, Training Loss 0.08670528722055199\n",
      "Epoch 160, Training Loss 0.08689772834301071\n",
      "Epoch 160, Training Loss 0.08704862995382846\n",
      "Epoch 160, Training Loss 0.0871874533255425\n",
      "Epoch 160, Training Loss 0.087299128325032\n",
      "Epoch 160, Training Loss 0.08745234185482001\n",
      "Epoch 160, Training Loss 0.08747441170122618\n",
      "Epoch 160, Training Loss 0.08755896369095349\n",
      "Epoch 160, Training Loss 0.08765358951352441\n",
      "Epoch 160, Training Loss 0.08778874652312539\n",
      "Epoch 160, Training Loss 0.08796568226326457\n",
      "Epoch 160, Training Loss 0.0880126360508487\n",
      "Epoch 160, Training Loss 0.08810110228217166\n",
      "Epoch 160, Training Loss 0.0882221558476653\n",
      "Epoch 160, Training Loss 0.08833795936439959\n",
      "Epoch 160, Training Loss 0.08842125412105295\n",
      "Epoch 160, Training Loss 0.08851960034626524\n",
      "Epoch 160, Training Loss 0.08862710412582168\n",
      "Epoch 160, Training Loss 0.08880994689967626\n",
      "Epoch 160, Training Loss 0.08899698932381238\n",
      "Epoch 160, Training Loss 0.08915467518369866\n",
      "Epoch 160, Training Loss 0.08923049233949093\n",
      "Epoch 160, Training Loss 0.08940795888109586\n",
      "Epoch 160, Training Loss 0.08959715522806663\n",
      "Epoch 160, Training Loss 0.08966065075753442\n",
      "Epoch 160, Training Loss 0.08977078838402505\n",
      "Epoch 160, Training Loss 0.0898648468572694\n",
      "Epoch 160, Training Loss 0.09009899070862766\n",
      "Epoch 160, Training Loss 0.09026692891040879\n",
      "Epoch 160, Training Loss 0.09039693531077689\n",
      "Epoch 160, Training Loss 0.09045405247155815\n",
      "Epoch 160, Training Loss 0.0906950648245223\n",
      "Epoch 160, Training Loss 0.09083742884171131\n",
      "Epoch 160, Training Loss 0.09098193824500836\n",
      "Epoch 160, Training Loss 0.09111607221938918\n",
      "Epoch 160, Training Loss 0.09137970283913338\n",
      "Epoch 160, Training Loss 0.09154524574594577\n",
      "Epoch 160, Training Loss 0.09164388974666443\n",
      "Epoch 160, Training Loss 0.09174916325875408\n",
      "Epoch 160, Training Loss 0.09184443435686476\n",
      "Epoch 160, Training Loss 0.09205558672642616\n",
      "Epoch 160, Training Loss 0.09218150799822472\n",
      "Epoch 160, Training Loss 0.09235039852140352\n",
      "Epoch 160, Training Loss 0.09253287469239338\n",
      "Epoch 160, Training Loss 0.0926911764256561\n",
      "Epoch 160, Training Loss 0.09290247271909281\n",
      "Epoch 160, Training Loss 0.09306253232252415\n",
      "Epoch 160, Training Loss 0.09321806059145105\n",
      "Epoch 160, Training Loss 0.09358046727393136\n",
      "Epoch 160, Training Loss 0.09367176284894461\n",
      "Epoch 160, Training Loss 0.09382739372056006\n",
      "Epoch 160, Training Loss 0.09400434828723025\n",
      "Epoch 160, Training Loss 0.09418089817875944\n",
      "Epoch 160, Training Loss 0.09431675685774488\n",
      "Epoch 160, Training Loss 0.09449238312499755\n",
      "Epoch 160, Training Loss 0.09462832432249775\n",
      "Epoch 160, Training Loss 0.09471816982588042\n",
      "Epoch 160, Training Loss 0.0949757689624415\n",
      "Epoch 160, Training Loss 0.09507140027992714\n",
      "Epoch 160, Training Loss 0.09517020581151976\n",
      "Epoch 160, Training Loss 0.09526537620293363\n",
      "Epoch 160, Training Loss 0.09536209901142151\n",
      "Epoch 160, Training Loss 0.0955913294621212\n",
      "Epoch 160, Training Loss 0.09571930962850524\n",
      "Epoch 160, Training Loss 0.09580061288879198\n",
      "Epoch 160, Training Loss 0.09587593901130702\n",
      "Epoch 160, Training Loss 0.09597240445082603\n",
      "Epoch 160, Training Loss 0.09604923594790651\n",
      "Epoch 160, Training Loss 0.0962711691742053\n",
      "Epoch 160, Training Loss 0.0964756426032242\n",
      "Epoch 160, Training Loss 0.09675217132129328\n",
      "Epoch 160, Training Loss 0.09680553330846912\n",
      "Epoch 160, Training Loss 0.09694953724417997\n",
      "Epoch 160, Training Loss 0.09707950599148603\n",
      "Epoch 160, Training Loss 0.09723940301600777\n",
      "Epoch 160, Training Loss 0.09728783607254247\n",
      "Epoch 160, Training Loss 0.09748184316984528\n",
      "Epoch 160, Training Loss 0.09769712304672622\n",
      "Epoch 160, Training Loss 0.09790583924792917\n",
      "Epoch 160, Training Loss 0.09804075343720138\n",
      "Epoch 160, Training Loss 0.09813536453963545\n",
      "Epoch 160, Training Loss 0.09842704800541138\n",
      "Epoch 160, Training Loss 0.09858596868946425\n",
      "Epoch 160, Training Loss 0.09872973435903754\n",
      "Epoch 160, Training Loss 0.09887479880200628\n",
      "Epoch 160, Training Loss 0.09909707488839889\n",
      "Epoch 160, Training Loss 0.0993006274561443\n",
      "Epoch 160, Training Loss 0.09940698480857607\n",
      "Epoch 160, Training Loss 0.09952172477874914\n",
      "Epoch 160, Training Loss 0.09964366644963889\n",
      "Epoch 160, Training Loss 0.09978064853707543\n",
      "Epoch 160, Training Loss 0.09990403426768225\n",
      "Epoch 160, Training Loss 0.1001544722148677\n",
      "Epoch 160, Training Loss 0.10034857965681863\n",
      "Epoch 160, Training Loss 0.10053213397064782\n",
      "Epoch 160, Training Loss 0.10083218352378481\n",
      "Epoch 160, Training Loss 0.10096665327925511\n",
      "Epoch 160, Training Loss 0.10115400009104968\n",
      "Epoch 160, Training Loss 0.10134805288270611\n",
      "Epoch 160, Training Loss 0.10148008004821779\n",
      "Epoch 160, Training Loss 0.10171711499162038\n",
      "Epoch 160, Training Loss 0.10182351664737667\n",
      "Epoch 160, Training Loss 0.10201477958723103\n",
      "Epoch 160, Training Loss 0.10215087153989336\n",
      "Epoch 160, Training Loss 0.10231833364766882\n",
      "Epoch 160, Training Loss 0.10248997507383452\n",
      "Epoch 160, Training Loss 0.10259174348791238\n",
      "Epoch 160, Training Loss 0.10279904798511653\n",
      "Epoch 160, Training Loss 0.10301590963359684\n",
      "Epoch 160, Training Loss 0.103167274352306\n",
      "Epoch 160, Training Loss 0.10335300606496804\n",
      "Epoch 160, Training Loss 0.1034856858613241\n",
      "Epoch 160, Training Loss 0.10358908817248272\n",
      "Epoch 160, Training Loss 0.10379830910764692\n",
      "Epoch 160, Training Loss 0.10400883284637995\n",
      "Epoch 160, Training Loss 0.10410414748560742\n",
      "Epoch 160, Training Loss 0.10420825721129127\n",
      "Epoch 160, Training Loss 0.1043846552900951\n",
      "Epoch 160, Training Loss 0.10454227770571514\n",
      "Epoch 160, Training Loss 0.1046286270289165\n",
      "Epoch 160, Training Loss 0.10485161374063443\n",
      "Epoch 160, Training Loss 0.10508810481070863\n",
      "Epoch 160, Training Loss 0.10522555443636901\n",
      "Epoch 160, Training Loss 0.1053418485576387\n",
      "Epoch 160, Training Loss 0.10550384980905086\n",
      "Epoch 160, Training Loss 0.1056026323886631\n",
      "Epoch 160, Training Loss 0.10578175848516662\n",
      "Epoch 160, Training Loss 0.10588166038589099\n",
      "Epoch 160, Training Loss 0.10600376315891286\n",
      "Epoch 160, Training Loss 0.1061806529188705\n",
      "Epoch 160, Training Loss 0.10627786333070081\n",
      "Epoch 160, Training Loss 0.10644028716913574\n",
      "Epoch 160, Training Loss 0.10672353686350386\n",
      "Epoch 160, Training Loss 0.10688048777411051\n",
      "Epoch 160, Training Loss 0.10705115736636055\n",
      "Epoch 160, Training Loss 0.10714527996985809\n",
      "Epoch 160, Training Loss 0.10730414510802234\n",
      "Epoch 160, Training Loss 0.1074878706899293\n",
      "Epoch 160, Training Loss 0.10788339560332201\n",
      "Epoch 160, Training Loss 0.10811668463870693\n",
      "Epoch 160, Training Loss 0.10817682386264015\n",
      "Epoch 160, Training Loss 0.10833742047953027\n",
      "Epoch 160, Training Loss 0.10848673259186775\n",
      "Epoch 160, Training Loss 0.1086314048694299\n",
      "Epoch 160, Training Loss 0.10883602399445708\n",
      "Epoch 160, Training Loss 0.10902029436434169\n",
      "Epoch 160, Training Loss 0.10922113004262032\n",
      "Epoch 160, Training Loss 0.10935340893676367\n",
      "Epoch 160, Training Loss 0.10946994780293663\n",
      "Epoch 160, Training Loss 0.10970544945115171\n",
      "Epoch 160, Training Loss 0.1099353471003911\n",
      "Epoch 160, Training Loss 0.1100263225481562\n",
      "Epoch 160, Training Loss 0.11014259183098135\n",
      "Epoch 160, Training Loss 0.11026880987788863\n",
      "Epoch 160, Training Loss 0.11036729205237783\n",
      "Epoch 160, Training Loss 0.11046036437172871\n",
      "Epoch 160, Training Loss 0.11065642927747096\n",
      "Epoch 160, Training Loss 0.11086187475001263\n",
      "Epoch 160, Training Loss 0.11109550001428407\n",
      "Epoch 160, Training Loss 0.11123348754899734\n",
      "Epoch 160, Training Loss 0.11148850144842244\n",
      "Epoch 160, Training Loss 0.111682181806325\n",
      "Epoch 160, Training Loss 0.11185028986609957\n",
      "Epoch 160, Training Loss 0.11197607244467339\n",
      "Epoch 160, Training Loss 0.11214623541649803\n",
      "Epoch 160, Training Loss 0.11238028871757752\n",
      "Epoch 160, Training Loss 0.11248734626261626\n",
      "Epoch 160, Training Loss 0.11261358697567601\n",
      "Epoch 160, Training Loss 0.11276053184228937\n",
      "Epoch 160, Training Loss 0.11292463798752374\n",
      "Epoch 160, Training Loss 0.1131215718291376\n",
      "Epoch 160, Training Loss 0.11324545327107162\n",
      "Epoch 170, Training Loss 7.244210947505044e-05\n",
      "Epoch 170, Training Loss 0.0001549739057145765\n",
      "Epoch 170, Training Loss 0.0003364531661543395\n",
      "Epoch 170, Training Loss 0.00041039649616269507\n",
      "Epoch 170, Training Loss 0.000573078875460893\n",
      "Epoch 170, Training Loss 0.000661997610460157\n",
      "Epoch 170, Training Loss 0.0007999834016232235\n",
      "Epoch 170, Training Loss 0.0008809956414696506\n",
      "Epoch 170, Training Loss 0.001028029876939781\n",
      "Epoch 170, Training Loss 0.001112643510217557\n",
      "Epoch 170, Training Loss 0.0012015302324920054\n",
      "Epoch 170, Training Loss 0.001320637672034371\n",
      "Epoch 170, Training Loss 0.0014722137795308667\n",
      "Epoch 170, Training Loss 0.001582397062264745\n",
      "Epoch 170, Training Loss 0.0017073514640255048\n",
      "Epoch 170, Training Loss 0.0017812188733797854\n",
      "Epoch 170, Training Loss 0.001871392192781124\n",
      "Epoch 170, Training Loss 0.001986185155446877\n",
      "Epoch 170, Training Loss 0.0020881955323697964\n",
      "Epoch 170, Training Loss 0.002280678843026576\n",
      "Epoch 170, Training Loss 0.00233528320498936\n",
      "Epoch 170, Training Loss 0.0024822031664650155\n",
      "Epoch 170, Training Loss 0.002560125897302652\n",
      "Epoch 170, Training Loss 0.0026462320571817707\n",
      "Epoch 170, Training Loss 0.0027500034579078254\n",
      "Epoch 170, Training Loss 0.002824770558215773\n",
      "Epoch 170, Training Loss 0.002942824255093894\n",
      "Epoch 170, Training Loss 0.003136796214620171\n",
      "Epoch 170, Training Loss 0.003367805639115136\n",
      "Epoch 170, Training Loss 0.0035045235072407883\n",
      "Epoch 170, Training Loss 0.0035583681719915947\n",
      "Epoch 170, Training Loss 0.0036308107340274867\n",
      "Epoch 170, Training Loss 0.003715830890800032\n",
      "Epoch 170, Training Loss 0.0037709948513895046\n",
      "Epoch 170, Training Loss 0.003884559449599222\n",
      "Epoch 170, Training Loss 0.0039607216187221616\n",
      "Epoch 170, Training Loss 0.004032180175337645\n",
      "Epoch 170, Training Loss 0.004163358632065451\n",
      "Epoch 170, Training Loss 0.004334390730313633\n",
      "Epoch 170, Training Loss 0.004417991379985724\n",
      "Epoch 170, Training Loss 0.004524183390504869\n",
      "Epoch 170, Training Loss 0.004619685609055602\n",
      "Epoch 170, Training Loss 0.004690270627970281\n",
      "Epoch 170, Training Loss 0.004749013573083731\n",
      "Epoch 170, Training Loss 0.004847548625734456\n",
      "Epoch 170, Training Loss 0.0049405988315334715\n",
      "Epoch 170, Training Loss 0.0050154438056528115\n",
      "Epoch 170, Training Loss 0.005144253844762092\n",
      "Epoch 170, Training Loss 0.005293671113184041\n",
      "Epoch 170, Training Loss 0.005581439079721566\n",
      "Epoch 170, Training Loss 0.005772921859341509\n",
      "Epoch 170, Training Loss 0.00589459895839929\n",
      "Epoch 170, Training Loss 0.005994612646415411\n",
      "Epoch 170, Training Loss 0.006106382073915523\n",
      "Epoch 170, Training Loss 0.006229611580519725\n",
      "Epoch 170, Training Loss 0.006382589731031976\n",
      "Epoch 170, Training Loss 0.006525559722424468\n",
      "Epoch 170, Training Loss 0.006633849500123497\n",
      "Epoch 170, Training Loss 0.0067055813034477135\n",
      "Epoch 170, Training Loss 0.006767959107675821\n",
      "Epoch 170, Training Loss 0.006814501965251725\n",
      "Epoch 170, Training Loss 0.006909234439739791\n",
      "Epoch 170, Training Loss 0.0070070382684011895\n",
      "Epoch 170, Training Loss 0.007143120236142212\n",
      "Epoch 170, Training Loss 0.007233926045048572\n",
      "Epoch 170, Training Loss 0.0073076093859989625\n",
      "Epoch 170, Training Loss 0.0073732230526483275\n",
      "Epoch 170, Training Loss 0.0075204242211397345\n",
      "Epoch 170, Training Loss 0.007652060850463865\n",
      "Epoch 170, Training Loss 0.007801839109996091\n",
      "Epoch 170, Training Loss 0.007882528707308843\n",
      "Epoch 170, Training Loss 0.00800603211326215\n",
      "Epoch 170, Training Loss 0.008082067408144017\n",
      "Epoch 170, Training Loss 0.008235396193268964\n",
      "Epoch 170, Training Loss 0.008295597127445823\n",
      "Epoch 170, Training Loss 0.008464150945358264\n",
      "Epoch 170, Training Loss 0.008708294180919752\n",
      "Epoch 170, Training Loss 0.00884255670163485\n",
      "Epoch 170, Training Loss 0.008931082680516535\n",
      "Epoch 170, Training Loss 0.009080621206661319\n",
      "Epoch 170, Training Loss 0.009192322730979956\n",
      "Epoch 170, Training Loss 0.009396957481265678\n",
      "Epoch 170, Training Loss 0.009603147497376823\n",
      "Epoch 170, Training Loss 0.009677400960184424\n",
      "Epoch 170, Training Loss 0.009787639684003332\n",
      "Epoch 170, Training Loss 0.00985763531626033\n",
      "Epoch 170, Training Loss 0.009959286883892611\n",
      "Epoch 170, Training Loss 0.010045680862939572\n",
      "Epoch 170, Training Loss 0.010185400228899763\n",
      "Epoch 170, Training Loss 0.01027108278230328\n",
      "Epoch 170, Training Loss 0.01037328696007009\n",
      "Epoch 170, Training Loss 0.010518832260842824\n",
      "Epoch 170, Training Loss 0.010628192911824912\n",
      "Epoch 170, Training Loss 0.010713654606009993\n",
      "Epoch 170, Training Loss 0.010910424839733812\n",
      "Epoch 170, Training Loss 0.0110512435950739\n",
      "Epoch 170, Training Loss 0.011166464280137015\n",
      "Epoch 170, Training Loss 0.011227223212304323\n",
      "Epoch 170, Training Loss 0.01134211728182595\n",
      "Epoch 170, Training Loss 0.011471823658174871\n",
      "Epoch 170, Training Loss 0.011499267271565049\n",
      "Epoch 170, Training Loss 0.011653147075715882\n",
      "Epoch 170, Training Loss 0.011825800599420773\n",
      "Epoch 170, Training Loss 0.011920598985822609\n",
      "Epoch 170, Training Loss 0.012062096572897928\n",
      "Epoch 170, Training Loss 0.01215199602153295\n",
      "Epoch 170, Training Loss 0.0122417125022015\n",
      "Epoch 170, Training Loss 0.012380259383060133\n",
      "Epoch 170, Training Loss 0.012494473296510595\n",
      "Epoch 170, Training Loss 0.012602496949379401\n",
      "Epoch 170, Training Loss 0.012780346023990675\n",
      "Epoch 170, Training Loss 0.012942619021515103\n",
      "Epoch 170, Training Loss 0.013053536100689407\n",
      "Epoch 170, Training Loss 0.013164065187544469\n",
      "Epoch 170, Training Loss 0.013233488525652214\n",
      "Epoch 170, Training Loss 0.013355318123422315\n",
      "Epoch 170, Training Loss 0.013422213042216838\n",
      "Epoch 170, Training Loss 0.013626950862043349\n",
      "Epoch 170, Training Loss 0.013809159383787524\n",
      "Epoch 170, Training Loss 0.013933800994549566\n",
      "Epoch 170, Training Loss 0.014021217856375152\n",
      "Epoch 170, Training Loss 0.014183659492360662\n",
      "Epoch 170, Training Loss 0.01429493582862265\n",
      "Epoch 170, Training Loss 0.01439497647973735\n",
      "Epoch 170, Training Loss 0.014515514535557888\n",
      "Epoch 170, Training Loss 0.014699956211630645\n",
      "Epoch 170, Training Loss 0.014881909298508064\n",
      "Epoch 170, Training Loss 0.015030007880857534\n",
      "Epoch 170, Training Loss 0.01511420980762795\n",
      "Epoch 170, Training Loss 0.015180399508008261\n",
      "Epoch 170, Training Loss 0.015255284393230057\n",
      "Epoch 170, Training Loss 0.015348278005104845\n",
      "Epoch 170, Training Loss 0.015402847086377156\n",
      "Epoch 170, Training Loss 0.015528250237941132\n",
      "Epoch 170, Training Loss 0.015579265201716777\n",
      "Epoch 170, Training Loss 0.01562180962708905\n",
      "Epoch 170, Training Loss 0.01573306540279742\n",
      "Epoch 170, Training Loss 0.015889958814358163\n",
      "Epoch 170, Training Loss 0.015981591571017605\n",
      "Epoch 170, Training Loss 0.016049029119789143\n",
      "Epoch 170, Training Loss 0.01612279790422648\n",
      "Epoch 170, Training Loss 0.016196384902119332\n",
      "Epoch 170, Training Loss 0.01628715517309011\n",
      "Epoch 170, Training Loss 0.01641326040372519\n",
      "Epoch 170, Training Loss 0.016501371970262062\n",
      "Epoch 170, Training Loss 0.0166208919356851\n",
      "Epoch 170, Training Loss 0.01667716352702559\n",
      "Epoch 170, Training Loss 0.016718225029613967\n",
      "Epoch 170, Training Loss 0.016947809983130613\n",
      "Epoch 170, Training Loss 0.017069444398555303\n",
      "Epoch 170, Training Loss 0.017160016819453606\n",
      "Epoch 170, Training Loss 0.0172858506850803\n",
      "Epoch 170, Training Loss 0.01733515676958939\n",
      "Epoch 170, Training Loss 0.017486997687107767\n",
      "Epoch 170, Training Loss 0.017545964768932908\n",
      "Epoch 170, Training Loss 0.0176136254159081\n",
      "Epoch 170, Training Loss 0.017674190699673065\n",
      "Epoch 170, Training Loss 0.017793981020179248\n",
      "Epoch 170, Training Loss 0.01786432093214196\n",
      "Epoch 170, Training Loss 0.017972303435320743\n",
      "Epoch 170, Training Loss 0.018048842406600638\n",
      "Epoch 170, Training Loss 0.018108837497051413\n",
      "Epoch 170, Training Loss 0.018273234310205024\n",
      "Epoch 170, Training Loss 0.018405943099037765\n",
      "Epoch 170, Training Loss 0.018496108253288755\n",
      "Epoch 170, Training Loss 0.018557418137788773\n",
      "Epoch 170, Training Loss 0.01868000532240819\n",
      "Epoch 170, Training Loss 0.018755988987243694\n",
      "Epoch 170, Training Loss 0.018818452959056094\n",
      "Epoch 170, Training Loss 0.019031582266816397\n",
      "Epoch 170, Training Loss 0.019206074407071713\n",
      "Epoch 170, Training Loss 0.01923088867769903\n",
      "Epoch 170, Training Loss 0.019259857804612125\n",
      "Epoch 170, Training Loss 0.0193967894148415\n",
      "Epoch 170, Training Loss 0.019498714919933272\n",
      "Epoch 170, Training Loss 0.019601394362804836\n",
      "Epoch 170, Training Loss 0.019706688911827933\n",
      "Epoch 170, Training Loss 0.019843154231948622\n",
      "Epoch 170, Training Loss 0.02012656620034324\n",
      "Epoch 170, Training Loss 0.020215910294895893\n",
      "Epoch 170, Training Loss 0.020268442657063988\n",
      "Epoch 170, Training Loss 0.02037751318319984\n",
      "Epoch 170, Training Loss 0.02043787137035976\n",
      "Epoch 170, Training Loss 0.020574111448567543\n",
      "Epoch 170, Training Loss 0.02076370681128691\n",
      "Epoch 170, Training Loss 0.02090487869747002\n",
      "Epoch 170, Training Loss 0.020996808229238176\n",
      "Epoch 170, Training Loss 0.02110389923519643\n",
      "Epoch 170, Training Loss 0.021216007864193233\n",
      "Epoch 170, Training Loss 0.021270130406064756\n",
      "Epoch 170, Training Loss 0.02132104381995128\n",
      "Epoch 170, Training Loss 0.02136946547671657\n",
      "Epoch 170, Training Loss 0.02145992372842396\n",
      "Epoch 170, Training Loss 0.021582054598328404\n",
      "Epoch 170, Training Loss 0.02177682794306589\n",
      "Epoch 170, Training Loss 0.021911079011609793\n",
      "Epoch 170, Training Loss 0.022011587839297322\n",
      "Epoch 170, Training Loss 0.022180553859152147\n",
      "Epoch 170, Training Loss 0.022262335299988233\n",
      "Epoch 170, Training Loss 0.02235930370133551\n",
      "Epoch 170, Training Loss 0.022525830808884043\n",
      "Epoch 170, Training Loss 0.022656696331699182\n",
      "Epoch 170, Training Loss 0.022695749959982265\n",
      "Epoch 170, Training Loss 0.02290072233018363\n",
      "Epoch 170, Training Loss 0.022976105251466223\n",
      "Epoch 170, Training Loss 0.02305084890912256\n",
      "Epoch 170, Training Loss 0.023098053758406577\n",
      "Epoch 170, Training Loss 0.023404423842954512\n",
      "Epoch 170, Training Loss 0.023728114183601517\n",
      "Epoch 170, Training Loss 0.023830355518995344\n",
      "Epoch 170, Training Loss 0.023964991788272664\n",
      "Epoch 170, Training Loss 0.024075771422337387\n",
      "Epoch 170, Training Loss 0.0242267658891123\n",
      "Epoch 170, Training Loss 0.024352720869547876\n",
      "Epoch 170, Training Loss 0.024442750407987848\n",
      "Epoch 170, Training Loss 0.024519529117418983\n",
      "Epoch 170, Training Loss 0.024623636956638693\n",
      "Epoch 170, Training Loss 0.024697192985078564\n",
      "Epoch 170, Training Loss 0.024771906564112208\n",
      "Epoch 170, Training Loss 0.0249412785548612\n",
      "Epoch 170, Training Loss 0.02504498478682602\n",
      "Epoch 170, Training Loss 0.0251543855394625\n",
      "Epoch 170, Training Loss 0.025278497311046057\n",
      "Epoch 170, Training Loss 0.025362944716344708\n",
      "Epoch 170, Training Loss 0.025426572850902978\n",
      "Epoch 170, Training Loss 0.025555536827391676\n",
      "Epoch 170, Training Loss 0.025643312346066354\n",
      "Epoch 170, Training Loss 0.025823708776089235\n",
      "Epoch 170, Training Loss 0.02591801788229162\n",
      "Epoch 170, Training Loss 0.026094649958869686\n",
      "Epoch 170, Training Loss 0.026166603459840846\n",
      "Epoch 170, Training Loss 0.02626203274463906\n",
      "Epoch 170, Training Loss 0.02637282038188499\n",
      "Epoch 170, Training Loss 0.026617764049898022\n",
      "Epoch 170, Training Loss 0.026720252264376798\n",
      "Epoch 170, Training Loss 0.026860580234157154\n",
      "Epoch 170, Training Loss 0.026942937075139006\n",
      "Epoch 170, Training Loss 0.027069813183620763\n",
      "Epoch 170, Training Loss 0.027205129647079634\n",
      "Epoch 170, Training Loss 0.02728310808577501\n",
      "Epoch 170, Training Loss 0.02749521506335729\n",
      "Epoch 170, Training Loss 0.027608418889591455\n",
      "Epoch 170, Training Loss 0.027859026275556104\n",
      "Epoch 170, Training Loss 0.027924496470890996\n",
      "Epoch 170, Training Loss 0.028045959046582126\n",
      "Epoch 170, Training Loss 0.02818848155534176\n",
      "Epoch 170, Training Loss 0.028283484251526617\n",
      "Epoch 170, Training Loss 0.028400715526260074\n",
      "Epoch 170, Training Loss 0.02848720494324289\n",
      "Epoch 170, Training Loss 0.02866280428550737\n",
      "Epoch 170, Training Loss 0.028882295855552034\n",
      "Epoch 170, Training Loss 0.02911750127173141\n",
      "Epoch 170, Training Loss 0.029205244777681272\n",
      "Epoch 170, Training Loss 0.029282274467827718\n",
      "Epoch 170, Training Loss 0.029475272554532646\n",
      "Epoch 170, Training Loss 0.029670608218025675\n",
      "Epoch 170, Training Loss 0.02975872747333306\n",
      "Epoch 170, Training Loss 0.02994347246997344\n",
      "Epoch 170, Training Loss 0.030089155415935286\n",
      "Epoch 170, Training Loss 0.030226593332179367\n",
      "Epoch 170, Training Loss 0.030325847070502197\n",
      "Epoch 170, Training Loss 0.030462782882401705\n",
      "Epoch 170, Training Loss 0.030603752164241604\n",
      "Epoch 170, Training Loss 0.030722821779225185\n",
      "Epoch 170, Training Loss 0.03081114279091968\n",
      "Epoch 170, Training Loss 0.03092742217299731\n",
      "Epoch 170, Training Loss 0.031038482108956104\n",
      "Epoch 170, Training Loss 0.03119153360290753\n",
      "Epoch 170, Training Loss 0.031289833552582795\n",
      "Epoch 170, Training Loss 0.03134452997494842\n",
      "Epoch 170, Training Loss 0.03149980621988816\n",
      "Epoch 170, Training Loss 0.03158235818604984\n",
      "Epoch 170, Training Loss 0.03169916251964886\n",
      "Epoch 170, Training Loss 0.03188207112919644\n",
      "Epoch 170, Training Loss 0.031925971355394024\n",
      "Epoch 170, Training Loss 0.03210719287528864\n",
      "Epoch 170, Training Loss 0.03228883307589137\n",
      "Epoch 170, Training Loss 0.03232860453474476\n",
      "Epoch 170, Training Loss 0.03249624968670747\n",
      "Epoch 170, Training Loss 0.032581440482736396\n",
      "Epoch 170, Training Loss 0.03269903144329939\n",
      "Epoch 170, Training Loss 0.03285882216842507\n",
      "Epoch 170, Training Loss 0.03296809837631786\n",
      "Epoch 170, Training Loss 0.0332102871397534\n",
      "Epoch 170, Training Loss 0.03330749070124172\n",
      "Epoch 170, Training Loss 0.033445469194265734\n",
      "Epoch 170, Training Loss 0.03370425003864195\n",
      "Epoch 170, Training Loss 0.033781688457921795\n",
      "Epoch 170, Training Loss 0.033873493606915404\n",
      "Epoch 170, Training Loss 0.03394905847194783\n",
      "Epoch 170, Training Loss 0.033987894649510186\n",
      "Epoch 170, Training Loss 0.03403084971906279\n",
      "Epoch 170, Training Loss 0.034101757466259514\n",
      "Epoch 170, Training Loss 0.03422634468397216\n",
      "Epoch 170, Training Loss 0.034330247632225454\n",
      "Epoch 170, Training Loss 0.03447594035350148\n",
      "Epoch 170, Training Loss 0.034602950686765145\n",
      "Epoch 170, Training Loss 0.034711084059437215\n",
      "Epoch 170, Training Loss 0.03479317655724942\n",
      "Epoch 170, Training Loss 0.03494402952015857\n",
      "Epoch 170, Training Loss 0.035088987873338376\n",
      "Epoch 170, Training Loss 0.03520358866437927\n",
      "Epoch 170, Training Loss 0.03538273179622562\n",
      "Epoch 170, Training Loss 0.0354897285837804\n",
      "Epoch 170, Training Loss 0.03562208102144244\n",
      "Epoch 170, Training Loss 0.03578076017139208\n",
      "Epoch 170, Training Loss 0.03584411585955973\n",
      "Epoch 170, Training Loss 0.03599704651500258\n",
      "Epoch 170, Training Loss 0.03619687263008274\n",
      "Epoch 170, Training Loss 0.03632633178435323\n",
      "Epoch 170, Training Loss 0.036384649817710335\n",
      "Epoch 170, Training Loss 0.03655044596804225\n",
      "Epoch 170, Training Loss 0.03673103256889469\n",
      "Epoch 170, Training Loss 0.036826940815505166\n",
      "Epoch 170, Training Loss 0.03708652165406348\n",
      "Epoch 170, Training Loss 0.0372409347273276\n",
      "Epoch 170, Training Loss 0.03733578470566541\n",
      "Epoch 170, Training Loss 0.03743258222003879\n",
      "Epoch 170, Training Loss 0.0375715230098542\n",
      "Epoch 170, Training Loss 0.03773798403399222\n",
      "Epoch 170, Training Loss 0.0378557828390766\n",
      "Epoch 170, Training Loss 0.03799373650318369\n",
      "Epoch 170, Training Loss 0.038139873420071725\n",
      "Epoch 170, Training Loss 0.03824596747736949\n",
      "Epoch 170, Training Loss 0.0383940688036668\n",
      "Epoch 170, Training Loss 0.03849115694784905\n",
      "Epoch 170, Training Loss 0.03864942988871461\n",
      "Epoch 170, Training Loss 0.03885727999326975\n",
      "Epoch 170, Training Loss 0.0390099833869492\n",
      "Epoch 170, Training Loss 0.03914320198795222\n",
      "Epoch 170, Training Loss 0.03932269693583326\n",
      "Epoch 170, Training Loss 0.03940064465279317\n",
      "Epoch 170, Training Loss 0.039600992156073565\n",
      "Epoch 170, Training Loss 0.03973820135283196\n",
      "Epoch 170, Training Loss 0.0398615643982311\n",
      "Epoch 170, Training Loss 0.03997940000842142\n",
      "Epoch 170, Training Loss 0.040028546224622166\n",
      "Epoch 170, Training Loss 0.04016027550982392\n",
      "Epoch 170, Training Loss 0.040254501068531096\n",
      "Epoch 170, Training Loss 0.0403700795720148\n",
      "Epoch 170, Training Loss 0.04049239032294439\n",
      "Epoch 170, Training Loss 0.04075685616992319\n",
      "Epoch 170, Training Loss 0.040864019671364514\n",
      "Epoch 170, Training Loss 0.04096667631469724\n",
      "Epoch 170, Training Loss 0.04103389916860539\n",
      "Epoch 170, Training Loss 0.041105629666648866\n",
      "Epoch 170, Training Loss 0.04122710660995577\n",
      "Epoch 170, Training Loss 0.04125602358042279\n",
      "Epoch 170, Training Loss 0.041355980588766314\n",
      "Epoch 170, Training Loss 0.04157513303353506\n",
      "Epoch 170, Training Loss 0.04171062974959536\n",
      "Epoch 170, Training Loss 0.041825819984459514\n",
      "Epoch 170, Training Loss 0.041952264788167555\n",
      "Epoch 170, Training Loss 0.04219563731737911\n",
      "Epoch 170, Training Loss 0.04226900658586903\n",
      "Epoch 170, Training Loss 0.04233907448494678\n",
      "Epoch 170, Training Loss 0.04257193052917338\n",
      "Epoch 170, Training Loss 0.04268476994389006\n",
      "Epoch 170, Training Loss 0.042811064764171305\n",
      "Epoch 170, Training Loss 0.04290519574242632\n",
      "Epoch 170, Training Loss 0.04306671947069333\n",
      "Epoch 170, Training Loss 0.04313607483892642\n",
      "Epoch 170, Training Loss 0.04321944072861653\n",
      "Epoch 170, Training Loss 0.04335820178984833\n",
      "Epoch 170, Training Loss 0.043423963472475784\n",
      "Epoch 170, Training Loss 0.043551985579340355\n",
      "Epoch 170, Training Loss 0.04367355874184605\n",
      "Epoch 170, Training Loss 0.04380924980182325\n",
      "Epoch 170, Training Loss 0.0439342176660781\n",
      "Epoch 170, Training Loss 0.0440372396165224\n",
      "Epoch 170, Training Loss 0.04409157141776341\n",
      "Epoch 170, Training Loss 0.04419696035668673\n",
      "Epoch 170, Training Loss 0.0442695303550919\n",
      "Epoch 170, Training Loss 0.04443391914600912\n",
      "Epoch 170, Training Loss 0.044550379638171864\n",
      "Epoch 170, Training Loss 0.04463658922964045\n",
      "Epoch 170, Training Loss 0.04469665390012972\n",
      "Epoch 170, Training Loss 0.04474922090940311\n",
      "Epoch 170, Training Loss 0.0449416535332456\n",
      "Epoch 170, Training Loss 0.04501892473367627\n",
      "Epoch 170, Training Loss 0.04508558359197186\n",
      "Epoch 170, Training Loss 0.04524467429598732\n",
      "Epoch 170, Training Loss 0.04534995218124384\n",
      "Epoch 170, Training Loss 0.04555028621250254\n",
      "Epoch 170, Training Loss 0.04561918100718495\n",
      "Epoch 170, Training Loss 0.04574493733722993\n",
      "Epoch 170, Training Loss 0.045931261304356256\n",
      "Epoch 170, Training Loss 0.04606405907141431\n",
      "Epoch 170, Training Loss 0.04616856692678026\n",
      "Epoch 170, Training Loss 0.04632721499770956\n",
      "Epoch 170, Training Loss 0.046503398400705186\n",
      "Epoch 170, Training Loss 0.046735732847124414\n",
      "Epoch 170, Training Loss 0.0468587623313641\n",
      "Epoch 170, Training Loss 0.04696346400662914\n",
      "Epoch 170, Training Loss 0.04712608331323737\n",
      "Epoch 170, Training Loss 0.04720481686637072\n",
      "Epoch 170, Training Loss 0.04733179741636719\n",
      "Epoch 170, Training Loss 0.04746620134567208\n",
      "Epoch 170, Training Loss 0.047522484875091205\n",
      "Epoch 170, Training Loss 0.04765189350451655\n",
      "Epoch 170, Training Loss 0.04774465804438457\n",
      "Epoch 170, Training Loss 0.04786143790158774\n",
      "Epoch 170, Training Loss 0.04799939141325329\n",
      "Epoch 170, Training Loss 0.04809614628210397\n",
      "Epoch 170, Training Loss 0.04824140550725905\n",
      "Epoch 170, Training Loss 0.04838728088209086\n",
      "Epoch 170, Training Loss 0.04857529683605484\n",
      "Epoch 170, Training Loss 0.04868798986877627\n",
      "Epoch 170, Training Loss 0.04874769341000511\n",
      "Epoch 170, Training Loss 0.048814196868435196\n",
      "Epoch 170, Training Loss 0.04889480311833227\n",
      "Epoch 170, Training Loss 0.048984798357424225\n",
      "Epoch 170, Training Loss 0.049212173746941645\n",
      "Epoch 170, Training Loss 0.04928287774171976\n",
      "Epoch 170, Training Loss 0.04942001011746619\n",
      "Epoch 170, Training Loss 0.04950995092539836\n",
      "Epoch 170, Training Loss 0.049579834284456184\n",
      "Epoch 170, Training Loss 0.04968593280066919\n",
      "Epoch 170, Training Loss 0.04978049569346411\n",
      "Epoch 170, Training Loss 0.04984716361250414\n",
      "Epoch 170, Training Loss 0.05004564908040149\n",
      "Epoch 170, Training Loss 0.05013112770512586\n",
      "Epoch 170, Training Loss 0.050274533850838764\n",
      "Epoch 170, Training Loss 0.05037316640891382\n",
      "Epoch 170, Training Loss 0.050431709017252066\n",
      "Epoch 170, Training Loss 0.05054914757437871\n",
      "Epoch 170, Training Loss 0.05069093854473832\n",
      "Epoch 170, Training Loss 0.05076982304358574\n",
      "Epoch 170, Training Loss 0.05087898853127761\n",
      "Epoch 170, Training Loss 0.05103780520255761\n",
      "Epoch 170, Training Loss 0.05111355087755586\n",
      "Epoch 170, Training Loss 0.05135939844772029\n",
      "Epoch 170, Training Loss 0.05152156136339278\n",
      "Epoch 170, Training Loss 0.0516115738760175\n",
      "Epoch 170, Training Loss 0.05172470842709627\n",
      "Epoch 170, Training Loss 0.05183294886137214\n",
      "Epoch 170, Training Loss 0.05193872514473813\n",
      "Epoch 170, Training Loss 0.05207785812523359\n",
      "Epoch 170, Training Loss 0.052222797804323914\n",
      "Epoch 170, Training Loss 0.052358355154009426\n",
      "Epoch 170, Training Loss 0.052486218683555\n",
      "Epoch 170, Training Loss 0.05258882884174357\n",
      "Epoch 170, Training Loss 0.05279317702097661\n",
      "Epoch 170, Training Loss 0.052858597082097815\n",
      "Epoch 170, Training Loss 0.05297461283557555\n",
      "Epoch 170, Training Loss 0.053039277482139484\n",
      "Epoch 170, Training Loss 0.05322362723595956\n",
      "Epoch 170, Training Loss 0.0534821750353212\n",
      "Epoch 170, Training Loss 0.05359343097299871\n",
      "Epoch 170, Training Loss 0.053708847373952644\n",
      "Epoch 170, Training Loss 0.05387903500319747\n",
      "Epoch 170, Training Loss 0.054004059899646\n",
      "Epoch 170, Training Loss 0.054191521823863546\n",
      "Epoch 170, Training Loss 0.05446788504757845\n",
      "Epoch 170, Training Loss 0.054575565249642445\n",
      "Epoch 170, Training Loss 0.05467908901860342\n",
      "Epoch 170, Training Loss 0.05473276291071149\n",
      "Epoch 170, Training Loss 0.05487594243300998\n",
      "Epoch 170, Training Loss 0.054958880381168\n",
      "Epoch 170, Training Loss 0.05506414196470662\n",
      "Epoch 170, Training Loss 0.05513659444020685\n",
      "Epoch 170, Training Loss 0.05521879234182103\n",
      "Epoch 170, Training Loss 0.055331303816660286\n",
      "Epoch 170, Training Loss 0.055503191245372036\n",
      "Epoch 170, Training Loss 0.0557326527784014\n",
      "Epoch 170, Training Loss 0.05586879629441692\n",
      "Epoch 170, Training Loss 0.05600179738991553\n",
      "Epoch 170, Training Loss 0.056050722875520397\n",
      "Epoch 170, Training Loss 0.0562537333611256\n",
      "Epoch 170, Training Loss 0.056299163209622166\n",
      "Epoch 170, Training Loss 0.05640566984043859\n",
      "Epoch 170, Training Loss 0.05651006989104821\n",
      "Epoch 170, Training Loss 0.05662181781476264\n",
      "Epoch 170, Training Loss 0.05668713891750102\n",
      "Epoch 170, Training Loss 0.05680212283225925\n",
      "Epoch 170, Training Loss 0.057057079642325106\n",
      "Epoch 170, Training Loss 0.05732323128320372\n",
      "Epoch 170, Training Loss 0.05746204166880349\n",
      "Epoch 170, Training Loss 0.05752136241978087\n",
      "Epoch 170, Training Loss 0.05767932899124787\n",
      "Epoch 170, Training Loss 0.05780521096171016\n",
      "Epoch 170, Training Loss 0.05793782878104988\n",
      "Epoch 170, Training Loss 0.0579902946787036\n",
      "Epoch 170, Training Loss 0.05823449852883511\n",
      "Epoch 170, Training Loss 0.05842934796095962\n",
      "Epoch 170, Training Loss 0.058554144137922454\n",
      "Epoch 170, Training Loss 0.05868278801574579\n",
      "Epoch 170, Training Loss 0.05875550927427571\n",
      "Epoch 170, Training Loss 0.058832111032417665\n",
      "Epoch 170, Training Loss 0.05893171979757526\n",
      "Epoch 170, Training Loss 0.05911092076193341\n",
      "Epoch 170, Training Loss 0.05915908078613031\n",
      "Epoch 170, Training Loss 0.059245765814200387\n",
      "Epoch 170, Training Loss 0.05931944839770684\n",
      "Epoch 170, Training Loss 0.05945267786017007\n",
      "Epoch 170, Training Loss 0.05954719560168436\n",
      "Epoch 170, Training Loss 0.05959168666749812\n",
      "Epoch 170, Training Loss 0.05964587010500376\n",
      "Epoch 170, Training Loss 0.05973305512229195\n",
      "Epoch 170, Training Loss 0.05984265930817255\n",
      "Epoch 170, Training Loss 0.05992228741688497\n",
      "Epoch 170, Training Loss 0.06013111916878034\n",
      "Epoch 170, Training Loss 0.06028930535135062\n",
      "Epoch 170, Training Loss 0.06033557472879167\n",
      "Epoch 170, Training Loss 0.06043749792820505\n",
      "Epoch 170, Training Loss 0.060515960883301544\n",
      "Epoch 170, Training Loss 0.06071318853694155\n",
      "Epoch 170, Training Loss 0.06078987867307023\n",
      "Epoch 170, Training Loss 0.06100742272613451\n",
      "Epoch 170, Training Loss 0.0610500950547283\n",
      "Epoch 170, Training Loss 0.0611614617792999\n",
      "Epoch 170, Training Loss 0.06121491114882862\n",
      "Epoch 170, Training Loss 0.06125282193950909\n",
      "Epoch 170, Training Loss 0.06135051641041589\n",
      "Epoch 170, Training Loss 0.06154667323841082\n",
      "Epoch 170, Training Loss 0.061655225348956594\n",
      "Epoch 170, Training Loss 0.06177513969971624\n",
      "Epoch 170, Training Loss 0.06191874483881323\n",
      "Epoch 170, Training Loss 0.062040066815760284\n",
      "Epoch 170, Training Loss 0.06217368739559446\n",
      "Epoch 170, Training Loss 0.062305411164317745\n",
      "Epoch 170, Training Loss 0.062336993063121196\n",
      "Epoch 170, Training Loss 0.06244137316294338\n",
      "Epoch 170, Training Loss 0.06250181726044249\n",
      "Epoch 170, Training Loss 0.0626353249954217\n",
      "Epoch 170, Training Loss 0.06287683762343185\n",
      "Epoch 170, Training Loss 0.06301632546879293\n",
      "Epoch 170, Training Loss 0.06308744187035677\n",
      "Epoch 170, Training Loss 0.06315945205576431\n",
      "Epoch 170, Training Loss 0.0633571938185207\n",
      "Epoch 170, Training Loss 0.063467782357579\n",
      "Epoch 170, Training Loss 0.06358875712032055\n",
      "Epoch 170, Training Loss 0.0636722508513028\n",
      "Epoch 170, Training Loss 0.0637996082939684\n",
      "Epoch 170, Training Loss 0.06401738339125196\n",
      "Epoch 170, Training Loss 0.06417087040117482\n",
      "Epoch 170, Training Loss 0.06426410352730233\n",
      "Epoch 170, Training Loss 0.06465669798062128\n",
      "Epoch 170, Training Loss 0.06499453722153936\n",
      "Epoch 170, Training Loss 0.06510879103656468\n",
      "Epoch 170, Training Loss 0.0654495833751262\n",
      "Epoch 170, Training Loss 0.0657032228806211\n",
      "Epoch 170, Training Loss 0.06586420450769269\n",
      "Epoch 170, Training Loss 0.0660552469027393\n",
      "Epoch 170, Training Loss 0.06617883442784361\n",
      "Epoch 170, Training Loss 0.06629769488349747\n",
      "Epoch 170, Training Loss 0.06639054823962166\n",
      "Epoch 170, Training Loss 0.06654219682354603\n",
      "Epoch 170, Training Loss 0.06670429763834343\n",
      "Epoch 170, Training Loss 0.06687983303137905\n",
      "Epoch 170, Training Loss 0.06698196281767105\n",
      "Epoch 170, Training Loss 0.0670816921926749\n",
      "Epoch 170, Training Loss 0.06721471816472843\n",
      "Epoch 170, Training Loss 0.06728386942802182\n",
      "Epoch 170, Training Loss 0.06740743125719792\n",
      "Epoch 170, Training Loss 0.06746992989517081\n",
      "Epoch 170, Training Loss 0.0675687856467255\n",
      "Epoch 170, Training Loss 0.06764067589874616\n",
      "Epoch 170, Training Loss 0.06767233850105721\n",
      "Epoch 170, Training Loss 0.06777757876183448\n",
      "Epoch 170, Training Loss 0.06791764727610228\n",
      "Epoch 170, Training Loss 0.06799735827490573\n",
      "Epoch 170, Training Loss 0.06822531075095353\n",
      "Epoch 170, Training Loss 0.06828103030381528\n",
      "Epoch 170, Training Loss 0.06838673634021103\n",
      "Epoch 170, Training Loss 0.06842766434211484\n",
      "Epoch 170, Training Loss 0.0686067559296632\n",
      "Epoch 170, Training Loss 0.06887283056850552\n",
      "Epoch 170, Training Loss 0.06895805050945267\n",
      "Epoch 170, Training Loss 0.06918254322932123\n",
      "Epoch 170, Training Loss 0.06928037211556187\n",
      "Epoch 170, Training Loss 0.06936135711600942\n",
      "Epoch 170, Training Loss 0.06950442889071715\n",
      "Epoch 170, Training Loss 0.06969082366694193\n",
      "Epoch 170, Training Loss 0.0698219189572784\n",
      "Epoch 170, Training Loss 0.06994340289384127\n",
      "Epoch 170, Training Loss 0.07021325025135827\n",
      "Epoch 170, Training Loss 0.07033220279123396\n",
      "Epoch 170, Training Loss 0.07045830967967086\n",
      "Epoch 170, Training Loss 0.07058582428957114\n",
      "Epoch 170, Training Loss 0.0708198118502336\n",
      "Epoch 170, Training Loss 0.07103658767412309\n",
      "Epoch 170, Training Loss 0.0712040611197386\n",
      "Epoch 170, Training Loss 0.0712452224572487\n",
      "Epoch 170, Training Loss 0.0713342180323151\n",
      "Epoch 170, Training Loss 0.07150132992469213\n",
      "Epoch 170, Training Loss 0.0715801738931433\n",
      "Epoch 170, Training Loss 0.071657979077257\n",
      "Epoch 170, Training Loss 0.0718604048351993\n",
      "Epoch 170, Training Loss 0.07207166334218762\n",
      "Epoch 170, Training Loss 0.07216422161911532\n",
      "Epoch 170, Training Loss 0.07233119221723369\n",
      "Epoch 170, Training Loss 0.07248548943492229\n",
      "Epoch 170, Training Loss 0.07255593976696663\n",
      "Epoch 170, Training Loss 0.07261180287689123\n",
      "Epoch 170, Training Loss 0.07276438950987348\n",
      "Epoch 170, Training Loss 0.07290724668499378\n",
      "Epoch 170, Training Loss 0.07306388121269777\n",
      "Epoch 170, Training Loss 0.07317522650970446\n",
      "Epoch 170, Training Loss 0.07325421417813244\n",
      "Epoch 170, Training Loss 0.07342217158754845\n",
      "Epoch 170, Training Loss 0.07348948192384923\n",
      "Epoch 170, Training Loss 0.07362962375297342\n",
      "Epoch 170, Training Loss 0.0737660971239133\n",
      "Epoch 170, Training Loss 0.07394756346493198\n",
      "Epoch 170, Training Loss 0.07403626729546071\n",
      "Epoch 170, Training Loss 0.07423054438579799\n",
      "Epoch 170, Training Loss 0.07430320466770922\n",
      "Epoch 170, Training Loss 0.0744397152248589\n",
      "Epoch 170, Training Loss 0.07454947560615933\n",
      "Epoch 170, Training Loss 0.07465767844453873\n",
      "Epoch 170, Training Loss 0.07484751500789542\n",
      "Epoch 170, Training Loss 0.0749821064784132\n",
      "Epoch 170, Training Loss 0.07503862209055963\n",
      "Epoch 170, Training Loss 0.07512482637515687\n",
      "Epoch 170, Training Loss 0.07522313860590424\n",
      "Epoch 170, Training Loss 0.07549904519096588\n",
      "Epoch 170, Training Loss 0.07566098094967853\n",
      "Epoch 170, Training Loss 0.07589293659790931\n",
      "Epoch 170, Training Loss 0.0759678089162311\n",
      "Epoch 170, Training Loss 0.07604369494701972\n",
      "Epoch 170, Training Loss 0.07623463270999968\n",
      "Epoch 170, Training Loss 0.07631464221793444\n",
      "Epoch 170, Training Loss 0.07641467938194875\n",
      "Epoch 170, Training Loss 0.07652485711009377\n",
      "Epoch 170, Training Loss 0.07671235958376275\n",
      "Epoch 170, Training Loss 0.07680063420558905\n",
      "Epoch 170, Training Loss 0.07690598260458854\n",
      "Epoch 170, Training Loss 0.07714218535053227\n",
      "Epoch 170, Training Loss 0.07719645803779973\n",
      "Epoch 170, Training Loss 0.07730276251206998\n",
      "Epoch 170, Training Loss 0.0773384486684752\n",
      "Epoch 170, Training Loss 0.07743695666275137\n",
      "Epoch 170, Training Loss 0.07767504059454745\n",
      "Epoch 170, Training Loss 0.07783319871834553\n",
      "Epoch 170, Training Loss 0.07796595222018946\n",
      "Epoch 170, Training Loss 0.0782039257652505\n",
      "Epoch 170, Training Loss 0.07831489568447594\n",
      "Epoch 170, Training Loss 0.07840014211214183\n",
      "Epoch 170, Training Loss 0.07869000445403483\n",
      "Epoch 170, Training Loss 0.07881039563242508\n",
      "Epoch 170, Training Loss 0.07895389942647628\n",
      "Epoch 170, Training Loss 0.07909332571403525\n",
      "Epoch 170, Training Loss 0.07918038000316953\n",
      "Epoch 170, Training Loss 0.07930402220715113\n",
      "Epoch 170, Training Loss 0.07937554126400548\n",
      "Epoch 170, Training Loss 0.07970875882021987\n",
      "Epoch 170, Training Loss 0.07993405690783506\n",
      "Epoch 170, Training Loss 0.08007810971058925\n",
      "Epoch 170, Training Loss 0.08019365445779794\n",
      "Epoch 170, Training Loss 0.0803596115864032\n",
      "Epoch 170, Training Loss 0.08059128544643483\n",
      "Epoch 170, Training Loss 0.080751909960128\n",
      "Epoch 170, Training Loss 0.0808501362424258\n",
      "Epoch 170, Training Loss 0.08109353722104103\n",
      "Epoch 170, Training Loss 0.08121710146069908\n",
      "Epoch 170, Training Loss 0.08125625645070125\n",
      "Epoch 170, Training Loss 0.08138470957651163\n",
      "Epoch 170, Training Loss 0.0815742954687999\n",
      "Epoch 170, Training Loss 0.08170058547764483\n",
      "Epoch 170, Training Loss 0.08183376264312993\n",
      "Epoch 170, Training Loss 0.08197099716424028\n",
      "Epoch 170, Training Loss 0.08205956748455687\n",
      "Epoch 170, Training Loss 0.08212991921550325\n",
      "Epoch 170, Training Loss 0.08224374746136806\n",
      "Epoch 170, Training Loss 0.08236364363347326\n",
      "Epoch 170, Training Loss 0.08244646497337563\n",
      "Epoch 170, Training Loss 0.08275876928339987\n",
      "Epoch 170, Training Loss 0.08291345264505395\n",
      "Epoch 170, Training Loss 0.08302811153537934\n",
      "Epoch 170, Training Loss 0.08312599313781237\n",
      "Epoch 170, Training Loss 0.08318120849025828\n",
      "Epoch 170, Training Loss 0.08332967863935034\n",
      "Epoch 170, Training Loss 0.08339849218268834\n",
      "Epoch 170, Training Loss 0.08375525864227043\n",
      "Epoch 170, Training Loss 0.08397003683402106\n",
      "Epoch 170, Training Loss 0.08414178222532162\n",
      "Epoch 170, Training Loss 0.08428475447475453\n",
      "Epoch 170, Training Loss 0.08439935069254902\n",
      "Epoch 170, Training Loss 0.08452965892717966\n",
      "Epoch 170, Training Loss 0.08460725611433044\n",
      "Epoch 170, Training Loss 0.08482634171348094\n",
      "Epoch 170, Training Loss 0.08497720366091374\n",
      "Epoch 170, Training Loss 0.08521055416835238\n",
      "Epoch 170, Training Loss 0.0853932110778511\n",
      "Epoch 170, Training Loss 0.08547461588326317\n",
      "Epoch 170, Training Loss 0.08557829506638105\n",
      "Epoch 170, Training Loss 0.0856928098422792\n",
      "Epoch 170, Training Loss 0.08578751935525929\n",
      "Epoch 170, Training Loss 0.08590302674476143\n",
      "Epoch 170, Training Loss 0.08594661468015913\n",
      "Epoch 170, Training Loss 0.08606982215895982\n",
      "Epoch 170, Training Loss 0.08625545146901284\n",
      "Epoch 170, Training Loss 0.0864184095960139\n",
      "Epoch 170, Training Loss 0.08659714866248543\n",
      "Epoch 170, Training Loss 0.08673431805294493\n",
      "Epoch 170, Training Loss 0.08694214218527155\n",
      "Epoch 170, Training Loss 0.08699009130658854\n",
      "Epoch 170, Training Loss 0.08718118766594268\n",
      "Epoch 170, Training Loss 0.08728378357084664\n",
      "Epoch 170, Training Loss 0.08733202737120106\n",
      "Epoch 170, Training Loss 0.0875168486457804\n",
      "Epoch 170, Training Loss 0.08760063527413951\n",
      "Epoch 170, Training Loss 0.08769055161520344\n",
      "Epoch 170, Training Loss 0.08777540813550315\n",
      "Epoch 170, Training Loss 0.08788463670541258\n",
      "Epoch 170, Training Loss 0.08806800046730834\n",
      "Epoch 170, Training Loss 0.0881768863581483\n",
      "Epoch 170, Training Loss 0.08833586206407193\n",
      "Epoch 170, Training Loss 0.08838397485520834\n",
      "Epoch 170, Training Loss 0.08849884862142146\n",
      "Epoch 170, Training Loss 0.08857301606908631\n",
      "Epoch 170, Training Loss 0.08861333321865715\n",
      "Epoch 170, Training Loss 0.08874166139003718\n",
      "Epoch 170, Training Loss 0.08894240348826131\n",
      "Epoch 170, Training Loss 0.08909430799295988\n",
      "Epoch 170, Training Loss 0.08930371805091801\n",
      "Epoch 170, Training Loss 0.08940792661588973\n",
      "Epoch 170, Training Loss 0.08958606359541721\n",
      "Epoch 170, Training Loss 0.08979106159485362\n",
      "Epoch 170, Training Loss 0.08995721655447617\n",
      "Epoch 170, Training Loss 0.0901163193299566\n",
      "Epoch 170, Training Loss 0.09024243633670118\n",
      "Epoch 170, Training Loss 0.09031996259565854\n",
      "Epoch 170, Training Loss 0.09048875730932521\n",
      "Epoch 170, Training Loss 0.09059655018474745\n",
      "Epoch 170, Training Loss 0.09078024352526726\n",
      "Epoch 170, Training Loss 0.09088180540963207\n",
      "Epoch 170, Training Loss 0.09098117386974643\n",
      "Epoch 170, Training Loss 0.09104823105780364\n",
      "Epoch 170, Training Loss 0.09130097885647089\n",
      "Epoch 170, Training Loss 0.09141908686065003\n",
      "Epoch 170, Training Loss 0.09156608400518632\n",
      "Epoch 170, Training Loss 0.09166581573350654\n",
      "Epoch 170, Training Loss 0.09179407197153172\n",
      "Epoch 170, Training Loss 0.09192766656960978\n",
      "Epoch 170, Training Loss 0.09202291222903734\n",
      "Epoch 170, Training Loss 0.09213676071151748\n",
      "Epoch 170, Training Loss 0.09227691444060991\n",
      "Epoch 170, Training Loss 0.0924327498506707\n",
      "Epoch 170, Training Loss 0.09264524694522629\n",
      "Epoch 170, Training Loss 0.09271548388768798\n",
      "Epoch 170, Training Loss 0.09287593619959891\n",
      "Epoch 170, Training Loss 0.09308338382036027\n",
      "Epoch 170, Training Loss 0.09325391526245858\n",
      "Epoch 170, Training Loss 0.09348349240334596\n",
      "Epoch 170, Training Loss 0.09357758358482966\n",
      "Epoch 170, Training Loss 0.09370481064233481\n",
      "Epoch 170, Training Loss 0.09385608286713548\n",
      "Epoch 170, Training Loss 0.09399590442609757\n",
      "Epoch 170, Training Loss 0.09407154844163934\n",
      "Epoch 170, Training Loss 0.09416030010070338\n",
      "Epoch 170, Training Loss 0.09435003323247061\n",
      "Epoch 170, Training Loss 0.09438621348289349\n",
      "Epoch 170, Training Loss 0.09449730390716163\n",
      "Epoch 170, Training Loss 0.09464940119325124\n",
      "Epoch 170, Training Loss 0.0947335114169037\n",
      "Epoch 170, Training Loss 0.09479764740094733\n",
      "Epoch 170, Training Loss 0.09488028542036214\n",
      "Epoch 170, Training Loss 0.09502063977205769\n",
      "Epoch 170, Training Loss 0.09514129206376232\n",
      "Epoch 170, Training Loss 0.09526845558172525\n",
      "Epoch 170, Training Loss 0.09535915023216125\n",
      "Epoch 170, Training Loss 0.09543097588231268\n",
      "Epoch 170, Training Loss 0.09558580979904936\n",
      "Epoch 170, Training Loss 0.09560495978364211\n",
      "Epoch 170, Training Loss 0.09575115029326142\n",
      "Epoch 170, Training Loss 0.09602498740810529\n",
      "Epoch 170, Training Loss 0.09610811349771478\n",
      "Epoch 170, Training Loss 0.0962806352738129\n",
      "Epoch 170, Training Loss 0.09641867641078504\n",
      "Epoch 170, Training Loss 0.09648869668140703\n",
      "Epoch 170, Training Loss 0.09661537417760857\n",
      "Epoch 170, Training Loss 0.09675192701823228\n",
      "Epoch 170, Training Loss 0.09692034668758359\n",
      "Epoch 170, Training Loss 0.09696904113373297\n",
      "Epoch 170, Training Loss 0.09714143927973669\n",
      "Epoch 170, Training Loss 0.0972466293014967\n",
      "Epoch 170, Training Loss 0.09739363808875613\n",
      "Epoch 170, Training Loss 0.09763407430794957\n",
      "Epoch 170, Training Loss 0.09774225532813259\n",
      "Epoch 170, Training Loss 0.09782535123431584\n",
      "Epoch 180, Training Loss 0.0003256729191831311\n",
      "Epoch 180, Training Loss 0.0003628548058440618\n",
      "Epoch 180, Training Loss 0.0005114835798930939\n",
      "Epoch 180, Training Loss 0.0006094396595492998\n",
      "Epoch 180, Training Loss 0.0007224367369356973\n",
      "Epoch 180, Training Loss 0.0008671552614520883\n",
      "Epoch 180, Training Loss 0.0009920092042335464\n",
      "Epoch 180, Training Loss 0.0011931933448328387\n",
      "Epoch 180, Training Loss 0.0012579254848915902\n",
      "Epoch 180, Training Loss 0.0012943516544940527\n",
      "Epoch 180, Training Loss 0.001395501737075541\n",
      "Epoch 180, Training Loss 0.0015351770855391118\n",
      "Epoch 180, Training Loss 0.0016517708116137158\n",
      "Epoch 180, Training Loss 0.0016996220749852908\n",
      "Epoch 180, Training Loss 0.0017854153581173219\n",
      "Epoch 180, Training Loss 0.0018443569726765614\n",
      "Epoch 180, Training Loss 0.0019429126906844662\n",
      "Epoch 180, Training Loss 0.001995034299104872\n",
      "Epoch 180, Training Loss 0.002171811328538696\n",
      "Epoch 180, Training Loss 0.002218231227715759\n",
      "Epoch 180, Training Loss 0.0022738032028688797\n",
      "Epoch 180, Training Loss 0.0023488882247863522\n",
      "Epoch 180, Training Loss 0.0024478676826562113\n",
      "Epoch 180, Training Loss 0.0025246359403137963\n",
      "Epoch 180, Training Loss 0.002564648988054079\n",
      "Epoch 180, Training Loss 0.0026540188547557273\n",
      "Epoch 180, Training Loss 0.0027254584657452298\n",
      "Epoch 180, Training Loss 0.002782049060077466\n",
      "Epoch 180, Training Loss 0.002904568646398499\n",
      "Epoch 180, Training Loss 0.0030786623616162164\n",
      "Epoch 180, Training Loss 0.003159355179256643\n",
      "Epoch 180, Training Loss 0.0032416358852134946\n",
      "Epoch 180, Training Loss 0.00329745992246415\n",
      "Epoch 180, Training Loss 0.0034102834260943907\n",
      "Epoch 180, Training Loss 0.0035004408534644814\n",
      "Epoch 180, Training Loss 0.0035827744471104554\n",
      "Epoch 180, Training Loss 0.003746787393394181\n",
      "Epoch 180, Training Loss 0.003813101986275457\n",
      "Epoch 180, Training Loss 0.0038833202498838723\n",
      "Epoch 180, Training Loss 0.003952644877326305\n",
      "Epoch 180, Training Loss 0.003995587370451302\n",
      "Epoch 180, Training Loss 0.0040296487810323614\n",
      "Epoch 180, Training Loss 0.004206014427897113\n",
      "Epoch 180, Training Loss 0.0042597002247372244\n",
      "Epoch 180, Training Loss 0.004338172140538388\n",
      "Epoch 180, Training Loss 0.004478904251914348\n",
      "Epoch 180, Training Loss 0.004504708427926311\n",
      "Epoch 180, Training Loss 0.004678797531787239\n",
      "Epoch 180, Training Loss 0.004788757306630807\n",
      "Epoch 180, Training Loss 0.004872424830503933\n",
      "Epoch 180, Training Loss 0.005015662185314214\n",
      "Epoch 180, Training Loss 0.005306308495971706\n",
      "Epoch 180, Training Loss 0.005439592637312229\n",
      "Epoch 180, Training Loss 0.0055767660770003145\n",
      "Epoch 180, Training Loss 0.0056837593774547055\n",
      "Epoch 180, Training Loss 0.005780798330655336\n",
      "Epoch 180, Training Loss 0.005914242964361787\n",
      "Epoch 180, Training Loss 0.005996728446953894\n",
      "Epoch 180, Training Loss 0.00613187846329892\n",
      "Epoch 180, Training Loss 0.006194972709450118\n",
      "Epoch 180, Training Loss 0.006368952588942807\n",
      "Epoch 180, Training Loss 0.006494230014341109\n",
      "Epoch 180, Training Loss 0.006566920136685109\n",
      "Epoch 180, Training Loss 0.006625105774558871\n",
      "Epoch 180, Training Loss 0.0067512442183959515\n",
      "Epoch 180, Training Loss 0.006961648233349213\n",
      "Epoch 180, Training Loss 0.00714328610921836\n",
      "Epoch 180, Training Loss 0.007207000854394167\n",
      "Epoch 180, Training Loss 0.0072952243797195235\n",
      "Epoch 180, Training Loss 0.007338743735476375\n",
      "Epoch 180, Training Loss 0.007428317235501678\n",
      "Epoch 180, Training Loss 0.007570469073112816\n",
      "Epoch 180, Training Loss 0.007674045825038877\n",
      "Epoch 180, Training Loss 0.007758120177289866\n",
      "Epoch 180, Training Loss 0.007886159862570293\n",
      "Epoch 180, Training Loss 0.00802280994184563\n",
      "Epoch 180, Training Loss 0.008065104753712712\n",
      "Epoch 180, Training Loss 0.008161159115545737\n",
      "Epoch 180, Training Loss 0.008291566056077894\n",
      "Epoch 180, Training Loss 0.00838158728645357\n",
      "Epoch 180, Training Loss 0.008484293292264652\n",
      "Epoch 180, Training Loss 0.008517603416119695\n",
      "Epoch 180, Training Loss 0.00857740187603037\n",
      "Epoch 180, Training Loss 0.00867143026112443\n",
      "Epoch 180, Training Loss 0.008723610330878012\n",
      "Epoch 180, Training Loss 0.00892292336582223\n",
      "Epoch 180, Training Loss 0.008965446797135236\n",
      "Epoch 180, Training Loss 0.009175606410178687\n",
      "Epoch 180, Training Loss 0.009340124052313282\n",
      "Epoch 180, Training Loss 0.009391988596647902\n",
      "Epoch 180, Training Loss 0.009492555742754656\n",
      "Epoch 180, Training Loss 0.009580197903658728\n",
      "Epoch 180, Training Loss 0.009710653863675759\n",
      "Epoch 180, Training Loss 0.009759091817395156\n",
      "Epoch 180, Training Loss 0.009815576236190088\n",
      "Epoch 180, Training Loss 0.009919431527404834\n",
      "Epoch 180, Training Loss 0.010001345363724263\n",
      "Epoch 180, Training Loss 0.010092266494660732\n",
      "Epoch 180, Training Loss 0.010316022940913735\n",
      "Epoch 180, Training Loss 0.010409747671974284\n",
      "Epoch 180, Training Loss 0.010506439029865557\n",
      "Epoch 180, Training Loss 0.010606031760077952\n",
      "Epoch 180, Training Loss 0.010808283734656966\n",
      "Epoch 180, Training Loss 0.010943529350906991\n",
      "Epoch 180, Training Loss 0.011088748350548927\n",
      "Epoch 180, Training Loss 0.011249589998170238\n",
      "Epoch 180, Training Loss 0.011375775622666034\n",
      "Epoch 180, Training Loss 0.011694831054305176\n",
      "Epoch 180, Training Loss 0.01178322136973786\n",
      "Epoch 180, Training Loss 0.011930931759688556\n",
      "Epoch 180, Training Loss 0.012044804356515865\n",
      "Epoch 180, Training Loss 0.01216633798902297\n",
      "Epoch 180, Training Loss 0.012261206012628877\n",
      "Epoch 180, Training Loss 0.012332047142869677\n",
      "Epoch 180, Training Loss 0.012367271653869572\n",
      "Epoch 180, Training Loss 0.012535787266119362\n",
      "Epoch 180, Training Loss 0.01264285811168306\n",
      "Epoch 180, Training Loss 0.012730012431054774\n",
      "Epoch 180, Training Loss 0.012830148310498203\n",
      "Epoch 180, Training Loss 0.013005191908048852\n",
      "Epoch 180, Training Loss 0.01309328370482263\n",
      "Epoch 180, Training Loss 0.013186435979764785\n",
      "Epoch 180, Training Loss 0.013286820434205367\n",
      "Epoch 180, Training Loss 0.01349297173016364\n",
      "Epoch 180, Training Loss 0.013780588893901052\n",
      "Epoch 180, Training Loss 0.013888433475590423\n",
      "Epoch 180, Training Loss 0.01394952960369532\n",
      "Epoch 180, Training Loss 0.014013403555011506\n",
      "Epoch 180, Training Loss 0.014145816635826359\n",
      "Epoch 180, Training Loss 0.014189291502470556\n",
      "Epoch 180, Training Loss 0.01432068039522604\n",
      "Epoch 180, Training Loss 0.014412732632911724\n",
      "Epoch 180, Training Loss 0.0144771893539697\n",
      "Epoch 180, Training Loss 0.014532406283232867\n",
      "Epoch 180, Training Loss 0.014673826353781668\n",
      "Epoch 180, Training Loss 0.014841318530652224\n",
      "Epoch 180, Training Loss 0.0148892075757084\n",
      "Epoch 180, Training Loss 0.014960656080709393\n",
      "Epoch 180, Training Loss 0.01501154936278415\n",
      "Epoch 180, Training Loss 0.015123681772662245\n",
      "Epoch 180, Training Loss 0.015225767217519339\n",
      "Epoch 180, Training Loss 0.015288376051675328\n",
      "Epoch 180, Training Loss 0.015374105490382066\n",
      "Epoch 180, Training Loss 0.015492248117847517\n",
      "Epoch 180, Training Loss 0.015596236909746819\n",
      "Epoch 180, Training Loss 0.01566317092979808\n",
      "Epoch 180, Training Loss 0.01580310121769338\n",
      "Epoch 180, Training Loss 0.01596081506013108\n",
      "Epoch 180, Training Loss 0.01600877487617533\n",
      "Epoch 180, Training Loss 0.016076426162286793\n",
      "Epoch 180, Training Loss 0.016229511726923915\n",
      "Epoch 180, Training Loss 0.016311487435456126\n",
      "Epoch 180, Training Loss 0.016490814488028627\n",
      "Epoch 180, Training Loss 0.016710592743457126\n",
      "Epoch 180, Training Loss 0.016794613255259327\n",
      "Epoch 180, Training Loss 0.01689787008954436\n",
      "Epoch 180, Training Loss 0.01694988477927492\n",
      "Epoch 180, Training Loss 0.017052397917946587\n",
      "Epoch 180, Training Loss 0.01710260298360339\n",
      "Epoch 180, Training Loss 0.017135356700576632\n",
      "Epoch 180, Training Loss 0.01717247763443786\n",
      "Epoch 180, Training Loss 0.01723121958391746\n",
      "Epoch 180, Training Loss 0.017289512140481064\n",
      "Epoch 180, Training Loss 0.017405514097998818\n",
      "Epoch 180, Training Loss 0.017538067224957144\n",
      "Epoch 180, Training Loss 0.01767559580581115\n",
      "Epoch 180, Training Loss 0.017716980798889306\n",
      "Epoch 180, Training Loss 0.017824270140827464\n",
      "Epoch 180, Training Loss 0.017906005630064805\n",
      "Epoch 180, Training Loss 0.017982561598577158\n",
      "Epoch 180, Training Loss 0.01807250381659364\n",
      "Epoch 180, Training Loss 0.018176608283044127\n",
      "Epoch 180, Training Loss 0.018263368664876274\n",
      "Epoch 180, Training Loss 0.018350110258287786\n",
      "Epoch 180, Training Loss 0.018506349695613013\n",
      "Epoch 180, Training Loss 0.01861670562792617\n",
      "Epoch 180, Training Loss 0.018734103178276736\n",
      "Epoch 180, Training Loss 0.01888380558861186\n",
      "Epoch 180, Training Loss 0.019028494339388656\n",
      "Epoch 180, Training Loss 0.01914241918555611\n",
      "Epoch 180, Training Loss 0.019466832594569685\n",
      "Epoch 180, Training Loss 0.019569700712438128\n",
      "Epoch 180, Training Loss 0.01966184724474807\n",
      "Epoch 180, Training Loss 0.019830463620860255\n",
      "Epoch 180, Training Loss 0.01994397532185325\n",
      "Epoch 180, Training Loss 0.02003679745604315\n",
      "Epoch 180, Training Loss 0.020150720234722128\n",
      "Epoch 180, Training Loss 0.020388897251137687\n",
      "Epoch 180, Training Loss 0.02052955596190889\n",
      "Epoch 180, Training Loss 0.020605156145742178\n",
      "Epoch 180, Training Loss 0.020710534185094907\n",
      "Epoch 180, Training Loss 0.020801511957593586\n",
      "Epoch 180, Training Loss 0.020954762968946907\n",
      "Epoch 180, Training Loss 0.021014633295519274\n",
      "Epoch 180, Training Loss 0.021163593734736027\n",
      "Epoch 180, Training Loss 0.02136645962953415\n",
      "Epoch 180, Training Loss 0.021437871343720598\n",
      "Epoch 180, Training Loss 0.02156360407390863\n",
      "Epoch 180, Training Loss 0.02177871083435805\n",
      "Epoch 180, Training Loss 0.021955009121114335\n",
      "Epoch 180, Training Loss 0.022143463287359614\n",
      "Epoch 180, Training Loss 0.022208492578867145\n",
      "Epoch 180, Training Loss 0.022300031174288686\n",
      "Epoch 180, Training Loss 0.0223493483536841\n",
      "Epoch 180, Training Loss 0.0224269406103036\n",
      "Epoch 180, Training Loss 0.022619932580291463\n",
      "Epoch 180, Training Loss 0.022742680009559292\n",
      "Epoch 180, Training Loss 0.022828387849204376\n",
      "Epoch 180, Training Loss 0.02290875864360491\n",
      "Epoch 180, Training Loss 0.02300140918577876\n",
      "Epoch 180, Training Loss 0.023084473003016408\n",
      "Epoch 180, Training Loss 0.02338441245524627\n",
      "Epoch 180, Training Loss 0.023562344705776485\n",
      "Epoch 180, Training Loss 0.023707201447137786\n",
      "Epoch 180, Training Loss 0.023842966355516783\n",
      "Epoch 180, Training Loss 0.023938667681782752\n",
      "Epoch 180, Training Loss 0.02407875402218393\n",
      "Epoch 180, Training Loss 0.02419478891660338\n",
      "Epoch 180, Training Loss 0.02424944079745456\n",
      "Epoch 180, Training Loss 0.024289716740169793\n",
      "Epoch 180, Training Loss 0.024410745486274095\n",
      "Epoch 180, Training Loss 0.024464110965314118\n",
      "Epoch 180, Training Loss 0.024674631121670804\n",
      "Epoch 180, Training Loss 0.024696749203917012\n",
      "Epoch 180, Training Loss 0.024854485569593243\n",
      "Epoch 180, Training Loss 0.025006562285601636\n",
      "Epoch 180, Training Loss 0.025080662816191267\n",
      "Epoch 180, Training Loss 0.025199910873532905\n",
      "Epoch 180, Training Loss 0.025326565798857937\n",
      "Epoch 180, Training Loss 0.025391522575827205\n",
      "Epoch 180, Training Loss 0.025464956846345418\n",
      "Epoch 180, Training Loss 0.025534074734467678\n",
      "Epoch 180, Training Loss 0.025591188670157472\n",
      "Epoch 180, Training Loss 0.025701780353322663\n",
      "Epoch 180, Training Loss 0.025904001915813102\n",
      "Epoch 180, Training Loss 0.025967067717324436\n",
      "Epoch 180, Training Loss 0.026078077404738387\n",
      "Epoch 180, Training Loss 0.0263027660501049\n",
      "Epoch 180, Training Loss 0.02639094599620308\n",
      "Epoch 180, Training Loss 0.02644893624212431\n",
      "Epoch 180, Training Loss 0.02651563512585352\n",
      "Epoch 180, Training Loss 0.02661524258573037\n",
      "Epoch 180, Training Loss 0.026772657871398778\n",
      "Epoch 180, Training Loss 0.02682948198712543\n",
      "Epoch 180, Training Loss 0.02697731272014968\n",
      "Epoch 180, Training Loss 0.02710167965029969\n",
      "Epoch 180, Training Loss 0.027350682036384293\n",
      "Epoch 180, Training Loss 0.027414863567103816\n",
      "Epoch 180, Training Loss 0.027453840581600168\n",
      "Epoch 180, Training Loss 0.027617122303894567\n",
      "Epoch 180, Training Loss 0.027717920351306647\n",
      "Epoch 180, Training Loss 0.027879519995463932\n",
      "Epoch 180, Training Loss 0.027919049886867518\n",
      "Epoch 180, Training Loss 0.027992139329843204\n",
      "Epoch 180, Training Loss 0.02817489856573017\n",
      "Epoch 180, Training Loss 0.0282515796839886\n",
      "Epoch 180, Training Loss 0.028415974534457298\n",
      "Epoch 180, Training Loss 0.028635400135422606\n",
      "Epoch 180, Training Loss 0.028748046673472274\n",
      "Epoch 180, Training Loss 0.028824121293509402\n",
      "Epoch 180, Training Loss 0.028950962664373695\n",
      "Epoch 180, Training Loss 0.02915067338120297\n",
      "Epoch 180, Training Loss 0.029364164661416007\n",
      "Epoch 180, Training Loss 0.02962617598988516\n",
      "Epoch 180, Training Loss 0.029856706652647395\n",
      "Epoch 180, Training Loss 0.029922412746512066\n",
      "Epoch 180, Training Loss 0.030045233109532415\n",
      "Epoch 180, Training Loss 0.030166171829375768\n",
      "Epoch 180, Training Loss 0.03022136361531132\n",
      "Epoch 180, Training Loss 0.030372724732588927\n",
      "Epoch 180, Training Loss 0.030461130215955513\n",
      "Epoch 180, Training Loss 0.030583487061397803\n",
      "Epoch 180, Training Loss 0.03078297881500038\n",
      "Epoch 180, Training Loss 0.030829486053656128\n",
      "Epoch 180, Training Loss 0.030991078171012043\n",
      "Epoch 180, Training Loss 0.031221020790507727\n",
      "Epoch 180, Training Loss 0.03152532902691523\n",
      "Epoch 180, Training Loss 0.0315993160885923\n",
      "Epoch 180, Training Loss 0.03175375122777031\n",
      "Epoch 180, Training Loss 0.03179556469116217\n",
      "Epoch 180, Training Loss 0.03187190355909297\n",
      "Epoch 180, Training Loss 0.03197077519315131\n",
      "Epoch 180, Training Loss 0.032110062253940135\n",
      "Epoch 180, Training Loss 0.03244284554229825\n",
      "Epoch 180, Training Loss 0.03256879466326188\n",
      "Epoch 180, Training Loss 0.03263954448101618\n",
      "Epoch 180, Training Loss 0.03272223736986023\n",
      "Epoch 180, Training Loss 0.032778291374711735\n",
      "Epoch 180, Training Loss 0.032886116586797076\n",
      "Epoch 180, Training Loss 0.032937018696190146\n",
      "Epoch 180, Training Loss 0.032989247856885576\n",
      "Epoch 180, Training Loss 0.033113342478795126\n",
      "Epoch 180, Training Loss 0.03315759747934616\n",
      "Epoch 180, Training Loss 0.033252428173828306\n",
      "Epoch 180, Training Loss 0.03335345627458961\n",
      "Epoch 180, Training Loss 0.03351581035196172\n",
      "Epoch 180, Training Loss 0.03373202881144593\n",
      "Epoch 180, Training Loss 0.034093648704993144\n",
      "Epoch 180, Training Loss 0.03427029583517395\n",
      "Epoch 180, Training Loss 0.03437572172211716\n",
      "Epoch 180, Training Loss 0.03452881818632488\n",
      "Epoch 180, Training Loss 0.03469306457778225\n",
      "Epoch 180, Training Loss 0.03486261429631954\n",
      "Epoch 180, Training Loss 0.034965731255957845\n",
      "Epoch 180, Training Loss 0.03508819144724123\n",
      "Epoch 180, Training Loss 0.035153157189678964\n",
      "Epoch 180, Training Loss 0.03520783019797576\n",
      "Epoch 180, Training Loss 0.0353254814396429\n",
      "Epoch 180, Training Loss 0.03549820379070614\n",
      "Epoch 180, Training Loss 0.035598050078848746\n",
      "Epoch 180, Training Loss 0.03574323282598534\n",
      "Epoch 180, Training Loss 0.035808334789236486\n",
      "Epoch 180, Training Loss 0.036043018474222144\n",
      "Epoch 180, Training Loss 0.03607206632052084\n",
      "Epoch 180, Training Loss 0.03613536690583315\n",
      "Epoch 180, Training Loss 0.036229235770376136\n",
      "Epoch 180, Training Loss 0.03630544128530013\n",
      "Epoch 180, Training Loss 0.036461745996190155\n",
      "Epoch 180, Training Loss 0.036607916707463585\n",
      "Epoch 180, Training Loss 0.03666521726495317\n",
      "Epoch 180, Training Loss 0.03678167141173654\n",
      "Epoch 180, Training Loss 0.03687741355422665\n",
      "Epoch 180, Training Loss 0.036966579530359533\n",
      "Epoch 180, Training Loss 0.037043747141995396\n",
      "Epoch 180, Training Loss 0.03711898288572841\n",
      "Epoch 180, Training Loss 0.03723501132996491\n",
      "Epoch 180, Training Loss 0.03734194857004049\n",
      "Epoch 180, Training Loss 0.03752034578634345\n",
      "Epoch 180, Training Loss 0.03760802729622177\n",
      "Epoch 180, Training Loss 0.037698333963866126\n",
      "Epoch 180, Training Loss 0.03773494519274253\n",
      "Epoch 180, Training Loss 0.03781891192126152\n",
      "Epoch 180, Training Loss 0.03787112764804564\n",
      "Epoch 180, Training Loss 0.03794838688657869\n",
      "Epoch 180, Training Loss 0.03802598438814016\n",
      "Epoch 180, Training Loss 0.038127194370721915\n",
      "Epoch 180, Training Loss 0.038205283558201\n",
      "Epoch 180, Training Loss 0.03833826914277223\n",
      "Epoch 180, Training Loss 0.03849126043184029\n",
      "Epoch 180, Training Loss 0.03877468076546479\n",
      "Epoch 180, Training Loss 0.03914442727975833\n",
      "Epoch 180, Training Loss 0.039318679839067755\n",
      "Epoch 180, Training Loss 0.039417215301405135\n",
      "Epoch 180, Training Loss 0.03952473545890025\n",
      "Epoch 180, Training Loss 0.03958299630762214\n",
      "Epoch 180, Training Loss 0.03969162499146236\n",
      "Epoch 180, Training Loss 0.03979699446550568\n",
      "Epoch 180, Training Loss 0.03990970800161514\n",
      "Epoch 180, Training Loss 0.039970243687901044\n",
      "Epoch 180, Training Loss 0.04006434436839865\n",
      "Epoch 180, Training Loss 0.04016133870386407\n",
      "Epoch 180, Training Loss 0.04024690058072815\n",
      "Epoch 180, Training Loss 0.040345402391594086\n",
      "Epoch 180, Training Loss 0.040444762429312976\n",
      "Epoch 180, Training Loss 0.04058715085620466\n",
      "Epoch 180, Training Loss 0.040798146115696945\n",
      "Epoch 180, Training Loss 0.04102652087388441\n",
      "Epoch 180, Training Loss 0.04119224261368632\n",
      "Epoch 180, Training Loss 0.04128964587360087\n",
      "Epoch 180, Training Loss 0.04139324057551906\n",
      "Epoch 180, Training Loss 0.04147577192396154\n",
      "Epoch 180, Training Loss 0.04172416084715168\n",
      "Epoch 180, Training Loss 0.04181508605589952\n",
      "Epoch 180, Training Loss 0.04184973936842378\n",
      "Epoch 180, Training Loss 0.042029480738065124\n",
      "Epoch 180, Training Loss 0.04221840970256292\n",
      "Epoch 180, Training Loss 0.042279831469630644\n",
      "Epoch 180, Training Loss 0.04233332869627744\n",
      "Epoch 180, Training Loss 0.042392958894067106\n",
      "Epoch 180, Training Loss 0.0426607397539765\n",
      "Epoch 180, Training Loss 0.0428014946553637\n",
      "Epoch 180, Training Loss 0.042898928803746655\n",
      "Epoch 180, Training Loss 0.04309017192619993\n",
      "Epoch 180, Training Loss 0.04314877709750172\n",
      "Epoch 180, Training Loss 0.04331732739020339\n",
      "Epoch 180, Training Loss 0.043481964070130795\n",
      "Epoch 180, Training Loss 0.043597736441151565\n",
      "Epoch 180, Training Loss 0.04367352690061797\n",
      "Epoch 180, Training Loss 0.04376585244694177\n",
      "Epoch 180, Training Loss 0.04388571037527393\n",
      "Epoch 180, Training Loss 0.04398781782887933\n",
      "Epoch 180, Training Loss 0.044020844864018284\n",
      "Epoch 180, Training Loss 0.044142779760786795\n",
      "Epoch 180, Training Loss 0.0442000150018374\n",
      "Epoch 180, Training Loss 0.04425323672373505\n",
      "Epoch 180, Training Loss 0.04437074404152687\n",
      "Epoch 180, Training Loss 0.04451971493251717\n",
      "Epoch 180, Training Loss 0.04459133166152879\n",
      "Epoch 180, Training Loss 0.0447129757569917\n",
      "Epoch 180, Training Loss 0.04483755916366568\n",
      "Epoch 180, Training Loss 0.04499593217287908\n",
      "Epoch 180, Training Loss 0.04512414965264099\n",
      "Epoch 180, Training Loss 0.04520035758757454\n",
      "Epoch 180, Training Loss 0.0453041020823676\n",
      "Epoch 180, Training Loss 0.04535218475677092\n",
      "Epoch 180, Training Loss 0.04544487658202115\n",
      "Epoch 180, Training Loss 0.04555505176629785\n",
      "Epoch 180, Training Loss 0.04568289920373265\n",
      "Epoch 180, Training Loss 0.045874293071820456\n",
      "Epoch 180, Training Loss 0.046150203163037674\n",
      "Epoch 180, Training Loss 0.046243438213739704\n",
      "Epoch 180, Training Loss 0.046363814814907055\n",
      "Epoch 180, Training Loss 0.04651338277179796\n",
      "Epoch 180, Training Loss 0.046703659452955286\n",
      "Epoch 180, Training Loss 0.04684066121487895\n",
      "Epoch 180, Training Loss 0.04693181055081089\n",
      "Epoch 180, Training Loss 0.047064503028398125\n",
      "Epoch 180, Training Loss 0.047176929574240654\n",
      "Epoch 180, Training Loss 0.04727062887614569\n",
      "Epoch 180, Training Loss 0.047392239043836853\n",
      "Epoch 180, Training Loss 0.0474757204746918\n",
      "Epoch 180, Training Loss 0.047539771367769566\n",
      "Epoch 180, Training Loss 0.04766482483748051\n",
      "Epoch 180, Training Loss 0.04789702808889358\n",
      "Epoch 180, Training Loss 0.0481449525226908\n",
      "Epoch 180, Training Loss 0.04823506616122659\n",
      "Epoch 180, Training Loss 0.048271531925138916\n",
      "Epoch 180, Training Loss 0.04833547621393752\n",
      "Epoch 180, Training Loss 0.048407868775146086\n",
      "Epoch 180, Training Loss 0.048483265873492526\n",
      "Epoch 180, Training Loss 0.048570575768990286\n",
      "Epoch 180, Training Loss 0.04865801764075713\n",
      "Epoch 180, Training Loss 0.04876046080875884\n",
      "Epoch 180, Training Loss 0.048943891416272846\n",
      "Epoch 180, Training Loss 0.04901297577678242\n",
      "Epoch 180, Training Loss 0.04908835713077537\n",
      "Epoch 180, Training Loss 0.04915591245493316\n",
      "Epoch 180, Training Loss 0.049261356234702916\n",
      "Epoch 180, Training Loss 0.04934536245511011\n",
      "Epoch 180, Training Loss 0.049418070003428426\n",
      "Epoch 180, Training Loss 0.04955805205475644\n",
      "Epoch 180, Training Loss 0.04964448831727743\n",
      "Epoch 180, Training Loss 0.049840814793658685\n",
      "Epoch 180, Training Loss 0.04996552753745747\n",
      "Epoch 180, Training Loss 0.050011371483888165\n",
      "Epoch 180, Training Loss 0.050129983917145474\n",
      "Epoch 180, Training Loss 0.05022506898893115\n",
      "Epoch 180, Training Loss 0.050436089858603296\n",
      "Epoch 180, Training Loss 0.05054589583898139\n",
      "Epoch 180, Training Loss 0.0507090730816507\n",
      "Epoch 180, Training Loss 0.050875453445155296\n",
      "Epoch 180, Training Loss 0.051013775169849396\n",
      "Epoch 180, Training Loss 0.05105344557901249\n",
      "Epoch 180, Training Loss 0.051096063495501685\n",
      "Epoch 180, Training Loss 0.0511645996380988\n",
      "Epoch 180, Training Loss 0.05128922688124506\n",
      "Epoch 180, Training Loss 0.0513628637037047\n",
      "Epoch 180, Training Loss 0.05141271937810018\n",
      "Epoch 180, Training Loss 0.051448928647200624\n",
      "Epoch 180, Training Loss 0.05151804098073403\n",
      "Epoch 180, Training Loss 0.05166538187143062\n",
      "Epoch 180, Training Loss 0.05175955401843085\n",
      "Epoch 180, Training Loss 0.051900840102387664\n",
      "Epoch 180, Training Loss 0.05209376305446524\n",
      "Epoch 180, Training Loss 0.05219040960406937\n",
      "Epoch 180, Training Loss 0.0522737170419539\n",
      "Epoch 180, Training Loss 0.0524206635092035\n",
      "Epoch 180, Training Loss 0.052515531770999324\n",
      "Epoch 180, Training Loss 0.052686075160703845\n",
      "Epoch 180, Training Loss 0.05279395751931402\n",
      "Epoch 180, Training Loss 0.05287593716274366\n",
      "Epoch 180, Training Loss 0.053138339560945776\n",
      "Epoch 180, Training Loss 0.053282496605135135\n",
      "Epoch 180, Training Loss 0.05339105503247751\n",
      "Epoch 180, Training Loss 0.05351739343198593\n",
      "Epoch 180, Training Loss 0.05360242011516219\n",
      "Epoch 180, Training Loss 0.05372172691013731\n",
      "Epoch 180, Training Loss 0.05383702995174605\n",
      "Epoch 180, Training Loss 0.053938992881237546\n",
      "Epoch 180, Training Loss 0.054008026950328095\n",
      "Epoch 180, Training Loss 0.05408205712914391\n",
      "Epoch 180, Training Loss 0.05425068404277801\n",
      "Epoch 180, Training Loss 0.054397280104077225\n",
      "Epoch 180, Training Loss 0.0545444015075293\n",
      "Epoch 180, Training Loss 0.05470699162158134\n",
      "Epoch 180, Training Loss 0.054784723257412536\n",
      "Epoch 180, Training Loss 0.054870434727071954\n",
      "Epoch 180, Training Loss 0.05496056746843907\n",
      "Epoch 180, Training Loss 0.055069477807092086\n",
      "Epoch 180, Training Loss 0.0551421243787917\n",
      "Epoch 180, Training Loss 0.05528838377293495\n",
      "Epoch 180, Training Loss 0.05535583982429922\n",
      "Epoch 180, Training Loss 0.05542055477180978\n",
      "Epoch 180, Training Loss 0.05554401219758155\n",
      "Epoch 180, Training Loss 0.055619495010951446\n",
      "Epoch 180, Training Loss 0.055765371481456875\n",
      "Epoch 180, Training Loss 0.055895628619586565\n",
      "Epoch 180, Training Loss 0.05596079280757157\n",
      "Epoch 180, Training Loss 0.05601556774209756\n",
      "Epoch 180, Training Loss 0.05610634581617001\n",
      "Epoch 180, Training Loss 0.056197999057162294\n",
      "Epoch 180, Training Loss 0.056371086353288434\n",
      "Epoch 180, Training Loss 0.05651175896482318\n",
      "Epoch 180, Training Loss 0.056542276054658856\n",
      "Epoch 180, Training Loss 0.05665538364263904\n",
      "Epoch 180, Training Loss 0.05677675057078719\n",
      "Epoch 180, Training Loss 0.05688063167226132\n",
      "Epoch 180, Training Loss 0.05706431396076899\n",
      "Epoch 180, Training Loss 0.057165321138928006\n",
      "Epoch 180, Training Loss 0.05721623563896055\n",
      "Epoch 180, Training Loss 0.05729645780285301\n",
      "Epoch 180, Training Loss 0.05746903539161243\n",
      "Epoch 180, Training Loss 0.05757465528900666\n",
      "Epoch 180, Training Loss 0.05790155940234204\n",
      "Epoch 180, Training Loss 0.057981117897669375\n",
      "Epoch 180, Training Loss 0.058049036718695365\n",
      "Epoch 180, Training Loss 0.05818179529874831\n",
      "Epoch 180, Training Loss 0.058318317832087005\n",
      "Epoch 180, Training Loss 0.058379927564345666\n",
      "Epoch 180, Training Loss 0.05859350714632465\n",
      "Epoch 180, Training Loss 0.05870869283652519\n",
      "Epoch 180, Training Loss 0.058853212682068196\n",
      "Epoch 180, Training Loss 0.058935350740847686\n",
      "Epoch 180, Training Loss 0.05901863441690612\n",
      "Epoch 180, Training Loss 0.05914915221102555\n",
      "Epoch 180, Training Loss 0.05926488460425068\n",
      "Epoch 180, Training Loss 0.05937829614161988\n",
      "Epoch 180, Training Loss 0.059432961642170504\n",
      "Epoch 180, Training Loss 0.05954267562407514\n",
      "Epoch 180, Training Loss 0.05976105116955612\n",
      "Epoch 180, Training Loss 0.059849987540136824\n",
      "Epoch 180, Training Loss 0.059998020343958874\n",
      "Epoch 180, Training Loss 0.0600931135618397\n",
      "Epoch 180, Training Loss 0.06015199586710966\n",
      "Epoch 180, Training Loss 0.06038113178499519\n",
      "Epoch 180, Training Loss 0.060564032562858305\n",
      "Epoch 180, Training Loss 0.060663471355691286\n",
      "Epoch 180, Training Loss 0.060765710194854786\n",
      "Epoch 180, Training Loss 0.06086030068909726\n",
      "Epoch 180, Training Loss 0.0609529397886275\n",
      "Epoch 180, Training Loss 0.061069660365124186\n",
      "Epoch 180, Training Loss 0.06114525414164871\n",
      "Epoch 180, Training Loss 0.061298278291397695\n",
      "Epoch 180, Training Loss 0.061364477343114135\n",
      "Epoch 180, Training Loss 0.061552744749409465\n",
      "Epoch 180, Training Loss 0.06161950489558527\n",
      "Epoch 180, Training Loss 0.061792310822726515\n",
      "Epoch 180, Training Loss 0.06201906444128517\n",
      "Epoch 180, Training Loss 0.06220871010971496\n",
      "Epoch 180, Training Loss 0.06227992410245149\n",
      "Epoch 180, Training Loss 0.0623895314229114\n",
      "Epoch 180, Training Loss 0.06251573096722593\n",
      "Epoch 180, Training Loss 0.06267381636687862\n",
      "Epoch 180, Training Loss 0.06278686477895588\n",
      "Epoch 180, Training Loss 0.06287830806982792\n",
      "Epoch 180, Training Loss 0.06301503092088663\n",
      "Epoch 180, Training Loss 0.06318597305956704\n",
      "Epoch 180, Training Loss 0.06337492866322512\n",
      "Epoch 180, Training Loss 0.06354823886700299\n",
      "Epoch 180, Training Loss 0.06376594841442144\n",
      "Epoch 180, Training Loss 0.0638781184297236\n",
      "Epoch 180, Training Loss 0.06394576756259822\n",
      "Epoch 180, Training Loss 0.0640544287260155\n",
      "Epoch 180, Training Loss 0.06416504858228404\n",
      "Epoch 180, Training Loss 0.06426056381553183\n",
      "Epoch 180, Training Loss 0.06438714579757675\n",
      "Epoch 180, Training Loss 0.06450937746945397\n",
      "Epoch 180, Training Loss 0.06457015859139392\n",
      "Epoch 180, Training Loss 0.06464503328685108\n",
      "Epoch 180, Training Loss 0.06470155873147727\n",
      "Epoch 180, Training Loss 0.06477192207180021\n",
      "Epoch 180, Training Loss 0.06490440592360314\n",
      "Epoch 180, Training Loss 0.06510971092125949\n",
      "Epoch 180, Training Loss 0.06526237098342927\n",
      "Epoch 180, Training Loss 0.06541908028371193\n",
      "Epoch 180, Training Loss 0.0654927762820745\n",
      "Epoch 180, Training Loss 0.06562112544275\n",
      "Epoch 180, Training Loss 0.06566723886772495\n",
      "Epoch 180, Training Loss 0.06586323627997237\n",
      "Epoch 180, Training Loss 0.06608018489635509\n",
      "Epoch 180, Training Loss 0.0662601146837482\n",
      "Epoch 180, Training Loss 0.06633920149634713\n",
      "Epoch 180, Training Loss 0.06650724997053213\n",
      "Epoch 180, Training Loss 0.0666048767002266\n",
      "Epoch 180, Training Loss 0.06672756803100524\n",
      "Epoch 180, Training Loss 0.06682405896637293\n",
      "Epoch 180, Training Loss 0.06687152547204434\n",
      "Epoch 180, Training Loss 0.06700228537192278\n",
      "Epoch 180, Training Loss 0.06722802374883534\n",
      "Epoch 180, Training Loss 0.06734919155021306\n",
      "Epoch 180, Training Loss 0.06746390235641271\n",
      "Epoch 180, Training Loss 0.06753877770927404\n",
      "Epoch 180, Training Loss 0.06758779183011074\n",
      "Epoch 180, Training Loss 0.06770498884360657\n",
      "Epoch 180, Training Loss 0.06782498917139856\n",
      "Epoch 180, Training Loss 0.06795111881173632\n",
      "Epoch 180, Training Loss 0.06800618611962136\n",
      "Epoch 180, Training Loss 0.06815762949816863\n",
      "Epoch 180, Training Loss 0.06821963832716045\n",
      "Epoch 180, Training Loss 0.06827468438850492\n",
      "Epoch 180, Training Loss 0.068382059859917\n",
      "Epoch 180, Training Loss 0.06843386267018897\n",
      "Epoch 180, Training Loss 0.06862022722487712\n",
      "Epoch 180, Training Loss 0.0687993445369365\n",
      "Epoch 180, Training Loss 0.06892273497417607\n",
      "Epoch 180, Training Loss 0.06902248598635197\n",
      "Epoch 180, Training Loss 0.06914081279178867\n",
      "Epoch 180, Training Loss 0.06926090082110804\n",
      "Epoch 180, Training Loss 0.0693854089788235\n",
      "Epoch 180, Training Loss 0.0695205098708801\n",
      "Epoch 180, Training Loss 0.06968822876644104\n",
      "Epoch 180, Training Loss 0.0698005822451447\n",
      "Epoch 180, Training Loss 0.06986684988126578\n",
      "Epoch 180, Training Loss 0.0700130148664536\n",
      "Epoch 180, Training Loss 0.07011929845623195\n",
      "Epoch 180, Training Loss 0.0702532022343496\n",
      "Epoch 180, Training Loss 0.07041318592189065\n",
      "Epoch 180, Training Loss 0.07047306415160447\n",
      "Epoch 180, Training Loss 0.07056946870025314\n",
      "Epoch 180, Training Loss 0.07074060802207426\n",
      "Epoch 180, Training Loss 0.07085325954305699\n",
      "Epoch 180, Training Loss 0.07099558926089798\n",
      "Epoch 180, Training Loss 0.0711436433303158\n",
      "Epoch 180, Training Loss 0.07123150733178077\n",
      "Epoch 180, Training Loss 0.07135109379030097\n",
      "Epoch 180, Training Loss 0.07157967840333271\n",
      "Epoch 180, Training Loss 0.07169402467415613\n",
      "Epoch 180, Training Loss 0.07192589727985432\n",
      "Epoch 180, Training Loss 0.07206085675379352\n",
      "Epoch 180, Training Loss 0.07214628548249412\n",
      "Epoch 180, Training Loss 0.07222589840421743\n",
      "Epoch 180, Training Loss 0.07234397489110679\n",
      "Epoch 180, Training Loss 0.07241869786911456\n",
      "Epoch 180, Training Loss 0.07248013873901361\n",
      "Epoch 180, Training Loss 0.0726110894099602\n",
      "Epoch 180, Training Loss 0.07267950921107436\n",
      "Epoch 180, Training Loss 0.07273749060585828\n",
      "Epoch 180, Training Loss 0.07286252537289696\n",
      "Epoch 180, Training Loss 0.0730150207315031\n",
      "Epoch 180, Training Loss 0.07313118394359451\n",
      "Epoch 180, Training Loss 0.0732440121348023\n",
      "Epoch 180, Training Loss 0.07332161677253368\n",
      "Epoch 180, Training Loss 0.07336988947961641\n",
      "Epoch 180, Training Loss 0.07343193007837934\n",
      "Epoch 180, Training Loss 0.07347466425536692\n",
      "Epoch 180, Training Loss 0.07358019332618207\n",
      "Epoch 180, Training Loss 0.07370359904568671\n",
      "Epoch 180, Training Loss 0.07387174843617565\n",
      "Epoch 180, Training Loss 0.07393203136008566\n",
      "Epoch 180, Training Loss 0.07396848527881343\n",
      "Epoch 180, Training Loss 0.07436820359238426\n",
      "Epoch 180, Training Loss 0.07451086967249813\n",
      "Epoch 180, Training Loss 0.07477318899005728\n",
      "Epoch 180, Training Loss 0.07487744263961645\n",
      "Epoch 180, Training Loss 0.07500918579699896\n",
      "Epoch 180, Training Loss 0.07508283152299769\n",
      "Epoch 180, Training Loss 0.07516031907132976\n",
      "Epoch 180, Training Loss 0.07529570304733865\n",
      "Epoch 180, Training Loss 0.07541830919187545\n",
      "Epoch 180, Training Loss 0.07565037999063959\n",
      "Epoch 180, Training Loss 0.07570660579711427\n",
      "Epoch 180, Training Loss 0.07581518327012239\n",
      "Epoch 180, Training Loss 0.07591170873350042\n",
      "Epoch 180, Training Loss 0.07602884908637884\n",
      "Epoch 180, Training Loss 0.07614659488944293\n",
      "Epoch 180, Training Loss 0.07620740881966204\n",
      "Epoch 180, Training Loss 0.07632421745020715\n",
      "Epoch 180, Training Loss 0.07638850360822952\n",
      "Epoch 180, Training Loss 0.07651998713860274\n",
      "Epoch 180, Training Loss 0.07658574074659201\n",
      "Epoch 180, Training Loss 0.07664132382615906\n",
      "Epoch 180, Training Loss 0.07678761036442522\n",
      "Epoch 180, Training Loss 0.07684073298502608\n",
      "Epoch 180, Training Loss 0.07710899527439528\n",
      "Epoch 180, Training Loss 0.0772160735419568\n",
      "Epoch 180, Training Loss 0.07734086604126732\n",
      "Epoch 180, Training Loss 0.07739250065134767\n",
      "Epoch 180, Training Loss 0.07755423575887442\n",
      "Epoch 180, Training Loss 0.07764500124699167\n",
      "Epoch 180, Training Loss 0.07782867509404869\n",
      "Epoch 180, Training Loss 0.07789143278737508\n",
      "Epoch 180, Training Loss 0.07794881877882401\n",
      "Epoch 180, Training Loss 0.07809873705591693\n",
      "Epoch 180, Training Loss 0.07818801393327506\n",
      "Epoch 180, Training Loss 0.07850962840115933\n",
      "Epoch 180, Training Loss 0.07859833878667458\n",
      "Epoch 180, Training Loss 0.0786425777903908\n",
      "Epoch 180, Training Loss 0.0787248686813485\n",
      "Epoch 180, Training Loss 0.07881385131793864\n",
      "Epoch 180, Training Loss 0.07895236559536147\n",
      "Epoch 180, Training Loss 0.07906867860032775\n",
      "Epoch 180, Training Loss 0.07926175363190338\n",
      "Epoch 180, Training Loss 0.07934581431205315\n",
      "Epoch 180, Training Loss 0.07941479447400174\n",
      "Epoch 180, Training Loss 0.07955426619866925\n",
      "Epoch 180, Training Loss 0.07960811609407063\n",
      "Epoch 180, Training Loss 0.07968126756170064\n",
      "Epoch 180, Training Loss 0.0798209549530464\n",
      "Epoch 180, Training Loss 0.0799030987283839\n",
      "Epoch 180, Training Loss 0.08006202044141719\n",
      "Epoch 180, Training Loss 0.08015081331686442\n",
      "Epoch 180, Training Loss 0.08022764418512354\n",
      "Epoch 180, Training Loss 0.08033494406458362\n",
      "Epoch 180, Training Loss 0.08042699011885907\n",
      "Epoch 180, Training Loss 0.0806293350637264\n",
      "Epoch 180, Training Loss 0.0806724386875663\n",
      "Epoch 180, Training Loss 0.08074975549660222\n",
      "Epoch 180, Training Loss 0.08086149627461915\n",
      "Epoch 180, Training Loss 0.08098518018089139\n",
      "Epoch 180, Training Loss 0.0810554305286816\n",
      "Epoch 180, Training Loss 0.08108787625894674\n",
      "Epoch 180, Training Loss 0.08122580430334639\n",
      "Epoch 180, Training Loss 0.08134912844756832\n",
      "Epoch 180, Training Loss 0.08151040108555266\n",
      "Epoch 180, Training Loss 0.08157692033597423\n",
      "Epoch 180, Training Loss 0.08171442692713513\n",
      "Epoch 180, Training Loss 0.08184706501167296\n",
      "Epoch 180, Training Loss 0.08195397786110106\n",
      "Epoch 180, Training Loss 0.0820400519034518\n",
      "Epoch 180, Training Loss 0.0821931687090784\n",
      "Epoch 180, Training Loss 0.0823348152029621\n",
      "Epoch 180, Training Loss 0.08240589541394996\n",
      "Epoch 180, Training Loss 0.08252531011848499\n",
      "Epoch 180, Training Loss 0.08266379705170536\n",
      "Epoch 180, Training Loss 0.0827990205829863\n",
      "Epoch 180, Training Loss 0.08293118203044547\n",
      "Epoch 180, Training Loss 0.08297896370901477\n",
      "Epoch 180, Training Loss 0.08307937545049221\n",
      "Epoch 180, Training Loss 0.08328553139591766\n",
      "Epoch 180, Training Loss 0.08337433518999068\n",
      "Epoch 180, Training Loss 0.08357448728226335\n",
      "Epoch 180, Training Loss 0.08365046859378247\n",
      "Epoch 180, Training Loss 0.08373396866061651\n",
      "Epoch 180, Training Loss 0.08387724289675351\n",
      "Epoch 180, Training Loss 0.08392205733872588\n",
      "Epoch 180, Training Loss 0.08400245026573348\n",
      "Epoch 180, Training Loss 0.08411322798475128\n",
      "Epoch 180, Training Loss 0.0842144360808689\n",
      "Epoch 180, Training Loss 0.08431404661815947\n",
      "Epoch 180, Training Loss 0.08450919456894287\n",
      "Epoch 180, Training Loss 0.08465826384189641\n",
      "Epoch 180, Training Loss 0.0848725735426635\n",
      "Epoch 180, Training Loss 0.08491843369553613\n",
      "Epoch 180, Training Loss 0.08506504529634552\n",
      "Epoch 180, Training Loss 0.08510580801826609\n",
      "Epoch 180, Training Loss 0.08527683805855339\n",
      "Epoch 180, Training Loss 0.08545323305041588\n",
      "Epoch 180, Training Loss 0.08558335112374457\n",
      "Epoch 180, Training Loss 0.08566066240201063\n",
      "Epoch 180, Training Loss 0.08581027488136078\n",
      "Epoch 180, Training Loss 0.08610864866362966\n",
      "Epoch 180, Training Loss 0.08623690657851184\n",
      "Epoch 180, Training Loss 0.08636116107349354\n",
      "Epoch 180, Training Loss 0.08642506565126921\n",
      "Epoch 180, Training Loss 0.08652727554559403\n",
      "Epoch 180, Training Loss 0.08657422168728184\n",
      "Epoch 180, Training Loss 0.0866831581887153\n",
      "Epoch 180, Training Loss 0.08684576507133748\n",
      "Epoch 180, Training Loss 0.08700422249505739\n",
      "Epoch 180, Training Loss 0.08723720211221282\n",
      "Epoch 180, Training Loss 0.08734711676435855\n",
      "Epoch 180, Training Loss 0.08749849797533754\n",
      "Epoch 180, Training Loss 0.08770411578304779\n",
      "Epoch 180, Training Loss 0.08783118035691931\n",
      "Epoch 180, Training Loss 0.08795870991561876\n",
      "Epoch 180, Training Loss 0.08815882406900148\n",
      "Epoch 180, Training Loss 0.08847340072988702\n",
      "Epoch 180, Training Loss 0.08857063237873032\n",
      "Epoch 180, Training Loss 0.08870219092939974\n",
      "Epoch 180, Training Loss 0.08875180521736974\n",
      "Epoch 180, Training Loss 0.08884155084295652\n",
      "Epoch 180, Training Loss 0.08890503977456361\n",
      "Epoch 180, Training Loss 0.0889379607937525\n",
      "Epoch 180, Training Loss 0.08907806476973512\n",
      "Epoch 180, Training Loss 0.08926328286871581\n",
      "Epoch 180, Training Loss 0.08932046204938761\n",
      "Epoch 180, Training Loss 0.0894908146394412\n",
      "Epoch 180, Training Loss 0.0895998663771564\n",
      "Epoch 180, Training Loss 0.0896422478853894\n",
      "Epoch 180, Training Loss 0.08972676751939841\n",
      "Epoch 180, Training Loss 0.0898072186028561\n",
      "Epoch 180, Training Loss 0.08989293618923258\n",
      "Epoch 180, Training Loss 0.08994112060407695\n",
      "Epoch 180, Training Loss 0.08999486262326503\n",
      "Epoch 180, Training Loss 0.09009206309781202\n",
      "Epoch 180, Training Loss 0.09018598505965127\n",
      "Epoch 180, Training Loss 0.09021646665204364\n",
      "Epoch 180, Training Loss 0.09041634349204848\n",
      "Epoch 180, Training Loss 0.09063135882568024\n",
      "Epoch 180, Training Loss 0.09073463923123944\n",
      "Epoch 180, Training Loss 0.0909129565872271\n",
      "Epoch 180, Training Loss 0.09098376895841735\n",
      "Epoch 190, Training Loss 0.000584500143899942\n",
      "Epoch 190, Training Loss 0.0006518243809642694\n",
      "Epoch 190, Training Loss 0.0008126615696703381\n",
      "Epoch 190, Training Loss 0.0010411351862008615\n",
      "Epoch 190, Training Loss 0.0011775237615303615\n",
      "Epoch 190, Training Loss 0.001356800489337243\n",
      "Epoch 190, Training Loss 0.0015241227510487637\n",
      "Epoch 190, Training Loss 0.0019274924588782707\n",
      "Epoch 190, Training Loss 0.002143038193816724\n",
      "Epoch 190, Training Loss 0.0021804771612367363\n",
      "Epoch 190, Training Loss 0.0022970556907946495\n",
      "Epoch 190, Training Loss 0.0023354016258702866\n",
      "Epoch 190, Training Loss 0.002418786018153133\n",
      "Epoch 190, Training Loss 0.0026405136770261523\n",
      "Epoch 190, Training Loss 0.0027804261959536608\n",
      "Epoch 190, Training Loss 0.0028085788792890053\n",
      "Epoch 190, Training Loss 0.002961545482354091\n",
      "Epoch 190, Training Loss 0.0030684028049487895\n",
      "Epoch 190, Training Loss 0.0032086618139844417\n",
      "Epoch 190, Training Loss 0.003311231460831964\n",
      "Epoch 190, Training Loss 0.0034173872283733715\n",
      "Epoch 190, Training Loss 0.0035209716023767695\n",
      "Epoch 190, Training Loss 0.0035546090706344456\n",
      "Epoch 190, Training Loss 0.0036028638705992333\n",
      "Epoch 190, Training Loss 0.0036643629350587535\n",
      "Epoch 190, Training Loss 0.0038707468949277382\n",
      "Epoch 190, Training Loss 0.003975455239510445\n",
      "Epoch 190, Training Loss 0.004062505669968055\n",
      "Epoch 190, Training Loss 0.00412480617204057\n",
      "Epoch 190, Training Loss 0.00419223304037624\n",
      "Epoch 190, Training Loss 0.004324815617135876\n",
      "Epoch 190, Training Loss 0.004643877317933628\n",
      "Epoch 190, Training Loss 0.0047346017039988355\n",
      "Epoch 190, Training Loss 0.004802501636564427\n",
      "Epoch 190, Training Loss 0.004864952136355135\n",
      "Epoch 190, Training Loss 0.004943230094107063\n",
      "Epoch 190, Training Loss 0.0049787556223780905\n",
      "Epoch 190, Training Loss 0.005022058939880422\n",
      "Epoch 190, Training Loss 0.005093106938063946\n",
      "Epoch 190, Training Loss 0.005128537388065892\n",
      "Epoch 190, Training Loss 0.005177973257496839\n",
      "Epoch 190, Training Loss 0.005278325985040506\n",
      "Epoch 190, Training Loss 0.005490542091715061\n",
      "Epoch 190, Training Loss 0.005567693616003942\n",
      "Epoch 190, Training Loss 0.005667848664971873\n",
      "Epoch 190, Training Loss 0.005868166537426622\n",
      "Epoch 190, Training Loss 0.005965689244820638\n",
      "Epoch 190, Training Loss 0.006022229061826416\n",
      "Epoch 190, Training Loss 0.006130610342563876\n",
      "Epoch 190, Training Loss 0.006309607321077296\n",
      "Epoch 190, Training Loss 0.006530664046592725\n",
      "Epoch 190, Training Loss 0.0066834899175273795\n",
      "Epoch 190, Training Loss 0.0067439437474664824\n",
      "Epoch 190, Training Loss 0.006818976648666365\n",
      "Epoch 190, Training Loss 0.0069676778610328885\n",
      "Epoch 190, Training Loss 0.007060248438087876\n",
      "Epoch 190, Training Loss 0.007180122953965841\n",
      "Epoch 190, Training Loss 0.007199123562516078\n",
      "Epoch 190, Training Loss 0.007242530687947941\n",
      "Epoch 190, Training Loss 0.007330562813383768\n",
      "Epoch 190, Training Loss 0.007465639857865889\n",
      "Epoch 190, Training Loss 0.007525156915445081\n",
      "Epoch 190, Training Loss 0.007600610683936521\n",
      "Epoch 190, Training Loss 0.0076372921002357055\n",
      "Epoch 190, Training Loss 0.007866511365536915\n",
      "Epoch 190, Training Loss 0.008051424981583186\n",
      "Epoch 190, Training Loss 0.008204465680767584\n",
      "Epoch 190, Training Loss 0.008263267153907386\n",
      "Epoch 190, Training Loss 0.008309254009524345\n",
      "Epoch 190, Training Loss 0.008441450356570122\n",
      "Epoch 190, Training Loss 0.008551821695007098\n",
      "Epoch 190, Training Loss 0.008628513608032556\n",
      "Epoch 190, Training Loss 0.00873047494641541\n",
      "Epoch 190, Training Loss 0.00889655580039105\n",
      "Epoch 190, Training Loss 0.009028751528143045\n",
      "Epoch 190, Training Loss 0.009085425322213212\n",
      "Epoch 190, Training Loss 0.009148283560267267\n",
      "Epoch 190, Training Loss 0.009398281732645562\n",
      "Epoch 190, Training Loss 0.009477133323174075\n",
      "Epoch 190, Training Loss 0.00967628198802052\n",
      "Epoch 190, Training Loss 0.009831807751189489\n",
      "Epoch 190, Training Loss 0.009911330013066683\n",
      "Epoch 190, Training Loss 0.009995345484770244\n",
      "Epoch 190, Training Loss 0.01010905262535376\n",
      "Epoch 190, Training Loss 0.0102349424560833\n",
      "Epoch 190, Training Loss 0.010298221728161853\n",
      "Epoch 190, Training Loss 0.01035999474913606\n",
      "Epoch 190, Training Loss 0.010511913726253964\n",
      "Epoch 190, Training Loss 0.010551684029886255\n",
      "Epoch 190, Training Loss 0.010765040189723301\n",
      "Epoch 190, Training Loss 0.010950272218050325\n",
      "Epoch 190, Training Loss 0.011021984270666642\n",
      "Epoch 190, Training Loss 0.011194893019631162\n",
      "Epoch 190, Training Loss 0.011231548051633265\n",
      "Epoch 190, Training Loss 0.0112739059647731\n",
      "Epoch 190, Training Loss 0.011361845701104005\n",
      "Epoch 190, Training Loss 0.01153833991335824\n",
      "Epoch 190, Training Loss 0.011743964066447886\n",
      "Epoch 190, Training Loss 0.011816043426971073\n",
      "Epoch 190, Training Loss 0.011849789526504095\n",
      "Epoch 190, Training Loss 0.011956137017873318\n",
      "Epoch 190, Training Loss 0.012061660314964898\n",
      "Epoch 190, Training Loss 0.012141953831028831\n",
      "Epoch 190, Training Loss 0.01223602744362429\n",
      "Epoch 190, Training Loss 0.012356047121965138\n",
      "Epoch 190, Training Loss 0.012418786403210954\n",
      "Epoch 190, Training Loss 0.012445120670882713\n",
      "Epoch 190, Training Loss 0.012564907809171607\n",
      "Epoch 190, Training Loss 0.012651082262863664\n",
      "Epoch 190, Training Loss 0.012830768925044924\n",
      "Epoch 190, Training Loss 0.013026991531567271\n",
      "Epoch 190, Training Loss 0.013166209307673109\n",
      "Epoch 190, Training Loss 0.013331406000677658\n",
      "Epoch 190, Training Loss 0.013384315427011618\n",
      "Epoch 190, Training Loss 0.013518842643417437\n",
      "Epoch 190, Training Loss 0.01378450902474239\n",
      "Epoch 190, Training Loss 0.013872370282259515\n",
      "Epoch 190, Training Loss 0.013975453485622812\n",
      "Epoch 190, Training Loss 0.014150002543740641\n",
      "Epoch 190, Training Loss 0.014320114904971759\n",
      "Epoch 190, Training Loss 0.014443253927990375\n",
      "Epoch 190, Training Loss 0.01450522613290059\n",
      "Epoch 190, Training Loss 0.014591483314714545\n",
      "Epoch 190, Training Loss 0.014649095972452094\n",
      "Epoch 190, Training Loss 0.014684200721561833\n",
      "Epoch 190, Training Loss 0.014753104939037347\n",
      "Epoch 190, Training Loss 0.014978073028814229\n",
      "Epoch 190, Training Loss 0.015246885079566551\n",
      "Epoch 190, Training Loss 0.015461987400155924\n",
      "Epoch 190, Training Loss 0.015613306272069893\n",
      "Epoch 190, Training Loss 0.01567751827204357\n",
      "Epoch 190, Training Loss 0.015741516815741426\n",
      "Epoch 190, Training Loss 0.015817415256224706\n",
      "Epoch 190, Training Loss 0.016094825086433947\n",
      "Epoch 190, Training Loss 0.016361860983559618\n",
      "Epoch 190, Training Loss 0.01662069060923079\n",
      "Epoch 190, Training Loss 0.016794504517989466\n",
      "Epoch 190, Training Loss 0.016875911057424135\n",
      "Epoch 190, Training Loss 0.01694413136971919\n",
      "Epoch 190, Training Loss 0.01702399056076127\n",
      "Epoch 190, Training Loss 0.017070752781722934\n",
      "Epoch 190, Training Loss 0.01723740300487565\n",
      "Epoch 190, Training Loss 0.017270550478364118\n",
      "Epoch 190, Training Loss 0.017315105169468447\n",
      "Epoch 190, Training Loss 0.01735297097321933\n",
      "Epoch 190, Training Loss 0.017505175397133508\n",
      "Epoch 190, Training Loss 0.01754974754279494\n",
      "Epoch 190, Training Loss 0.017713427899734062\n",
      "Epoch 190, Training Loss 0.017750288053746798\n",
      "Epoch 190, Training Loss 0.01789277295708237\n",
      "Epoch 190, Training Loss 0.01805958131928464\n",
      "Epoch 190, Training Loss 0.01830721761950332\n",
      "Epoch 190, Training Loss 0.01852751977841758\n",
      "Epoch 190, Training Loss 0.018600982545977435\n",
      "Epoch 190, Training Loss 0.01867181395330583\n",
      "Epoch 190, Training Loss 0.018758439329093147\n",
      "Epoch 190, Training Loss 0.018817696164188255\n",
      "Epoch 190, Training Loss 0.018910989095040066\n",
      "Epoch 190, Training Loss 0.019019852386420722\n",
      "Epoch 190, Training Loss 0.019149316733231402\n",
      "Epoch 190, Training Loss 0.01918659447466054\n",
      "Epoch 190, Training Loss 0.01927395792477919\n",
      "Epoch 190, Training Loss 0.01935675517891717\n",
      "Epoch 190, Training Loss 0.01947395181130913\n",
      "Epoch 190, Training Loss 0.019610410395418974\n",
      "Epoch 190, Training Loss 0.0197628187480123\n",
      "Epoch 190, Training Loss 0.019911604165278204\n",
      "Epoch 190, Training Loss 0.019957985949304785\n",
      "Epoch 190, Training Loss 0.020045728759740093\n",
      "Epoch 190, Training Loss 0.020324041748948186\n",
      "Epoch 190, Training Loss 0.020491830405571957\n",
      "Epoch 190, Training Loss 0.020589643428364144\n",
      "Epoch 190, Training Loss 0.02075682980749194\n",
      "Epoch 190, Training Loss 0.020873675452750127\n",
      "Epoch 190, Training Loss 0.021063465940649322\n",
      "Epoch 190, Training Loss 0.02115991325867946\n",
      "Epoch 190, Training Loss 0.021270860063955378\n",
      "Epoch 190, Training Loss 0.02135039653147921\n",
      "Epoch 190, Training Loss 0.021407524024939065\n",
      "Epoch 190, Training Loss 0.021456959413226378\n",
      "Epoch 190, Training Loss 0.021522038186307223\n",
      "Epoch 190, Training Loss 0.021593101485811002\n",
      "Epoch 190, Training Loss 0.0216810343813275\n",
      "Epoch 190, Training Loss 0.02176403141964008\n",
      "Epoch 190, Training Loss 0.021852670310069915\n",
      "Epoch 190, Training Loss 0.021922054561212316\n",
      "Epoch 190, Training Loss 0.022004747926436193\n",
      "Epoch 190, Training Loss 0.022141990736555643\n",
      "Epoch 190, Training Loss 0.022235202720946135\n",
      "Epoch 190, Training Loss 0.022324837645387178\n",
      "Epoch 190, Training Loss 0.022412524557412813\n",
      "Epoch 190, Training Loss 0.022494599324367617\n",
      "Epoch 190, Training Loss 0.022666692625388236\n",
      "Epoch 190, Training Loss 0.022723943015317553\n",
      "Epoch 190, Training Loss 0.022927019260394983\n",
      "Epoch 190, Training Loss 0.02307711121604761\n",
      "Epoch 190, Training Loss 0.023109604810576532\n",
      "Epoch 190, Training Loss 0.023221685447494315\n",
      "Epoch 190, Training Loss 0.023252569087792923\n",
      "Epoch 190, Training Loss 0.023313819648712264\n",
      "Epoch 190, Training Loss 0.02344597979089069\n",
      "Epoch 190, Training Loss 0.02353786198955858\n",
      "Epoch 190, Training Loss 0.02370906308831652\n",
      "Epoch 190, Training Loss 0.023798184751597284\n",
      "Epoch 190, Training Loss 0.02387565682711237\n",
      "Epoch 190, Training Loss 0.023964130509015927\n",
      "Epoch 190, Training Loss 0.024052410961731392\n",
      "Epoch 190, Training Loss 0.024093041807422628\n",
      "Epoch 190, Training Loss 0.024188088301969383\n",
      "Epoch 190, Training Loss 0.024295075533344694\n",
      "Epoch 190, Training Loss 0.02437487204232827\n",
      "Epoch 190, Training Loss 0.024458858140451296\n",
      "Epoch 190, Training Loss 0.024524867154724534\n",
      "Epoch 190, Training Loss 0.024644636686014777\n",
      "Epoch 190, Training Loss 0.024755308477212783\n",
      "Epoch 190, Training Loss 0.024883025739094258\n",
      "Epoch 190, Training Loss 0.02502244977814043\n",
      "Epoch 190, Training Loss 0.025233588199300307\n",
      "Epoch 190, Training Loss 0.02535475879226385\n",
      "Epoch 190, Training Loss 0.02553284478009395\n",
      "Epoch 190, Training Loss 0.0257074235452582\n",
      "Epoch 190, Training Loss 0.025790836644189817\n",
      "Epoch 190, Training Loss 0.025887431277919686\n",
      "Epoch 190, Training Loss 0.025939071685542613\n",
      "Epoch 190, Training Loss 0.025994184921207406\n",
      "Epoch 190, Training Loss 0.02604406996441962\n",
      "Epoch 190, Training Loss 0.026103613894585223\n",
      "Epoch 190, Training Loss 0.026204955205559502\n",
      "Epoch 190, Training Loss 0.026292869693759227\n",
      "Epoch 190, Training Loss 0.026400669267081924\n",
      "Epoch 190, Training Loss 0.026449761004484904\n",
      "Epoch 190, Training Loss 0.026551859483094242\n",
      "Epoch 190, Training Loss 0.026709530441819326\n",
      "Epoch 190, Training Loss 0.026829199066333225\n",
      "Epoch 190, Training Loss 0.026932835651566383\n",
      "Epoch 190, Training Loss 0.027105049876843954\n",
      "Epoch 190, Training Loss 0.0272273178393369\n",
      "Epoch 190, Training Loss 0.027331637543723787\n",
      "Epoch 190, Training Loss 0.02744946833295019\n",
      "Epoch 190, Training Loss 0.02753214576803243\n",
      "Epoch 190, Training Loss 0.027652330645371963\n",
      "Epoch 190, Training Loss 0.027740603237820176\n",
      "Epoch 190, Training Loss 0.02779053310837473\n",
      "Epoch 190, Training Loss 0.027914609875568114\n",
      "Epoch 190, Training Loss 0.028085272379286225\n",
      "Epoch 190, Training Loss 0.028214189927796344\n",
      "Epoch 190, Training Loss 0.02837803652224219\n",
      "Epoch 190, Training Loss 0.028591139209897393\n",
      "Epoch 190, Training Loss 0.02877066739003562\n",
      "Epoch 190, Training Loss 0.028995569987112984\n",
      "Epoch 190, Training Loss 0.029121464657861633\n",
      "Epoch 190, Training Loss 0.029177716036405787\n",
      "Epoch 190, Training Loss 0.029292947878284605\n",
      "Epoch 190, Training Loss 0.02938044642972405\n",
      "Epoch 190, Training Loss 0.02946131403112541\n",
      "Epoch 190, Training Loss 0.029574192404184883\n",
      "Epoch 190, Training Loss 0.029659594388921625\n",
      "Epoch 190, Training Loss 0.02981672409440741\n",
      "Epoch 190, Training Loss 0.029941615166233095\n",
      "Epoch 190, Training Loss 0.030058888562471436\n",
      "Epoch 190, Training Loss 0.030140795367424523\n",
      "Epoch 190, Training Loss 0.030262506756302723\n",
      "Epoch 190, Training Loss 0.03047129094643552\n",
      "Epoch 190, Training Loss 0.03070117142694571\n",
      "Epoch 190, Training Loss 0.030908110998142176\n",
      "Epoch 190, Training Loss 0.0309846579011463\n",
      "Epoch 190, Training Loss 0.031124938899875068\n",
      "Epoch 190, Training Loss 0.031178857751257356\n",
      "Epoch 190, Training Loss 0.031247355516695077\n",
      "Epoch 190, Training Loss 0.03142269441257695\n",
      "Epoch 190, Training Loss 0.03148264477096136\n",
      "Epoch 190, Training Loss 0.03155226482059378\n",
      "Epoch 190, Training Loss 0.0316563414474068\n",
      "Epoch 190, Training Loss 0.03171608361351254\n",
      "Epoch 190, Training Loss 0.03180558838145545\n",
      "Epoch 190, Training Loss 0.032031142170710104\n",
      "Epoch 190, Training Loss 0.032085773309983336\n",
      "Epoch 190, Training Loss 0.03230482266615609\n",
      "Epoch 190, Training Loss 0.03235698858266482\n",
      "Epoch 190, Training Loss 0.032457657054762175\n",
      "Epoch 190, Training Loss 0.03254587311045174\n",
      "Epoch 190, Training Loss 0.03266381142813417\n",
      "Epoch 190, Training Loss 0.03272605813739588\n",
      "Epoch 190, Training Loss 0.03281561225828955\n",
      "Epoch 190, Training Loss 0.032897838914190486\n",
      "Epoch 190, Training Loss 0.03298556534668712\n",
      "Epoch 190, Training Loss 0.033195769987152436\n",
      "Epoch 190, Training Loss 0.033350564173811006\n",
      "Epoch 190, Training Loss 0.03345923634160243\n",
      "Epoch 190, Training Loss 0.03351012785154421\n",
      "Epoch 190, Training Loss 0.033577579353481074\n",
      "Epoch 190, Training Loss 0.033679919604383546\n",
      "Epoch 190, Training Loss 0.0337374958762294\n",
      "Epoch 190, Training Loss 0.03379688648354558\n",
      "Epoch 190, Training Loss 0.033884742310332595\n",
      "Epoch 190, Training Loss 0.03397026225624373\n",
      "Epoch 190, Training Loss 0.034035404449771815\n",
      "Epoch 190, Training Loss 0.03415424638377774\n",
      "Epoch 190, Training Loss 0.034181669469960894\n",
      "Epoch 190, Training Loss 0.03431869732797184\n",
      "Epoch 190, Training Loss 0.03447732473354396\n",
      "Epoch 190, Training Loss 0.03457118591884403\n",
      "Epoch 190, Training Loss 0.03467606565536326\n",
      "Epoch 190, Training Loss 0.03478129289191588\n",
      "Epoch 190, Training Loss 0.03509744816123868\n",
      "Epoch 190, Training Loss 0.03526374986490516\n",
      "Epoch 190, Training Loss 0.035363430201368944\n",
      "Epoch 190, Training Loss 0.035467248649367365\n",
      "Epoch 190, Training Loss 0.0355375281039654\n",
      "Epoch 190, Training Loss 0.03558817150099846\n",
      "Epoch 190, Training Loss 0.03561280505693591\n",
      "Epoch 190, Training Loss 0.035738101146896094\n",
      "Epoch 190, Training Loss 0.035876733052027425\n",
      "Epoch 190, Training Loss 0.03598274083693733\n",
      "Epoch 190, Training Loss 0.036152054575960275\n",
      "Epoch 190, Training Loss 0.036205777187433924\n",
      "Epoch 190, Training Loss 0.036236268416990326\n",
      "Epoch 190, Training Loss 0.036442759409880315\n",
      "Epoch 190, Training Loss 0.03649890705671571\n",
      "Epoch 190, Training Loss 0.036774714573827166\n",
      "Epoch 190, Training Loss 0.03687375986739002\n",
      "Epoch 190, Training Loss 0.03695355965386685\n",
      "Epoch 190, Training Loss 0.037038743132701536\n",
      "Epoch 190, Training Loss 0.03710715565949564\n",
      "Epoch 190, Training Loss 0.03718553445256694\n",
      "Epoch 190, Training Loss 0.03727558602833801\n",
      "Epoch 190, Training Loss 0.037419756182500395\n",
      "Epoch 190, Training Loss 0.03753031591963395\n",
      "Epoch 190, Training Loss 0.037663520896173724\n",
      "Epoch 190, Training Loss 0.037864616430123975\n",
      "Epoch 190, Training Loss 0.03818990451062236\n",
      "Epoch 190, Training Loss 0.03836437147896727\n",
      "Epoch 190, Training Loss 0.03855747457884271\n",
      "Epoch 190, Training Loss 0.03861788492959441\n",
      "Epoch 190, Training Loss 0.03869224465845148\n",
      "Epoch 190, Training Loss 0.03885707417693551\n",
      "Epoch 190, Training Loss 0.03900142044872236\n",
      "Epoch 190, Training Loss 0.039029427921956836\n",
      "Epoch 190, Training Loss 0.039115748452761064\n",
      "Epoch 190, Training Loss 0.039221504004076695\n",
      "Epoch 190, Training Loss 0.03930971290454116\n",
      "Epoch 190, Training Loss 0.03939291082270196\n",
      "Epoch 190, Training Loss 0.03954822189100753\n",
      "Epoch 190, Training Loss 0.03972797940039764\n",
      "Epoch 190, Training Loss 0.03986716957505592\n",
      "Epoch 190, Training Loss 0.039939406917423315\n",
      "Epoch 190, Training Loss 0.03999904785757823\n",
      "Epoch 190, Training Loss 0.0401048565252453\n",
      "Epoch 190, Training Loss 0.04016006294080554\n",
      "Epoch 190, Training Loss 0.04029246937135792\n",
      "Epoch 190, Training Loss 0.04043700435625203\n",
      "Epoch 190, Training Loss 0.04061086418802667\n",
      "Epoch 190, Training Loss 0.04064936925302191\n",
      "Epoch 190, Training Loss 0.040702773552254566\n",
      "Epoch 190, Training Loss 0.04084113081488425\n",
      "Epoch 190, Training Loss 0.04098626028370026\n",
      "Epoch 190, Training Loss 0.04111530346310009\n",
      "Epoch 190, Training Loss 0.041225985839457996\n",
      "Epoch 190, Training Loss 0.041305202770444666\n",
      "Epoch 190, Training Loss 0.04138795112895653\n",
      "Epoch 190, Training Loss 0.041429774720183644\n",
      "Epoch 190, Training Loss 0.04151583475696728\n",
      "Epoch 190, Training Loss 0.041618074691804396\n",
      "Epoch 190, Training Loss 0.04174840961323332\n",
      "Epoch 190, Training Loss 0.04185818153245332\n",
      "Epoch 190, Training Loss 0.04203681503315373\n",
      "Epoch 190, Training Loss 0.04219152060363566\n",
      "Epoch 190, Training Loss 0.04230912477897523\n",
      "Epoch 190, Training Loss 0.04242953991927111\n",
      "Epoch 190, Training Loss 0.042517003923645026\n",
      "Epoch 190, Training Loss 0.0426466265619468\n",
      "Epoch 190, Training Loss 0.04285713745747952\n",
      "Epoch 190, Training Loss 0.042927731770564755\n",
      "Epoch 190, Training Loss 0.043087556280548236\n",
      "Epoch 190, Training Loss 0.043181949448736046\n",
      "Epoch 190, Training Loss 0.04328923676129612\n",
      "Epoch 190, Training Loss 0.04333527904370671\n",
      "Epoch 190, Training Loss 0.043521385895960094\n",
      "Epoch 190, Training Loss 0.043668173833528674\n",
      "Epoch 190, Training Loss 0.04381783123549712\n",
      "Epoch 190, Training Loss 0.04388474362314014\n",
      "Epoch 190, Training Loss 0.043982000148538355\n",
      "Epoch 190, Training Loss 0.04414729935848309\n",
      "Epoch 190, Training Loss 0.04422359690999093\n",
      "Epoch 190, Training Loss 0.04435253573362442\n",
      "Epoch 190, Training Loss 0.04443786142136701\n",
      "Epoch 190, Training Loss 0.04448015058813307\n",
      "Epoch 190, Training Loss 0.044567352869426424\n",
      "Epoch 190, Training Loss 0.04461775861008812\n",
      "Epoch 190, Training Loss 0.04469750442272981\n",
      "Epoch 190, Training Loss 0.044779301574572807\n",
      "Epoch 190, Training Loss 0.04489984945334551\n",
      "Epoch 190, Training Loss 0.045004249856476206\n",
      "Epoch 190, Training Loss 0.04514315375543731\n",
      "Epoch 190, Training Loss 0.04522087103318032\n",
      "Epoch 190, Training Loss 0.045346287456686465\n",
      "Epoch 190, Training Loss 0.0455725960412046\n",
      "Epoch 190, Training Loss 0.04567866344981448\n",
      "Epoch 190, Training Loss 0.045707873008011476\n",
      "Epoch 190, Training Loss 0.04574397076612048\n",
      "Epoch 190, Training Loss 0.04577047175958829\n",
      "Epoch 190, Training Loss 0.04582122255645483\n",
      "Epoch 190, Training Loss 0.0459118752187723\n",
      "Epoch 190, Training Loss 0.046038206434472824\n",
      "Epoch 190, Training Loss 0.04607965121322009\n",
      "Epoch 190, Training Loss 0.04622335455325596\n",
      "Epoch 190, Training Loss 0.046356985575336095\n",
      "Epoch 190, Training Loss 0.04649390373259897\n",
      "Epoch 190, Training Loss 0.04662054282305834\n",
      "Epoch 190, Training Loss 0.04665917570790862\n",
      "Epoch 190, Training Loss 0.04676557789721033\n",
      "Epoch 190, Training Loss 0.04687910242234845\n",
      "Epoch 190, Training Loss 0.04698133375376577\n",
      "Epoch 190, Training Loss 0.04701227721308008\n",
      "Epoch 190, Training Loss 0.04707529659614043\n",
      "Epoch 190, Training Loss 0.04723661386138757\n",
      "Epoch 190, Training Loss 0.047336971295563995\n",
      "Epoch 190, Training Loss 0.04739514720099776\n",
      "Epoch 190, Training Loss 0.04743434954077348\n",
      "Epoch 190, Training Loss 0.04759692101884643\n",
      "Epoch 190, Training Loss 0.04762981158783636\n",
      "Epoch 190, Training Loss 0.047699389020528865\n",
      "Epoch 190, Training Loss 0.04783786452063324\n",
      "Epoch 190, Training Loss 0.04792939142569366\n",
      "Epoch 190, Training Loss 0.047972446263474806\n",
      "Epoch 190, Training Loss 0.04817629620656752\n",
      "Epoch 190, Training Loss 0.04829227996637563\n",
      "Epoch 190, Training Loss 0.04839162852334054\n",
      "Epoch 190, Training Loss 0.04851075904824964\n",
      "Epoch 190, Training Loss 0.048573508519260095\n",
      "Epoch 190, Training Loss 0.048666689110219555\n",
      "Epoch 190, Training Loss 0.04875378727393649\n",
      "Epoch 190, Training Loss 0.04883555155557097\n",
      "Epoch 190, Training Loss 0.04900287724721729\n",
      "Epoch 190, Training Loss 0.049164437332793194\n",
      "Epoch 190, Training Loss 0.04924857207095189\n",
      "Epoch 190, Training Loss 0.049300437696668725\n",
      "Epoch 190, Training Loss 0.049314670832803865\n",
      "Epoch 190, Training Loss 0.04935928930640411\n",
      "Epoch 190, Training Loss 0.04948597341475775\n",
      "Epoch 190, Training Loss 0.04952699597329473\n",
      "Epoch 190, Training Loss 0.04973756513960869\n",
      "Epoch 190, Training Loss 0.04986548289070692\n",
      "Epoch 190, Training Loss 0.04992895920777603\n",
      "Epoch 190, Training Loss 0.05005076114693299\n",
      "Epoch 190, Training Loss 0.050154516436493074\n",
      "Epoch 190, Training Loss 0.05021859283018333\n",
      "Epoch 190, Training Loss 0.050254904099351835\n",
      "Epoch 190, Training Loss 0.05031652394515436\n",
      "Epoch 190, Training Loss 0.050468672127665384\n",
      "Epoch 190, Training Loss 0.050562765510023935\n",
      "Epoch 190, Training Loss 0.050674895652334025\n",
      "Epoch 190, Training Loss 0.05077528421316877\n",
      "Epoch 190, Training Loss 0.05083892278520919\n",
      "Epoch 190, Training Loss 0.050926022092237726\n",
      "Epoch 190, Training Loss 0.05100439024298354\n",
      "Epoch 190, Training Loss 0.051069550643748865\n",
      "Epoch 190, Training Loss 0.05110632503271827\n",
      "Epoch 190, Training Loss 0.05122618030285096\n",
      "Epoch 190, Training Loss 0.0512997049140408\n",
      "Epoch 190, Training Loss 0.051399317585513034\n",
      "Epoch 190, Training Loss 0.051468738451209445\n",
      "Epoch 190, Training Loss 0.051677875219584654\n",
      "Epoch 190, Training Loss 0.05172902635177192\n",
      "Epoch 190, Training Loss 0.051886336881514936\n",
      "Epoch 190, Training Loss 0.05192356172572735\n",
      "Epoch 190, Training Loss 0.052097858454974943\n",
      "Epoch 190, Training Loss 0.052407182913864286\n",
      "Epoch 190, Training Loss 0.052451050603319234\n",
      "Epoch 190, Training Loss 0.05256243713576433\n",
      "Epoch 190, Training Loss 0.05269814024815131\n",
      "Epoch 190, Training Loss 0.0527741923544775\n",
      "Epoch 190, Training Loss 0.05297725522280921\n",
      "Epoch 190, Training Loss 0.05309720537827715\n",
      "Epoch 190, Training Loss 0.053231139063520734\n",
      "Epoch 190, Training Loss 0.0533067418769707\n",
      "Epoch 190, Training Loss 0.05340886419362691\n",
      "Epoch 190, Training Loss 0.05352674445013523\n",
      "Epoch 190, Training Loss 0.05373455970274175\n",
      "Epoch 190, Training Loss 0.05384192585616427\n",
      "Epoch 190, Training Loss 0.05393550110876065\n",
      "Epoch 190, Training Loss 0.05402606754632824\n",
      "Epoch 190, Training Loss 0.05413385614385957\n",
      "Epoch 190, Training Loss 0.05424803680599765\n",
      "Epoch 190, Training Loss 0.054316509628187286\n",
      "Epoch 190, Training Loss 0.05448838533795512\n",
      "Epoch 190, Training Loss 0.05462720199271351\n",
      "Epoch 190, Training Loss 0.054672092793609406\n",
      "Epoch 190, Training Loss 0.05481527094393397\n",
      "Epoch 190, Training Loss 0.05487559024897187\n",
      "Epoch 190, Training Loss 0.05499181405657812\n",
      "Epoch 190, Training Loss 0.05505500168270429\n",
      "Epoch 190, Training Loss 0.055225813845910916\n",
      "Epoch 190, Training Loss 0.055296581618421145\n",
      "Epoch 190, Training Loss 0.05542310213793993\n",
      "Epoch 190, Training Loss 0.055552805389237146\n",
      "Epoch 190, Training Loss 0.055604130981247064\n",
      "Epoch 190, Training Loss 0.055720007193543954\n",
      "Epoch 190, Training Loss 0.05583423753257107\n",
      "Epoch 190, Training Loss 0.055863107293796586\n",
      "Epoch 190, Training Loss 0.05593985633905548\n",
      "Epoch 190, Training Loss 0.05600559939999524\n",
      "Epoch 190, Training Loss 0.05608419855684995\n",
      "Epoch 190, Training Loss 0.05623407898080128\n",
      "Epoch 190, Training Loss 0.056271382320500773\n",
      "Epoch 190, Training Loss 0.05635005332972578\n",
      "Epoch 190, Training Loss 0.0563926105030224\n",
      "Epoch 190, Training Loss 0.05660502566024661\n",
      "Epoch 190, Training Loss 0.05673174856497389\n",
      "Epoch 190, Training Loss 0.05684235094763015\n",
      "Epoch 190, Training Loss 0.056947246881063715\n",
      "Epoch 190, Training Loss 0.057065496979819615\n",
      "Epoch 190, Training Loss 0.05714274291306391\n",
      "Epoch 190, Training Loss 0.05720376228685003\n",
      "Epoch 190, Training Loss 0.05729456333195805\n",
      "Epoch 190, Training Loss 0.057360435363210144\n",
      "Epoch 190, Training Loss 0.05744557229973509\n",
      "Epoch 190, Training Loss 0.05752749811224354\n",
      "Epoch 190, Training Loss 0.05757123908942656\n",
      "Epoch 190, Training Loss 0.05764649036314214\n",
      "Epoch 190, Training Loss 0.057769699442579085\n",
      "Epoch 190, Training Loss 0.05783466291744881\n",
      "Epoch 190, Training Loss 0.05797470330625125\n",
      "Epoch 190, Training Loss 0.05804718467895218\n",
      "Epoch 190, Training Loss 0.05814687500986487\n",
      "Epoch 190, Training Loss 0.058305022815275755\n",
      "Epoch 190, Training Loss 0.05842502920261925\n",
      "Epoch 190, Training Loss 0.05856580352124846\n",
      "Epoch 190, Training Loss 0.05867723172740139\n",
      "Epoch 190, Training Loss 0.058778534003812104\n",
      "Epoch 190, Training Loss 0.058872509129640774\n",
      "Epoch 190, Training Loss 0.05900461357348906\n",
      "Epoch 190, Training Loss 0.05907516647487536\n",
      "Epoch 190, Training Loss 0.05909770683687933\n",
      "Epoch 190, Training Loss 0.05933821371272969\n",
      "Epoch 190, Training Loss 0.05951011798623235\n",
      "Epoch 190, Training Loss 0.059568230585550025\n",
      "Epoch 190, Training Loss 0.05969198663478427\n",
      "Epoch 190, Training Loss 0.059785778373432207\n",
      "Epoch 190, Training Loss 0.05986995479482633\n",
      "Epoch 190, Training Loss 0.06005106295537575\n",
      "Epoch 190, Training Loss 0.06030444647340328\n",
      "Epoch 190, Training Loss 0.060467626707737934\n",
      "Epoch 190, Training Loss 0.060657244936689315\n",
      "Epoch 190, Training Loss 0.0608481208084013\n",
      "Epoch 190, Training Loss 0.06095462794418035\n",
      "Epoch 190, Training Loss 0.06112271109881723\n",
      "Epoch 190, Training Loss 0.061170469572924824\n",
      "Epoch 190, Training Loss 0.061292168223406274\n",
      "Epoch 190, Training Loss 0.06146197991274164\n",
      "Epoch 190, Training Loss 0.06155286611550871\n",
      "Epoch 190, Training Loss 0.061613367450044816\n",
      "Epoch 190, Training Loss 0.06170934891623571\n",
      "Epoch 190, Training Loss 0.06178278157182628\n",
      "Epoch 190, Training Loss 0.06189081399008403\n",
      "Epoch 190, Training Loss 0.06199924969245368\n",
      "Epoch 190, Training Loss 0.062075904371631345\n",
      "Epoch 190, Training Loss 0.062110538226421305\n",
      "Epoch 190, Training Loss 0.062164541616168856\n",
      "Epoch 190, Training Loss 0.06222131324912924\n",
      "Epoch 190, Training Loss 0.06234947409685653\n",
      "Epoch 190, Training Loss 0.06240887285026786\n",
      "Epoch 190, Training Loss 0.06244762599958902\n",
      "Epoch 190, Training Loss 0.06254295613068868\n",
      "Epoch 190, Training Loss 0.06261491463841189\n",
      "Epoch 190, Training Loss 0.06291702504881927\n",
      "Epoch 190, Training Loss 0.06297791831057205\n",
      "Epoch 190, Training Loss 0.06305150588012069\n",
      "Epoch 190, Training Loss 0.06316028580503047\n",
      "Epoch 190, Training Loss 0.06321457389842176\n",
      "Epoch 190, Training Loss 0.06328597862887984\n",
      "Epoch 190, Training Loss 0.06337806861490354\n",
      "Epoch 190, Training Loss 0.0634503910625282\n",
      "Epoch 190, Training Loss 0.0635230651832736\n",
      "Epoch 190, Training Loss 0.06371305430250819\n",
      "Epoch 190, Training Loss 0.06379073370805444\n",
      "Epoch 190, Training Loss 0.06398610083167167\n",
      "Epoch 190, Training Loss 0.06403205842803926\n",
      "Epoch 190, Training Loss 0.0640850901315489\n",
      "Epoch 190, Training Loss 0.0641549077311345\n",
      "Epoch 190, Training Loss 0.06425928518228481\n",
      "Epoch 190, Training Loss 0.06432037382169872\n",
      "Epoch 190, Training Loss 0.06458579862128248\n",
      "Epoch 190, Training Loss 0.06464018861232015\n",
      "Epoch 190, Training Loss 0.06478106402827764\n",
      "Epoch 190, Training Loss 0.0648213070245636\n",
      "Epoch 190, Training Loss 0.06486804549322675\n",
      "Epoch 190, Training Loss 0.06495219479908076\n",
      "Epoch 190, Training Loss 0.06510134399904276\n",
      "Epoch 190, Training Loss 0.06515278204527619\n",
      "Epoch 190, Training Loss 0.06520168912832809\n",
      "Epoch 190, Training Loss 0.06539072327749312\n",
      "Epoch 190, Training Loss 0.06546894598947577\n",
      "Epoch 190, Training Loss 0.06553198369886831\n",
      "Epoch 190, Training Loss 0.06562069846707803\n",
      "Epoch 190, Training Loss 0.06572980559824029\n",
      "Epoch 190, Training Loss 0.06585798912164767\n",
      "Epoch 190, Training Loss 0.06604886299256435\n",
      "Epoch 190, Training Loss 0.0661903842557174\n",
      "Epoch 190, Training Loss 0.0663539590659406\n",
      "Epoch 190, Training Loss 0.06646838200533443\n",
      "Epoch 190, Training Loss 0.06658635131628884\n",
      "Epoch 190, Training Loss 0.06668502063183185\n",
      "Epoch 190, Training Loss 0.06685328639238654\n",
      "Epoch 190, Training Loss 0.06691878650437498\n",
      "Epoch 190, Training Loss 0.06699026479264317\n",
      "Epoch 190, Training Loss 0.0670877442066379\n",
      "Epoch 190, Training Loss 0.06722502328831788\n",
      "Epoch 190, Training Loss 0.06727404505028711\n",
      "Epoch 190, Training Loss 0.06732894028024868\n",
      "Epoch 190, Training Loss 0.06754870757894099\n",
      "Epoch 190, Training Loss 0.06776013779227653\n",
      "Epoch 190, Training Loss 0.06783121609298126\n",
      "Epoch 190, Training Loss 0.067928658301711\n",
      "Epoch 190, Training Loss 0.06804057359671616\n",
      "Epoch 190, Training Loss 0.06834854744255657\n",
      "Epoch 190, Training Loss 0.06853519304229132\n",
      "Epoch 190, Training Loss 0.06865656528683842\n",
      "Epoch 190, Training Loss 0.06869810910495308\n",
      "Epoch 190, Training Loss 0.06884301522784793\n",
      "Epoch 190, Training Loss 0.06895936195097883\n",
      "Epoch 190, Training Loss 0.06916516541343783\n",
      "Epoch 190, Training Loss 0.06929970769297399\n",
      "Epoch 190, Training Loss 0.06951267825191855\n",
      "Epoch 190, Training Loss 0.0695751503650623\n",
      "Epoch 190, Training Loss 0.06966686733614873\n",
      "Epoch 190, Training Loss 0.06971449966011259\n",
      "Epoch 190, Training Loss 0.06983471059542902\n",
      "Epoch 190, Training Loss 0.06990106616650357\n",
      "Epoch 190, Training Loss 0.0700010763960021\n",
      "Epoch 190, Training Loss 0.07010667262684621\n",
      "Epoch 190, Training Loss 0.07020108027698095\n",
      "Epoch 190, Training Loss 0.07035081738682317\n",
      "Epoch 190, Training Loss 0.07039583777256138\n",
      "Epoch 190, Training Loss 0.07045033736430738\n",
      "Epoch 190, Training Loss 0.07060357418906925\n",
      "Epoch 190, Training Loss 0.07066112463636433\n",
      "Epoch 190, Training Loss 0.07081967298193928\n",
      "Epoch 190, Training Loss 0.07087034585259264\n",
      "Epoch 190, Training Loss 0.07095742376063906\n",
      "Epoch 190, Training Loss 0.07102494849644773\n",
      "Epoch 190, Training Loss 0.0710576898037263\n",
      "Epoch 190, Training Loss 0.07112986633144415\n",
      "Epoch 190, Training Loss 0.07119428604731665\n",
      "Epoch 190, Training Loss 0.07124394676802903\n",
      "Epoch 190, Training Loss 0.07130729965031947\n",
      "Epoch 190, Training Loss 0.07138138433651584\n",
      "Epoch 190, Training Loss 0.07144348431602501\n",
      "Epoch 190, Training Loss 0.07150250747728416\n",
      "Epoch 190, Training Loss 0.07156645225199021\n",
      "Epoch 190, Training Loss 0.07180964631026092\n",
      "Epoch 190, Training Loss 0.07190469953129092\n",
      "Epoch 190, Training Loss 0.07196142546989767\n",
      "Epoch 190, Training Loss 0.07215063991930212\n",
      "Epoch 190, Training Loss 0.07231970628380509\n",
      "Epoch 190, Training Loss 0.07241149123429376\n",
      "Epoch 190, Training Loss 0.07251225460125395\n",
      "Epoch 190, Training Loss 0.0727319242420804\n",
      "Epoch 190, Training Loss 0.07276096237975808\n",
      "Epoch 190, Training Loss 0.07279337532079928\n",
      "Epoch 190, Training Loss 0.07286794997794586\n",
      "Epoch 190, Training Loss 0.07301757988327033\n",
      "Epoch 190, Training Loss 0.07312651159232268\n",
      "Epoch 190, Training Loss 0.07323516068427498\n",
      "Epoch 190, Training Loss 0.07327977164060144\n",
      "Epoch 190, Training Loss 0.073390471623958\n",
      "Epoch 190, Training Loss 0.07349138016290867\n",
      "Epoch 190, Training Loss 0.07363001763215646\n",
      "Epoch 190, Training Loss 0.07379386232108297\n",
      "Epoch 190, Training Loss 0.0738487358948173\n",
      "Epoch 190, Training Loss 0.07395307341580996\n",
      "Epoch 190, Training Loss 0.07406544597828975\n",
      "Epoch 190, Training Loss 0.07415426608360827\n",
      "Epoch 190, Training Loss 0.07423683498030924\n",
      "Epoch 190, Training Loss 0.07426791168544489\n",
      "Epoch 190, Training Loss 0.0743951703078778\n",
      "Epoch 190, Training Loss 0.074589457345026\n",
      "Epoch 190, Training Loss 0.07469809090699572\n",
      "Epoch 190, Training Loss 0.0748347477222819\n",
      "Epoch 190, Training Loss 0.0749065697567104\n",
      "Epoch 190, Training Loss 0.07500789977748261\n",
      "Epoch 190, Training Loss 0.075082983336914\n",
      "Epoch 190, Training Loss 0.07523780819290625\n",
      "Epoch 190, Training Loss 0.07533791390797862\n",
      "Epoch 190, Training Loss 0.07542815872246537\n",
      "Epoch 190, Training Loss 0.0754837684793031\n",
      "Epoch 190, Training Loss 0.07555079083923069\n",
      "Epoch 190, Training Loss 0.07563651256058412\n",
      "Epoch 190, Training Loss 0.07568901652575034\n",
      "Epoch 190, Training Loss 0.07583731150997759\n",
      "Epoch 190, Training Loss 0.0758745278246091\n",
      "Epoch 190, Training Loss 0.07596576031740479\n",
      "Epoch 190, Training Loss 0.07599966648056189\n",
      "Epoch 190, Training Loss 0.07606716998412139\n",
      "Epoch 190, Training Loss 0.07618679658240636\n",
      "Epoch 190, Training Loss 0.07631090995462617\n",
      "Epoch 190, Training Loss 0.07641833714183296\n",
      "Epoch 190, Training Loss 0.07660104992115856\n",
      "Epoch 190, Training Loss 0.07673468652641034\n",
      "Epoch 190, Training Loss 0.07683680336469846\n",
      "Epoch 190, Training Loss 0.07707522468774787\n",
      "Epoch 190, Training Loss 0.07733228660124304\n",
      "Epoch 190, Training Loss 0.07750224204295698\n",
      "Epoch 190, Training Loss 0.07760324103691046\n",
      "Epoch 190, Training Loss 0.07767897721765862\n",
      "Epoch 190, Training Loss 0.07776037448674174\n",
      "Epoch 190, Training Loss 0.07785389333596582\n",
      "Epoch 190, Training Loss 0.07794299882138743\n",
      "Epoch 190, Training Loss 0.07802771339002434\n",
      "Epoch 190, Training Loss 0.0781724846974263\n",
      "Epoch 190, Training Loss 0.07827412478430458\n",
      "Epoch 190, Training Loss 0.0784032332563244\n",
      "Epoch 190, Training Loss 0.07846493209066713\n",
      "Epoch 190, Training Loss 0.07854617356448947\n",
      "Epoch 190, Training Loss 0.07869504538752958\n",
      "Epoch 190, Training Loss 0.07876368962071092\n",
      "Epoch 190, Training Loss 0.07887911739761529\n",
      "Epoch 190, Training Loss 0.07894183982930639\n",
      "Epoch 190, Training Loss 0.07917138375699177\n",
      "Epoch 190, Training Loss 0.07930368476468698\n",
      "Epoch 190, Training Loss 0.07937339790370268\n",
      "Epoch 190, Training Loss 0.07955237032007188\n",
      "Epoch 190, Training Loss 0.07976495047621525\n",
      "Epoch 190, Training Loss 0.07987015457023554\n",
      "Epoch 190, Training Loss 0.07994518643292739\n",
      "Epoch 190, Training Loss 0.08000947215744411\n",
      "Epoch 190, Training Loss 0.0800396345562928\n",
      "Epoch 190, Training Loss 0.0802249047331169\n",
      "Epoch 190, Training Loss 0.08031335391004181\n",
      "Epoch 190, Training Loss 0.08040518883277503\n",
      "Epoch 190, Training Loss 0.08071380920227036\n",
      "Epoch 190, Training Loss 0.08081325050086126\n",
      "Epoch 190, Training Loss 0.08087885932749152\n",
      "Epoch 190, Training Loss 0.0810618838688354\n",
      "Epoch 190, Training Loss 0.08115000673510192\n",
      "Epoch 190, Training Loss 0.0813788519850682\n",
      "Epoch 190, Training Loss 0.0814356275433984\n",
      "Epoch 190, Training Loss 0.08154610213240052\n",
      "Epoch 190, Training Loss 0.08158849045286512\n",
      "Epoch 190, Training Loss 0.08167506134985468\n",
      "Epoch 190, Training Loss 0.0817831393481349\n",
      "Epoch 190, Training Loss 0.08196551999544054\n",
      "Epoch 190, Training Loss 0.08206043569514018\n",
      "Epoch 190, Training Loss 0.082208997528295\n",
      "Epoch 190, Training Loss 0.08239967711246036\n",
      "Epoch 190, Training Loss 0.08252457089012351\n",
      "Epoch 190, Training Loss 0.08260529169031536\n",
      "Epoch 190, Training Loss 0.08269913045718047\n",
      "Epoch 190, Training Loss 0.08275411585155312\n",
      "Epoch 190, Training Loss 0.08294510491111357\n",
      "Epoch 190, Training Loss 0.08307871083750407\n",
      "Epoch 190, Training Loss 0.08322697399003083\n",
      "Epoch 190, Training Loss 0.08354308455587005\n",
      "Epoch 190, Training Loss 0.08366454292036345\n",
      "Epoch 190, Training Loss 0.08372087447363359\n",
      "Epoch 190, Training Loss 0.08379843085408782\n",
      "Epoch 190, Training Loss 0.08386735163529015\n",
      "Epoch 190, Training Loss 0.08397125808017143\n",
      "Epoch 190, Training Loss 0.0840797927171049\n",
      "Epoch 190, Training Loss 0.08418870032133768\n",
      "Epoch 190, Training Loss 0.08424850915203733\n",
      "Epoch 190, Training Loss 0.08441575309690422\n",
      "Epoch 190, Training Loss 0.08459125840178955\n",
      "Epoch 190, Training Loss 0.0846817963518431\n",
      "Epoch 190, Training Loss 0.08474136949838389\n",
      "Epoch 190, Training Loss 0.08489105898929793\n",
      "Epoch 190, Training Loss 0.0849959723772886\n",
      "Epoch 190, Training Loss 0.08508282245191581\n",
      "Epoch 190, Training Loss 0.08526443831903664\n",
      "Epoch 190, Training Loss 0.08532304289486364\n",
      "Epoch 190, Training Loss 0.08537976361234742\n",
      "Epoch 190, Training Loss 0.08548033328326729\n",
      "Epoch 190, Training Loss 0.08566233922925104\n",
      "Epoch 190, Training Loss 0.08575762793436037\n",
      "Epoch 190, Training Loss 0.08588094958235197\n",
      "Epoch 190, Training Loss 0.08592441707463636\n",
      "Epoch 190, Training Loss 0.08594721354558454\n",
      "Epoch 190, Training Loss 0.0860585391776317\n",
      "Epoch 190, Training Loss 0.08612869532130983\n",
      "Epoch 190, Training Loss 0.08617274251068606\n",
      "Epoch 190, Training Loss 0.0862846904758202\n",
      "Epoch 190, Training Loss 0.0863140474140282\n",
      "Epoch 200, Training Loss 0.00010996150886616134\n",
      "Epoch 200, Training Loss 0.00015350784201298832\n",
      "Epoch 200, Training Loss 0.00023382291902818947\n",
      "Epoch 200, Training Loss 0.0003877441872797354\n",
      "Epoch 200, Training Loss 0.0005057681580562421\n",
      "Epoch 200, Training Loss 0.0005581733077535849\n",
      "Epoch 200, Training Loss 0.0006366285312053798\n",
      "Epoch 200, Training Loss 0.0007041084730183072\n",
      "Epoch 200, Training Loss 0.00088005385759389\n",
      "Epoch 200, Training Loss 0.0009251321044266986\n",
      "Epoch 200, Training Loss 0.001008441247751036\n",
      "Epoch 200, Training Loss 0.0010881203433970356\n",
      "Epoch 200, Training Loss 0.0011271494881385732\n",
      "Epoch 200, Training Loss 0.0011815662326677071\n",
      "Epoch 200, Training Loss 0.0012869124117371677\n",
      "Epoch 200, Training Loss 0.00132742322876554\n",
      "Epoch 200, Training Loss 0.001342796684835878\n",
      "Epoch 200, Training Loss 0.0013757556905526944\n",
      "Epoch 200, Training Loss 0.0015368149103715902\n",
      "Epoch 200, Training Loss 0.0016620802452497166\n",
      "Epoch 200, Training Loss 0.0017577412983645563\n",
      "Epoch 200, Training Loss 0.0018818011445462552\n",
      "Epoch 200, Training Loss 0.0019697404235525205\n",
      "Epoch 200, Training Loss 0.0020227120650927427\n",
      "Epoch 200, Training Loss 0.0020920170275756465\n",
      "Epoch 200, Training Loss 0.002137430383802375\n",
      "Epoch 200, Training Loss 0.002199696114910838\n",
      "Epoch 200, Training Loss 0.002355689940321476\n",
      "Epoch 200, Training Loss 0.002425414202806285\n",
      "Epoch 200, Training Loss 0.002511833029825364\n",
      "Epoch 200, Training Loss 0.0025665408567242002\n",
      "Epoch 200, Training Loss 0.0026237654268665388\n",
      "Epoch 200, Training Loss 0.0026480590786470476\n",
      "Epoch 200, Training Loss 0.0026750679954391002\n",
      "Epoch 200, Training Loss 0.002818074086895379\n",
      "Epoch 200, Training Loss 0.00296492900346856\n",
      "Epoch 200, Training Loss 0.0031242683205915532\n",
      "Epoch 200, Training Loss 0.0031730003007080245\n",
      "Epoch 200, Training Loss 0.0032488637082183454\n",
      "Epoch 200, Training Loss 0.0032950530109731742\n",
      "Epoch 200, Training Loss 0.0034040712782412844\n",
      "Epoch 200, Training Loss 0.003500639349031631\n",
      "Epoch 200, Training Loss 0.0035703739060846435\n",
      "Epoch 200, Training Loss 0.0036152835239839677\n",
      "Epoch 200, Training Loss 0.0036671217673879754\n",
      "Epoch 200, Training Loss 0.003744676942601228\n",
      "Epoch 200, Training Loss 0.003952627868184348\n",
      "Epoch 200, Training Loss 0.004009783939670419\n",
      "Epoch 200, Training Loss 0.004158194912859546\n",
      "Epoch 200, Training Loss 0.004203798968697448\n",
      "Epoch 200, Training Loss 0.004279679330566046\n",
      "Epoch 200, Training Loss 0.004360230834892643\n",
      "Epoch 200, Training Loss 0.004461307316789846\n",
      "Epoch 200, Training Loss 0.004516264904871621\n",
      "Epoch 200, Training Loss 0.004660454800214304\n",
      "Epoch 200, Training Loss 0.004810900401200175\n",
      "Epoch 200, Training Loss 0.0048930387267523716\n",
      "Epoch 200, Training Loss 0.005015091756192009\n",
      "Epoch 200, Training Loss 0.00510566214771222\n",
      "Epoch 200, Training Loss 0.005209269311726855\n",
      "Epoch 200, Training Loss 0.005273927165114362\n",
      "Epoch 200, Training Loss 0.005340806434831351\n",
      "Epoch 200, Training Loss 0.005414458215617768\n",
      "Epoch 200, Training Loss 0.00548264966405871\n",
      "Epoch 200, Training Loss 0.0056287788921762305\n",
      "Epoch 200, Training Loss 0.005721433293026731\n",
      "Epoch 200, Training Loss 0.0057851089369458\n",
      "Epoch 200, Training Loss 0.0059874304820357075\n",
      "Epoch 200, Training Loss 0.006220873102278966\n",
      "Epoch 200, Training Loss 0.0064253753046397966\n",
      "Epoch 200, Training Loss 0.006538147464051576\n",
      "Epoch 200, Training Loss 0.006609535306844565\n",
      "Epoch 200, Training Loss 0.006812783143938045\n",
      "Epoch 200, Training Loss 0.006863453084855434\n",
      "Epoch 200, Training Loss 0.006940585968401426\n",
      "Epoch 200, Training Loss 0.006992563743458685\n",
      "Epoch 200, Training Loss 0.007062747936381403\n",
      "Epoch 200, Training Loss 0.0071618971664963475\n",
      "Epoch 200, Training Loss 0.007220704089421446\n",
      "Epoch 200, Training Loss 0.007280658468451646\n",
      "Epoch 200, Training Loss 0.0073074590095588005\n",
      "Epoch 200, Training Loss 0.007435979304925713\n",
      "Epoch 200, Training Loss 0.0074881594556638655\n",
      "Epoch 200, Training Loss 0.007563132675998199\n",
      "Epoch 200, Training Loss 0.007619682954305121\n",
      "Epoch 200, Training Loss 0.00769499204147731\n",
      "Epoch 200, Training Loss 0.007763699118686302\n",
      "Epoch 200, Training Loss 0.007821356017818994\n",
      "Epoch 200, Training Loss 0.007947941001056863\n",
      "Epoch 200, Training Loss 0.007990686549231067\n",
      "Epoch 200, Training Loss 0.008020461239206517\n",
      "Epoch 200, Training Loss 0.008100195399597478\n",
      "Epoch 200, Training Loss 0.00823196766378782\n",
      "Epoch 200, Training Loss 0.008309739706156503\n",
      "Epoch 200, Training Loss 0.008392616191788403\n",
      "Epoch 200, Training Loss 0.00848442373220878\n",
      "Epoch 200, Training Loss 0.008547634076889214\n",
      "Epoch 200, Training Loss 0.008659284114075438\n",
      "Epoch 200, Training Loss 0.008684594124612753\n",
      "Epoch 200, Training Loss 0.008752670646418848\n",
      "Epoch 200, Training Loss 0.008844574882417841\n",
      "Epoch 200, Training Loss 0.009051365251450436\n",
      "Epoch 200, Training Loss 0.009182981386914126\n",
      "Epoch 200, Training Loss 0.00925419456504114\n",
      "Epoch 200, Training Loss 0.009338310133675328\n",
      "Epoch 200, Training Loss 0.00949822083982589\n",
      "Epoch 200, Training Loss 0.009535526882980943\n",
      "Epoch 200, Training Loss 0.009702125923884341\n",
      "Epoch 200, Training Loss 0.009787854943376825\n",
      "Epoch 200, Training Loss 0.009861991288202346\n",
      "Epoch 200, Training Loss 0.00993815035132877\n",
      "Epoch 200, Training Loss 0.0099788604999709\n",
      "Epoch 200, Training Loss 0.010015587574895233\n",
      "Epoch 200, Training Loss 0.010132670743138436\n",
      "Epoch 200, Training Loss 0.010165155469856756\n",
      "Epoch 200, Training Loss 0.010236951722608655\n",
      "Epoch 200, Training Loss 0.010325464763014061\n",
      "Epoch 200, Training Loss 0.010406095384503418\n",
      "Epoch 200, Training Loss 0.01053781306509243\n",
      "Epoch 200, Training Loss 0.01059685840183283\n",
      "Epoch 200, Training Loss 0.010659312531637872\n",
      "Epoch 200, Training Loss 0.010691727237666355\n",
      "Epoch 200, Training Loss 0.010784954790149808\n",
      "Epoch 200, Training Loss 0.010858647863540198\n",
      "Epoch 200, Training Loss 0.010933432523208811\n",
      "Epoch 200, Training Loss 0.011078283681398463\n",
      "Epoch 200, Training Loss 0.011167626851774238\n",
      "Epoch 200, Training Loss 0.01126750689619185\n",
      "Epoch 200, Training Loss 0.011356927716480496\n",
      "Epoch 200, Training Loss 0.0114155442827879\n",
      "Epoch 200, Training Loss 0.011497442617707545\n",
      "Epoch 200, Training Loss 0.01164457031890102\n",
      "Epoch 200, Training Loss 0.011695351118169477\n",
      "Epoch 200, Training Loss 0.011758835519404362\n",
      "Epoch 200, Training Loss 0.011820923427448553\n",
      "Epoch 200, Training Loss 0.011881506251518988\n",
      "Epoch 200, Training Loss 0.011926159205491586\n",
      "Epoch 200, Training Loss 0.012004476526509161\n",
      "Epoch 200, Training Loss 0.012077085835778194\n",
      "Epoch 200, Training Loss 0.012125367741755513\n",
      "Epoch 200, Training Loss 0.012200403496470598\n",
      "Epoch 200, Training Loss 0.012320716805813257\n",
      "Epoch 200, Training Loss 0.012484908451700149\n",
      "Epoch 200, Training Loss 0.01258779998840121\n",
      "Epoch 200, Training Loss 0.01268995453691696\n",
      "Epoch 200, Training Loss 0.012799426374952202\n",
      "Epoch 200, Training Loss 0.012950305929383659\n",
      "Epoch 200, Training Loss 0.013052279891832101\n",
      "Epoch 200, Training Loss 0.013139705499991432\n",
      "Epoch 200, Training Loss 0.013221151874307781\n",
      "Epoch 200, Training Loss 0.013284749620596467\n",
      "Epoch 200, Training Loss 0.013313855118382617\n",
      "Epoch 200, Training Loss 0.013383312998792094\n",
      "Epoch 200, Training Loss 0.013476118488270608\n",
      "Epoch 200, Training Loss 0.013622923308740492\n",
      "Epoch 200, Training Loss 0.01371393185537642\n",
      "Epoch 200, Training Loss 0.01379638280995819\n",
      "Epoch 200, Training Loss 0.013864359992277591\n",
      "Epoch 200, Training Loss 0.013919630223680335\n",
      "Epoch 200, Training Loss 0.01395741324928944\n",
      "Epoch 200, Training Loss 0.01405764051744014\n",
      "Epoch 200, Training Loss 0.014122897899611984\n",
      "Epoch 200, Training Loss 0.01417579319652008\n",
      "Epoch 200, Training Loss 0.014316762945212214\n",
      "Epoch 200, Training Loss 0.014404973789306401\n",
      "Epoch 200, Training Loss 0.0144631982473728\n",
      "Epoch 200, Training Loss 0.014546537126326347\n",
      "Epoch 200, Training Loss 0.014611095673573748\n",
      "Epoch 200, Training Loss 0.014652930683625475\n",
      "Epoch 200, Training Loss 0.014754515729577797\n",
      "Epoch 200, Training Loss 0.014821943655476698\n",
      "Epoch 200, Training Loss 0.01490069830270908\n",
      "Epoch 200, Training Loss 0.014981290975780895\n",
      "Epoch 200, Training Loss 0.015093071884034998\n",
      "Epoch 200, Training Loss 0.0152118423324831\n",
      "Epoch 200, Training Loss 0.015243697508959972\n",
      "Epoch 200, Training Loss 0.015286581321617069\n",
      "Epoch 200, Training Loss 0.015370613142676518\n",
      "Epoch 200, Training Loss 0.015390001095669426\n",
      "Epoch 200, Training Loss 0.015685961213763183\n",
      "Epoch 200, Training Loss 0.015799231275849406\n",
      "Epoch 200, Training Loss 0.015896603618712757\n",
      "Epoch 200, Training Loss 0.01594038114375661\n",
      "Epoch 200, Training Loss 0.01597364147400955\n",
      "Epoch 200, Training Loss 0.016094506640806604\n",
      "Epoch 200, Training Loss 0.01613651993601104\n",
      "Epoch 200, Training Loss 0.0162020137026201\n",
      "Epoch 200, Training Loss 0.01624957840923039\n",
      "Epoch 200, Training Loss 0.016317352053502102\n",
      "Epoch 200, Training Loss 0.016397243924200763\n",
      "Epoch 200, Training Loss 0.016451856908638537\n",
      "Epoch 200, Training Loss 0.016520703126869315\n",
      "Epoch 200, Training Loss 0.01663371022728741\n",
      "Epoch 200, Training Loss 0.01676042591962401\n",
      "Epoch 200, Training Loss 0.01681060725203751\n",
      "Epoch 200, Training Loss 0.016848117345348572\n",
      "Epoch 200, Training Loss 0.016949596892222953\n",
      "Epoch 200, Training Loss 0.017001641970223098\n",
      "Epoch 200, Training Loss 0.017054021547850975\n",
      "Epoch 200, Training Loss 0.017138853105847413\n",
      "Epoch 200, Training Loss 0.017194892609696786\n",
      "Epoch 200, Training Loss 0.017260739678645606\n",
      "Epoch 200, Training Loss 0.017349217676550455\n",
      "Epoch 200, Training Loss 0.01742447056042988\n",
      "Epoch 200, Training Loss 0.017493152160845373\n",
      "Epoch 200, Training Loss 0.017519601819622317\n",
      "Epoch 200, Training Loss 0.017603443870011268\n",
      "Epoch 200, Training Loss 0.017677513131028628\n",
      "Epoch 200, Training Loss 0.01772828776117824\n",
      "Epoch 200, Training Loss 0.01783098106670296\n",
      "Epoch 200, Training Loss 0.017928026136620652\n",
      "Epoch 200, Training Loss 0.018048240149350803\n",
      "Epoch 200, Training Loss 0.01811778229182524\n",
      "Epoch 200, Training Loss 0.018150512034983357\n",
      "Epoch 200, Training Loss 0.01827320452812878\n",
      "Epoch 200, Training Loss 0.01836875206945688\n",
      "Epoch 200, Training Loss 0.01844194857403636\n",
      "Epoch 200, Training Loss 0.01854029968690575\n",
      "Epoch 200, Training Loss 0.01869146467860588\n",
      "Epoch 200, Training Loss 0.018750625699425063\n",
      "Epoch 200, Training Loss 0.018836836196015328\n",
      "Epoch 200, Training Loss 0.018913543019729577\n",
      "Epoch 200, Training Loss 0.018996229953825702\n",
      "Epoch 200, Training Loss 0.019104970062908996\n",
      "Epoch 200, Training Loss 0.01919959689540517\n",
      "Epoch 200, Training Loss 0.019269606614213848\n",
      "Epoch 200, Training Loss 0.01931346445213384\n",
      "Epoch 200, Training Loss 0.01937602969277126\n",
      "Epoch 200, Training Loss 0.019505137602662873\n",
      "Epoch 200, Training Loss 0.019670970529517936\n",
      "Epoch 200, Training Loss 0.01977315177435956\n",
      "Epoch 200, Training Loss 0.01990845275547384\n",
      "Epoch 200, Training Loss 0.019993547808683818\n",
      "Epoch 200, Training Loss 0.02010360696588826\n",
      "Epoch 200, Training Loss 0.020356592878251506\n",
      "Epoch 200, Training Loss 0.020454891589339202\n",
      "Epoch 200, Training Loss 0.02055527970237691\n",
      "Epoch 200, Training Loss 0.020611179493548695\n",
      "Epoch 200, Training Loss 0.02070076920001594\n",
      "Epoch 200, Training Loss 0.0207814547576277\n",
      "Epoch 200, Training Loss 0.020872092852249856\n",
      "Epoch 200, Training Loss 0.02097191911338426\n",
      "Epoch 200, Training Loss 0.021034388654076915\n",
      "Epoch 200, Training Loss 0.02117649081479901\n",
      "Epoch 200, Training Loss 0.021485634039744468\n",
      "Epoch 200, Training Loss 0.02164051708076959\n",
      "Epoch 200, Training Loss 0.021730792593172826\n",
      "Epoch 200, Training Loss 0.02184390156265453\n",
      "Epoch 200, Training Loss 0.021956101637524185\n",
      "Epoch 200, Training Loss 0.022147128998023242\n",
      "Epoch 200, Training Loss 0.022208837331379846\n",
      "Epoch 200, Training Loss 0.022264753776790616\n",
      "Epoch 200, Training Loss 0.022316089301320063\n",
      "Epoch 200, Training Loss 0.022464459210567538\n",
      "Epoch 200, Training Loss 0.02256755699115374\n",
      "Epoch 200, Training Loss 0.022812245834066207\n",
      "Epoch 200, Training Loss 0.022853352978606436\n",
      "Epoch 200, Training Loss 0.022873820283252487\n",
      "Epoch 200, Training Loss 0.022908629307671048\n",
      "Epoch 200, Training Loss 0.022917525185381666\n",
      "Epoch 200, Training Loss 0.023017690343129664\n",
      "Epoch 200, Training Loss 0.023102785072401356\n",
      "Epoch 200, Training Loss 0.02321170844480662\n",
      "Epoch 200, Training Loss 0.023305100310222267\n",
      "Epoch 200, Training Loss 0.02333056114499679\n",
      "Epoch 200, Training Loss 0.02347972623699004\n",
      "Epoch 200, Training Loss 0.023598202987743157\n",
      "Epoch 200, Training Loss 0.023690144276565604\n",
      "Epoch 200, Training Loss 0.023846317640007914\n",
      "Epoch 200, Training Loss 0.02394812080123083\n",
      "Epoch 200, Training Loss 0.02399309551167061\n",
      "Epoch 200, Training Loss 0.02407228531282576\n",
      "Epoch 200, Training Loss 0.024141740415940808\n",
      "Epoch 200, Training Loss 0.024233219025613706\n",
      "Epoch 200, Training Loss 0.02426318295985994\n",
      "Epoch 200, Training Loss 0.024364855030880254\n",
      "Epoch 200, Training Loss 0.024436370533941043\n",
      "Epoch 200, Training Loss 0.02450154329676305\n",
      "Epoch 200, Training Loss 0.024541377392418854\n",
      "Epoch 200, Training Loss 0.02461448208431301\n",
      "Epoch 200, Training Loss 0.024664324896452983\n",
      "Epoch 200, Training Loss 0.024747830794176178\n",
      "Epoch 200, Training Loss 0.02484755385238344\n",
      "Epoch 200, Training Loss 0.024924522599257776\n",
      "Epoch 200, Training Loss 0.02496994280582651\n",
      "Epoch 200, Training Loss 0.025174028252053747\n",
      "Epoch 200, Training Loss 0.02520094320530553\n",
      "Epoch 200, Training Loss 0.02535015309605833\n",
      "Epoch 200, Training Loss 0.025505392307229816\n",
      "Epoch 200, Training Loss 0.025545528732583193\n",
      "Epoch 200, Training Loss 0.025607082861787676\n",
      "Epoch 200, Training Loss 0.02568257820394719\n",
      "Epoch 200, Training Loss 0.025799567201424897\n",
      "Epoch 200, Training Loss 0.02593158034231428\n",
      "Epoch 200, Training Loss 0.026070733902418552\n",
      "Epoch 200, Training Loss 0.02615162417949999\n",
      "Epoch 200, Training Loss 0.02622726450786185\n",
      "Epoch 200, Training Loss 0.026279268107469885\n",
      "Epoch 200, Training Loss 0.0263505150561633\n",
      "Epoch 200, Training Loss 0.026426247506857375\n",
      "Epoch 200, Training Loss 0.026459065032527422\n",
      "Epoch 200, Training Loss 0.026518723570153384\n",
      "Epoch 200, Training Loss 0.026531823721411815\n",
      "Epoch 200, Training Loss 0.026614080446407846\n",
      "Epoch 200, Training Loss 0.02666461687234928\n",
      "Epoch 200, Training Loss 0.02672053500890846\n",
      "Epoch 200, Training Loss 0.02683533445986755\n",
      "Epoch 200, Training Loss 0.0269211405671447\n",
      "Epoch 200, Training Loss 0.027168362302577024\n",
      "Epoch 200, Training Loss 0.02737248919384978\n",
      "Epoch 200, Training Loss 0.02760179523650147\n",
      "Epoch 200, Training Loss 0.027773760829616308\n",
      "Epoch 200, Training Loss 0.02786633262620367\n",
      "Epoch 200, Training Loss 0.028002486698796302\n",
      "Epoch 200, Training Loss 0.02810473661855473\n",
      "Epoch 200, Training Loss 0.02813216963249361\n",
      "Epoch 200, Training Loss 0.02835432791968098\n",
      "Epoch 200, Training Loss 0.02842099354733401\n",
      "Epoch 200, Training Loss 0.028477581855043045\n",
      "Epoch 200, Training Loss 0.02852209568228525\n",
      "Epoch 200, Training Loss 0.028554636779029276\n",
      "Epoch 200, Training Loss 0.028654290771688264\n",
      "Epoch 200, Training Loss 0.028804254924159143\n",
      "Epoch 200, Training Loss 0.028924916343301382\n",
      "Epoch 200, Training Loss 0.029001411606731545\n",
      "Epoch 200, Training Loss 0.02907132204798291\n",
      "Epoch 200, Training Loss 0.0291534137264218\n",
      "Epoch 200, Training Loss 0.02932323700131472\n",
      "Epoch 200, Training Loss 0.02942228589630912\n",
      "Epoch 200, Training Loss 0.029526989829614568\n",
      "Epoch 200, Training Loss 0.0296641451001758\n",
      "Epoch 200, Training Loss 0.029755740499128696\n",
      "Epoch 200, Training Loss 0.029782792990384124\n",
      "Epoch 200, Training Loss 0.029846653074283355\n",
      "Epoch 200, Training Loss 0.029921415353627384\n",
      "Epoch 200, Training Loss 0.0299842187413074\n",
      "Epoch 200, Training Loss 0.030152385986512504\n",
      "Epoch 200, Training Loss 0.030216243250243118\n",
      "Epoch 200, Training Loss 0.030271984387875137\n",
      "Epoch 200, Training Loss 0.030336364292684954\n",
      "Epoch 200, Training Loss 0.030417572267477395\n",
      "Epoch 200, Training Loss 0.03043810202313773\n",
      "Epoch 200, Training Loss 0.03052812077633827\n",
      "Epoch 200, Training Loss 0.030618648627139342\n",
      "Epoch 200, Training Loss 0.030673877270582614\n",
      "Epoch 200, Training Loss 0.03080145869751835\n",
      "Epoch 200, Training Loss 0.030922065399672904\n",
      "Epoch 200, Training Loss 0.03112143135088903\n",
      "Epoch 200, Training Loss 0.03119532375589318\n",
      "Epoch 200, Training Loss 0.03128669743814394\n",
      "Epoch 200, Training Loss 0.03151123892025227\n",
      "Epoch 200, Training Loss 0.03161264992559619\n",
      "Epoch 200, Training Loss 0.031661032752641254\n",
      "Epoch 200, Training Loss 0.031777754100873266\n",
      "Epoch 200, Training Loss 0.031894766517183586\n",
      "Epoch 200, Training Loss 0.031993254160513275\n",
      "Epoch 200, Training Loss 0.03203924930513934\n",
      "Epoch 200, Training Loss 0.032120991616135894\n",
      "Epoch 200, Training Loss 0.03220312331048081\n",
      "Epoch 200, Training Loss 0.03222295157539913\n",
      "Epoch 200, Training Loss 0.032340819789025255\n",
      "Epoch 200, Training Loss 0.03244774313786489\n",
      "Epoch 200, Training Loss 0.032531149996220685\n",
      "Epoch 200, Training Loss 0.03256912297352463\n",
      "Epoch 200, Training Loss 0.0326346518922016\n",
      "Epoch 200, Training Loss 0.0326806263285963\n",
      "Epoch 200, Training Loss 0.03279112870959789\n",
      "Epoch 200, Training Loss 0.03300857060777066\n",
      "Epoch 200, Training Loss 0.03313921735791103\n",
      "Epoch 200, Training Loss 0.033248945069082485\n",
      "Epoch 200, Training Loss 0.03334552475401317\n",
      "Epoch 200, Training Loss 0.03342778511855113\n",
      "Epoch 200, Training Loss 0.03350011767971965\n",
      "Epoch 200, Training Loss 0.03354327376727063\n",
      "Epoch 200, Training Loss 0.03360757948544896\n",
      "Epoch 200, Training Loss 0.03373327657289784\n",
      "Epoch 200, Training Loss 0.033855612381406684\n",
      "Epoch 200, Training Loss 0.03398669015049287\n",
      "Epoch 200, Training Loss 0.034051321259916514\n",
      "Epoch 200, Training Loss 0.03409368185507481\n",
      "Epoch 200, Training Loss 0.03421693033112399\n",
      "Epoch 200, Training Loss 0.03429424508219904\n",
      "Epoch 200, Training Loss 0.034364488774962974\n",
      "Epoch 200, Training Loss 0.034469986957309726\n",
      "Epoch 200, Training Loss 0.034544889725830474\n",
      "Epoch 200, Training Loss 0.0345802214794585\n",
      "Epoch 200, Training Loss 0.03461536191894537\n",
      "Epoch 200, Training Loss 0.034706197139542656\n",
      "Epoch 200, Training Loss 0.034747799123634994\n",
      "Epoch 200, Training Loss 0.03501319522609758\n",
      "Epoch 200, Training Loss 0.03513161964176218\n",
      "Epoch 200, Training Loss 0.03519249581100652\n",
      "Epoch 200, Training Loss 0.03528125438710575\n",
      "Epoch 200, Training Loss 0.03538645443189746\n",
      "Epoch 200, Training Loss 0.03557381096184063\n",
      "Epoch 200, Training Loss 0.03561550405357614\n",
      "Epoch 200, Training Loss 0.03592233634089379\n",
      "Epoch 200, Training Loss 0.035948721247741865\n",
      "Epoch 200, Training Loss 0.036044767911276775\n",
      "Epoch 200, Training Loss 0.03611663928912843\n",
      "Epoch 200, Training Loss 0.036270072753958\n",
      "Epoch 200, Training Loss 0.03644999360446544\n",
      "Epoch 200, Training Loss 0.03649313812551405\n",
      "Epoch 200, Training Loss 0.0365459568681829\n",
      "Epoch 200, Training Loss 0.036738870711878056\n",
      "Epoch 200, Training Loss 0.03687769279736654\n",
      "Epoch 200, Training Loss 0.036939673053095944\n",
      "Epoch 200, Training Loss 0.0370525199755588\n",
      "Epoch 200, Training Loss 0.037218404770650144\n",
      "Epoch 200, Training Loss 0.03732589258195456\n",
      "Epoch 200, Training Loss 0.03742861765248658\n",
      "Epoch 200, Training Loss 0.03760572445109639\n",
      "Epoch 200, Training Loss 0.03769142087668066\n",
      "Epoch 200, Training Loss 0.03772986486739934\n",
      "Epoch 200, Training Loss 0.03775849266577979\n",
      "Epoch 200, Training Loss 0.03782981667367508\n",
      "Epoch 200, Training Loss 0.037925339557582996\n",
      "Epoch 200, Training Loss 0.03804259709037288\n",
      "Epoch 200, Training Loss 0.03812523006968905\n",
      "Epoch 200, Training Loss 0.0382209466305821\n",
      "Epoch 200, Training Loss 0.03830757207306259\n",
      "Epoch 200, Training Loss 0.038413409714151144\n",
      "Epoch 200, Training Loss 0.03847127566781\n",
      "Epoch 200, Training Loss 0.03855945225066655\n",
      "Epoch 200, Training Loss 0.03876007170966633\n",
      "Epoch 200, Training Loss 0.03896897608090354\n",
      "Epoch 200, Training Loss 0.0390521729605563\n",
      "Epoch 200, Training Loss 0.03915646809327138\n",
      "Epoch 200, Training Loss 0.03924664581442237\n",
      "Epoch 200, Training Loss 0.039362246240906014\n",
      "Epoch 200, Training Loss 0.03952216496929298\n",
      "Epoch 200, Training Loss 0.03956015081838002\n",
      "Epoch 200, Training Loss 0.039598749439610774\n",
      "Epoch 200, Training Loss 0.03972201181642349\n",
      "Epoch 200, Training Loss 0.039863028726719145\n",
      "Epoch 200, Training Loss 0.039977110530514164\n",
      "Epoch 200, Training Loss 0.0401040997506236\n",
      "Epoch 200, Training Loss 0.040253933079664586\n",
      "Epoch 200, Training Loss 0.04049466979925704\n",
      "Epoch 200, Training Loss 0.040609344572086085\n",
      "Epoch 200, Training Loss 0.04070668751278611\n",
      "Epoch 200, Training Loss 0.04076879757725636\n",
      "Epoch 200, Training Loss 0.04081274580824977\n",
      "Epoch 200, Training Loss 0.04096516006795303\n",
      "Epoch 200, Training Loss 0.04115828067955116\n",
      "Epoch 200, Training Loss 0.04127235441347179\n",
      "Epoch 200, Training Loss 0.04145893886509111\n",
      "Epoch 200, Training Loss 0.04152963842477297\n",
      "Epoch 200, Training Loss 0.04166838217436162\n",
      "Epoch 200, Training Loss 0.0417586066093191\n",
      "Epoch 200, Training Loss 0.0418710578602198\n",
      "Epoch 200, Training Loss 0.04204351465810862\n",
      "Epoch 200, Training Loss 0.042136924797454685\n",
      "Epoch 200, Training Loss 0.04218601239035311\n",
      "Epoch 200, Training Loss 0.042316127472016435\n",
      "Epoch 200, Training Loss 0.042358803416333156\n",
      "Epoch 200, Training Loss 0.0424085432637359\n",
      "Epoch 200, Training Loss 0.042496928100800496\n",
      "Epoch 200, Training Loss 0.04267241295946338\n",
      "Epoch 200, Training Loss 0.04273145374677637\n",
      "Epoch 200, Training Loss 0.04281287231003804\n",
      "Epoch 200, Training Loss 0.042852957805623405\n",
      "Epoch 200, Training Loss 0.042936139698366606\n",
      "Epoch 200, Training Loss 0.043110929899956185\n",
      "Epoch 200, Training Loss 0.04327712091200454\n",
      "Epoch 200, Training Loss 0.04339064739029998\n",
      "Epoch 200, Training Loss 0.043484135160503715\n",
      "Epoch 200, Training Loss 0.043673828028647416\n",
      "Epoch 200, Training Loss 0.0437880805098092\n",
      "Epoch 200, Training Loss 0.043841871920058416\n",
      "Epoch 200, Training Loss 0.043978900606970274\n",
      "Epoch 200, Training Loss 0.04415480450838995\n",
      "Epoch 200, Training Loss 0.0443788679338077\n",
      "Epoch 200, Training Loss 0.04442298613236193\n",
      "Epoch 200, Training Loss 0.04448093577642994\n",
      "Epoch 200, Training Loss 0.04454622038847307\n",
      "Epoch 200, Training Loss 0.04459025382476352\n",
      "Epoch 200, Training Loss 0.044712404359627485\n",
      "Epoch 200, Training Loss 0.04476891026791671\n",
      "Epoch 200, Training Loss 0.044819500901119406\n",
      "Epoch 200, Training Loss 0.044869181410650084\n",
      "Epoch 200, Training Loss 0.045051898706056505\n",
      "Epoch 200, Training Loss 0.04517015272065464\n",
      "Epoch 200, Training Loss 0.045255287780243035\n",
      "Epoch 200, Training Loss 0.04532717425933541\n",
      "Epoch 200, Training Loss 0.0454316599725667\n",
      "Epoch 200, Training Loss 0.04549335271401136\n",
      "Epoch 200, Training Loss 0.04553617402087049\n",
      "Epoch 200, Training Loss 0.045641050441786075\n",
      "Epoch 200, Training Loss 0.04575789409577656\n",
      "Epoch 200, Training Loss 0.04587876925702252\n",
      "Epoch 200, Training Loss 0.04604824711842572\n",
      "Epoch 200, Training Loss 0.046218991807192716\n",
      "Epoch 200, Training Loss 0.046335262261912265\n",
      "Epoch 200, Training Loss 0.04640495847991627\n",
      "Epoch 200, Training Loss 0.04655036969882105\n",
      "Epoch 200, Training Loss 0.04665053396097496\n",
      "Epoch 200, Training Loss 0.046771796600049945\n",
      "Epoch 200, Training Loss 0.04684627113168311\n",
      "Epoch 200, Training Loss 0.04692590255599917\n",
      "Epoch 200, Training Loss 0.047107527379060876\n",
      "Epoch 200, Training Loss 0.0471413902535825\n",
      "Epoch 200, Training Loss 0.04731295002824472\n",
      "Epoch 200, Training Loss 0.04735833886043762\n",
      "Epoch 200, Training Loss 0.04739959989948308\n",
      "Epoch 200, Training Loss 0.047525349493517215\n",
      "Epoch 200, Training Loss 0.04758604060348762\n",
      "Epoch 200, Training Loss 0.04776336369759706\n",
      "Epoch 200, Training Loss 0.04803991108975562\n",
      "Epoch 200, Training Loss 0.048101610881621806\n",
      "Epoch 200, Training Loss 0.04831806909354866\n",
      "Epoch 200, Training Loss 0.04841462781776667\n",
      "Epoch 200, Training Loss 0.04843776490267776\n",
      "Epoch 200, Training Loss 0.048502796871439956\n",
      "Epoch 200, Training Loss 0.048583367259463155\n",
      "Epoch 200, Training Loss 0.048616180152339324\n",
      "Epoch 200, Training Loss 0.04866512024374988\n",
      "Epoch 200, Training Loss 0.04871190683154003\n",
      "Epoch 200, Training Loss 0.04875167674930466\n",
      "Epoch 200, Training Loss 0.04882195091132275\n",
      "Epoch 200, Training Loss 0.04888695603131753\n",
      "Epoch 200, Training Loss 0.04904196503669824\n",
      "Epoch 200, Training Loss 0.04913901382237863\n",
      "Epoch 200, Training Loss 0.04920593370824023\n",
      "Epoch 200, Training Loss 0.0492842315786692\n",
      "Epoch 200, Training Loss 0.049388493526765075\n",
      "Epoch 200, Training Loss 0.04943305977960796\n",
      "Epoch 200, Training Loss 0.04952179413655644\n",
      "Epoch 200, Training Loss 0.04961107079477985\n",
      "Epoch 200, Training Loss 0.049708659899995074\n",
      "Epoch 200, Training Loss 0.0497414093581326\n",
      "Epoch 200, Training Loss 0.04986865266019007\n",
      "Epoch 200, Training Loss 0.050043002953586144\n",
      "Epoch 200, Training Loss 0.05013602799700235\n",
      "Epoch 200, Training Loss 0.05019686251159405\n",
      "Epoch 200, Training Loss 0.05023089746880295\n",
      "Epoch 200, Training Loss 0.05030393565330855\n",
      "Epoch 200, Training Loss 0.05051210335077113\n",
      "Epoch 200, Training Loss 0.05057934227297106\n",
      "Epoch 200, Training Loss 0.05062320451264072\n",
      "Epoch 200, Training Loss 0.05079479601419986\n",
      "Epoch 200, Training Loss 0.05100417152509246\n",
      "Epoch 200, Training Loss 0.051110928236266305\n",
      "Epoch 200, Training Loss 0.05120325805095341\n",
      "Epoch 200, Training Loss 0.051306147405834834\n",
      "Epoch 200, Training Loss 0.051393777209446025\n",
      "Epoch 200, Training Loss 0.05148896620825619\n",
      "Epoch 200, Training Loss 0.051621965236266325\n",
      "Epoch 200, Training Loss 0.051695484264284525\n",
      "Epoch 200, Training Loss 0.051760114101774016\n",
      "Epoch 200, Training Loss 0.05179589810421514\n",
      "Epoch 200, Training Loss 0.05185102900404416\n",
      "Epoch 200, Training Loss 0.05210980216917746\n",
      "Epoch 200, Training Loss 0.05214011538749956\n",
      "Epoch 200, Training Loss 0.052338793770174315\n",
      "Epoch 200, Training Loss 0.05252235919318121\n",
      "Epoch 200, Training Loss 0.052677395942923436\n",
      "Epoch 200, Training Loss 0.05275939883368895\n",
      "Epoch 200, Training Loss 0.05291513622383518\n",
      "Epoch 200, Training Loss 0.052946651384205845\n",
      "Epoch 200, Training Loss 0.05304457735665657\n",
      "Epoch 200, Training Loss 0.05307721827641282\n",
      "Epoch 200, Training Loss 0.05315773570052613\n",
      "Epoch 200, Training Loss 0.05320008807813229\n",
      "Epoch 200, Training Loss 0.05328864557430377\n",
      "Epoch 200, Training Loss 0.05357719758582656\n",
      "Epoch 200, Training Loss 0.05367596963744448\n",
      "Epoch 200, Training Loss 0.05376563161073248\n",
      "Epoch 200, Training Loss 0.05383707700497316\n",
      "Epoch 200, Training Loss 0.05398225059608936\n",
      "Epoch 200, Training Loss 0.054019778720377123\n",
      "Epoch 200, Training Loss 0.05415761955868443\n",
      "Epoch 200, Training Loss 0.054259788455760766\n",
      "Epoch 200, Training Loss 0.05442151756090162\n",
      "Epoch 200, Training Loss 0.05456332391832986\n",
      "Epoch 200, Training Loss 0.05466702430035033\n",
      "Epoch 200, Training Loss 0.05470519212891569\n",
      "Epoch 200, Training Loss 0.0547663884111168\n",
      "Epoch 200, Training Loss 0.05492279772549067\n",
      "Epoch 200, Training Loss 0.05505582963323692\n",
      "Epoch 200, Training Loss 0.05516505313446493\n",
      "Epoch 200, Training Loss 0.05518706891175998\n",
      "Epoch 200, Training Loss 0.05524858832240219\n",
      "Epoch 200, Training Loss 0.055320913123343224\n",
      "Epoch 200, Training Loss 0.055361416489791\n",
      "Epoch 200, Training Loss 0.055391178515208576\n",
      "Epoch 200, Training Loss 0.05551042521963148\n",
      "Epoch 200, Training Loss 0.05561202455697881\n",
      "Epoch 200, Training Loss 0.05582612400274257\n",
      "Epoch 200, Training Loss 0.05588534822963807\n",
      "Epoch 200, Training Loss 0.05596650544139545\n",
      "Epoch 200, Training Loss 0.056047505485918137\n",
      "Epoch 200, Training Loss 0.056094987840746596\n",
      "Epoch 200, Training Loss 0.05623987971988442\n",
      "Epoch 200, Training Loss 0.05629846184392986\n",
      "Epoch 200, Training Loss 0.05637303826368183\n",
      "Epoch 200, Training Loss 0.056506157958703805\n",
      "Epoch 200, Training Loss 0.056603833107644566\n",
      "Epoch 200, Training Loss 0.05673609499502784\n",
      "Epoch 200, Training Loss 0.056910916161306606\n",
      "Epoch 200, Training Loss 0.05694956919464195\n",
      "Epoch 200, Training Loss 0.05708113199461947\n",
      "Epoch 200, Training Loss 0.05719842136863743\n",
      "Epoch 200, Training Loss 0.057333730448208886\n",
      "Epoch 200, Training Loss 0.057462288206085906\n",
      "Epoch 200, Training Loss 0.05756545248929688\n",
      "Epoch 200, Training Loss 0.057651585831409294\n",
      "Epoch 200, Training Loss 0.05770056476683148\n",
      "Epoch 200, Training Loss 0.057723049154205015\n",
      "Epoch 200, Training Loss 0.05779847523550053\n",
      "Epoch 200, Training Loss 0.05791737279161582\n",
      "Epoch 200, Training Loss 0.05801688763372543\n",
      "Epoch 200, Training Loss 0.05808259038340367\n",
      "Epoch 200, Training Loss 0.0581981009837059\n",
      "Epoch 200, Training Loss 0.0583341815890006\n",
      "Epoch 200, Training Loss 0.05845096716276177\n",
      "Epoch 200, Training Loss 0.05855085184711896\n",
      "Epoch 200, Training Loss 0.058602203554271357\n",
      "Epoch 200, Training Loss 0.05870656983907837\n",
      "Epoch 200, Training Loss 0.05885700978067182\n",
      "Epoch 200, Training Loss 0.05900191428387523\n",
      "Epoch 200, Training Loss 0.059043067584857535\n",
      "Epoch 200, Training Loss 0.05924092804241325\n",
      "Epoch 200, Training Loss 0.059330759778418736\n",
      "Epoch 200, Training Loss 0.059508102956701954\n",
      "Epoch 200, Training Loss 0.05962013055348907\n",
      "Epoch 200, Training Loss 0.05972949389125342\n",
      "Epoch 200, Training Loss 0.05983833303970411\n",
      "Epoch 200, Training Loss 0.05989268401046963\n",
      "Epoch 200, Training Loss 0.05995259911198255\n",
      "Epoch 200, Training Loss 0.060025823303757955\n",
      "Epoch 200, Training Loss 0.06017235497755887\n",
      "Epoch 200, Training Loss 0.060281894528223655\n",
      "Epoch 200, Training Loss 0.06036361202935848\n",
      "Epoch 200, Training Loss 0.06047562799295005\n",
      "Epoch 200, Training Loss 0.0605571585245278\n",
      "Epoch 200, Training Loss 0.06065017364018828\n",
      "Epoch 200, Training Loss 0.06077113840848093\n",
      "Epoch 200, Training Loss 0.06082714774795925\n",
      "Epoch 200, Training Loss 0.06089496141290078\n",
      "Epoch 200, Training Loss 0.0610060032368983\n",
      "Epoch 200, Training Loss 0.06114307491232634\n",
      "Epoch 200, Training Loss 0.06122592608313388\n",
      "Epoch 200, Training Loss 0.06136149024505101\n",
      "Epoch 200, Training Loss 0.06158248022022417\n",
      "Epoch 200, Training Loss 0.061710480337395617\n",
      "Epoch 200, Training Loss 0.061788115973162756\n",
      "Epoch 200, Training Loss 0.06191013387435347\n",
      "Epoch 200, Training Loss 0.06211675329924659\n",
      "Epoch 200, Training Loss 0.062228700283038264\n",
      "Epoch 200, Training Loss 0.06230858141137168\n",
      "Epoch 200, Training Loss 0.06233146192286821\n",
      "Epoch 200, Training Loss 0.06236388279325174\n",
      "Epoch 200, Training Loss 0.06247696238915291\n",
      "Epoch 200, Training Loss 0.06255347405434074\n",
      "Epoch 200, Training Loss 0.06256744296044645\n",
      "Epoch 200, Training Loss 0.06261222080215621\n",
      "Epoch 200, Training Loss 0.0627252903083325\n",
      "Epoch 200, Training Loss 0.06279341157649637\n",
      "Epoch 200, Training Loss 0.06292019023671937\n",
      "Epoch 200, Training Loss 0.0629553130096601\n",
      "Epoch 200, Training Loss 0.06301271854220983\n",
      "Epoch 200, Training Loss 0.06307051713576022\n",
      "Epoch 200, Training Loss 0.0630918467093421\n",
      "Epoch 200, Training Loss 0.06319022280356996\n",
      "Epoch 200, Training Loss 0.06324762777399148\n",
      "Epoch 200, Training Loss 0.06331171383819235\n",
      "Epoch 200, Training Loss 0.0633573217479431\n",
      "Epoch 200, Training Loss 0.06351753857577472\n",
      "Epoch 200, Training Loss 0.06357696123392609\n",
      "Epoch 200, Training Loss 0.0636692338068124\n",
      "Epoch 200, Training Loss 0.06377495731682042\n",
      "Epoch 200, Training Loss 0.06389315890105408\n",
      "Epoch 200, Training Loss 0.06398822772828743\n",
      "Epoch 200, Training Loss 0.06406577442870344\n",
      "Epoch 200, Training Loss 0.06416042523263284\n",
      "Epoch 200, Training Loss 0.06423786978768495\n",
      "Epoch 200, Training Loss 0.06439804888623374\n",
      "Epoch 200, Training Loss 0.06448279958351723\n",
      "Epoch 200, Training Loss 0.06467492031195508\n",
      "Epoch 200, Training Loss 0.06476893511069629\n",
      "Epoch 200, Training Loss 0.06491548921245976\n",
      "Epoch 200, Training Loss 0.06504138322580424\n",
      "Epoch 200, Training Loss 0.06513776185938998\n",
      "Epoch 200, Training Loss 0.06531137039127481\n",
      "Epoch 200, Training Loss 0.06532870966802969\n",
      "Epoch 200, Training Loss 0.06536288440222744\n",
      "Epoch 200, Training Loss 0.06548378579294704\n",
      "Epoch 200, Training Loss 0.06569271423089344\n",
      "Epoch 200, Training Loss 0.06581373826083739\n",
      "Epoch 200, Training Loss 0.0660024653736721\n",
      "Epoch 200, Training Loss 0.06607084448718469\n",
      "Epoch 200, Training Loss 0.06615134073145058\n",
      "Epoch 200, Training Loss 0.06631771854155928\n",
      "Epoch 200, Training Loss 0.06639539045841454\n",
      "Epoch 200, Training Loss 0.06647887020288488\n",
      "Epoch 200, Training Loss 0.06657281804167668\n",
      "Epoch 200, Training Loss 0.06672502096975817\n",
      "Epoch 200, Training Loss 0.06680283060802332\n",
      "Epoch 200, Training Loss 0.06690579288712967\n",
      "Epoch 200, Training Loss 0.06696982462021053\n",
      "Epoch 200, Training Loss 0.06706276284816587\n",
      "Epoch 200, Training Loss 0.06712529752903697\n",
      "Epoch 200, Training Loss 0.06728801296909562\n",
      "Epoch 200, Training Loss 0.06737012783770481\n",
      "Epoch 200, Training Loss 0.0674902767578945\n",
      "Epoch 200, Training Loss 0.06765297647741864\n",
      "Epoch 200, Training Loss 0.06785942943614272\n",
      "Epoch 200, Training Loss 0.06800376198819036\n",
      "Epoch 200, Training Loss 0.06809430902757113\n",
      "Epoch 200, Training Loss 0.06828850953508635\n",
      "Epoch 200, Training Loss 0.06838874535901887\n",
      "Epoch 200, Training Loss 0.0685002199169296\n",
      "Epoch 200, Training Loss 0.06851958276827813\n",
      "Epoch 200, Training Loss 0.06865045163527969\n",
      "Epoch 200, Training Loss 0.0687437766169667\n",
      "Epoch 200, Training Loss 0.06881082665456263\n",
      "Epoch 200, Training Loss 0.06892460126005819\n",
      "Epoch 200, Training Loss 0.06904662328669825\n",
      "Epoch 200, Training Loss 0.0691631387626214\n",
      "Epoch 200, Training Loss 0.06936016765153964\n",
      "Epoch 200, Training Loss 0.0694482981398835\n",
      "Epoch 200, Training Loss 0.06958117873033942\n",
      "Epoch 200, Training Loss 0.06969538612691376\n",
      "Epoch 200, Training Loss 0.06990798440454599\n",
      "Epoch 200, Training Loss 0.07018731627494211\n",
      "Epoch 200, Training Loss 0.07041059205394305\n",
      "Epoch 200, Training Loss 0.07063468259490092\n",
      "Epoch 200, Training Loss 0.0708229292500907\n",
      "Epoch 200, Training Loss 0.07093401188078477\n",
      "Epoch 200, Training Loss 0.07097892758681837\n",
      "Epoch 200, Training Loss 0.07114550517157406\n",
      "Epoch 200, Training Loss 0.07125802285840635\n",
      "Epoch 200, Training Loss 0.0714171107041428\n",
      "Epoch 200, Training Loss 0.07148475431101135\n",
      "Epoch 200, Training Loss 0.07159620100070181\n",
      "Epoch 200, Training Loss 0.07166412574312914\n",
      "Epoch 200, Training Loss 0.07175485888505569\n",
      "Epoch 200, Training Loss 0.07190344550901705\n",
      "Epoch 200, Training Loss 0.07202977164650855\n",
      "Epoch 200, Training Loss 0.07211541184974486\n",
      "Epoch 200, Training Loss 0.07229059763714824\n",
      "Epoch 200, Training Loss 0.07245638054412078\n",
      "Epoch 200, Training Loss 0.07256209309739263\n",
      "Epoch 200, Training Loss 0.0727005388523402\n",
      "Epoch 200, Training Loss 0.0728238407686791\n",
      "Epoch 200, Training Loss 0.07289554279349993\n",
      "Epoch 200, Training Loss 0.07292149595492294\n",
      "Epoch 200, Training Loss 0.07303408256438954\n",
      "Epoch 200, Training Loss 0.073120870671528\n",
      "Epoch 200, Training Loss 0.07327138995894653\n",
      "Epoch 200, Training Loss 0.07331708687074158\n",
      "Epoch 200, Training Loss 0.07344252824702341\n",
      "Epoch 200, Training Loss 0.07352030645851093\n",
      "Epoch 200, Training Loss 0.07358581515010018\n",
      "Epoch 200, Training Loss 0.07365246326603053\n",
      "Epoch 200, Training Loss 0.07370374057218052\n",
      "Epoch 200, Training Loss 0.07381127335374123\n",
      "Epoch 200, Training Loss 0.07401309014581468\n",
      "Epoch 200, Training Loss 0.07429701612804972\n",
      "Epoch 200, Training Loss 0.07446585543086881\n",
      "Epoch 200, Training Loss 0.07450189278163302\n",
      "Epoch 200, Training Loss 0.07454283143538991\n",
      "Epoch 200, Training Loss 0.07471886474062758\n",
      "Epoch 200, Training Loss 0.07476220863616413\n",
      "Epoch 200, Training Loss 0.07491399820946405\n",
      "Epoch 200, Training Loss 0.07503356115387567\n",
      "Epoch 200, Training Loss 0.07515096957521404\n",
      "Epoch 200, Training Loss 0.07520448631437882\n",
      "Epoch 200, Training Loss 0.07527600962530508\n",
      "Epoch 200, Training Loss 0.07530226202710244\n",
      "Epoch 200, Training Loss 0.07536310224396074\n",
      "Epoch 200, Training Loss 0.07543971653089232\n",
      "Epoch 200, Training Loss 0.075553357950114\n",
      "Epoch 200, Training Loss 0.07564421493174209\n",
      "Epoch 200, Training Loss 0.07572107345861433\n",
      "Epoch 200, Training Loss 0.07582654916536054\n",
      "Epoch 200, Training Loss 0.0759058631396831\n",
      "Epoch 200, Training Loss 0.07594344157682699\n",
      "Epoch 200, Training Loss 0.07604536755829859\n",
      "Epoch 200, Training Loss 0.07613705052539249\n",
      "Epoch 200, Training Loss 0.0762914404656519\n",
      "Epoch 210, Training Loss 0.00018654988549859323\n",
      "Epoch 210, Training Loss 0.0002833999731504094\n",
      "Epoch 210, Training Loss 0.00041604801402677353\n",
      "Epoch 210, Training Loss 0.0005332498770693074\n",
      "Epoch 210, Training Loss 0.0005663141270008538\n",
      "Epoch 210, Training Loss 0.0006994452551388375\n",
      "Epoch 210, Training Loss 0.0007605702709168425\n",
      "Epoch 210, Training Loss 0.0008223093688831\n",
      "Epoch 210, Training Loss 0.0008501584934609015\n",
      "Epoch 210, Training Loss 0.0009301478981666858\n",
      "Epoch 210, Training Loss 0.0009699846877504493\n",
      "Epoch 210, Training Loss 0.001082743241754182\n",
      "Epoch 210, Training Loss 0.0011658074496232946\n",
      "Epoch 210, Training Loss 0.0012458628329360271\n",
      "Epoch 210, Training Loss 0.0013292683955386778\n",
      "Epoch 210, Training Loss 0.0014302318706117627\n",
      "Epoch 210, Training Loss 0.0014615812365089537\n",
      "Epoch 210, Training Loss 0.0015258119105721068\n",
      "Epoch 210, Training Loss 0.0015670690433029323\n",
      "Epoch 210, Training Loss 0.001629274459960668\n",
      "Epoch 210, Training Loss 0.0016811165668051261\n",
      "Epoch 210, Training Loss 0.0017514616189062443\n",
      "Epoch 210, Training Loss 0.0018506713016220675\n",
      "Epoch 210, Training Loss 0.001983505540320178\n",
      "Epoch 210, Training Loss 0.0020935936423633105\n",
      "Epoch 210, Training Loss 0.002155877113856775\n",
      "Epoch 210, Training Loss 0.0022085450632531017\n",
      "Epoch 210, Training Loss 0.0022568151264277564\n",
      "Epoch 210, Training Loss 0.002395191501416361\n",
      "Epoch 210, Training Loss 0.002449169247637472\n",
      "Epoch 210, Training Loss 0.0024903762795012016\n",
      "Epoch 210, Training Loss 0.002540386701121812\n",
      "Epoch 210, Training Loss 0.002595476818073284\n",
      "Epoch 210, Training Loss 0.00269535049805632\n",
      "Epoch 210, Training Loss 0.0027286747846838153\n",
      "Epoch 210, Training Loss 0.0028015346983280938\n",
      "Epoch 210, Training Loss 0.00288693003280236\n",
      "Epoch 210, Training Loss 0.0030263846840166376\n",
      "Epoch 210, Training Loss 0.0030869395922288262\n",
      "Epoch 210, Training Loss 0.0031773525116312535\n",
      "Epoch 210, Training Loss 0.0032650551060810114\n",
      "Epoch 210, Training Loss 0.0033553024358532924\n",
      "Epoch 210, Training Loss 0.0034420052805291417\n",
      "Epoch 210, Training Loss 0.0034770515159039243\n",
      "Epoch 210, Training Loss 0.00361571436666924\n",
      "Epoch 210, Training Loss 0.0038155920641577763\n",
      "Epoch 210, Training Loss 0.003914431999901981\n",
      "Epoch 210, Training Loss 0.003973005715843357\n",
      "Epoch 210, Training Loss 0.004010504566590347\n",
      "Epoch 210, Training Loss 0.004060960184220615\n",
      "Epoch 210, Training Loss 0.004125890455415944\n",
      "Epoch 210, Training Loss 0.00417314239484651\n",
      "Epoch 210, Training Loss 0.004217758524657973\n",
      "Epoch 210, Training Loss 0.004287464060651524\n",
      "Epoch 210, Training Loss 0.004327561656284668\n",
      "Epoch 210, Training Loss 0.004380751232071148\n",
      "Epoch 210, Training Loss 0.004425183303006317\n",
      "Epoch 210, Training Loss 0.0044590207340810305\n",
      "Epoch 210, Training Loss 0.004629153655865766\n",
      "Epoch 210, Training Loss 0.004669460191693909\n",
      "Epoch 210, Training Loss 0.00482795504934114\n",
      "Epoch 210, Training Loss 0.004857773678210537\n",
      "Epoch 210, Training Loss 0.004985474200104661\n",
      "Epoch 210, Training Loss 0.0051879241581425034\n",
      "Epoch 210, Training Loss 0.005330460196089409\n",
      "Epoch 210, Training Loss 0.005363078856997935\n",
      "Epoch 210, Training Loss 0.005466941589265681\n",
      "Epoch 210, Training Loss 0.005570551992663185\n",
      "Epoch 210, Training Loss 0.005645753565670737\n",
      "Epoch 210, Training Loss 0.005800274300777241\n",
      "Epoch 210, Training Loss 0.005838647727733073\n",
      "Epoch 210, Training Loss 0.005897801050254147\n",
      "Epoch 210, Training Loss 0.005960032053272743\n",
      "Epoch 210, Training Loss 0.0060165727849278\n",
      "Epoch 210, Training Loss 0.006080662255244487\n",
      "Epoch 210, Training Loss 0.006121991221290415\n",
      "Epoch 210, Training Loss 0.006206425897719915\n",
      "Epoch 210, Training Loss 0.0062738948780328724\n",
      "Epoch 210, Training Loss 0.0063828085037067415\n",
      "Epoch 210, Training Loss 0.006574548947651063\n",
      "Epoch 210, Training Loss 0.006610616624755475\n",
      "Epoch 210, Training Loss 0.006663478822792735\n",
      "Epoch 210, Training Loss 0.006758699157868352\n",
      "Epoch 210, Training Loss 0.006877924415671154\n",
      "Epoch 210, Training Loss 0.006945867694037802\n",
      "Epoch 210, Training Loss 0.007041946113052423\n",
      "Epoch 210, Training Loss 0.007103524694833762\n",
      "Epoch 210, Training Loss 0.007196175141731644\n",
      "Epoch 210, Training Loss 0.0072283445240553384\n",
      "Epoch 210, Training Loss 0.007322726030464825\n",
      "Epoch 210, Training Loss 0.007352599247699351\n",
      "Epoch 210, Training Loss 0.007404895111099076\n",
      "Epoch 210, Training Loss 0.00749383505452853\n",
      "Epoch 210, Training Loss 0.007627079037049085\n",
      "Epoch 210, Training Loss 0.007682265863870569\n",
      "Epoch 210, Training Loss 0.0078053741413347255\n",
      "Epoch 210, Training Loss 0.00792167111135581\n",
      "Epoch 210, Training Loss 0.007957613736372013\n",
      "Epoch 210, Training Loss 0.008071236538669795\n",
      "Epoch 210, Training Loss 0.00811361360227894\n",
      "Epoch 210, Training Loss 0.008138078631704572\n",
      "Epoch 210, Training Loss 0.008202640527902205\n",
      "Epoch 210, Training Loss 0.008278424999274104\n",
      "Epoch 210, Training Loss 0.008484251556627433\n",
      "Epoch 210, Training Loss 0.008573878811353155\n",
      "Epoch 210, Training Loss 0.008681278578136735\n",
      "Epoch 210, Training Loss 0.008781974851761175\n",
      "Epoch 210, Training Loss 0.008864977168362312\n",
      "Epoch 210, Training Loss 0.008930869650600665\n",
      "Epoch 210, Training Loss 0.009092416473762\n",
      "Epoch 210, Training Loss 0.009230439975152693\n",
      "Epoch 210, Training Loss 0.009369640973160792\n",
      "Epoch 210, Training Loss 0.009418312800795678\n",
      "Epoch 210, Training Loss 0.009474799863498687\n",
      "Epoch 210, Training Loss 0.009568057589880798\n",
      "Epoch 210, Training Loss 0.009757547558325788\n",
      "Epoch 210, Training Loss 0.009891250828171478\n",
      "Epoch 210, Training Loss 0.00993591163764753\n",
      "Epoch 210, Training Loss 0.009989504669042653\n",
      "Epoch 210, Training Loss 0.010115674096624108\n",
      "Epoch 210, Training Loss 0.010200687326833872\n",
      "Epoch 210, Training Loss 0.010275742489263377\n",
      "Epoch 210, Training Loss 0.01032620579566416\n",
      "Epoch 210, Training Loss 0.010585847077295756\n",
      "Epoch 210, Training Loss 0.010693949732996162\n",
      "Epoch 210, Training Loss 0.01084682482349522\n",
      "Epoch 210, Training Loss 0.01090050911021126\n",
      "Epoch 210, Training Loss 0.010987365101476002\n",
      "Epoch 210, Training Loss 0.011130742216601854\n",
      "Epoch 210, Training Loss 0.011235977465863271\n",
      "Epoch 210, Training Loss 0.011257997437682755\n",
      "Epoch 210, Training Loss 0.011371794890355119\n",
      "Epoch 210, Training Loss 0.011455241364457875\n",
      "Epoch 210, Training Loss 0.011522568421690818\n",
      "Epoch 210, Training Loss 0.011632075473723356\n",
      "Epoch 210, Training Loss 0.011698411760942252\n",
      "Epoch 210, Training Loss 0.011732338984851794\n",
      "Epoch 210, Training Loss 0.011909401785020176\n",
      "Epoch 210, Training Loss 0.01199943509638843\n",
      "Epoch 210, Training Loss 0.012138030749014424\n",
      "Epoch 210, Training Loss 0.012264154701376968\n",
      "Epoch 210, Training Loss 0.012337400449339843\n",
      "Epoch 210, Training Loss 0.012458408511031771\n",
      "Epoch 210, Training Loss 0.012562066771547356\n",
      "Epoch 210, Training Loss 0.012645927819964068\n",
      "Epoch 210, Training Loss 0.012694633251432415\n",
      "Epoch 210, Training Loss 0.01271888626801312\n",
      "Epoch 210, Training Loss 0.012769042962061628\n",
      "Epoch 210, Training Loss 0.012820471390186217\n",
      "Epoch 210, Training Loss 0.012849697400160762\n",
      "Epoch 210, Training Loss 0.01292329534049839\n",
      "Epoch 210, Training Loss 0.013093061945246308\n",
      "Epoch 210, Training Loss 0.013142105230056416\n",
      "Epoch 210, Training Loss 0.0132363622393602\n",
      "Epoch 210, Training Loss 0.013305451149297187\n",
      "Epoch 210, Training Loss 0.013369366798140204\n",
      "Epoch 210, Training Loss 0.013448590612815468\n",
      "Epoch 210, Training Loss 0.013517516729471933\n",
      "Epoch 210, Training Loss 0.01355556496288008\n",
      "Epoch 210, Training Loss 0.013647154645274972\n",
      "Epoch 210, Training Loss 0.013709256911407347\n",
      "Epoch 210, Training Loss 0.013866547271228203\n",
      "Epoch 210, Training Loss 0.013924096861039586\n",
      "Epoch 210, Training Loss 0.013993853064792236\n",
      "Epoch 210, Training Loss 0.014120950404068699\n",
      "Epoch 210, Training Loss 0.014181912397903859\n",
      "Epoch 210, Training Loss 0.014206666744711912\n",
      "Epoch 210, Training Loss 0.014245979745141076\n",
      "Epoch 210, Training Loss 0.014315539551779742\n",
      "Epoch 210, Training Loss 0.014385862399816818\n",
      "Epoch 210, Training Loss 0.014525481811760332\n",
      "Epoch 210, Training Loss 0.014607415303511694\n",
      "Epoch 210, Training Loss 0.0147043177929452\n",
      "Epoch 210, Training Loss 0.014776509764897245\n",
      "Epoch 210, Training Loss 0.014895800744061884\n",
      "Epoch 210, Training Loss 0.0149336375481904\n",
      "Epoch 210, Training Loss 0.014974942690480853\n",
      "Epoch 210, Training Loss 0.015057825311885008\n",
      "Epoch 210, Training Loss 0.015187931742490557\n",
      "Epoch 210, Training Loss 0.015242535227914447\n",
      "Epoch 210, Training Loss 0.015346509890387888\n",
      "Epoch 210, Training Loss 0.015376662061361552\n",
      "Epoch 210, Training Loss 0.015399227723898486\n",
      "Epoch 210, Training Loss 0.015536878143658723\n",
      "Epoch 210, Training Loss 0.015573409157316856\n",
      "Epoch 210, Training Loss 0.015754509257519488\n",
      "Epoch 210, Training Loss 0.01581478649821809\n",
      "Epoch 210, Training Loss 0.01596826923025005\n",
      "Epoch 210, Training Loss 0.016112515709989363\n",
      "Epoch 210, Training Loss 0.016146150870186747\n",
      "Epoch 210, Training Loss 0.016179971594620698\n",
      "Epoch 210, Training Loss 0.016209173132963193\n",
      "Epoch 210, Training Loss 0.016291361726115425\n",
      "Epoch 210, Training Loss 0.01637011637811161\n",
      "Epoch 210, Training Loss 0.016424484369928573\n",
      "Epoch 210, Training Loss 0.01650749470876611\n",
      "Epoch 210, Training Loss 0.01663900133403366\n",
      "Epoch 210, Training Loss 0.01674581946009565\n",
      "Epoch 210, Training Loss 0.016813009643874815\n",
      "Epoch 210, Training Loss 0.016921884520813023\n",
      "Epoch 210, Training Loss 0.01698964662240137\n",
      "Epoch 210, Training Loss 0.017094797866842936\n",
      "Epoch 210, Training Loss 0.01717967417596094\n",
      "Epoch 210, Training Loss 0.01721444452310081\n",
      "Epoch 210, Training Loss 0.017275448471707915\n",
      "Epoch 210, Training Loss 0.01732511815788877\n",
      "Epoch 210, Training Loss 0.0174162812516703\n",
      "Epoch 210, Training Loss 0.017437018375472187\n",
      "Epoch 210, Training Loss 0.017513243283819206\n",
      "Epoch 210, Training Loss 0.017566184622838217\n",
      "Epoch 210, Training Loss 0.017730463427179457\n",
      "Epoch 210, Training Loss 0.017789292444601236\n",
      "Epoch 210, Training Loss 0.01785935620393823\n",
      "Epoch 210, Training Loss 0.01790388601848765\n",
      "Epoch 210, Training Loss 0.017957047359241397\n",
      "Epoch 210, Training Loss 0.018005836685962232\n",
      "Epoch 210, Training Loss 0.018095737335074434\n",
      "Epoch 210, Training Loss 0.018169452483906313\n",
      "Epoch 210, Training Loss 0.01829773468463241\n",
      "Epoch 210, Training Loss 0.01841683297768197\n",
      "Epoch 210, Training Loss 0.018450453189080177\n",
      "Epoch 210, Training Loss 0.018485670844974267\n",
      "Epoch 210, Training Loss 0.018552806140269006\n",
      "Epoch 210, Training Loss 0.018641176895545723\n",
      "Epoch 210, Training Loss 0.018698187566378994\n",
      "Epoch 210, Training Loss 0.018790062247773114\n",
      "Epoch 210, Training Loss 0.018874508481177374\n",
      "Epoch 210, Training Loss 0.01893134648813997\n",
      "Epoch 210, Training Loss 0.018993706632967646\n",
      "Epoch 210, Training Loss 0.01912933765955822\n",
      "Epoch 210, Training Loss 0.019183432171240335\n",
      "Epoch 210, Training Loss 0.01939758098901004\n",
      "Epoch 210, Training Loss 0.019464096547487902\n",
      "Epoch 210, Training Loss 0.019508236807192224\n",
      "Epoch 210, Training Loss 0.019601396627990944\n",
      "Epoch 210, Training Loss 0.01965865068604498\n",
      "Epoch 210, Training Loss 0.0197918180571607\n",
      "Epoch 210, Training Loss 0.01993328178315745\n",
      "Epoch 210, Training Loss 0.01999812884032345\n",
      "Epoch 210, Training Loss 0.020040105816329378\n",
      "Epoch 210, Training Loss 0.020074819050291005\n",
      "Epoch 210, Training Loss 0.020135651707001354\n",
      "Epoch 210, Training Loss 0.020180412980220507\n",
      "Epoch 210, Training Loss 0.020337936737577018\n",
      "Epoch 210, Training Loss 0.02037309723503678\n",
      "Epoch 210, Training Loss 0.020493545054989244\n",
      "Epoch 210, Training Loss 0.020564862565063607\n",
      "Epoch 210, Training Loss 0.020695327671573442\n",
      "Epoch 210, Training Loss 0.020785366308868236\n",
      "Epoch 210, Training Loss 0.020854172973281435\n",
      "Epoch 210, Training Loss 0.020970041544445792\n",
      "Epoch 210, Training Loss 0.021019661677596363\n",
      "Epoch 210, Training Loss 0.021122523221423103\n",
      "Epoch 210, Training Loss 0.021213650853489825\n",
      "Epoch 210, Training Loss 0.021251830996473885\n",
      "Epoch 210, Training Loss 0.021273916977865008\n",
      "Epoch 210, Training Loss 0.021288976933130675\n",
      "Epoch 210, Training Loss 0.021394964328983707\n",
      "Epoch 210, Training Loss 0.02153820897240544\n",
      "Epoch 210, Training Loss 0.021607188824707132\n",
      "Epoch 210, Training Loss 0.021647132902175112\n",
      "Epoch 210, Training Loss 0.02176039636163684\n",
      "Epoch 210, Training Loss 0.02186046314694921\n",
      "Epoch 210, Training Loss 0.022015555985653034\n",
      "Epoch 210, Training Loss 0.02211982205919826\n",
      "Epoch 210, Training Loss 0.022171404486631647\n",
      "Epoch 210, Training Loss 0.022299561637651434\n",
      "Epoch 210, Training Loss 0.022388960982737183\n",
      "Epoch 210, Training Loss 0.022423991268915137\n",
      "Epoch 210, Training Loss 0.022473755602241324\n",
      "Epoch 210, Training Loss 0.022552470867152865\n",
      "Epoch 210, Training Loss 0.02264968663349252\n",
      "Epoch 210, Training Loss 0.02271393131788658\n",
      "Epoch 210, Training Loss 0.022794294271075054\n",
      "Epoch 210, Training Loss 0.022894826879643875\n",
      "Epoch 210, Training Loss 0.022966007587721434\n",
      "Epoch 210, Training Loss 0.023107334097270924\n",
      "Epoch 210, Training Loss 0.023145700264198092\n",
      "Epoch 210, Training Loss 0.023207363975055687\n",
      "Epoch 210, Training Loss 0.023303659023035824\n",
      "Epoch 210, Training Loss 0.02346118611505117\n",
      "Epoch 210, Training Loss 0.023589132972957227\n",
      "Epoch 210, Training Loss 0.023664284668996206\n",
      "Epoch 210, Training Loss 0.023757241123243976\n",
      "Epoch 210, Training Loss 0.02391103998093349\n",
      "Epoch 210, Training Loss 0.023937584427387817\n",
      "Epoch 210, Training Loss 0.024036981517930166\n",
      "Epoch 210, Training Loss 0.02415224547733736\n",
      "Epoch 210, Training Loss 0.02426848948344855\n",
      "Epoch 210, Training Loss 0.02433031014716991\n",
      "Epoch 210, Training Loss 0.024343115545552022\n",
      "Epoch 210, Training Loss 0.024474100058521988\n",
      "Epoch 210, Training Loss 0.02452316726236354\n",
      "Epoch 210, Training Loss 0.024722930609636828\n",
      "Epoch 210, Training Loss 0.02480369050155782\n",
      "Epoch 210, Training Loss 0.02493806905887278\n",
      "Epoch 210, Training Loss 0.02504465677906447\n",
      "Epoch 210, Training Loss 0.02513921155434702\n",
      "Epoch 210, Training Loss 0.025190012918932893\n",
      "Epoch 210, Training Loss 0.02529857174643432\n",
      "Epoch 210, Training Loss 0.025560374992192172\n",
      "Epoch 210, Training Loss 0.02562965352512191\n",
      "Epoch 210, Training Loss 0.025665280257906677\n",
      "Epoch 210, Training Loss 0.02573669364895014\n",
      "Epoch 210, Training Loss 0.02581497017756257\n",
      "Epoch 210, Training Loss 0.025879054731639373\n",
      "Epoch 210, Training Loss 0.02595167357326888\n",
      "Epoch 210, Training Loss 0.026010098477677844\n",
      "Epoch 210, Training Loss 0.026054949348416092\n",
      "Epoch 210, Training Loss 0.026092234954876288\n",
      "Epoch 210, Training Loss 0.026113768585759888\n",
      "Epoch 210, Training Loss 0.026147624409859977\n",
      "Epoch 210, Training Loss 0.026185326277018736\n",
      "Epoch 210, Training Loss 0.02630495600035543\n",
      "Epoch 210, Training Loss 0.026360256381837838\n",
      "Epoch 210, Training Loss 0.026456855741255652\n",
      "Epoch 210, Training Loss 0.026532759355223926\n",
      "Epoch 210, Training Loss 0.026559004234983717\n",
      "Epoch 210, Training Loss 0.026665404261520986\n",
      "Epoch 210, Training Loss 0.02673965717172798\n",
      "Epoch 210, Training Loss 0.02676548872290708\n",
      "Epoch 210, Training Loss 0.02678591856266112\n",
      "Epoch 210, Training Loss 0.026820511648864925\n",
      "Epoch 210, Training Loss 0.02691270977311084\n",
      "Epoch 210, Training Loss 0.027070826137452708\n",
      "Epoch 210, Training Loss 0.027236065495035152\n",
      "Epoch 210, Training Loss 0.027332286351620008\n",
      "Epoch 210, Training Loss 0.027416406846021677\n",
      "Epoch 210, Training Loss 0.02747092336830695\n",
      "Epoch 210, Training Loss 0.027586016488378235\n",
      "Epoch 210, Training Loss 0.02762075643533903\n",
      "Epoch 210, Training Loss 0.02766413268183008\n",
      "Epoch 210, Training Loss 0.02778340545375748\n",
      "Epoch 210, Training Loss 0.027893707307431934\n",
      "Epoch 210, Training Loss 0.028099385062780452\n",
      "Epoch 210, Training Loss 0.028143737530645432\n",
      "Epoch 210, Training Loss 0.028166035765453297\n",
      "Epoch 210, Training Loss 0.028270534483854044\n",
      "Epoch 210, Training Loss 0.028303513366519414\n",
      "Epoch 210, Training Loss 0.028376831829100085\n",
      "Epoch 210, Training Loss 0.028578590941103293\n",
      "Epoch 210, Training Loss 0.02864697904390333\n",
      "Epoch 210, Training Loss 0.028739817298786795\n",
      "Epoch 210, Training Loss 0.028880213467104128\n",
      "Epoch 210, Training Loss 0.0289211893928192\n",
      "Epoch 210, Training Loss 0.028965109822285527\n",
      "Epoch 210, Training Loss 0.029053670481619095\n",
      "Epoch 210, Training Loss 0.029174661546078562\n",
      "Epoch 210, Training Loss 0.029252371530446325\n",
      "Epoch 210, Training Loss 0.029326615603807407\n",
      "Epoch 210, Training Loss 0.02941626049411457\n",
      "Epoch 210, Training Loss 0.02948158511253612\n",
      "Epoch 210, Training Loss 0.029532809740604113\n",
      "Epoch 210, Training Loss 0.029627645579988465\n",
      "Epoch 210, Training Loss 0.029767147916824083\n",
      "Epoch 210, Training Loss 0.02985148534507436\n",
      "Epoch 210, Training Loss 0.030009291767168916\n",
      "Epoch 210, Training Loss 0.030060416241140697\n",
      "Epoch 210, Training Loss 0.030110985865992736\n",
      "Epoch 210, Training Loss 0.030190206908137368\n",
      "Epoch 210, Training Loss 0.030324955536362233\n",
      "Epoch 210, Training Loss 0.03036509416259044\n",
      "Epoch 210, Training Loss 0.030417794934557298\n",
      "Epoch 210, Training Loss 0.03048406705817641\n",
      "Epoch 210, Training Loss 0.030557072653537593\n",
      "Epoch 210, Training Loss 0.030723209205843375\n",
      "Epoch 210, Training Loss 0.030876787747387464\n",
      "Epoch 210, Training Loss 0.031079575077623434\n",
      "Epoch 210, Training Loss 0.031149335659306755\n",
      "Epoch 210, Training Loss 0.031238738024640646\n",
      "Epoch 210, Training Loss 0.03133486667314492\n",
      "Epoch 210, Training Loss 0.03137620592899525\n",
      "Epoch 210, Training Loss 0.03142326025177946\n",
      "Epoch 210, Training Loss 0.03145982347347814\n",
      "Epoch 210, Training Loss 0.03153829499864784\n",
      "Epoch 210, Training Loss 0.031622942578752554\n",
      "Epoch 210, Training Loss 0.0316807006514343\n",
      "Epoch 210, Training Loss 0.03174069254537639\n",
      "Epoch 210, Training Loss 0.03183584143300457\n",
      "Epoch 210, Training Loss 0.03195603728613547\n",
      "Epoch 210, Training Loss 0.03203252770001893\n",
      "Epoch 210, Training Loss 0.03212889413828092\n",
      "Epoch 210, Training Loss 0.03221491490231108\n",
      "Epoch 210, Training Loss 0.03231231867671584\n",
      "Epoch 210, Training Loss 0.0323577268118196\n",
      "Epoch 210, Training Loss 0.03240416881502094\n",
      "Epoch 210, Training Loss 0.03247580299382586\n",
      "Epoch 210, Training Loss 0.03261204159525616\n",
      "Epoch 210, Training Loss 0.032688731740912434\n",
      "Epoch 210, Training Loss 0.03271763589914383\n",
      "Epoch 210, Training Loss 0.03277320776949339\n",
      "Epoch 210, Training Loss 0.03287613259561722\n",
      "Epoch 210, Training Loss 0.03292984291172736\n",
      "Epoch 210, Training Loss 0.03299676938592206\n",
      "Epoch 210, Training Loss 0.033067995593037525\n",
      "Epoch 210, Training Loss 0.033174479786363786\n",
      "Epoch 210, Training Loss 0.03323333263706864\n",
      "Epoch 210, Training Loss 0.033376568619905\n",
      "Epoch 210, Training Loss 0.03354052219020627\n",
      "Epoch 210, Training Loss 0.03365908595649978\n",
      "Epoch 210, Training Loss 0.033838132554736666\n",
      "Epoch 210, Training Loss 0.033909958576464366\n",
      "Epoch 210, Training Loss 0.03400568773284021\n",
      "Epoch 210, Training Loss 0.0340915538353936\n",
      "Epoch 210, Training Loss 0.03420416573110177\n",
      "Epoch 210, Training Loss 0.03426998244512759\n",
      "Epoch 210, Training Loss 0.034412939478988616\n",
      "Epoch 210, Training Loss 0.03445175330957298\n",
      "Epoch 210, Training Loss 0.034535880968727346\n",
      "Epoch 210, Training Loss 0.034602273020969555\n",
      "Epoch 210, Training Loss 0.034652030899349\n",
      "Epoch 210, Training Loss 0.03470832882973049\n",
      "Epoch 210, Training Loss 0.034768997392643365\n",
      "Epoch 210, Training Loss 0.034961574988396804\n",
      "Epoch 210, Training Loss 0.03503115297368992\n",
      "Epoch 210, Training Loss 0.03506236925573491\n",
      "Epoch 210, Training Loss 0.03509143615484504\n",
      "Epoch 210, Training Loss 0.035184634390913544\n",
      "Epoch 210, Training Loss 0.035234837713020155\n",
      "Epoch 210, Training Loss 0.03528678784851947\n",
      "Epoch 210, Training Loss 0.03534094439438351\n",
      "Epoch 210, Training Loss 0.03543663572620057\n",
      "Epoch 210, Training Loss 0.035455054908400147\n",
      "Epoch 210, Training Loss 0.03551065394669162\n",
      "Epoch 210, Training Loss 0.035562267672280067\n",
      "Epoch 210, Training Loss 0.035648331701126704\n",
      "Epoch 210, Training Loss 0.03572793348032571\n",
      "Epoch 210, Training Loss 0.035797322080816\n",
      "Epoch 210, Training Loss 0.035896364601848224\n",
      "Epoch 210, Training Loss 0.03596203055594812\n",
      "Epoch 210, Training Loss 0.03602677392427955\n",
      "Epoch 210, Training Loss 0.03605852235356331\n",
      "Epoch 210, Training Loss 0.03615717816850185\n",
      "Epoch 210, Training Loss 0.03637935013731804\n",
      "Epoch 210, Training Loss 0.03652604387553833\n",
      "Epoch 210, Training Loss 0.036698982807929104\n",
      "Epoch 210, Training Loss 0.036928376161476685\n",
      "Epoch 210, Training Loss 0.036989525239200086\n",
      "Epoch 210, Training Loss 0.0370608603419817\n",
      "Epoch 210, Training Loss 0.0371689850445408\n",
      "Epoch 210, Training Loss 0.037228260120219736\n",
      "Epoch 210, Training Loss 0.03729933514819502\n",
      "Epoch 210, Training Loss 0.03738792287543073\n",
      "Epoch 210, Training Loss 0.03742496286521254\n",
      "Epoch 210, Training Loss 0.03755396052175547\n",
      "Epoch 210, Training Loss 0.037736585513805335\n",
      "Epoch 210, Training Loss 0.03783648101198475\n",
      "Epoch 210, Training Loss 0.03797012758310265\n",
      "Epoch 210, Training Loss 0.038062621110483356\n",
      "Epoch 210, Training Loss 0.03820966890372355\n",
      "Epoch 210, Training Loss 0.03841620709632745\n",
      "Epoch 210, Training Loss 0.038519081883513565\n",
      "Epoch 210, Training Loss 0.03861073756357059\n",
      "Epoch 210, Training Loss 0.038740703023855794\n",
      "Epoch 210, Training Loss 0.038796476312362785\n",
      "Epoch 210, Training Loss 0.03887835968895565\n",
      "Epoch 210, Training Loss 0.03899112411195894\n",
      "Epoch 210, Training Loss 0.03913718199743258\n",
      "Epoch 210, Training Loss 0.039293940507752055\n",
      "Epoch 210, Training Loss 0.03934156118422899\n",
      "Epoch 210, Training Loss 0.03946112926401522\n",
      "Epoch 210, Training Loss 0.039611311217346006\n",
      "Epoch 210, Training Loss 0.039750386327219285\n",
      "Epoch 210, Training Loss 0.039877477464030316\n",
      "Epoch 210, Training Loss 0.03997462401238015\n",
      "Epoch 210, Training Loss 0.040094187871441055\n",
      "Epoch 210, Training Loss 0.04015402267913303\n",
      "Epoch 210, Training Loss 0.04020839406753821\n",
      "Epoch 210, Training Loss 0.040269734921491204\n",
      "Epoch 210, Training Loss 0.040382082893243985\n",
      "Epoch 210, Training Loss 0.04048743432201921\n",
      "Epoch 210, Training Loss 0.040694192697382184\n",
      "Epoch 210, Training Loss 0.040740177780866166\n",
      "Epoch 210, Training Loss 0.04079842749659134\n",
      "Epoch 210, Training Loss 0.04082772152884232\n",
      "Epoch 210, Training Loss 0.04090844089989467\n",
      "Epoch 210, Training Loss 0.040987498591394375\n",
      "Epoch 210, Training Loss 0.04114802942975708\n",
      "Epoch 210, Training Loss 0.04128939584088143\n",
      "Epoch 210, Training Loss 0.04140444101808626\n",
      "Epoch 210, Training Loss 0.041461845897995606\n",
      "Epoch 210, Training Loss 0.04153017254779711\n",
      "Epoch 210, Training Loss 0.04164223862654718\n",
      "Epoch 210, Training Loss 0.04173281566832986\n",
      "Epoch 210, Training Loss 0.041799121048025155\n",
      "Epoch 210, Training Loss 0.041841957641913154\n",
      "Epoch 210, Training Loss 0.04202948399173939\n",
      "Epoch 210, Training Loss 0.04217975064540458\n",
      "Epoch 210, Training Loss 0.042226971820225494\n",
      "Epoch 210, Training Loss 0.042535829445933135\n",
      "Epoch 210, Training Loss 0.042590284400888724\n",
      "Epoch 210, Training Loss 0.04271222927305095\n",
      "Epoch 210, Training Loss 0.04273668089714806\n",
      "Epoch 210, Training Loss 0.04284503632951576\n",
      "Epoch 210, Training Loss 0.04289834550045945\n",
      "Epoch 210, Training Loss 0.0430610938678922\n",
      "Epoch 210, Training Loss 0.04328676661871888\n",
      "Epoch 210, Training Loss 0.04345476703570627\n",
      "Epoch 210, Training Loss 0.043520318020297134\n",
      "Epoch 210, Training Loss 0.04369796120354434\n",
      "Epoch 210, Training Loss 0.04376339559893474\n",
      "Epoch 210, Training Loss 0.04388939301528589\n",
      "Epoch 210, Training Loss 0.043989350738199165\n",
      "Epoch 210, Training Loss 0.04411718842890257\n",
      "Epoch 210, Training Loss 0.04428061782894537\n",
      "Epoch 210, Training Loss 0.04435972430174003\n",
      "Epoch 210, Training Loss 0.044464586840947264\n",
      "Epoch 210, Training Loss 0.044547363667917984\n",
      "Epoch 210, Training Loss 0.044616611879271316\n",
      "Epoch 210, Training Loss 0.04477887550164062\n",
      "Epoch 210, Training Loss 0.044904885818357665\n",
      "Epoch 210, Training Loss 0.04497304232433781\n",
      "Epoch 210, Training Loss 0.045031274737947435\n",
      "Epoch 210, Training Loss 0.04510708801124407\n",
      "Epoch 210, Training Loss 0.045222854351296145\n",
      "Epoch 210, Training Loss 0.045346800056869724\n",
      "Epoch 210, Training Loss 0.04545900460970981\n",
      "Epoch 210, Training Loss 0.04566761545474877\n",
      "Epoch 210, Training Loss 0.04576794822197741\n",
      "Epoch 210, Training Loss 0.04585490873097764\n",
      "Epoch 210, Training Loss 0.04592909825884778\n",
      "Epoch 210, Training Loss 0.04603174206850779\n",
      "Epoch 210, Training Loss 0.04610541429551666\n",
      "Epoch 210, Training Loss 0.04618223052462349\n",
      "Epoch 210, Training Loss 0.0463148409219654\n",
      "Epoch 210, Training Loss 0.046486209096658564\n",
      "Epoch 210, Training Loss 0.04655330868252098\n",
      "Epoch 210, Training Loss 0.04666159141932607\n",
      "Epoch 210, Training Loss 0.046712355964514604\n",
      "Epoch 210, Training Loss 0.04676895551478771\n",
      "Epoch 210, Training Loss 0.04680649572244995\n",
      "Epoch 210, Training Loss 0.04686632134553874\n",
      "Epoch 210, Training Loss 0.0469570581746452\n",
      "Epoch 210, Training Loss 0.04707929361945071\n",
      "Epoch 210, Training Loss 0.047208659879653656\n",
      "Epoch 210, Training Loss 0.0473320014689051\n",
      "Epoch 210, Training Loss 0.04740220450741403\n",
      "Epoch 210, Training Loss 0.047498972067023484\n",
      "Epoch 210, Training Loss 0.04761335642917839\n",
      "Epoch 210, Training Loss 0.047638661728319154\n",
      "Epoch 210, Training Loss 0.047731230618989526\n",
      "Epoch 210, Training Loss 0.047775841203739726\n",
      "Epoch 210, Training Loss 0.047873068612802515\n",
      "Epoch 210, Training Loss 0.04798506675978832\n",
      "Epoch 210, Training Loss 0.04804510745884436\n",
      "Epoch 210, Training Loss 0.048135108004807664\n",
      "Epoch 210, Training Loss 0.048236623527887074\n",
      "Epoch 210, Training Loss 0.04845737615157195\n",
      "Epoch 210, Training Loss 0.04860044219066648\n",
      "Epoch 210, Training Loss 0.04884745062226453\n",
      "Epoch 210, Training Loss 0.04895734255109221\n",
      "Epoch 210, Training Loss 0.04905574761874154\n",
      "Epoch 210, Training Loss 0.04912745896631571\n",
      "Epoch 210, Training Loss 0.049160626997975895\n",
      "Epoch 210, Training Loss 0.049257421596904695\n",
      "Epoch 210, Training Loss 0.04934955003630856\n",
      "Epoch 210, Training Loss 0.049417641425929255\n",
      "Epoch 210, Training Loss 0.049500677651246945\n",
      "Epoch 210, Training Loss 0.049569371237379055\n",
      "Epoch 210, Training Loss 0.04966351978452233\n",
      "Epoch 210, Training Loss 0.04976214165263392\n",
      "Epoch 210, Training Loss 0.049809716610938234\n",
      "Epoch 210, Training Loss 0.04985214099812005\n",
      "Epoch 210, Training Loss 0.050131994428213145\n",
      "Epoch 210, Training Loss 0.05026552428627182\n",
      "Epoch 210, Training Loss 0.050334434372746886\n",
      "Epoch 210, Training Loss 0.050491856756474815\n",
      "Epoch 210, Training Loss 0.05056488088777532\n",
      "Epoch 210, Training Loss 0.0507230793514177\n",
      "Epoch 210, Training Loss 0.05077466738107793\n",
      "Epoch 210, Training Loss 0.05080829594103272\n",
      "Epoch 210, Training Loss 0.050828656886735234\n",
      "Epoch 210, Training Loss 0.05098287667364568\n",
      "Epoch 210, Training Loss 0.051086145865223594\n",
      "Epoch 210, Training Loss 0.05115232540442206\n",
      "Epoch 210, Training Loss 0.051310182560968885\n",
      "Epoch 210, Training Loss 0.051379573181309666\n",
      "Epoch 210, Training Loss 0.051462351990020486\n",
      "Epoch 210, Training Loss 0.0515085390585539\n",
      "Epoch 210, Training Loss 0.05155581213972148\n",
      "Epoch 210, Training Loss 0.05159670735240135\n",
      "Epoch 210, Training Loss 0.05167929954407618\n",
      "Epoch 210, Training Loss 0.05190656942498806\n",
      "Epoch 210, Training Loss 0.051935778194776426\n",
      "Epoch 210, Training Loss 0.05203603008223693\n",
      "Epoch 210, Training Loss 0.05206113526135531\n",
      "Epoch 210, Training Loss 0.052107552449931115\n",
      "Epoch 210, Training Loss 0.05216665545245037\n",
      "Epoch 210, Training Loss 0.05222356599777022\n",
      "Epoch 210, Training Loss 0.05231495997380189\n",
      "Epoch 210, Training Loss 0.052340135976310126\n",
      "Epoch 210, Training Loss 0.05253322931992657\n",
      "Epoch 210, Training Loss 0.052597957648946654\n",
      "Epoch 210, Training Loss 0.052640592500738936\n",
      "Epoch 210, Training Loss 0.05267213000570569\n",
      "Epoch 210, Training Loss 0.052724020355535896\n",
      "Epoch 210, Training Loss 0.05280635411115101\n",
      "Epoch 210, Training Loss 0.05310485903126047\n",
      "Epoch 210, Training Loss 0.053211148737755884\n",
      "Epoch 210, Training Loss 0.05328689577520046\n",
      "Epoch 210, Training Loss 0.053522243781391615\n",
      "Epoch 210, Training Loss 0.05358829893781553\n",
      "Epoch 210, Training Loss 0.05364717099615528\n",
      "Epoch 210, Training Loss 0.05368118693628122\n",
      "Epoch 210, Training Loss 0.05373481515309085\n",
      "Epoch 210, Training Loss 0.053827608694963135\n",
      "Epoch 210, Training Loss 0.05389785185894545\n",
      "Epoch 210, Training Loss 0.05395240363810221\n",
      "Epoch 210, Training Loss 0.0540150983349594\n",
      "Epoch 210, Training Loss 0.05410895840553066\n",
      "Epoch 210, Training Loss 0.054154103879085584\n",
      "Epoch 210, Training Loss 0.054224299462250124\n",
      "Epoch 210, Training Loss 0.054279334042840605\n",
      "Epoch 210, Training Loss 0.05441842940838441\n",
      "Epoch 210, Training Loss 0.054534001280660827\n",
      "Epoch 210, Training Loss 0.054667220815368324\n",
      "Epoch 210, Training Loss 0.0546968388454536\n",
      "Epoch 210, Training Loss 0.054894709478482566\n",
      "Epoch 210, Training Loss 0.05495660048921395\n",
      "Epoch 210, Training Loss 0.055129803050204616\n",
      "Epoch 210, Training Loss 0.05525343623155218\n",
      "Epoch 210, Training Loss 0.0553567829872946\n",
      "Epoch 210, Training Loss 0.055398040634515644\n",
      "Epoch 210, Training Loss 0.05549119413851777\n",
      "Epoch 210, Training Loss 0.05556329806118518\n",
      "Epoch 210, Training Loss 0.055597509028833085\n",
      "Epoch 210, Training Loss 0.05569620823244686\n",
      "Epoch 210, Training Loss 0.055788469210248014\n",
      "Epoch 210, Training Loss 0.05595040443541525\n",
      "Epoch 210, Training Loss 0.056078551563403335\n",
      "Epoch 210, Training Loss 0.05621994203170928\n",
      "Epoch 210, Training Loss 0.056270941151091665\n",
      "Epoch 210, Training Loss 0.05641330573993647\n",
      "Epoch 210, Training Loss 0.05644394280365132\n",
      "Epoch 210, Training Loss 0.05658444971122476\n",
      "Epoch 210, Training Loss 0.05665938290612548\n",
      "Epoch 210, Training Loss 0.05675478788602459\n",
      "Epoch 210, Training Loss 0.05683509134889945\n",
      "Epoch 210, Training Loss 0.05688952507636965\n",
      "Epoch 210, Training Loss 0.05696126277251241\n",
      "Epoch 210, Training Loss 0.057074130903405455\n",
      "Epoch 210, Training Loss 0.057132941256265356\n",
      "Epoch 210, Training Loss 0.05717504168134135\n",
      "Epoch 210, Training Loss 0.0572494610005518\n",
      "Epoch 210, Training Loss 0.057318291369625525\n",
      "Epoch 210, Training Loss 0.057357881639791115\n",
      "Epoch 210, Training Loss 0.057416998019056094\n",
      "Epoch 210, Training Loss 0.05753180407263967\n",
      "Epoch 210, Training Loss 0.05760678918341465\n",
      "Epoch 210, Training Loss 0.05774546013263714\n",
      "Epoch 210, Training Loss 0.05778543454120912\n",
      "Epoch 210, Training Loss 0.057896575623708764\n",
      "Epoch 210, Training Loss 0.057985362401494135\n",
      "Epoch 210, Training Loss 0.0580771476473383\n",
      "Epoch 210, Training Loss 0.058130656364266675\n",
      "Epoch 210, Training Loss 0.05819122353449578\n",
      "Epoch 210, Training Loss 0.05828143230608433\n",
      "Epoch 210, Training Loss 0.058406278531512486\n",
      "Epoch 210, Training Loss 0.05847477701628376\n",
      "Epoch 210, Training Loss 0.05852755945880928\n",
      "Epoch 210, Training Loss 0.05860615149378548\n",
      "Epoch 210, Training Loss 0.05872522238546701\n",
      "Epoch 210, Training Loss 0.05875323580392181\n",
      "Epoch 210, Training Loss 0.05894287392649504\n",
      "Epoch 210, Training Loss 0.05918009595378586\n",
      "Epoch 210, Training Loss 0.05925785174207462\n",
      "Epoch 210, Training Loss 0.05932368385746046\n",
      "Epoch 210, Training Loss 0.059390843171826406\n",
      "Epoch 210, Training Loss 0.05946593195238077\n",
      "Epoch 210, Training Loss 0.05954560613655068\n",
      "Epoch 210, Training Loss 0.059601741249832654\n",
      "Epoch 210, Training Loss 0.05970172843207484\n",
      "Epoch 210, Training Loss 0.0597761288390059\n",
      "Epoch 210, Training Loss 0.05982126743839982\n",
      "Epoch 210, Training Loss 0.05988495831694597\n",
      "Epoch 210, Training Loss 0.05997807981775088\n",
      "Epoch 210, Training Loss 0.06011750363766232\n",
      "Epoch 210, Training Loss 0.06018926224211597\n",
      "Epoch 210, Training Loss 0.060311281924967264\n",
      "Epoch 210, Training Loss 0.06034090316347073\n",
      "Epoch 210, Training Loss 0.060413668534773236\n",
      "Epoch 210, Training Loss 0.0604693291070478\n",
      "Epoch 210, Training Loss 0.06057400045597264\n",
      "Epoch 210, Training Loss 0.06063508549395501\n",
      "Epoch 210, Training Loss 0.06075601863062671\n",
      "Epoch 210, Training Loss 0.06084412925393152\n",
      "Epoch 210, Training Loss 0.06088084458371105\n",
      "Epoch 210, Training Loss 0.061082263846340995\n",
      "Epoch 210, Training Loss 0.06121369748069045\n",
      "Epoch 210, Training Loss 0.06139357532362651\n",
      "Epoch 210, Training Loss 0.061493534999697105\n",
      "Epoch 210, Training Loss 0.06155809743420395\n",
      "Epoch 210, Training Loss 0.061628675845730334\n",
      "Epoch 210, Training Loss 0.061706493839697764\n",
      "Epoch 210, Training Loss 0.06180601879058744\n",
      "Epoch 210, Training Loss 0.0619341436716373\n",
      "Epoch 210, Training Loss 0.0621239839507567\n",
      "Epoch 210, Training Loss 0.06219309530771144\n",
      "Epoch 210, Training Loss 0.06230169794310237\n",
      "Epoch 210, Training Loss 0.0623384891316066\n",
      "Epoch 210, Training Loss 0.0623966477730352\n",
      "Epoch 210, Training Loss 0.062433832696617564\n",
      "Epoch 210, Training Loss 0.06258092934737348\n",
      "Epoch 210, Training Loss 0.0626148091309973\n",
      "Epoch 210, Training Loss 0.06265313059205899\n",
      "Epoch 210, Training Loss 0.06272408091808523\n",
      "Epoch 210, Training Loss 0.06275361297352006\n",
      "Epoch 210, Training Loss 0.06283136667526515\n",
      "Epoch 210, Training Loss 0.06292222649611705\n",
      "Epoch 210, Training Loss 0.06305608740957726\n",
      "Epoch 210, Training Loss 0.06313783715209921\n",
      "Epoch 210, Training Loss 0.06323617161072963\n",
      "Epoch 210, Training Loss 0.06327608013358872\n",
      "Epoch 210, Training Loss 0.0633237742535446\n",
      "Epoch 210, Training Loss 0.06336073114839204\n",
      "Epoch 210, Training Loss 0.06344679690173367\n",
      "Epoch 210, Training Loss 0.06349314903111561\n",
      "Epoch 210, Training Loss 0.06358630683206384\n",
      "Epoch 210, Training Loss 0.06365901565231631\n",
      "Epoch 210, Training Loss 0.06368211938587524\n",
      "Epoch 210, Training Loss 0.06371083156303371\n",
      "Epoch 210, Training Loss 0.0637594830968877\n",
      "Epoch 210, Training Loss 0.06378549391456197\n",
      "Epoch 210, Training Loss 0.06384856775260109\n",
      "Epoch 210, Training Loss 0.0639846451566233\n",
      "Epoch 210, Training Loss 0.06402648467322826\n",
      "Epoch 210, Training Loss 0.0640980709949151\n",
      "Epoch 210, Training Loss 0.06420574559475226\n",
      "Epoch 210, Training Loss 0.0642473601695636\n",
      "Epoch 210, Training Loss 0.06429851852366915\n",
      "Epoch 210, Training Loss 0.06438316273100464\n",
      "Epoch 210, Training Loss 0.06449037972037368\n",
      "Epoch 210, Training Loss 0.06457186951194807\n",
      "Epoch 210, Training Loss 0.0646459187935952\n",
      "Epoch 210, Training Loss 0.06471688766508837\n",
      "Epoch 210, Training Loss 0.06480456869858686\n",
      "Epoch 210, Training Loss 0.06494624621432532\n",
      "Epoch 210, Training Loss 0.06501281451757836\n",
      "Epoch 210, Training Loss 0.06518420658748397\n",
      "Epoch 210, Training Loss 0.06527018021849339\n",
      "Epoch 210, Training Loss 0.06535068968706348\n",
      "Epoch 210, Training Loss 0.06545224528321449\n",
      "Epoch 210, Training Loss 0.06554626076794265\n",
      "Epoch 210, Training Loss 0.06567413889376632\n",
      "Epoch 210, Training Loss 0.06579132790408095\n",
      "Epoch 210, Training Loss 0.06584200877315172\n",
      "Epoch 210, Training Loss 0.0659118624823287\n",
      "Epoch 210, Training Loss 0.0659674805948687\n",
      "Epoch 210, Training Loss 0.06601538641325881\n",
      "Epoch 210, Training Loss 0.06621374819389618\n",
      "Epoch 210, Training Loss 0.06638670980196704\n",
      "Epoch 210, Training Loss 0.0665066962480507\n",
      "Epoch 210, Training Loss 0.06653913896283149\n",
      "Epoch 210, Training Loss 0.06657710191472184\n",
      "Epoch 210, Training Loss 0.06682247811656855\n",
      "Epoch 210, Training Loss 0.06698163646652038\n",
      "Epoch 210, Training Loss 0.06709697295952102\n",
      "Epoch 210, Training Loss 0.06722005388325514\n",
      "Epoch 210, Training Loss 0.06727020520373912\n",
      "Epoch 210, Training Loss 0.06751358087705758\n",
      "Epoch 210, Training Loss 0.06754007360771717\n",
      "Epoch 210, Training Loss 0.0676087685848784\n",
      "Epoch 210, Training Loss 0.06763690226780408\n",
      "Epoch 210, Training Loss 0.0676614881071555\n",
      "Epoch 210, Training Loss 0.06793417280201641\n",
      "Epoch 210, Training Loss 0.06802514494370546\n",
      "Epoch 210, Training Loss 0.06810834434817133\n",
      "Epoch 210, Training Loss 0.06823154655821106\n",
      "Epoch 210, Training Loss 0.06827997886682943\n",
      "Epoch 210, Training Loss 0.06837661792774258\n",
      "Epoch 210, Training Loss 0.06847763977840048\n",
      "Epoch 210, Training Loss 0.06865716378545136\n",
      "Epoch 210, Training Loss 0.06871678854774713\n",
      "Epoch 210, Training Loss 0.06882369712881191\n",
      "Epoch 210, Training Loss 0.06902520907590227\n",
      "Epoch 210, Training Loss 0.0690767584616304\n",
      "Epoch 210, Training Loss 0.06919116424182263\n",
      "Epoch 210, Training Loss 0.0692896322011376\n",
      "Epoch 210, Training Loss 0.06936766858667592\n",
      "Epoch 210, Training Loss 0.06942136145299277\n",
      "Epoch 210, Training Loss 0.0695125631716512\n",
      "Epoch 210, Training Loss 0.06960837087353401\n",
      "Epoch 220, Training Loss 6.933736107538424e-05\n",
      "Epoch 220, Training Loss 0.00011973128751720614\n",
      "Epoch 220, Training Loss 0.0002100278940194708\n",
      "Epoch 220, Training Loss 0.00026196668691494884\n",
      "Epoch 220, Training Loss 0.0003328344610798389\n",
      "Epoch 220, Training Loss 0.00042405096656831023\n",
      "Epoch 220, Training Loss 0.000586564878902167\n",
      "Epoch 220, Training Loss 0.0006204311071378191\n",
      "Epoch 220, Training Loss 0.00069461739562509\n",
      "Epoch 220, Training Loss 0.0007433730613468858\n",
      "Epoch 220, Training Loss 0.0008460289710546698\n",
      "Epoch 220, Training Loss 0.0008894682711804919\n",
      "Epoch 220, Training Loss 0.0009800558504850967\n",
      "Epoch 220, Training Loss 0.0010076528534178843\n",
      "Epoch 220, Training Loss 0.0011479779553916448\n",
      "Epoch 220, Training Loss 0.0012108255177736282\n",
      "Epoch 220, Training Loss 0.001316555125443527\n",
      "Epoch 220, Training Loss 0.0013579255794091603\n",
      "Epoch 220, Training Loss 0.0013967091432007988\n",
      "Epoch 220, Training Loss 0.0014571885118627793\n",
      "Epoch 220, Training Loss 0.001563950072583335\n",
      "Epoch 220, Training Loss 0.0016415335018845165\n",
      "Epoch 220, Training Loss 0.0016857048644281715\n",
      "Epoch 220, Training Loss 0.0017412132619287048\n",
      "Epoch 220, Training Loss 0.0017878277908505687\n",
      "Epoch 220, Training Loss 0.001802401998273247\n",
      "Epoch 220, Training Loss 0.0018533533796325058\n",
      "Epoch 220, Training Loss 0.0019730479668473343\n",
      "Epoch 220, Training Loss 0.0020150566078207988\n",
      "Epoch 220, Training Loss 0.0020636869427721823\n",
      "Epoch 220, Training Loss 0.002140389202767626\n",
      "Epoch 220, Training Loss 0.0022723562015063317\n",
      "Epoch 220, Training Loss 0.002370214890069364\n",
      "Epoch 220, Training Loss 0.0024240513658508314\n",
      "Epoch 220, Training Loss 0.0024895784838120344\n",
      "Epoch 220, Training Loss 0.0025758525866376772\n",
      "Epoch 220, Training Loss 0.002598223662303994\n",
      "Epoch 220, Training Loss 0.002628960301313559\n",
      "Epoch 220, Training Loss 0.002698277392427025\n",
      "Epoch 220, Training Loss 0.0027427873562287796\n",
      "Epoch 220, Training Loss 0.0027938963145093844\n",
      "Epoch 220, Training Loss 0.002859022739864981\n",
      "Epoch 220, Training Loss 0.002881062355206903\n",
      "Epoch 220, Training Loss 0.0029290663126065298\n",
      "Epoch 220, Training Loss 0.002983305710451225\n",
      "Epoch 220, Training Loss 0.0030198775331877993\n",
      "Epoch 220, Training Loss 0.003076780337335356\n",
      "Epoch 220, Training Loss 0.0032039397107937453\n",
      "Epoch 220, Training Loss 0.0033730961682508363\n",
      "Epoch 220, Training Loss 0.003445156668896413\n",
      "Epoch 220, Training Loss 0.0034807890944201928\n",
      "Epoch 220, Training Loss 0.0035350242429567726\n",
      "Epoch 220, Training Loss 0.003559199004622219\n",
      "Epoch 220, Training Loss 0.0035825261412679083\n",
      "Epoch 220, Training Loss 0.0037315407448717396\n",
      "Epoch 220, Training Loss 0.0038914896185745667\n",
      "Epoch 220, Training Loss 0.003945440144451987\n",
      "Epoch 220, Training Loss 0.004021032715735533\n",
      "Epoch 220, Training Loss 0.004135981549882828\n",
      "Epoch 220, Training Loss 0.004254402802385333\n",
      "Epoch 220, Training Loss 0.004306834800850095\n",
      "Epoch 220, Training Loss 0.004401119735539721\n",
      "Epoch 220, Training Loss 0.004435256152125575\n",
      "Epoch 220, Training Loss 0.004496778092344704\n",
      "Epoch 220, Training Loss 0.004581913999889208\n",
      "Epoch 220, Training Loss 0.004603764501964802\n",
      "Epoch 220, Training Loss 0.004710235518624868\n",
      "Epoch 220, Training Loss 0.004803785732697191\n",
      "Epoch 220, Training Loss 0.004866274209488231\n",
      "Epoch 220, Training Loss 0.004895480818894056\n",
      "Epoch 220, Training Loss 0.004911215829393824\n",
      "Epoch 220, Training Loss 0.0050682742112909285\n",
      "Epoch 220, Training Loss 0.005108824139103636\n",
      "Epoch 220, Training Loss 0.00518251141971525\n",
      "Epoch 220, Training Loss 0.005269271153670824\n",
      "Epoch 220, Training Loss 0.005310390245817278\n",
      "Epoch 220, Training Loss 0.0053549638492009985\n",
      "Epoch 220, Training Loss 0.005437515186541297\n",
      "Epoch 220, Training Loss 0.005585423188374552\n",
      "Epoch 220, Training Loss 0.005674613069247483\n",
      "Epoch 220, Training Loss 0.005719934874797797\n",
      "Epoch 220, Training Loss 0.005760552410436485\n",
      "Epoch 220, Training Loss 0.005997556941750486\n",
      "Epoch 220, Training Loss 0.006068624943123221\n",
      "Epoch 220, Training Loss 0.006119988407329907\n",
      "Epoch 220, Training Loss 0.006153087409170311\n",
      "Epoch 220, Training Loss 0.006211234884255607\n",
      "Epoch 220, Training Loss 0.0062824918416893236\n",
      "Epoch 220, Training Loss 0.006350793705448089\n",
      "Epoch 220, Training Loss 0.0064022779557615745\n",
      "Epoch 220, Training Loss 0.006454481834979237\n",
      "Epoch 220, Training Loss 0.006550949237183155\n",
      "Epoch 220, Training Loss 0.0066378910911491\n",
      "Epoch 220, Training Loss 0.006813882665513345\n",
      "Epoch 220, Training Loss 0.006900559052053239\n",
      "Epoch 220, Training Loss 0.0070928858624070956\n",
      "Epoch 220, Training Loss 0.00711580815241503\n",
      "Epoch 220, Training Loss 0.0071558791422344685\n",
      "Epoch 220, Training Loss 0.007229148118478029\n",
      "Epoch 220, Training Loss 0.007296097920993176\n",
      "Epoch 220, Training Loss 0.007332654142290201\n",
      "Epoch 220, Training Loss 0.007410336425170645\n",
      "Epoch 220, Training Loss 0.00745887576681002\n",
      "Epoch 220, Training Loss 0.007505169882655829\n",
      "Epoch 220, Training Loss 0.007559403411500975\n",
      "Epoch 220, Training Loss 0.007601239736361043\n",
      "Epoch 220, Training Loss 0.0076740819477307065\n",
      "Epoch 220, Training Loss 0.007772410546889162\n",
      "Epoch 220, Training Loss 0.007814748752195283\n",
      "Epoch 220, Training Loss 0.007854151771739697\n",
      "Epoch 220, Training Loss 0.007871930925723384\n",
      "Epoch 220, Training Loss 0.007892410490004456\n",
      "Epoch 220, Training Loss 0.008018769602508038\n",
      "Epoch 220, Training Loss 0.00809467256383594\n",
      "Epoch 220, Training Loss 0.008122424049603056\n",
      "Epoch 220, Training Loss 0.008152766212763842\n",
      "Epoch 220, Training Loss 0.008231300558142193\n",
      "Epoch 220, Training Loss 0.00827465053704922\n",
      "Epoch 220, Training Loss 0.008308551736328456\n",
      "Epoch 220, Training Loss 0.008360051688121255\n",
      "Epoch 220, Training Loss 0.00840184491966158\n",
      "Epoch 220, Training Loss 0.008420036555460805\n",
      "Epoch 220, Training Loss 0.008491128871260245\n",
      "Epoch 220, Training Loss 0.008548417890830266\n",
      "Epoch 220, Training Loss 0.008592709386840348\n",
      "Epoch 220, Training Loss 0.008650231441897352\n",
      "Epoch 220, Training Loss 0.00869081348723844\n",
      "Epoch 220, Training Loss 0.008748771163070445\n",
      "Epoch 220, Training Loss 0.008782064087231598\n",
      "Epoch 220, Training Loss 0.008858303372721995\n",
      "Epoch 220, Training Loss 0.008924849162264097\n",
      "Epoch 220, Training Loss 0.008960672034917738\n",
      "Epoch 220, Training Loss 0.009074006484502264\n",
      "Epoch 220, Training Loss 0.00912040199303185\n",
      "Epoch 220, Training Loss 0.009146111217015387\n",
      "Epoch 220, Training Loss 0.009176760090186316\n",
      "Epoch 220, Training Loss 0.009227412600365593\n",
      "Epoch 220, Training Loss 0.009266857755706286\n",
      "Epoch 220, Training Loss 0.009310185301886953\n",
      "Epoch 220, Training Loss 0.009371770881688045\n",
      "Epoch 220, Training Loss 0.009418683443361383\n",
      "Epoch 220, Training Loss 0.009458651711397311\n",
      "Epoch 220, Training Loss 0.009578032678712512\n",
      "Epoch 220, Training Loss 0.0095917275818565\n",
      "Epoch 220, Training Loss 0.00961765773651545\n",
      "Epoch 220, Training Loss 0.00965677654546926\n",
      "Epoch 220, Training Loss 0.009689712580150504\n",
      "Epoch 220, Training Loss 0.009750323067597873\n",
      "Epoch 220, Training Loss 0.009854101880794138\n",
      "Epoch 220, Training Loss 0.009885989751218988\n",
      "Epoch 220, Training Loss 0.009986970423604065\n",
      "Epoch 220, Training Loss 0.010119300280861995\n",
      "Epoch 220, Training Loss 0.010191787384412326\n",
      "Epoch 220, Training Loss 0.010274846485489621\n",
      "Epoch 220, Training Loss 0.010321165439779954\n",
      "Epoch 220, Training Loss 0.01035752522108881\n",
      "Epoch 220, Training Loss 0.010404730703958007\n",
      "Epoch 220, Training Loss 0.01047043631191525\n",
      "Epoch 220, Training Loss 0.010549444507550248\n",
      "Epoch 220, Training Loss 0.010637687259565686\n",
      "Epoch 220, Training Loss 0.01067153897250781\n",
      "Epoch 220, Training Loss 0.010718893843329014\n",
      "Epoch 220, Training Loss 0.010841188273485512\n",
      "Epoch 220, Training Loss 0.010910929928598044\n",
      "Epoch 220, Training Loss 0.010946910001832963\n",
      "Epoch 220, Training Loss 0.01102168574843489\n",
      "Epoch 220, Training Loss 0.011129507711609763\n",
      "Epoch 220, Training Loss 0.011183860687934376\n",
      "Epoch 220, Training Loss 0.011207062174635166\n",
      "Epoch 220, Training Loss 0.011275993974503044\n",
      "Epoch 220, Training Loss 0.011337991236992504\n",
      "Epoch 220, Training Loss 0.01142852714814036\n",
      "Epoch 220, Training Loss 0.0115742501767014\n",
      "Epoch 220, Training Loss 0.011595646931749323\n",
      "Epoch 220, Training Loss 0.011713437352548628\n",
      "Epoch 220, Training Loss 0.011774892609118653\n",
      "Epoch 220, Training Loss 0.011796900364177307\n",
      "Epoch 220, Training Loss 0.011846912819939806\n",
      "Epoch 220, Training Loss 0.011874605220316165\n",
      "Epoch 220, Training Loss 0.011907529869519384\n",
      "Epoch 220, Training Loss 0.011926820932808892\n",
      "Epoch 220, Training Loss 0.01199590268275698\n",
      "Epoch 220, Training Loss 0.012159572902333248\n",
      "Epoch 220, Training Loss 0.012268459183804672\n",
      "Epoch 220, Training Loss 0.01238921500237473\n",
      "Epoch 220, Training Loss 0.012418089767970392\n",
      "Epoch 220, Training Loss 0.012486955122379085\n",
      "Epoch 220, Training Loss 0.012545064487077696\n",
      "Epoch 220, Training Loss 0.012606744599692962\n",
      "Epoch 220, Training Loss 0.012702359548767509\n",
      "Epoch 220, Training Loss 0.01295619219770212\n",
      "Epoch 220, Training Loss 0.013233504789259732\n",
      "Epoch 220, Training Loss 0.013301285422023605\n",
      "Epoch 220, Training Loss 0.01334623110187633\n",
      "Epoch 220, Training Loss 0.01343092389042725\n",
      "Epoch 220, Training Loss 0.013494484093221253\n",
      "Epoch 220, Training Loss 0.013592254499073529\n",
      "Epoch 220, Training Loss 0.013751829033503141\n",
      "Epoch 220, Training Loss 0.013853831721655548\n",
      "Epoch 220, Training Loss 0.0139051649452704\n",
      "Epoch 220, Training Loss 0.013989642715019643\n",
      "Epoch 220, Training Loss 0.014032428036146152\n",
      "Epoch 220, Training Loss 0.014100318974660485\n",
      "Epoch 220, Training Loss 0.014204245265525625\n",
      "Epoch 220, Training Loss 0.014244848767014416\n",
      "Epoch 220, Training Loss 0.014276143844665773\n",
      "Epoch 220, Training Loss 0.014333264768847723\n",
      "Epoch 220, Training Loss 0.014552567844919841\n",
      "Epoch 220, Training Loss 0.014734548676158766\n",
      "Epoch 220, Training Loss 0.014850240786705176\n",
      "Epoch 220, Training Loss 0.014904395250789345\n",
      "Epoch 220, Training Loss 0.015078585937885983\n",
      "Epoch 220, Training Loss 0.015158869850132472\n",
      "Epoch 220, Training Loss 0.015370110235631923\n",
      "Epoch 220, Training Loss 0.015471171216129342\n",
      "Epoch 220, Training Loss 0.015602659281638577\n",
      "Epoch 220, Training Loss 0.015619961681830533\n",
      "Epoch 220, Training Loss 0.015677156992723495\n",
      "Epoch 220, Training Loss 0.015720687267106133\n",
      "Epoch 220, Training Loss 0.015782837871500217\n",
      "Epoch 220, Training Loss 0.015941093214894727\n",
      "Epoch 220, Training Loss 0.01599099426446935\n",
      "Epoch 220, Training Loss 0.016025730636199494\n",
      "Epoch 220, Training Loss 0.016056733511988542\n",
      "Epoch 220, Training Loss 0.016137983941751748\n",
      "Epoch 220, Training Loss 0.0161796711930705\n",
      "Epoch 220, Training Loss 0.016189552302284124\n",
      "Epoch 220, Training Loss 0.016271179796213193\n",
      "Epoch 220, Training Loss 0.01629267268471153\n",
      "Epoch 220, Training Loss 0.016303272256294212\n",
      "Epoch 220, Training Loss 0.016354875439596947\n",
      "Epoch 220, Training Loss 0.01644120137254848\n",
      "Epoch 220, Training Loss 0.016480837715789676\n",
      "Epoch 220, Training Loss 0.016538229361867243\n",
      "Epoch 220, Training Loss 0.016594411722501107\n",
      "Epoch 220, Training Loss 0.01676934601946274\n",
      "Epoch 220, Training Loss 0.016834366631329707\n",
      "Epoch 220, Training Loss 0.016875845576038637\n",
      "Epoch 220, Training Loss 0.01700264841730199\n",
      "Epoch 220, Training Loss 0.01704714125524873\n",
      "Epoch 220, Training Loss 0.01716446125096239\n",
      "Epoch 220, Training Loss 0.017220071279336616\n",
      "Epoch 220, Training Loss 0.017251087962995137\n",
      "Epoch 220, Training Loss 0.017422150301835155\n",
      "Epoch 220, Training Loss 0.01750716728591801\n",
      "Epoch 220, Training Loss 0.01767023090246941\n",
      "Epoch 220, Training Loss 0.017717815540811938\n",
      "Epoch 220, Training Loss 0.017732186582954147\n",
      "Epoch 220, Training Loss 0.017799993187946547\n",
      "Epoch 220, Training Loss 0.017902837162885023\n",
      "Epoch 220, Training Loss 0.018159680665754106\n",
      "Epoch 220, Training Loss 0.018202023349030663\n",
      "Epoch 220, Training Loss 0.018255921125369112\n",
      "Epoch 220, Training Loss 0.018347202799618816\n",
      "Epoch 220, Training Loss 0.01841145663050568\n",
      "Epoch 220, Training Loss 0.018489383401044304\n",
      "Epoch 220, Training Loss 0.01852501311834873\n",
      "Epoch 220, Training Loss 0.018571992482766128\n",
      "Epoch 220, Training Loss 0.018645975071236567\n",
      "Epoch 220, Training Loss 0.01883367836758818\n",
      "Epoch 220, Training Loss 0.018882155727447412\n",
      "Epoch 220, Training Loss 0.019025803607343066\n",
      "Epoch 220, Training Loss 0.019040601972914527\n",
      "Epoch 220, Training Loss 0.019165017017427725\n",
      "Epoch 220, Training Loss 0.019211270612404893\n",
      "Epoch 220, Training Loss 0.019358316204770257\n",
      "Epoch 220, Training Loss 0.01948872035371659\n",
      "Epoch 220, Training Loss 0.019548517555984503\n",
      "Epoch 220, Training Loss 0.019603537268760372\n",
      "Epoch 220, Training Loss 0.019768540151393437\n",
      "Epoch 220, Training Loss 0.01983471335474011\n",
      "Epoch 220, Training Loss 0.019878770748172386\n",
      "Epoch 220, Training Loss 0.019918783595833135\n",
      "Epoch 220, Training Loss 0.019967113416094112\n",
      "Epoch 220, Training Loss 0.02002195179012254\n",
      "Epoch 220, Training Loss 0.020112375704371525\n",
      "Epoch 220, Training Loss 0.02015107782626682\n",
      "Epoch 220, Training Loss 0.020243709284664534\n",
      "Epoch 220, Training Loss 0.020327507784409102\n",
      "Epoch 220, Training Loss 0.020403809594752652\n",
      "Epoch 220, Training Loss 0.020465377405585954\n",
      "Epoch 220, Training Loss 0.02050842224891819\n",
      "Epoch 220, Training Loss 0.020554376791691042\n",
      "Epoch 220, Training Loss 0.020695782809034752\n",
      "Epoch 220, Training Loss 0.020751920270869303\n",
      "Epoch 220, Training Loss 0.020797337080849826\n",
      "Epoch 220, Training Loss 0.020901161273989034\n",
      "Epoch 220, Training Loss 0.020927729417958185\n",
      "Epoch 220, Training Loss 0.020970038897560342\n",
      "Epoch 220, Training Loss 0.021035095271261413\n",
      "Epoch 220, Training Loss 0.02113789534303443\n",
      "Epoch 220, Training Loss 0.021306722469584982\n",
      "Epoch 220, Training Loss 0.02134885013882366\n",
      "Epoch 220, Training Loss 0.021431447885087346\n",
      "Epoch 220, Training Loss 0.021557744668053505\n",
      "Epoch 220, Training Loss 0.02163934521793919\n",
      "Epoch 220, Training Loss 0.021657665275022997\n",
      "Epoch 220, Training Loss 0.021749640901303848\n",
      "Epoch 220, Training Loss 0.021826376374501286\n",
      "Epoch 220, Training Loss 0.02185477195200427\n",
      "Epoch 220, Training Loss 0.021911544075635885\n",
      "Epoch 220, Training Loss 0.021975723672253283\n",
      "Epoch 220, Training Loss 0.022009754908935683\n",
      "Epoch 220, Training Loss 0.022059371740774006\n",
      "Epoch 220, Training Loss 0.02210209367956366\n",
      "Epoch 220, Training Loss 0.02216208718366959\n",
      "Epoch 220, Training Loss 0.022204027854768403\n",
      "Epoch 220, Training Loss 0.022266578027511686\n",
      "Epoch 220, Training Loss 0.02233013467651213\n",
      "Epoch 220, Training Loss 0.022454182527449622\n",
      "Epoch 220, Training Loss 0.022618372305927566\n",
      "Epoch 220, Training Loss 0.022735278622921355\n",
      "Epoch 220, Training Loss 0.02287444288251669\n",
      "Epoch 220, Training Loss 0.022955367382725373\n",
      "Epoch 220, Training Loss 0.023004397571853855\n",
      "Epoch 220, Training Loss 0.023032660710999308\n",
      "Epoch 220, Training Loss 0.023090982726891824\n",
      "Epoch 220, Training Loss 0.023215156675571256\n",
      "Epoch 220, Training Loss 0.023253981572459153\n",
      "Epoch 220, Training Loss 0.023302195322773684\n",
      "Epoch 220, Training Loss 0.02336766240734826\n",
      "Epoch 220, Training Loss 0.02346191816853688\n",
      "Epoch 220, Training Loss 0.023522882934902675\n",
      "Epoch 220, Training Loss 0.023586376787513456\n",
      "Epoch 220, Training Loss 0.023627743782961497\n",
      "Epoch 220, Training Loss 0.023889940670903418\n",
      "Epoch 220, Training Loss 0.02397646373935768\n",
      "Epoch 220, Training Loss 0.024049765561990764\n",
      "Epoch 220, Training Loss 0.024081784165333336\n",
      "Epoch 220, Training Loss 0.024192817033389987\n",
      "Epoch 220, Training Loss 0.024290601263785034\n",
      "Epoch 220, Training Loss 0.02431716796432329\n",
      "Epoch 220, Training Loss 0.024350053409848103\n",
      "Epoch 220, Training Loss 0.024366065971148404\n",
      "Epoch 220, Training Loss 0.02444073261724085\n",
      "Epoch 220, Training Loss 0.024513226271013887\n",
      "Epoch 220, Training Loss 0.024597842286190833\n",
      "Epoch 220, Training Loss 0.024613176970897467\n",
      "Epoch 220, Training Loss 0.024693447287263505\n",
      "Epoch 220, Training Loss 0.02475683368763903\n",
      "Epoch 220, Training Loss 0.024811879739455898\n",
      "Epoch 220, Training Loss 0.024931739935356065\n",
      "Epoch 220, Training Loss 0.025055102828313667\n",
      "Epoch 220, Training Loss 0.02509143903596646\n",
      "Epoch 220, Training Loss 0.025162844452411508\n",
      "Epoch 220, Training Loss 0.02538609620281364\n",
      "Epoch 220, Training Loss 0.02545391774231382\n",
      "Epoch 220, Training Loss 0.02549950853097336\n",
      "Epoch 220, Training Loss 0.02554252480104318\n",
      "Epoch 220, Training Loss 0.02562294260460569\n",
      "Epoch 220, Training Loss 0.025653857400145412\n",
      "Epoch 220, Training Loss 0.025709633213465515\n",
      "Epoch 220, Training Loss 0.02579815850636024\n",
      "Epoch 220, Training Loss 0.025862676794747904\n",
      "Epoch 220, Training Loss 0.025920967602997523\n",
      "Epoch 220, Training Loss 0.02606479369658891\n",
      "Epoch 220, Training Loss 0.026111237714876948\n",
      "Epoch 220, Training Loss 0.026176931122746654\n",
      "Epoch 220, Training Loss 0.026271058671066865\n",
      "Epoch 220, Training Loss 0.02639293668808325\n",
      "Epoch 220, Training Loss 0.0264561452749221\n",
      "Epoch 220, Training Loss 0.026536648312364432\n",
      "Epoch 220, Training Loss 0.026630611376255713\n",
      "Epoch 220, Training Loss 0.02677499683504767\n",
      "Epoch 220, Training Loss 0.02688951972845818\n",
      "Epoch 220, Training Loss 0.026934762354916358\n",
      "Epoch 220, Training Loss 0.026998501562196617\n",
      "Epoch 220, Training Loss 0.027049750423706744\n",
      "Epoch 220, Training Loss 0.027135685591808404\n",
      "Epoch 220, Training Loss 0.02721816775879687\n",
      "Epoch 220, Training Loss 0.027261775035215804\n",
      "Epoch 220, Training Loss 0.027416542925707558\n",
      "Epoch 220, Training Loss 0.02750251546104336\n",
      "Epoch 220, Training Loss 0.02755831527437948\n",
      "Epoch 220, Training Loss 0.02768951510150662\n",
      "Epoch 220, Training Loss 0.027863617201009407\n",
      "Epoch 220, Training Loss 0.028036291056605593\n",
      "Epoch 220, Training Loss 0.028116912817430048\n",
      "Epoch 220, Training Loss 0.028254145699793762\n",
      "Epoch 220, Training Loss 0.02830044617531035\n",
      "Epoch 220, Training Loss 0.02837570025013936\n",
      "Epoch 220, Training Loss 0.0284094090920056\n",
      "Epoch 220, Training Loss 0.028433636717541177\n",
      "Epoch 220, Training Loss 0.028484425629558198\n",
      "Epoch 220, Training Loss 0.02857404074612576\n",
      "Epoch 220, Training Loss 0.028622152184343438\n",
      "Epoch 220, Training Loss 0.028655394581277544\n",
      "Epoch 220, Training Loss 0.02871558654045834\n",
      "Epoch 220, Training Loss 0.028808246924639548\n",
      "Epoch 220, Training Loss 0.028963151097874087\n",
      "Epoch 220, Training Loss 0.029072673727527185\n",
      "Epoch 220, Training Loss 0.029114659630891195\n",
      "Epoch 220, Training Loss 0.02918308905487799\n",
      "Epoch 220, Training Loss 0.029260558076798344\n",
      "Epoch 220, Training Loss 0.029343158976696527\n",
      "Epoch 220, Training Loss 0.029420285057538496\n",
      "Epoch 220, Training Loss 0.029594368502007\n",
      "Epoch 220, Training Loss 0.0296269903856251\n",
      "Epoch 220, Training Loss 0.029657727946429645\n",
      "Epoch 220, Training Loss 0.02974616933930808\n",
      "Epoch 220, Training Loss 0.029802312894040704\n",
      "Epoch 220, Training Loss 0.02996552805654476\n",
      "Epoch 220, Training Loss 0.030054007845637783\n",
      "Epoch 220, Training Loss 0.030147484011228774\n",
      "Epoch 220, Training Loss 0.030202423520695867\n",
      "Epoch 220, Training Loss 0.030240621477903802\n",
      "Epoch 220, Training Loss 0.03031993218902451\n",
      "Epoch 220, Training Loss 0.03040808783022358\n",
      "Epoch 220, Training Loss 0.030484348228868204\n",
      "Epoch 220, Training Loss 0.030534512892751324\n",
      "Epoch 220, Training Loss 0.030588300291882337\n",
      "Epoch 220, Training Loss 0.030638430189565205\n",
      "Epoch 220, Training Loss 0.0306997862162159\n",
      "Epoch 220, Training Loss 0.03076982085445367\n",
      "Epoch 220, Training Loss 0.03080363405685005\n",
      "Epoch 220, Training Loss 0.030947356375858492\n",
      "Epoch 220, Training Loss 0.03105264382682207\n",
      "Epoch 220, Training Loss 0.03121005908390769\n",
      "Epoch 220, Training Loss 0.03127890498772778\n",
      "Epoch 220, Training Loss 0.03136611443377384\n",
      "Epoch 220, Training Loss 0.031413300580385585\n",
      "Epoch 220, Training Loss 0.03155842904880395\n",
      "Epoch 220, Training Loss 0.0316742197985587\n",
      "Epoch 220, Training Loss 0.031785238089392445\n",
      "Epoch 220, Training Loss 0.03185132704496555\n",
      "Epoch 220, Training Loss 0.03194885418269202\n",
      "Epoch 220, Training Loss 0.03213342141343371\n",
      "Epoch 220, Training Loss 0.032390564044727885\n",
      "Epoch 220, Training Loss 0.03244064879589392\n",
      "Epoch 220, Training Loss 0.032499295659958745\n",
      "Epoch 220, Training Loss 0.032556194567319265\n",
      "Epoch 220, Training Loss 0.03263262404269441\n",
      "Epoch 220, Training Loss 0.03279697843898764\n",
      "Epoch 220, Training Loss 0.03287201652320011\n",
      "Epoch 220, Training Loss 0.03294861893398244\n",
      "Epoch 220, Training Loss 0.03304464679004153\n",
      "Epoch 220, Training Loss 0.03309558346555533\n",
      "Epoch 220, Training Loss 0.033134018750432544\n",
      "Epoch 220, Training Loss 0.033225354789148856\n",
      "Epoch 220, Training Loss 0.03327293984670564\n",
      "Epoch 220, Training Loss 0.03342391084700994\n",
      "Epoch 220, Training Loss 0.03349624268884492\n",
      "Epoch 220, Training Loss 0.033602981068922774\n",
      "Epoch 220, Training Loss 0.0337386852198178\n",
      "Epoch 220, Training Loss 0.03381599613603519\n",
      "Epoch 220, Training Loss 0.03383210480101216\n",
      "Epoch 220, Training Loss 0.033892482896085326\n",
      "Epoch 220, Training Loss 0.03401727758674308\n",
      "Epoch 220, Training Loss 0.0340621468647981\n",
      "Epoch 220, Training Loss 0.03416909009773198\n",
      "Epoch 220, Training Loss 0.034250082463011755\n",
      "Epoch 220, Training Loss 0.03432954250968269\n",
      "Epoch 220, Training Loss 0.03441494823876492\n",
      "Epoch 220, Training Loss 0.034519908607334775\n",
      "Epoch 220, Training Loss 0.03460219545859148\n",
      "Epoch 220, Training Loss 0.034670307032425726\n",
      "Epoch 220, Training Loss 0.034693430478582185\n",
      "Epoch 220, Training Loss 0.03476248630393973\n",
      "Epoch 220, Training Loss 0.03482534517557896\n",
      "Epoch 220, Training Loss 0.03497911828970227\n",
      "Epoch 220, Training Loss 0.03502656360599867\n",
      "Epoch 220, Training Loss 0.035047301542028175\n",
      "Epoch 220, Training Loss 0.03508640008816576\n",
      "Epoch 220, Training Loss 0.03522497819095869\n",
      "Epoch 220, Training Loss 0.03530816126512387\n",
      "Epoch 220, Training Loss 0.03538492610713805\n",
      "Epoch 220, Training Loss 0.035410981637823497\n",
      "Epoch 220, Training Loss 0.03546106596977529\n",
      "Epoch 220, Training Loss 0.03555406935696426\n",
      "Epoch 220, Training Loss 0.03563049591213103\n",
      "Epoch 220, Training Loss 0.03566628438539689\n",
      "Epoch 220, Training Loss 0.035712612486180025\n",
      "Epoch 220, Training Loss 0.03574146135576793\n",
      "Epoch 220, Training Loss 0.035789321741813325\n",
      "Epoch 220, Training Loss 0.035821251990988165\n",
      "Epoch 220, Training Loss 0.03595133037234911\n",
      "Epoch 220, Training Loss 0.03602567162243244\n",
      "Epoch 220, Training Loss 0.03605727064947281\n",
      "Epoch 220, Training Loss 0.03607990548236158\n",
      "Epoch 220, Training Loss 0.03615334866594647\n",
      "Epoch 220, Training Loss 0.03622558340251617\n",
      "Epoch 220, Training Loss 0.03627961043023106\n",
      "Epoch 220, Training Loss 0.0363740166893367\n",
      "Epoch 220, Training Loss 0.03646042041511506\n",
      "Epoch 220, Training Loss 0.03651941615595099\n",
      "Epoch 220, Training Loss 0.03668705315229095\n",
      "Epoch 220, Training Loss 0.03692786395847512\n",
      "Epoch 220, Training Loss 0.03701319498167185\n",
      "Epoch 220, Training Loss 0.03706018156800753\n",
      "Epoch 220, Training Loss 0.03711884581993627\n",
      "Epoch 220, Training Loss 0.03723628330997208\n",
      "Epoch 220, Training Loss 0.037322532240887316\n",
      "Epoch 220, Training Loss 0.03743233986001204\n",
      "Epoch 220, Training Loss 0.03756187311040185\n",
      "Epoch 220, Training Loss 0.03762509669844166\n",
      "Epoch 220, Training Loss 0.037685600700704834\n",
      "Epoch 220, Training Loss 0.03777849966543429\n",
      "Epoch 220, Training Loss 0.03783851220567837\n",
      "Epoch 220, Training Loss 0.03797897895443184\n",
      "Epoch 220, Training Loss 0.038109300403839065\n",
      "Epoch 220, Training Loss 0.038232838442606276\n",
      "Epoch 220, Training Loss 0.0383224739863411\n",
      "Epoch 220, Training Loss 0.03837057266234303\n",
      "Epoch 220, Training Loss 0.03845177286261659\n",
      "Epoch 220, Training Loss 0.038549000824279984\n",
      "Epoch 220, Training Loss 0.03865440787516458\n",
      "Epoch 220, Training Loss 0.03888131606945163\n",
      "Epoch 220, Training Loss 0.03895763657293985\n",
      "Epoch 220, Training Loss 0.03898456560083381\n",
      "Epoch 220, Training Loss 0.03902471923779058\n",
      "Epoch 220, Training Loss 0.03910649105575403\n",
      "Epoch 220, Training Loss 0.03916601126220392\n",
      "Epoch 220, Training Loss 0.03926298215978152\n",
      "Epoch 220, Training Loss 0.03935389228634384\n",
      "Epoch 220, Training Loss 0.03940482153033223\n",
      "Epoch 220, Training Loss 0.03943750491220494\n",
      "Epoch 220, Training Loss 0.03947856805830374\n",
      "Epoch 220, Training Loss 0.039586861113496026\n",
      "Epoch 220, Training Loss 0.039652443529628316\n",
      "Epoch 220, Training Loss 0.039767746418795505\n",
      "Epoch 220, Training Loss 0.039942965095462586\n",
      "Epoch 220, Training Loss 0.040127346061689356\n",
      "Epoch 220, Training Loss 0.040233070743591774\n",
      "Epoch 220, Training Loss 0.040307626540860744\n",
      "Epoch 220, Training Loss 0.04037272063431704\n",
      "Epoch 220, Training Loss 0.040387932347524386\n",
      "Epoch 220, Training Loss 0.0404798577062052\n",
      "Epoch 220, Training Loss 0.04052716623479619\n",
      "Epoch 220, Training Loss 0.040704213104822945\n",
      "Epoch 220, Training Loss 0.040828476479524735\n",
      "Epoch 220, Training Loss 0.04092414529763086\n",
      "Epoch 220, Training Loss 0.04103588432256999\n",
      "Epoch 220, Training Loss 0.041248235683006895\n",
      "Epoch 220, Training Loss 0.04133005785431875\n",
      "Epoch 220, Training Loss 0.04148295605876733\n",
      "Epoch 220, Training Loss 0.04160787450337825\n",
      "Epoch 220, Training Loss 0.04172210921557225\n",
      "Epoch 220, Training Loss 0.04176727162443025\n",
      "Epoch 220, Training Loss 0.0418173294458443\n",
      "Epoch 220, Training Loss 0.04186869832648969\n",
      "Epoch 220, Training Loss 0.04190553294654335\n",
      "Epoch 220, Training Loss 0.04198655353256447\n",
      "Epoch 220, Training Loss 0.04205145933689154\n",
      "Epoch 220, Training Loss 0.04207757519155531\n",
      "Epoch 220, Training Loss 0.0421589444590318\n",
      "Epoch 220, Training Loss 0.0423037536862685\n",
      "Epoch 220, Training Loss 0.04235886614543417\n",
      "Epoch 220, Training Loss 0.04259813666317488\n",
      "Epoch 220, Training Loss 0.04273672202599647\n",
      "Epoch 220, Training Loss 0.04290187449189727\n",
      "Epoch 220, Training Loss 0.04295364134025562\n",
      "Epoch 220, Training Loss 0.04299144729878992\n",
      "Epoch 220, Training Loss 0.04308965098162842\n",
      "Epoch 220, Training Loss 0.04315074797674461\n",
      "Epoch 220, Training Loss 0.04327729130532983\n",
      "Epoch 220, Training Loss 0.043401269233235354\n",
      "Epoch 220, Training Loss 0.0434557786669887\n",
      "Epoch 220, Training Loss 0.04365179267630953\n",
      "Epoch 220, Training Loss 0.043690757011956606\n",
      "Epoch 220, Training Loss 0.043706864291859215\n",
      "Epoch 220, Training Loss 0.043785600665161184\n",
      "Epoch 220, Training Loss 0.04381471772468589\n",
      "Epoch 220, Training Loss 0.04388279190746701\n",
      "Epoch 220, Training Loss 0.04398883241014388\n",
      "Epoch 220, Training Loss 0.04408597675761889\n",
      "Epoch 220, Training Loss 0.044124352021018984\n",
      "Epoch 220, Training Loss 0.04414860782918551\n",
      "Epoch 220, Training Loss 0.04426393067866773\n",
      "Epoch 220, Training Loss 0.044313400190042526\n",
      "Epoch 220, Training Loss 0.044479267016086545\n",
      "Epoch 220, Training Loss 0.04451703370901306\n",
      "Epoch 220, Training Loss 0.0445332427865223\n",
      "Epoch 220, Training Loss 0.04462645998250817\n",
      "Epoch 220, Training Loss 0.044709255559789135\n",
      "Epoch 220, Training Loss 0.04479220765642822\n",
      "Epoch 220, Training Loss 0.04506636164008218\n",
      "Epoch 220, Training Loss 0.04516708101812855\n",
      "Epoch 220, Training Loss 0.04520704735206235\n",
      "Epoch 220, Training Loss 0.045274640105383665\n",
      "Epoch 220, Training Loss 0.04539114139471537\n",
      "Epoch 220, Training Loss 0.045438076246201116\n",
      "Epoch 220, Training Loss 0.045512392376775825\n",
      "Epoch 220, Training Loss 0.04555008243209661\n",
      "Epoch 220, Training Loss 0.04569463421414366\n",
      "Epoch 220, Training Loss 0.045734039939525524\n",
      "Epoch 220, Training Loss 0.0457680600860854\n",
      "Epoch 220, Training Loss 0.045797581763585546\n",
      "Epoch 220, Training Loss 0.045851927308384756\n",
      "Epoch 220, Training Loss 0.045923534176333825\n",
      "Epoch 220, Training Loss 0.04598097757631651\n",
      "Epoch 220, Training Loss 0.04605266745345157\n",
      "Epoch 220, Training Loss 0.046103052423952524\n",
      "Epoch 220, Training Loss 0.046157325835317335\n",
      "Epoch 220, Training Loss 0.04630113426474449\n",
      "Epoch 220, Training Loss 0.04632635134072198\n",
      "Epoch 220, Training Loss 0.04641835489238867\n",
      "Epoch 220, Training Loss 0.04655724634830852\n",
      "Epoch 220, Training Loss 0.046698021210010744\n",
      "Epoch 220, Training Loss 0.04678954276056184\n",
      "Epoch 220, Training Loss 0.04682781352289974\n",
      "Epoch 220, Training Loss 0.04704126570185604\n",
      "Epoch 220, Training Loss 0.04716158831966064\n",
      "Epoch 220, Training Loss 0.04722637690596587\n",
      "Epoch 220, Training Loss 0.04733814546645469\n",
      "Epoch 220, Training Loss 0.047409011735259306\n",
      "Epoch 220, Training Loss 0.047566793709540804\n",
      "Epoch 220, Training Loss 0.047630193196070356\n",
      "Epoch 220, Training Loss 0.04774250014199187\n",
      "Epoch 220, Training Loss 0.047808021043196244\n",
      "Epoch 220, Training Loss 0.04787248593893217\n",
      "Epoch 220, Training Loss 0.047911464499281076\n",
      "Epoch 220, Training Loss 0.04798490652049918\n",
      "Epoch 220, Training Loss 0.0480843718000509\n",
      "Epoch 220, Training Loss 0.04816413026534097\n",
      "Epoch 220, Training Loss 0.048258133307027884\n",
      "Epoch 220, Training Loss 0.048302642056061065\n",
      "Epoch 220, Training Loss 0.04837195213486338\n",
      "Epoch 220, Training Loss 0.048395179903439584\n",
      "Epoch 220, Training Loss 0.048444345479716766\n",
      "Epoch 220, Training Loss 0.04848345439664334\n",
      "Epoch 220, Training Loss 0.04857042922562136\n",
      "Epoch 220, Training Loss 0.048601453867205005\n",
      "Epoch 220, Training Loss 0.048640384782424856\n",
      "Epoch 220, Training Loss 0.04871908251655853\n",
      "Epoch 220, Training Loss 0.04877687233519238\n",
      "Epoch 220, Training Loss 0.04885512870341025\n",
      "Epoch 220, Training Loss 0.049041143843018074\n",
      "Epoch 220, Training Loss 0.0490568390365004\n",
      "Epoch 220, Training Loss 0.04908549021322118\n",
      "Epoch 220, Training Loss 0.04913150020660666\n",
      "Epoch 220, Training Loss 0.04923195482226913\n",
      "Epoch 220, Training Loss 0.04933002669080768\n",
      "Epoch 220, Training Loss 0.049468341603270155\n",
      "Epoch 220, Training Loss 0.04954693152788369\n",
      "Epoch 220, Training Loss 0.04966468802091125\n",
      "Epoch 220, Training Loss 0.04976329716968129\n",
      "Epoch 220, Training Loss 0.04985047054424634\n",
      "Epoch 220, Training Loss 0.04990311001565622\n",
      "Epoch 220, Training Loss 0.04995752224410453\n",
      "Epoch 220, Training Loss 0.05001834057607328\n",
      "Epoch 220, Training Loss 0.050103043949784105\n",
      "Epoch 220, Training Loss 0.05013956260614936\n",
      "Epoch 220, Training Loss 0.050266659659597916\n",
      "Epoch 220, Training Loss 0.05037172925253601\n",
      "Epoch 220, Training Loss 0.0504115279055322\n",
      "Epoch 220, Training Loss 0.05046915721750873\n",
      "Epoch 220, Training Loss 0.05068742275735854\n",
      "Epoch 220, Training Loss 0.050828609316040524\n",
      "Epoch 220, Training Loss 0.05096295183998487\n",
      "Epoch 220, Training Loss 0.05102914359832607\n",
      "Epoch 220, Training Loss 0.051131425997661545\n",
      "Epoch 220, Training Loss 0.05117930521023795\n",
      "Epoch 220, Training Loss 0.05123514238128062\n",
      "Epoch 220, Training Loss 0.051253992500250485\n",
      "Epoch 220, Training Loss 0.05132978244046288\n",
      "Epoch 220, Training Loss 0.05142304908467547\n",
      "Epoch 220, Training Loss 0.051504395695527075\n",
      "Epoch 220, Training Loss 0.05153241595479629\n",
      "Epoch 220, Training Loss 0.051577334633440046\n",
      "Epoch 220, Training Loss 0.051671282586563005\n",
      "Epoch 220, Training Loss 0.051832028519948156\n",
      "Epoch 220, Training Loss 0.051961754008656\n",
      "Epoch 220, Training Loss 0.052085733861136047\n",
      "Epoch 220, Training Loss 0.05211942109679494\n",
      "Epoch 220, Training Loss 0.05232164378354178\n",
      "Epoch 220, Training Loss 0.05242778709851434\n",
      "Epoch 220, Training Loss 0.052521997060545764\n",
      "Epoch 220, Training Loss 0.05260822899422854\n",
      "Epoch 220, Training Loss 0.05267603606130938\n",
      "Epoch 220, Training Loss 0.05270590705146818\n",
      "Epoch 220, Training Loss 0.052741290980597474\n",
      "Epoch 220, Training Loss 0.05287781610544247\n",
      "Epoch 220, Training Loss 0.052925977816023984\n",
      "Epoch 220, Training Loss 0.053043689986671826\n",
      "Epoch 220, Training Loss 0.053192127475162485\n",
      "Epoch 220, Training Loss 0.053433000420332145\n",
      "Epoch 220, Training Loss 0.053465793743524746\n",
      "Epoch 220, Training Loss 0.05352721783532606\n",
      "Epoch 220, Training Loss 0.05357461134055177\n",
      "Epoch 220, Training Loss 0.05368973431058342\n",
      "Epoch 220, Training Loss 0.053835820207191284\n",
      "Epoch 220, Training Loss 0.05393182067140994\n",
      "Epoch 220, Training Loss 0.05406410114713432\n",
      "Epoch 220, Training Loss 0.0541910207109845\n",
      "Epoch 220, Training Loss 0.05430491472584913\n",
      "Epoch 220, Training Loss 0.054448091218371866\n",
      "Epoch 220, Training Loss 0.05455132490059699\n",
      "Epoch 220, Training Loss 0.05460462456299921\n",
      "Epoch 220, Training Loss 0.05465464045822411\n",
      "Epoch 220, Training Loss 0.05478299887019598\n",
      "Epoch 220, Training Loss 0.054840277366535475\n",
      "Epoch 220, Training Loss 0.05490292277237129\n",
      "Epoch 220, Training Loss 0.054923420248534104\n",
      "Epoch 220, Training Loss 0.05503521322825319\n",
      "Epoch 220, Training Loss 0.055143123045396966\n",
      "Epoch 220, Training Loss 0.055329839568561336\n",
      "Epoch 220, Training Loss 0.05543365428174186\n",
      "Epoch 220, Training Loss 0.05549277352881344\n",
      "Epoch 220, Training Loss 0.055556169399620055\n",
      "Epoch 220, Training Loss 0.055579562629203855\n",
      "Epoch 220, Training Loss 0.05569314454858432\n",
      "Epoch 220, Training Loss 0.05573936336783364\n",
      "Epoch 220, Training Loss 0.05584962182330525\n",
      "Epoch 220, Training Loss 0.05600498498850943\n",
      "Epoch 220, Training Loss 0.05610657372164166\n",
      "Epoch 220, Training Loss 0.05613002741097203\n",
      "Epoch 220, Training Loss 0.056227072919159175\n",
      "Epoch 220, Training Loss 0.056281751291492904\n",
      "Epoch 220, Training Loss 0.05634348704527273\n",
      "Epoch 220, Training Loss 0.0563935027261267\n",
      "Epoch 220, Training Loss 0.05659820302627752\n",
      "Epoch 220, Training Loss 0.0567235806915204\n",
      "Epoch 220, Training Loss 0.05686826435456057\n",
      "Epoch 220, Training Loss 0.05696827813785266\n",
      "Epoch 220, Training Loss 0.05716414273540249\n",
      "Epoch 220, Training Loss 0.05739649461732363\n",
      "Epoch 220, Training Loss 0.057411964262819006\n",
      "Epoch 220, Training Loss 0.0575640701904388\n",
      "Epoch 220, Training Loss 0.057690069960106324\n",
      "Epoch 220, Training Loss 0.05788290712570347\n",
      "Epoch 220, Training Loss 0.058042075041362357\n",
      "Epoch 220, Training Loss 0.05808784404848143\n",
      "Epoch 220, Training Loss 0.05812120095998182\n",
      "Epoch 220, Training Loss 0.05814412017972173\n",
      "Epoch 220, Training Loss 0.058231886456627635\n",
      "Epoch 220, Training Loss 0.05828827036404149\n",
      "Epoch 220, Training Loss 0.0583265545364717\n",
      "Epoch 220, Training Loss 0.05839272741111629\n",
      "Epoch 220, Training Loss 0.058502336884813044\n",
      "Epoch 220, Training Loss 0.05858535941897432\n",
      "Epoch 220, Training Loss 0.05867070907815014\n",
      "Epoch 220, Training Loss 0.0587452803768189\n",
      "Epoch 220, Training Loss 0.0587663549014017\n",
      "Epoch 220, Training Loss 0.05882210255114605\n",
      "Epoch 220, Training Loss 0.05885013315500334\n",
      "Epoch 220, Training Loss 0.05893955213646583\n",
      "Epoch 220, Training Loss 0.05905189664017342\n",
      "Epoch 220, Training Loss 0.05913937545043257\n",
      "Epoch 220, Training Loss 0.05922593231327222\n",
      "Epoch 220, Training Loss 0.05929989997199867\n",
      "Epoch 220, Training Loss 0.059374459041997105\n",
      "Epoch 220, Training Loss 0.05942569128261484\n",
      "Epoch 220, Training Loss 0.059525830544355084\n",
      "Epoch 220, Training Loss 0.059674480622110275\n",
      "Epoch 220, Training Loss 0.05987953247152307\n",
      "Epoch 220, Training Loss 0.059925802296761164\n",
      "Epoch 220, Training Loss 0.05999737013014667\n",
      "Epoch 220, Training Loss 0.06003984680954281\n",
      "Epoch 220, Training Loss 0.06009252012297606\n",
      "Epoch 220, Training Loss 0.06012135108782793\n",
      "Epoch 220, Training Loss 0.06018534250011968\n",
      "Epoch 220, Training Loss 0.06023638373147935\n",
      "Epoch 220, Training Loss 0.06032181589011475\n",
      "Epoch 220, Training Loss 0.06041757013265263\n",
      "Epoch 220, Training Loss 0.06052101989124266\n",
      "Epoch 220, Training Loss 0.060569554478611294\n",
      "Epoch 220, Training Loss 0.06063218328260991\n",
      "Epoch 220, Training Loss 0.060770308500622656\n",
      "Epoch 220, Training Loss 0.060805731828746094\n",
      "Epoch 220, Training Loss 0.06093446788606722\n",
      "Epoch 220, Training Loss 0.06096530825440365\n",
      "Epoch 220, Training Loss 0.061075713644471126\n",
      "Epoch 220, Training Loss 0.06114402313507102\n",
      "Epoch 220, Training Loss 0.061250689010138214\n",
      "Epoch 220, Training Loss 0.06135612676846688\n",
      "Epoch 220, Training Loss 0.06149351553183973\n",
      "Epoch 220, Training Loss 0.061510249041616345\n",
      "Epoch 220, Training Loss 0.06156084533421147\n",
      "Epoch 220, Training Loss 0.06159416904918316\n",
      "Epoch 220, Training Loss 0.06179901919937442\n",
      "Epoch 220, Training Loss 0.06183953390366224\n",
      "Epoch 220, Training Loss 0.06203795365019775\n",
      "Epoch 220, Training Loss 0.06211472813890356\n",
      "Epoch 220, Training Loss 0.062194030460975394\n",
      "Epoch 220, Training Loss 0.06247940572405048\n",
      "Epoch 220, Training Loss 0.06262295554592119\n",
      "Epoch 220, Training Loss 0.06265395260273057\n",
      "Epoch 220, Training Loss 0.06266237145213557\n",
      "Epoch 220, Training Loss 0.06272115316027609\n",
      "Epoch 220, Training Loss 0.06277750517406008\n",
      "Epoch 220, Training Loss 0.06291826770705221\n",
      "Epoch 220, Training Loss 0.06294768631739346\n",
      "Epoch 230, Training Loss 0.00021201404540435127\n",
      "Epoch 230, Training Loss 0.000391373453695146\n",
      "Epoch 230, Training Loss 0.0005233486652221827\n",
      "Epoch 230, Training Loss 0.0005999631236505021\n",
      "Epoch 230, Training Loss 0.0006372074708533104\n",
      "Epoch 230, Training Loss 0.0007049380646794653\n",
      "Epoch 230, Training Loss 0.0008179859908492974\n",
      "Epoch 230, Training Loss 0.00085437618186483\n",
      "Epoch 230, Training Loss 0.0008824733383667743\n",
      "Epoch 230, Training Loss 0.000989121558797329\n",
      "Epoch 230, Training Loss 0.0010741257854282398\n",
      "Epoch 230, Training Loss 0.0011634929939304166\n",
      "Epoch 230, Training Loss 0.0013069305807123404\n",
      "Epoch 230, Training Loss 0.0013347770304173764\n",
      "Epoch 230, Training Loss 0.0013593116517909957\n",
      "Epoch 230, Training Loss 0.0013695474906974588\n",
      "Epoch 230, Training Loss 0.0014663547899126245\n",
      "Epoch 230, Training Loss 0.0015294903239039966\n",
      "Epoch 230, Training Loss 0.0016025413706650972\n",
      "Epoch 230, Training Loss 0.001703542765572934\n",
      "Epoch 230, Training Loss 0.0017340817879361416\n",
      "Epoch 230, Training Loss 0.0017767277441423416\n",
      "Epoch 230, Training Loss 0.0018148737068733444\n",
      "Epoch 230, Training Loss 0.001835126666795186\n",
      "Epoch 230, Training Loss 0.0019842055912041453\n",
      "Epoch 230, Training Loss 0.0020999047616996882\n",
      "Epoch 230, Training Loss 0.002256636899631575\n",
      "Epoch 230, Training Loss 0.0023787098131654665\n",
      "Epoch 230, Training Loss 0.0024994924042821692\n",
      "Epoch 230, Training Loss 0.002598012222305817\n",
      "Epoch 230, Training Loss 0.0026289782596423347\n",
      "Epoch 230, Training Loss 0.0026677853685072466\n",
      "Epoch 230, Training Loss 0.002721397340764551\n",
      "Epoch 230, Training Loss 0.0028053365535843557\n",
      "Epoch 230, Training Loss 0.002829414355280378\n",
      "Epoch 230, Training Loss 0.003036758506341892\n",
      "Epoch 230, Training Loss 0.0030960041369833147\n",
      "Epoch 230, Training Loss 0.0031681723089988733\n",
      "Epoch 230, Training Loss 0.003217941653840911\n",
      "Epoch 230, Training Loss 0.0033084422366603103\n",
      "Epoch 230, Training Loss 0.0033829498290539246\n",
      "Epoch 230, Training Loss 0.0034061405471885755\n",
      "Epoch 230, Training Loss 0.0035524368798241133\n",
      "Epoch 230, Training Loss 0.0036051028427165335\n",
      "Epoch 230, Training Loss 0.003654530885226815\n",
      "Epoch 230, Training Loss 0.0037711097291954184\n",
      "Epoch 230, Training Loss 0.0038194153017228672\n",
      "Epoch 230, Training Loss 0.0038743032049625885\n",
      "Epoch 230, Training Loss 0.003919438641194416\n",
      "Epoch 230, Training Loss 0.003959430756804812\n",
      "Epoch 230, Training Loss 0.0039929388236263985\n",
      "Epoch 230, Training Loss 0.004109772083008914\n",
      "Epoch 230, Training Loss 0.004146034935313036\n",
      "Epoch 230, Training Loss 0.00419183882772732\n",
      "Epoch 230, Training Loss 0.00425745118909594\n",
      "Epoch 230, Training Loss 0.00431556427432105\n",
      "Epoch 230, Training Loss 0.004359249153014872\n",
      "Epoch 230, Training Loss 0.004428785655152081\n",
      "Epoch 230, Training Loss 0.004513982968057132\n",
      "Epoch 230, Training Loss 0.004565775007381082\n",
      "Epoch 230, Training Loss 0.00462674270824665\n",
      "Epoch 230, Training Loss 0.0046882028524737685\n",
      "Epoch 230, Training Loss 0.0047152783814579475\n",
      "Epoch 230, Training Loss 0.004758702170656389\n",
      "Epoch 230, Training Loss 0.0047788057807366105\n",
      "Epoch 230, Training Loss 0.004841527421637188\n",
      "Epoch 230, Training Loss 0.004951637351404295\n",
      "Epoch 230, Training Loss 0.0050125664750309395\n",
      "Epoch 230, Training Loss 0.005077909510296858\n",
      "Epoch 230, Training Loss 0.005163154184884961\n",
      "Epoch 230, Training Loss 0.0052712016853282365\n",
      "Epoch 230, Training Loss 0.005319956045769288\n",
      "Epoch 230, Training Loss 0.005451036492110137\n",
      "Epoch 230, Training Loss 0.005503526161117551\n",
      "Epoch 230, Training Loss 0.0055781576741972694\n",
      "Epoch 230, Training Loss 0.005705132078893883\n",
      "Epoch 230, Training Loss 0.0057677659134635385\n",
      "Epoch 230, Training Loss 0.005844269985156824\n",
      "Epoch 230, Training Loss 0.005873626585135146\n",
      "Epoch 230, Training Loss 0.00591412334419463\n",
      "Epoch 230, Training Loss 0.006074524321414702\n",
      "Epoch 230, Training Loss 0.006132742243549784\n",
      "Epoch 230, Training Loss 0.006161746008040579\n",
      "Epoch 230, Training Loss 0.006221604625196637\n",
      "Epoch 230, Training Loss 0.006289451865388838\n",
      "Epoch 230, Training Loss 0.0064075958838834025\n",
      "Epoch 230, Training Loss 0.006506172171972521\n",
      "Epoch 230, Training Loss 0.006532209433491349\n",
      "Epoch 230, Training Loss 0.006586687616727618\n",
      "Epoch 230, Training Loss 0.006646620320475391\n",
      "Epoch 230, Training Loss 0.006672349702471586\n",
      "Epoch 230, Training Loss 0.006682993415409647\n",
      "Epoch 230, Training Loss 0.006768477632833259\n",
      "Epoch 230, Training Loss 0.00680158752233476\n",
      "Epoch 230, Training Loss 0.006903799493675647\n",
      "Epoch 230, Training Loss 0.00705531658723836\n",
      "Epoch 230, Training Loss 0.007174709587908157\n",
      "Epoch 230, Training Loss 0.00722318001167701\n",
      "Epoch 230, Training Loss 0.007290062806604768\n",
      "Epoch 230, Training Loss 0.00737224671217944\n",
      "Epoch 230, Training Loss 0.007482486617420335\n",
      "Epoch 230, Training Loss 0.007532261845553318\n",
      "Epoch 230, Training Loss 0.00759815077876191\n",
      "Epoch 230, Training Loss 0.007623661793006198\n",
      "Epoch 230, Training Loss 0.0077116639350953\n",
      "Epoch 230, Training Loss 0.007834675126349376\n",
      "Epoch 230, Training Loss 0.007921705444050414\n",
      "Epoch 230, Training Loss 0.007935324272907832\n",
      "Epoch 230, Training Loss 0.007967535559983585\n",
      "Epoch 230, Training Loss 0.00803016152475839\n",
      "Epoch 230, Training Loss 0.00809614074147304\n",
      "Epoch 230, Training Loss 0.008186729416451263\n",
      "Epoch 230, Training Loss 0.00824574536531973\n",
      "Epoch 230, Training Loss 0.008263556309320661\n",
      "Epoch 230, Training Loss 0.008421153714999442\n",
      "Epoch 230, Training Loss 0.008449903609531234\n",
      "Epoch 230, Training Loss 0.008485755655685883\n",
      "Epoch 230, Training Loss 0.008536912490139761\n",
      "Epoch 230, Training Loss 0.008646989330563628\n",
      "Epoch 230, Training Loss 0.00870275662144851\n",
      "Epoch 230, Training Loss 0.008748964140968173\n",
      "Epoch 230, Training Loss 0.008769205031806932\n",
      "Epoch 230, Training Loss 0.008805557872261614\n",
      "Epoch 230, Training Loss 0.008864217508069771\n",
      "Epoch 230, Training Loss 0.008919167173954913\n",
      "Epoch 230, Training Loss 0.00895631085971699\n",
      "Epoch 230, Training Loss 0.008998752820550862\n",
      "Epoch 230, Training Loss 0.009084366003523017\n",
      "Epoch 230, Training Loss 0.009129465573117175\n",
      "Epoch 230, Training Loss 0.009186727920180316\n",
      "Epoch 230, Training Loss 0.009238257645712712\n",
      "Epoch 230, Training Loss 0.009302772647079528\n",
      "Epoch 230, Training Loss 0.00938836394516213\n",
      "Epoch 230, Training Loss 0.009540038139266354\n",
      "Epoch 230, Training Loss 0.00961398169793703\n",
      "Epoch 230, Training Loss 0.009719104502502534\n",
      "Epoch 230, Training Loss 0.009880719638532003\n",
      "Epoch 230, Training Loss 0.009930627371715692\n",
      "Epoch 230, Training Loss 0.010028491872112693\n",
      "Epoch 230, Training Loss 0.010090990876898055\n",
      "Epoch 230, Training Loss 0.010136642913712794\n",
      "Epoch 230, Training Loss 0.01017415829007621\n",
      "Epoch 230, Training Loss 0.010229231586055758\n",
      "Epoch 230, Training Loss 0.010290246391359269\n",
      "Epoch 230, Training Loss 0.010306693804081139\n",
      "Epoch 230, Training Loss 0.010416318569570551\n",
      "Epoch 230, Training Loss 0.010511089497438782\n",
      "Epoch 230, Training Loss 0.010536167549107538\n",
      "Epoch 230, Training Loss 0.010626817353146002\n",
      "Epoch 230, Training Loss 0.01069854260148371\n",
      "Epoch 230, Training Loss 0.010742167037104249\n",
      "Epoch 230, Training Loss 0.010788443869889697\n",
      "Epoch 230, Training Loss 0.010808501118684517\n",
      "Epoch 230, Training Loss 0.010884234698398797\n",
      "Epoch 230, Training Loss 0.011112560713401689\n",
      "Epoch 230, Training Loss 0.011204647745870416\n",
      "Epoch 230, Training Loss 0.011252940132204072\n",
      "Epoch 230, Training Loss 0.01140852620505997\n",
      "Epoch 230, Training Loss 0.011490444509822237\n",
      "Epoch 230, Training Loss 0.011545728780138676\n",
      "Epoch 230, Training Loss 0.011583485490525776\n",
      "Epoch 230, Training Loss 0.011621355067204941\n",
      "Epoch 230, Training Loss 0.011683470443310336\n",
      "Epoch 230, Training Loss 0.01173290136791861\n",
      "Epoch 230, Training Loss 0.011840006065033281\n",
      "Epoch 230, Training Loss 0.011894723371890804\n",
      "Epoch 230, Training Loss 0.01212305830472418\n",
      "Epoch 230, Training Loss 0.01219575178078221\n",
      "Epoch 230, Training Loss 0.012212115492376373\n",
      "Epoch 230, Training Loss 0.01232639166867108\n",
      "Epoch 230, Training Loss 0.012447707224017977\n",
      "Epoch 230, Training Loss 0.012505977409784598\n",
      "Epoch 230, Training Loss 0.012590886655804294\n",
      "Epoch 230, Training Loss 0.0126713622918786\n",
      "Epoch 230, Training Loss 0.012712632082021603\n",
      "Epoch 230, Training Loss 0.012799089543445184\n",
      "Epoch 230, Training Loss 0.012884065654138318\n",
      "Epoch 230, Training Loss 0.012946729333904545\n",
      "Epoch 230, Training Loss 0.012968647832055683\n",
      "Epoch 230, Training Loss 0.013050701047586816\n",
      "Epoch 230, Training Loss 0.013093004605787642\n",
      "Epoch 230, Training Loss 0.013301530636161032\n",
      "Epoch 230, Training Loss 0.013386481889354452\n",
      "Epoch 230, Training Loss 0.013531227300748648\n",
      "Epoch 230, Training Loss 0.013599502344322783\n",
      "Epoch 230, Training Loss 0.013675186151872054\n",
      "Epoch 230, Training Loss 0.013716281153490323\n",
      "Epoch 230, Training Loss 0.013782337034011589\n",
      "Epoch 230, Training Loss 0.013903391123523035\n",
      "Epoch 230, Training Loss 0.0139436824342517\n",
      "Epoch 230, Training Loss 0.014000640326963208\n",
      "Epoch 230, Training Loss 0.014091619995453625\n",
      "Epoch 230, Training Loss 0.014220018442386708\n",
      "Epoch 230, Training Loss 0.0143287757297268\n",
      "Epoch 230, Training Loss 0.014345925414453611\n",
      "Epoch 230, Training Loss 0.014479929156115522\n",
      "Epoch 230, Training Loss 0.014595808741181632\n",
      "Epoch 230, Training Loss 0.01467701855480023\n",
      "Epoch 230, Training Loss 0.014720091933284498\n",
      "Epoch 230, Training Loss 0.014752779883639815\n",
      "Epoch 230, Training Loss 0.014819413107936568\n",
      "Epoch 230, Training Loss 0.014888994866157966\n",
      "Epoch 230, Training Loss 0.014947329044027631\n",
      "Epoch 230, Training Loss 0.01499351419275031\n",
      "Epoch 230, Training Loss 0.015030744005604398\n",
      "Epoch 230, Training Loss 0.015054889665225812\n",
      "Epoch 230, Training Loss 0.015095017982594421\n",
      "Epoch 230, Training Loss 0.015163914154014548\n",
      "Epoch 230, Training Loss 0.015286918856975291\n",
      "Epoch 230, Training Loss 0.015369889208489595\n",
      "Epoch 230, Training Loss 0.015406441290045867\n",
      "Epoch 230, Training Loss 0.015435088277006011\n",
      "Epoch 230, Training Loss 0.015542183456051609\n",
      "Epoch 230, Training Loss 0.01571105378072547\n",
      "Epoch 230, Training Loss 0.015774978880944382\n",
      "Epoch 230, Training Loss 0.015807427726733637\n",
      "Epoch 230, Training Loss 0.015883980860785983\n",
      "Epoch 230, Training Loss 0.01596551860475441\n",
      "Epoch 230, Training Loss 0.01601463782450999\n",
      "Epoch 230, Training Loss 0.016094274489004214\n",
      "Epoch 230, Training Loss 0.01621557444767536\n",
      "Epoch 230, Training Loss 0.016306602773601974\n",
      "Epoch 230, Training Loss 0.016391835643497804\n",
      "Epoch 230, Training Loss 0.01648445874619324\n",
      "Epoch 230, Training Loss 0.016517434298963574\n",
      "Epoch 230, Training Loss 0.01671189732987748\n",
      "Epoch 230, Training Loss 0.016935372712028683\n",
      "Epoch 230, Training Loss 0.017116787146700695\n",
      "Epoch 230, Training Loss 0.017161448737439675\n",
      "Epoch 230, Training Loss 0.0172322394095285\n",
      "Epoch 230, Training Loss 0.017284445937418038\n",
      "Epoch 230, Training Loss 0.017400366081463177\n",
      "Epoch 230, Training Loss 0.017449618363633866\n",
      "Epoch 230, Training Loss 0.017511523146506235\n",
      "Epoch 230, Training Loss 0.0176563923118536\n",
      "Epoch 230, Training Loss 0.017748648411525257\n",
      "Epoch 230, Training Loss 0.017801579017706615\n",
      "Epoch 230, Training Loss 0.017852309439807673\n",
      "Epoch 230, Training Loss 0.01789433344403077\n",
      "Epoch 230, Training Loss 0.017957045509696692\n",
      "Epoch 230, Training Loss 0.01800846627763356\n",
      "Epoch 230, Training Loss 0.01805775678928589\n",
      "Epoch 230, Training Loss 0.018118241159936124\n",
      "Epoch 230, Training Loss 0.0181454847080876\n",
      "Epoch 230, Training Loss 0.0182994436008184\n",
      "Epoch 230, Training Loss 0.018423288047094557\n",
      "Epoch 230, Training Loss 0.018444941351738048\n",
      "Epoch 230, Training Loss 0.01855163167461829\n",
      "Epoch 230, Training Loss 0.018657253934856493\n",
      "Epoch 230, Training Loss 0.01869231135086597\n",
      "Epoch 230, Training Loss 0.01880489562607139\n",
      "Epoch 230, Training Loss 0.018981555885280412\n",
      "Epoch 230, Training Loss 0.019050624653878037\n",
      "Epoch 230, Training Loss 0.01917870863295539\n",
      "Epoch 230, Training Loss 0.019241679072751643\n",
      "Epoch 230, Training Loss 0.01932553046971293\n",
      "Epoch 230, Training Loss 0.019358325948524276\n",
      "Epoch 230, Training Loss 0.0194346948140932\n",
      "Epoch 230, Training Loss 0.01949394683775199\n",
      "Epoch 230, Training Loss 0.0195292537434079\n",
      "Epoch 230, Training Loss 0.01958795556265032\n",
      "Epoch 230, Training Loss 0.019622316794789128\n",
      "Epoch 230, Training Loss 0.01970862971304361\n",
      "Epoch 230, Training Loss 0.019788749969762078\n",
      "Epoch 230, Training Loss 0.019826802411986054\n",
      "Epoch 230, Training Loss 0.019912398778750083\n",
      "Epoch 230, Training Loss 0.019987516118275463\n",
      "Epoch 230, Training Loss 0.020049659152993993\n",
      "Epoch 230, Training Loss 0.0200962191840629\n",
      "Epoch 230, Training Loss 0.020168713885871575\n",
      "Epoch 230, Training Loss 0.020202548146638494\n",
      "Epoch 230, Training Loss 0.020254306034292176\n",
      "Epoch 230, Training Loss 0.020321911606994814\n",
      "Epoch 230, Training Loss 0.020390807306798903\n",
      "Epoch 230, Training Loss 0.020450548967942024\n",
      "Epoch 230, Training Loss 0.02047231210433804\n",
      "Epoch 230, Training Loss 0.02050053046377914\n",
      "Epoch 230, Training Loss 0.0206076792736664\n",
      "Epoch 230, Training Loss 0.02066679181330992\n",
      "Epoch 230, Training Loss 0.020777015464470895\n",
      "Epoch 230, Training Loss 0.020856063871327643\n",
      "Epoch 230, Training Loss 0.02091618882292105\n",
      "Epoch 230, Training Loss 0.020987846406267198\n",
      "Epoch 230, Training Loss 0.021059464550126927\n",
      "Epoch 230, Training Loss 0.02115303319057121\n",
      "Epoch 230, Training Loss 0.021287910612847877\n",
      "Epoch 230, Training Loss 0.02134230888813086\n",
      "Epoch 230, Training Loss 0.021422819080798294\n",
      "Epoch 230, Training Loss 0.021528923408845272\n",
      "Epoch 230, Training Loss 0.02162540999010129\n",
      "Epoch 230, Training Loss 0.02165588902195206\n",
      "Epoch 230, Training Loss 0.021676090635273537\n",
      "Epoch 230, Training Loss 0.021739874822337687\n",
      "Epoch 230, Training Loss 0.021849779832555587\n",
      "Epoch 230, Training Loss 0.021924410430986023\n",
      "Epoch 230, Training Loss 0.022002449236533907\n",
      "Epoch 230, Training Loss 0.02216073113334034\n",
      "Epoch 230, Training Loss 0.02226442100642168\n",
      "Epoch 230, Training Loss 0.022368263216246197\n",
      "Epoch 230, Training Loss 0.02246019229068971\n",
      "Epoch 230, Training Loss 0.022512086556362144\n",
      "Epoch 230, Training Loss 0.022586643901865575\n",
      "Epoch 230, Training Loss 0.022662651714394844\n",
      "Epoch 230, Training Loss 0.022744959936050885\n",
      "Epoch 230, Training Loss 0.022877003165090556\n",
      "Epoch 230, Training Loss 0.02303279925475988\n",
      "Epoch 230, Training Loss 0.023078268356954732\n",
      "Epoch 230, Training Loss 0.023114545226735457\n",
      "Epoch 230, Training Loss 0.023136624193433528\n",
      "Epoch 230, Training Loss 0.023182283075826475\n",
      "Epoch 230, Training Loss 0.023200631785966323\n",
      "Epoch 230, Training Loss 0.023301326401788943\n",
      "Epoch 230, Training Loss 0.023335502365046678\n",
      "Epoch 230, Training Loss 0.023391144072679837\n",
      "Epoch 230, Training Loss 0.023430384334672212\n",
      "Epoch 230, Training Loss 0.02347060233530829\n",
      "Epoch 230, Training Loss 0.02355462530523043\n",
      "Epoch 230, Training Loss 0.023729197401077012\n",
      "Epoch 230, Training Loss 0.02377613484406906\n",
      "Epoch 230, Training Loss 0.023873776284372792\n",
      "Epoch 230, Training Loss 0.023974155870211476\n",
      "Epoch 230, Training Loss 0.02400516618943542\n",
      "Epoch 230, Training Loss 0.024052744749171275\n",
      "Epoch 230, Training Loss 0.024074312384047395\n",
      "Epoch 230, Training Loss 0.024174827738138643\n",
      "Epoch 230, Training Loss 0.024270327412821067\n",
      "Epoch 230, Training Loss 0.024349707861185607\n",
      "Epoch 230, Training Loss 0.02438733613718768\n",
      "Epoch 230, Training Loss 0.024481359977618126\n",
      "Epoch 230, Training Loss 0.024630664553626648\n",
      "Epoch 230, Training Loss 0.024677363301739287\n",
      "Epoch 230, Training Loss 0.024897279288105267\n",
      "Epoch 230, Training Loss 0.024960210350344476\n",
      "Epoch 230, Training Loss 0.025070870117715482\n",
      "Epoch 230, Training Loss 0.0251054482741753\n",
      "Epoch 230, Training Loss 0.02515622598173864\n",
      "Epoch 230, Training Loss 0.025283296405554503\n",
      "Epoch 230, Training Loss 0.025331487080029896\n",
      "Epoch 230, Training Loss 0.025412585401712248\n",
      "Epoch 230, Training Loss 0.02549143962662124\n",
      "Epoch 230, Training Loss 0.025583044800888317\n",
      "Epoch 230, Training Loss 0.02570386852263985\n",
      "Epoch 230, Training Loss 0.025769709941723843\n",
      "Epoch 230, Training Loss 0.025825698334780878\n",
      "Epoch 230, Training Loss 0.02587143919737938\n",
      "Epoch 230, Training Loss 0.025987558115673874\n",
      "Epoch 230, Training Loss 0.026027610398057247\n",
      "Epoch 230, Training Loss 0.026069612784163497\n",
      "Epoch 230, Training Loss 0.026129535750707474\n",
      "Epoch 230, Training Loss 0.02616877650813488\n",
      "Epoch 230, Training Loss 0.026201124313761436\n",
      "Epoch 230, Training Loss 0.02624418976528527\n",
      "Epoch 230, Training Loss 0.026318353501951223\n",
      "Epoch 230, Training Loss 0.026376445002406074\n",
      "Epoch 230, Training Loss 0.026465342509919953\n",
      "Epoch 230, Training Loss 0.026534451942300172\n",
      "Epoch 230, Training Loss 0.02666442368127158\n",
      "Epoch 230, Training Loss 0.026714861134538793\n",
      "Epoch 230, Training Loss 0.026801641486213564\n",
      "Epoch 230, Training Loss 0.02684862571923286\n",
      "Epoch 230, Training Loss 0.02702573941582266\n",
      "Epoch 230, Training Loss 0.027073688789620957\n",
      "Epoch 230, Training Loss 0.027101265320130397\n",
      "Epoch 230, Training Loss 0.02728659794082308\n",
      "Epoch 230, Training Loss 0.027376848290843504\n",
      "Epoch 230, Training Loss 0.0274058440426255\n",
      "Epoch 230, Training Loss 0.02742743454135173\n",
      "Epoch 230, Training Loss 0.027536835313043403\n",
      "Epoch 230, Training Loss 0.027596179482853398\n",
      "Epoch 230, Training Loss 0.02769101676090485\n",
      "Epoch 230, Training Loss 0.027851476370954833\n",
      "Epoch 230, Training Loss 0.02789381833966164\n",
      "Epoch 230, Training Loss 0.02802566620533157\n",
      "Epoch 230, Training Loss 0.028149303207245402\n",
      "Epoch 230, Training Loss 0.028330172377559915\n",
      "Epoch 230, Training Loss 0.028418522162596358\n",
      "Epoch 230, Training Loss 0.028509985108900328\n",
      "Epoch 230, Training Loss 0.02854482813492951\n",
      "Epoch 230, Training Loss 0.028567316228538142\n",
      "Epoch 230, Training Loss 0.02865245492766847\n",
      "Epoch 230, Training Loss 0.028750275858365894\n",
      "Epoch 230, Training Loss 0.028815934052238302\n",
      "Epoch 230, Training Loss 0.028954557477808594\n",
      "Epoch 230, Training Loss 0.028980881505695832\n",
      "Epoch 230, Training Loss 0.02905030967905889\n",
      "Epoch 230, Training Loss 0.02925780484252764\n",
      "Epoch 230, Training Loss 0.02937487877852967\n",
      "Epoch 230, Training Loss 0.02945382772204097\n",
      "Epoch 230, Training Loss 0.029526853515907094\n",
      "Epoch 230, Training Loss 0.029609073855011436\n",
      "Epoch 230, Training Loss 0.02971352197234626\n",
      "Epoch 230, Training Loss 0.02975137550931643\n",
      "Epoch 230, Training Loss 0.029794736954686055\n",
      "Epoch 230, Training Loss 0.02987212519211423\n",
      "Epoch 230, Training Loss 0.029988857154088937\n",
      "Epoch 230, Training Loss 0.030140328210304535\n",
      "Epoch 230, Training Loss 0.03037028752214959\n",
      "Epoch 230, Training Loss 0.030596648699001354\n",
      "Epoch 230, Training Loss 0.030779637302250584\n",
      "Epoch 230, Training Loss 0.030803571851766857\n",
      "Epoch 230, Training Loss 0.030887546164252797\n",
      "Epoch 230, Training Loss 0.030999546750303347\n",
      "Epoch 230, Training Loss 0.031140922955797075\n",
      "Epoch 230, Training Loss 0.031205943762979773\n",
      "Epoch 230, Training Loss 0.031235335791326316\n",
      "Epoch 230, Training Loss 0.03126367067089281\n",
      "Epoch 230, Training Loss 0.03134841931974301\n",
      "Epoch 230, Training Loss 0.03139795941631774\n",
      "Epoch 230, Training Loss 0.03153791531439289\n",
      "Epoch 230, Training Loss 0.031720644119136096\n",
      "Epoch 230, Training Loss 0.03179019479357335\n",
      "Epoch 230, Training Loss 0.03193801535112436\n",
      "Epoch 230, Training Loss 0.03198538275549898\n",
      "Epoch 230, Training Loss 0.03219644179272339\n",
      "Epoch 230, Training Loss 0.03231316347442129\n",
      "Epoch 230, Training Loss 0.032410580577933806\n",
      "Epoch 230, Training Loss 0.03246215622171836\n",
      "Epoch 230, Training Loss 0.03250247143718707\n",
      "Epoch 230, Training Loss 0.03252554523503727\n",
      "Epoch 230, Training Loss 0.03272939018566933\n",
      "Epoch 230, Training Loss 0.0327709531494895\n",
      "Epoch 230, Training Loss 0.03284184089349702\n",
      "Epoch 230, Training Loss 0.03293722375031666\n",
      "Epoch 230, Training Loss 0.03301370836665754\n",
      "Epoch 230, Training Loss 0.03306873256216764\n",
      "Epoch 230, Training Loss 0.033104110658025876\n",
      "Epoch 230, Training Loss 0.033166903427199405\n",
      "Epoch 230, Training Loss 0.03319950484077606\n",
      "Epoch 230, Training Loss 0.03326618283524957\n",
      "Epoch 230, Training Loss 0.03333394403171623\n",
      "Epoch 230, Training Loss 0.03336683939070539\n",
      "Epoch 230, Training Loss 0.03342105921410272\n",
      "Epoch 230, Training Loss 0.033504060739913334\n",
      "Epoch 230, Training Loss 0.0335842528442383\n",
      "Epoch 230, Training Loss 0.03365908839556453\n",
      "Epoch 230, Training Loss 0.03376820052871504\n",
      "Epoch 230, Training Loss 0.03393607765860151\n",
      "Epoch 230, Training Loss 0.034065568501658525\n",
      "Epoch 230, Training Loss 0.034076784406324175\n",
      "Epoch 230, Training Loss 0.03415275470870535\n",
      "Epoch 230, Training Loss 0.03423099319362427\n",
      "Epoch 230, Training Loss 0.03426187377794624\n",
      "Epoch 230, Training Loss 0.03431573039983087\n",
      "Epoch 230, Training Loss 0.03436830907088259\n",
      "Epoch 230, Training Loss 0.03440030460314982\n",
      "Epoch 230, Training Loss 0.03443901119348795\n",
      "Epoch 230, Training Loss 0.034476603517103986\n",
      "Epoch 230, Training Loss 0.034541968165722955\n",
      "Epoch 230, Training Loss 0.034646923217893866\n",
      "Epoch 230, Training Loss 0.03474297248246267\n",
      "Epoch 230, Training Loss 0.03480763155538255\n",
      "Epoch 230, Training Loss 0.034841606988931254\n",
      "Epoch 230, Training Loss 0.03492606519852453\n",
      "Epoch 230, Training Loss 0.03502715188447777\n",
      "Epoch 230, Training Loss 0.03519227588191971\n",
      "Epoch 230, Training Loss 0.03522482145663418\n",
      "Epoch 230, Training Loss 0.03535582070879619\n",
      "Epoch 230, Training Loss 0.035431178043717926\n",
      "Epoch 230, Training Loss 0.03556594478390406\n",
      "Epoch 230, Training Loss 0.035730914872549376\n",
      "Epoch 230, Training Loss 0.03590019095851028\n",
      "Epoch 230, Training Loss 0.03602132489880942\n",
      "Epoch 230, Training Loss 0.03605292099373191\n",
      "Epoch 230, Training Loss 0.036188178319398245\n",
      "Epoch 230, Training Loss 0.03627927515827253\n",
      "Epoch 230, Training Loss 0.03632717788972132\n",
      "Epoch 230, Training Loss 0.03641701587583022\n",
      "Epoch 230, Training Loss 0.03648194416528544\n",
      "Epoch 230, Training Loss 0.03653359027517497\n",
      "Epoch 230, Training Loss 0.03657442226748713\n",
      "Epoch 230, Training Loss 0.036676434988198836\n",
      "Epoch 230, Training Loss 0.036716551086424715\n",
      "Epoch 230, Training Loss 0.03680354746682641\n",
      "Epoch 230, Training Loss 0.037066703516980416\n",
      "Epoch 230, Training Loss 0.03725047174441006\n",
      "Epoch 230, Training Loss 0.037291181659626076\n",
      "Epoch 230, Training Loss 0.03733570503352015\n",
      "Epoch 230, Training Loss 0.037441519131917324\n",
      "Epoch 230, Training Loss 0.0374961100694964\n",
      "Epoch 230, Training Loss 0.03756619644734789\n",
      "Epoch 230, Training Loss 0.03759431711319462\n",
      "Epoch 230, Training Loss 0.03762636135529984\n",
      "Epoch 230, Training Loss 0.037652497668095565\n",
      "Epoch 230, Training Loss 0.03773350173326404\n",
      "Epoch 230, Training Loss 0.03782285462178842\n",
      "Epoch 230, Training Loss 0.037853067974224115\n",
      "Epoch 230, Training Loss 0.03790819544888213\n",
      "Epoch 230, Training Loss 0.03794189487510096\n",
      "Epoch 230, Training Loss 0.03806383041021845\n",
      "Epoch 230, Training Loss 0.03823781825478196\n",
      "Epoch 230, Training Loss 0.038318089752569985\n",
      "Epoch 230, Training Loss 0.03840281516361191\n",
      "Epoch 230, Training Loss 0.038455082977767036\n",
      "Epoch 230, Training Loss 0.03850482321580124\n",
      "Epoch 230, Training Loss 0.03854625399850899\n",
      "Epoch 230, Training Loss 0.03862015302519283\n",
      "Epoch 230, Training Loss 0.03878143237060522\n",
      "Epoch 230, Training Loss 0.03883948145896349\n",
      "Epoch 230, Training Loss 0.03891415060128626\n",
      "Epoch 230, Training Loss 0.038971771181220444\n",
      "Epoch 230, Training Loss 0.039062985190478586\n",
      "Epoch 230, Training Loss 0.03914083147188053\n",
      "Epoch 230, Training Loss 0.03918013084784645\n",
      "Epoch 230, Training Loss 0.03921526226708118\n",
      "Epoch 230, Training Loss 0.039255495064074886\n",
      "Epoch 230, Training Loss 0.039305289180668265\n",
      "Epoch 230, Training Loss 0.03952766578320576\n",
      "Epoch 230, Training Loss 0.039599547936769244\n",
      "Epoch 230, Training Loss 0.0397106895718695\n",
      "Epoch 230, Training Loss 0.03982926212260714\n",
      "Epoch 230, Training Loss 0.03987708288814177\n",
      "Epoch 230, Training Loss 0.03995783926437959\n",
      "Epoch 230, Training Loss 0.04001141355022826\n",
      "Epoch 230, Training Loss 0.04018168372657064\n",
      "Epoch 230, Training Loss 0.04028271497143885\n",
      "Epoch 230, Training Loss 0.04034349285399594\n",
      "Epoch 230, Training Loss 0.040358031675924576\n",
      "Epoch 230, Training Loss 0.04044309348734977\n",
      "Epoch 230, Training Loss 0.04053738677774168\n",
      "Epoch 230, Training Loss 0.04056225692057777\n",
      "Epoch 230, Training Loss 0.040632623419299\n",
      "Epoch 230, Training Loss 0.040711620172290394\n",
      "Epoch 230, Training Loss 0.04081775531258501\n",
      "Epoch 230, Training Loss 0.04105291101376495\n",
      "Epoch 230, Training Loss 0.04114913598627157\n",
      "Epoch 230, Training Loss 0.04127092737837902\n",
      "Epoch 230, Training Loss 0.041360187048897566\n",
      "Epoch 230, Training Loss 0.0413896569582012\n",
      "Epoch 230, Training Loss 0.041421738870041754\n",
      "Epoch 230, Training Loss 0.041501147858322124\n",
      "Epoch 230, Training Loss 0.0415367599781555\n",
      "Epoch 230, Training Loss 0.041552916539551885\n",
      "Epoch 230, Training Loss 0.04156844014101817\n",
      "Epoch 230, Training Loss 0.041591244407922334\n",
      "Epoch 230, Training Loss 0.04165547181878363\n",
      "Epoch 230, Training Loss 0.04171988399356337\n",
      "Epoch 230, Training Loss 0.04180259014482198\n",
      "Epoch 230, Training Loss 0.04182799648174354\n",
      "Epoch 230, Training Loss 0.04186520120843559\n",
      "Epoch 230, Training Loss 0.04189741124025048\n",
      "Epoch 230, Training Loss 0.04191530153364934\n",
      "Epoch 230, Training Loss 0.041950354967123404\n",
      "Epoch 230, Training Loss 0.04205852346804441\n",
      "Epoch 230, Training Loss 0.04220988280366144\n",
      "Epoch 230, Training Loss 0.042310467147080186\n",
      "Epoch 230, Training Loss 0.042338648236945005\n",
      "Epoch 230, Training Loss 0.04238459417629806\n",
      "Epoch 230, Training Loss 0.04241239898087804\n",
      "Epoch 230, Training Loss 0.04249052977537179\n",
      "Epoch 230, Training Loss 0.04252755798428031\n",
      "Epoch 230, Training Loss 0.04263105522841215\n",
      "Epoch 230, Training Loss 0.04274935102152169\n",
      "Epoch 230, Training Loss 0.042797679465044945\n",
      "Epoch 230, Training Loss 0.04285727860286946\n",
      "Epoch 230, Training Loss 0.04290253371047928\n",
      "Epoch 230, Training Loss 0.04296280905994994\n",
      "Epoch 230, Training Loss 0.0430738398986285\n",
      "Epoch 230, Training Loss 0.04313697700943712\n",
      "Epoch 230, Training Loss 0.04330780407380494\n",
      "Epoch 230, Training Loss 0.04342219122754567\n",
      "Epoch 230, Training Loss 0.043559721532894675\n",
      "Epoch 230, Training Loss 0.04367830600265576\n",
      "Epoch 230, Training Loss 0.04374701555703988\n",
      "Epoch 230, Training Loss 0.04379154216555302\n",
      "Epoch 230, Training Loss 0.043878959522813635\n",
      "Epoch 230, Training Loss 0.04393166131423334\n",
      "Epoch 230, Training Loss 0.04402663313147738\n",
      "Epoch 230, Training Loss 0.04417473811637181\n",
      "Epoch 230, Training Loss 0.04432381397289464\n",
      "Epoch 230, Training Loss 0.04445979980718526\n",
      "Epoch 230, Training Loss 0.044495729934376525\n",
      "Epoch 230, Training Loss 0.04459493572030531\n",
      "Epoch 230, Training Loss 0.0447098948728398\n",
      "Epoch 230, Training Loss 0.04476593907855813\n",
      "Epoch 230, Training Loss 0.044815410228679554\n",
      "Epoch 230, Training Loss 0.04485368434711338\n",
      "Epoch 230, Training Loss 0.0449099816296183\n",
      "Epoch 230, Training Loss 0.04493598109039733\n",
      "Epoch 230, Training Loss 0.0449956699400721\n",
      "Epoch 230, Training Loss 0.04502990379777101\n",
      "Epoch 230, Training Loss 0.045226638912773495\n",
      "Epoch 230, Training Loss 0.04534968463203791\n",
      "Epoch 230, Training Loss 0.04545486928023341\n",
      "Epoch 230, Training Loss 0.04554882607496608\n",
      "Epoch 230, Training Loss 0.045648503724647606\n",
      "Epoch 230, Training Loss 0.04569894103976467\n",
      "Epoch 230, Training Loss 0.04579191390053391\n",
      "Epoch 230, Training Loss 0.04583461614101744\n",
      "Epoch 230, Training Loss 0.0459503029446925\n",
      "Epoch 230, Training Loss 0.0460799355488604\n",
      "Epoch 230, Training Loss 0.04615117650469551\n",
      "Epoch 230, Training Loss 0.04621297689369115\n",
      "Epoch 230, Training Loss 0.046314512948741386\n",
      "Epoch 230, Training Loss 0.046408280477766184\n",
      "Epoch 230, Training Loss 0.04645590305499866\n",
      "Epoch 230, Training Loss 0.04647444589349353\n",
      "Epoch 230, Training Loss 0.04657770560511276\n",
      "Epoch 230, Training Loss 0.04665053963704068\n",
      "Epoch 230, Training Loss 0.0467475442813657\n",
      "Epoch 230, Training Loss 0.04679041362636725\n",
      "Epoch 230, Training Loss 0.04687835673546738\n",
      "Epoch 230, Training Loss 0.04693958468739982\n",
      "Epoch 230, Training Loss 0.047001045045997863\n",
      "Epoch 230, Training Loss 0.04709037980350578\n",
      "Epoch 230, Training Loss 0.04720938916596801\n",
      "Epoch 230, Training Loss 0.04728873931181134\n",
      "Epoch 230, Training Loss 0.04737693557868262\n",
      "Epoch 230, Training Loss 0.04742064790161865\n",
      "Epoch 230, Training Loss 0.04748513282912657\n",
      "Epoch 230, Training Loss 0.04759693877590473\n",
      "Epoch 230, Training Loss 0.04767632082966931\n",
      "Epoch 230, Training Loss 0.04773495299265246\n",
      "Epoch 230, Training Loss 0.047808848484597925\n",
      "Epoch 230, Training Loss 0.047828971923988724\n",
      "Epoch 230, Training Loss 0.04793581048674558\n",
      "Epoch 230, Training Loss 0.04804084839089714\n",
      "Epoch 230, Training Loss 0.048196215062256895\n",
      "Epoch 230, Training Loss 0.0483158933953918\n",
      "Epoch 230, Training Loss 0.04836336388359861\n",
      "Epoch 230, Training Loss 0.04841994810396867\n",
      "Epoch 230, Training Loss 0.04856263309636194\n",
      "Epoch 230, Training Loss 0.0487088841443305\n",
      "Epoch 230, Training Loss 0.04881173718001341\n",
      "Epoch 230, Training Loss 0.048874600686828425\n",
      "Epoch 230, Training Loss 0.04891855848829269\n",
      "Epoch 230, Training Loss 0.04895392052776864\n",
      "Epoch 230, Training Loss 0.048986798648953514\n",
      "Epoch 230, Training Loss 0.04905541092181183\n",
      "Epoch 230, Training Loss 0.04907134957516285\n",
      "Epoch 230, Training Loss 0.04909817640290922\n",
      "Epoch 230, Training Loss 0.04915815081847522\n",
      "Epoch 230, Training Loss 0.04920855956985746\n",
      "Epoch 230, Training Loss 0.049277341486338305\n",
      "Epoch 230, Training Loss 0.04932851285990471\n",
      "Epoch 230, Training Loss 0.049381995252083484\n",
      "Epoch 230, Training Loss 0.04945794963861442\n",
      "Epoch 230, Training Loss 0.04963638753775517\n",
      "Epoch 230, Training Loss 0.04966762797940341\n",
      "Epoch 230, Training Loss 0.04980035055466854\n",
      "Epoch 230, Training Loss 0.04983238441407528\n",
      "Epoch 230, Training Loss 0.049892777224521505\n",
      "Epoch 230, Training Loss 0.04992703044229685\n",
      "Epoch 230, Training Loss 0.04997695084718411\n",
      "Epoch 230, Training Loss 0.05001135317899306\n",
      "Epoch 230, Training Loss 0.0500643287601945\n",
      "Epoch 230, Training Loss 0.05010619526848083\n",
      "Epoch 230, Training Loss 0.050123564661968774\n",
      "Epoch 230, Training Loss 0.050208161802976714\n",
      "Epoch 230, Training Loss 0.05032337818156613\n",
      "Epoch 230, Training Loss 0.05044709876908556\n",
      "Epoch 230, Training Loss 0.05062443680246658\n",
      "Epoch 230, Training Loss 0.05067440663176158\n",
      "Epoch 230, Training Loss 0.05073677455587194\n",
      "Epoch 230, Training Loss 0.05081170953243323\n",
      "Epoch 230, Training Loss 0.05085159564400306\n",
      "Epoch 230, Training Loss 0.05096707085881125\n",
      "Epoch 230, Training Loss 0.05104659086741183\n",
      "Epoch 230, Training Loss 0.05109572420230188\n",
      "Epoch 230, Training Loss 0.05117819359278321\n",
      "Epoch 230, Training Loss 0.051249765689768224\n",
      "Epoch 230, Training Loss 0.05130203212242183\n",
      "Epoch 230, Training Loss 0.051324043495585316\n",
      "Epoch 230, Training Loss 0.05148016405708688\n",
      "Epoch 230, Training Loss 0.051651058881722216\n",
      "Epoch 230, Training Loss 0.051791193974835566\n",
      "Epoch 230, Training Loss 0.051818065970535857\n",
      "Epoch 230, Training Loss 0.051898700250622334\n",
      "Epoch 230, Training Loss 0.05196557533296058\n",
      "Epoch 230, Training Loss 0.05211593497596929\n",
      "Epoch 230, Training Loss 0.052226559510287804\n",
      "Epoch 230, Training Loss 0.05228372246069867\n",
      "Epoch 230, Training Loss 0.05232525231135662\n",
      "Epoch 230, Training Loss 0.052526858600709214\n",
      "Epoch 230, Training Loss 0.052603487483918895\n",
      "Epoch 230, Training Loss 0.052702743165871445\n",
      "Epoch 230, Training Loss 0.05282381562458928\n",
      "Epoch 230, Training Loss 0.05289206803059372\n",
      "Epoch 230, Training Loss 0.053007895690257975\n",
      "Epoch 230, Training Loss 0.05306561739610322\n",
      "Epoch 230, Training Loss 0.05313668166742186\n",
      "Epoch 230, Training Loss 0.05318594680232046\n",
      "Epoch 230, Training Loss 0.05321118016096065\n",
      "Epoch 230, Training Loss 0.05326307360725978\n",
      "Epoch 230, Training Loss 0.05331548135799101\n",
      "Epoch 230, Training Loss 0.05333187462895385\n",
      "Epoch 230, Training Loss 0.05335305786817847\n",
      "Epoch 230, Training Loss 0.05346936059286794\n",
      "Epoch 230, Training Loss 0.05355533722507031\n",
      "Epoch 230, Training Loss 0.05365283319802807\n",
      "Epoch 230, Training Loss 0.05376201648327053\n",
      "Epoch 230, Training Loss 0.053857939316631506\n",
      "Epoch 230, Training Loss 0.054023010188194415\n",
      "Epoch 230, Training Loss 0.05405555642984065\n",
      "Epoch 230, Training Loss 0.05413358933304239\n",
      "Epoch 230, Training Loss 0.0541667738950828\n",
      "Epoch 230, Training Loss 0.05418269853691197\n",
      "Epoch 230, Training Loss 0.05420375965616625\n",
      "Epoch 230, Training Loss 0.05428179145416678\n",
      "Epoch 230, Training Loss 0.054337221434782916\n",
      "Epoch 230, Training Loss 0.05445374923941615\n",
      "Epoch 230, Training Loss 0.05452541162467102\n",
      "Epoch 230, Training Loss 0.05462330216398972\n",
      "Epoch 230, Training Loss 0.05473381848386524\n",
      "Epoch 230, Training Loss 0.05479599977664821\n",
      "Epoch 230, Training Loss 0.05489111083971761\n",
      "Epoch 230, Training Loss 0.05496244938672542\n",
      "Epoch 230, Training Loss 0.055117971205469365\n",
      "Epoch 230, Training Loss 0.05521664346503975\n",
      "Epoch 230, Training Loss 0.05525401499374862\n",
      "Epoch 230, Training Loss 0.055373150673087526\n",
      "Epoch 230, Training Loss 0.055417561192599024\n",
      "Epoch 230, Training Loss 0.05552800004358601\n",
      "Epoch 230, Training Loss 0.0555643843536925\n",
      "Epoch 230, Training Loss 0.05562226176428635\n",
      "Epoch 230, Training Loss 0.05576075242279702\n",
      "Epoch 230, Training Loss 0.05592542119167002\n",
      "Epoch 230, Training Loss 0.056044177320140325\n",
      "Epoch 230, Training Loss 0.05616955943477085\n",
      "Epoch 230, Training Loss 0.056250267601369516\n",
      "Epoch 230, Training Loss 0.05635453955344189\n",
      "Epoch 230, Training Loss 0.05641381769700695\n",
      "Epoch 230, Training Loss 0.056458693606269254\n",
      "Epoch 230, Training Loss 0.05651281799649453\n",
      "Epoch 230, Training Loss 0.05655328256180486\n",
      "Epoch 230, Training Loss 0.056598371703444464\n",
      "Epoch 230, Training Loss 0.05662757894405357\n",
      "Epoch 230, Training Loss 0.056662410160626675\n",
      "Epoch 230, Training Loss 0.056725881580511096\n",
      "Epoch 230, Training Loss 0.05678331599120632\n",
      "Epoch 230, Training Loss 0.05686569209698864\n",
      "Epoch 230, Training Loss 0.05689303452492027\n",
      "Epoch 230, Training Loss 0.057021551037831304\n",
      "Epoch 230, Training Loss 0.05707297766047632\n",
      "Epoch 230, Training Loss 0.05713344189931479\n",
      "Epoch 230, Training Loss 0.057189913146207326\n",
      "Epoch 230, Training Loss 0.057257146799646295\n",
      "Epoch 230, Training Loss 0.057291991471567806\n",
      "Epoch 230, Training Loss 0.057345967788649414\n",
      "Epoch 230, Training Loss 0.05744618178485795\n",
      "Epoch 230, Training Loss 0.05762200172909576\n",
      "Epoch 230, Training Loss 0.05778562385336403\n",
      "Epoch 230, Training Loss 0.0580066598658843\n",
      "Epoch 230, Training Loss 0.05808888413035847\n",
      "Epoch 230, Training Loss 0.058123178092900976\n",
      "Epoch 230, Training Loss 0.05822763439444134\n",
      "Epoch 230, Training Loss 0.058294529351346255\n",
      "Epoch 230, Training Loss 0.05834222398579235\n",
      "Epoch 230, Training Loss 0.05844656088749123\n",
      "Epoch 230, Training Loss 0.05854481558108231\n",
      "Epoch 230, Training Loss 0.05865061208200843\n",
      "Epoch 230, Training Loss 0.05886486709437064\n",
      "Epoch 230, Training Loss 0.058927789981931905\n",
      "Epoch 230, Training Loss 0.059065882815643574\n",
      "Epoch 230, Training Loss 0.059123838809854765\n",
      "Epoch 230, Training Loss 0.05919984074862069\n",
      "Epoch 230, Training Loss 0.05933986048159354\n",
      "Epoch 230, Training Loss 0.05936746691684703\n",
      "Epoch 230, Training Loss 0.059424747171028115\n",
      "Epoch 230, Training Loss 0.059612861011520295\n",
      "Epoch 230, Training Loss 0.059732832775577484\n",
      "Epoch 230, Training Loss 0.05981669652983165\n",
      "Epoch 230, Training Loss 0.05997167039743584\n",
      "Epoch 230, Training Loss 0.060008403734373085\n",
      "Epoch 230, Training Loss 0.0600774882711432\n",
      "Epoch 230, Training Loss 0.06020695568584001\n",
      "Epoch 230, Training Loss 0.060350606319211096\n",
      "Epoch 230, Training Loss 0.060508556846205305\n",
      "Epoch 230, Training Loss 0.06060823219020844\n",
      "Epoch 230, Training Loss 0.06076204195699614\n",
      "Epoch 230, Training Loss 0.06081229680910935\n",
      "Epoch 230, Training Loss 0.06093342085023556\n",
      "Epoch 230, Training Loss 0.060976780247171895\n",
      "Epoch 230, Training Loss 0.06101069846988448\n",
      "Epoch 230, Training Loss 0.06111442432869845\n",
      "Epoch 230, Training Loss 0.061188911541562786\n",
      "Epoch 230, Training Loss 0.06123697359824691\n",
      "Epoch 230, Training Loss 0.06128296231174522\n",
      "Epoch 230, Training Loss 0.06146705238139043\n",
      "Epoch 230, Training Loss 0.06159086259501174\n",
      "Epoch 230, Training Loss 0.06165605591362357\n",
      "Epoch 230, Training Loss 0.061768526743735425\n",
      "Epoch 230, Training Loss 0.06183548570703477\n",
      "Epoch 240, Training Loss 0.00015813788718274793\n",
      "Epoch 240, Training Loss 0.00025740473547859874\n",
      "Epoch 240, Training Loss 0.0002924050264003332\n",
      "Epoch 240, Training Loss 0.0004771301675292537\n",
      "Epoch 240, Training Loss 0.0005572010712969639\n",
      "Epoch 240, Training Loss 0.0005854415728727265\n",
      "Epoch 240, Training Loss 0.0006554327081040958\n",
      "Epoch 240, Training Loss 0.000725286717400374\n",
      "Epoch 240, Training Loss 0.0008260927108280799\n",
      "Epoch 240, Training Loss 0.0008445176369774982\n",
      "Epoch 240, Training Loss 0.0009323079424822117\n",
      "Epoch 240, Training Loss 0.0009876929433144572\n",
      "Epoch 240, Training Loss 0.0010597821601364009\n",
      "Epoch 240, Training Loss 0.0011029253473214787\n",
      "Epoch 240, Training Loss 0.0011583764077452443\n",
      "Epoch 240, Training Loss 0.001254761405765553\n",
      "Epoch 240, Training Loss 0.0012947910601068336\n",
      "Epoch 240, Training Loss 0.001389381239938614\n",
      "Epoch 240, Training Loss 0.0014834726310294607\n",
      "Epoch 240, Training Loss 0.0015362916404710097\n",
      "Epoch 240, Training Loss 0.001589817273647279\n",
      "Epoch 240, Training Loss 0.001622371763810325\n",
      "Epoch 240, Training Loss 0.0016488066357572366\n",
      "Epoch 240, Training Loss 0.001750424342310947\n",
      "Epoch 240, Training Loss 0.0017990314823282344\n",
      "Epoch 240, Training Loss 0.0018335979508088372\n",
      "Epoch 240, Training Loss 0.0018719475587729909\n",
      "Epoch 240, Training Loss 0.0019542945768026743\n",
      "Epoch 240, Training Loss 0.001987684058392292\n",
      "Epoch 240, Training Loss 0.0020767470412051585\n",
      "Epoch 240, Training Loss 0.002147111772557201\n",
      "Epoch 240, Training Loss 0.002185440686581385\n",
      "Epoch 240, Training Loss 0.002286520198254329\n",
      "Epoch 240, Training Loss 0.0023155650147772813\n",
      "Epoch 240, Training Loss 0.002359671218087301\n",
      "Epoch 240, Training Loss 0.002448804686060342\n",
      "Epoch 240, Training Loss 0.0025017029174682123\n",
      "Epoch 240, Training Loss 0.0025166008488067887\n",
      "Epoch 240, Training Loss 0.002559816593404316\n",
      "Epoch 240, Training Loss 0.0025873775724940895\n",
      "Epoch 240, Training Loss 0.0026200095315456694\n",
      "Epoch 240, Training Loss 0.0026730707277308036\n",
      "Epoch 240, Training Loss 0.0027763378708754354\n",
      "Epoch 240, Training Loss 0.0028138527422762283\n",
      "Epoch 240, Training Loss 0.0028235754212531288\n",
      "Epoch 240, Training Loss 0.002866468366111636\n",
      "Epoch 240, Training Loss 0.0028924617459020955\n",
      "Epoch 240, Training Loss 0.002938283164330456\n",
      "Epoch 240, Training Loss 0.002983968838325242\n",
      "Epoch 240, Training Loss 0.0030478838392916847\n",
      "Epoch 240, Training Loss 0.0030835964252500583\n",
      "Epoch 240, Training Loss 0.0031237347989969545\n",
      "Epoch 240, Training Loss 0.0031353974514795693\n",
      "Epoch 240, Training Loss 0.003145622245519591\n",
      "Epoch 240, Training Loss 0.003170008150636769\n",
      "Epoch 240, Training Loss 0.0032421855310745097\n",
      "Epoch 240, Training Loss 0.0032958085029421713\n",
      "Epoch 240, Training Loss 0.0033269381172040388\n",
      "Epoch 240, Training Loss 0.0033703792561560184\n",
      "Epoch 240, Training Loss 0.0033927129812614844\n",
      "Epoch 240, Training Loss 0.003467414465011157\n",
      "Epoch 240, Training Loss 0.00349276220483129\n",
      "Epoch 240, Training Loss 0.00353107080601937\n",
      "Epoch 240, Training Loss 0.00358022588288502\n",
      "Epoch 240, Training Loss 0.003638229229251671\n",
      "Epoch 240, Training Loss 0.003771444581344228\n",
      "Epoch 240, Training Loss 0.003819945226649723\n",
      "Epoch 240, Training Loss 0.003843278227531163\n",
      "Epoch 240, Training Loss 0.003924049905089238\n",
      "Epoch 240, Training Loss 0.003973347057476449\n",
      "Epoch 240, Training Loss 0.004117438123058861\n",
      "Epoch 240, Training Loss 0.004471519172353589\n",
      "Epoch 240, Training Loss 0.0045603842098180134\n",
      "Epoch 240, Training Loss 0.0045899236181755655\n",
      "Epoch 240, Training Loss 0.0047314631838418185\n",
      "Epoch 240, Training Loss 0.004800697003760378\n",
      "Epoch 240, Training Loss 0.004851278414249496\n",
      "Epoch 240, Training Loss 0.00490461630136956\n",
      "Epoch 240, Training Loss 0.004948915862490225\n",
      "Epoch 240, Training Loss 0.005011595281846155\n",
      "Epoch 240, Training Loss 0.005065187108000298\n",
      "Epoch 240, Training Loss 0.0051078002702187545\n",
      "Epoch 240, Training Loss 0.005204194338511095\n",
      "Epoch 240, Training Loss 0.005282676372619922\n",
      "Epoch 240, Training Loss 0.005402280209476457\n",
      "Epoch 240, Training Loss 0.005532928836553375\n",
      "Epoch 240, Training Loss 0.005720307451465627\n",
      "Epoch 240, Training Loss 0.0057988548544152165\n",
      "Epoch 240, Training Loss 0.005858571719987999\n",
      "Epoch 240, Training Loss 0.005878076331256448\n",
      "Epoch 240, Training Loss 0.005911487747755502\n",
      "Epoch 240, Training Loss 0.005948533968585531\n",
      "Epoch 240, Training Loss 0.006099029275043236\n",
      "Epoch 240, Training Loss 0.006135292203568132\n",
      "Epoch 240, Training Loss 0.006227619379110958\n",
      "Epoch 240, Training Loss 0.006292939214678981\n",
      "Epoch 240, Training Loss 0.006357267608537394\n",
      "Epoch 240, Training Loss 0.006415117960756697\n",
      "Epoch 240, Training Loss 0.006508001957631782\n",
      "Epoch 240, Training Loss 0.006553144672947466\n",
      "Epoch 240, Training Loss 0.006574249631055938\n",
      "Epoch 240, Training Loss 0.006637838316600189\n",
      "Epoch 240, Training Loss 0.006688423638167741\n",
      "Epoch 240, Training Loss 0.0067166121786131575\n",
      "Epoch 240, Training Loss 0.006733247588681595\n",
      "Epoch 240, Training Loss 0.006783588598851505\n",
      "Epoch 240, Training Loss 0.006811414140245646\n",
      "Epoch 240, Training Loss 0.006838289029953425\n",
      "Epoch 240, Training Loss 0.0068547860421526155\n",
      "Epoch 240, Training Loss 0.006929383770374538\n",
      "Epoch 240, Training Loss 0.006950346438590523\n",
      "Epoch 240, Training Loss 0.0069950543048665345\n",
      "Epoch 240, Training Loss 0.0070523634419569275\n",
      "Epoch 240, Training Loss 0.007102476823550966\n",
      "Epoch 240, Training Loss 0.007326466391039322\n",
      "Epoch 240, Training Loss 0.007397881301734454\n",
      "Epoch 240, Training Loss 0.00752188728841217\n",
      "Epoch 240, Training Loss 0.007534085237004263\n",
      "Epoch 240, Training Loss 0.007598754266262664\n",
      "Epoch 240, Training Loss 0.007790422853072891\n",
      "Epoch 240, Training Loss 0.007806835965255794\n",
      "Epoch 240, Training Loss 0.00781515445036199\n",
      "Epoch 240, Training Loss 0.00784079972511667\n",
      "Epoch 240, Training Loss 0.007930879083359639\n",
      "Epoch 240, Training Loss 0.007953310606863035\n",
      "Epoch 240, Training Loss 0.007982489228953638\n",
      "Epoch 240, Training Loss 0.008062224751790923\n",
      "Epoch 240, Training Loss 0.008089645310779056\n",
      "Epoch 240, Training Loss 0.008109053187167554\n",
      "Epoch 240, Training Loss 0.008121823185764234\n",
      "Epoch 240, Training Loss 0.008211783372828036\n",
      "Epoch 240, Training Loss 0.008334028364285408\n",
      "Epoch 240, Training Loss 0.008381231551003806\n",
      "Epoch 240, Training Loss 0.008441681679471603\n",
      "Epoch 240, Training Loss 0.008532194467142339\n",
      "Epoch 240, Training Loss 0.00856251025315174\n",
      "Epoch 240, Training Loss 0.00858511224679668\n",
      "Epoch 240, Training Loss 0.008651023493636676\n",
      "Epoch 240, Training Loss 0.008727548788051548\n",
      "Epoch 240, Training Loss 0.00879059684917788\n",
      "Epoch 240, Training Loss 0.008831834126397244\n",
      "Epoch 240, Training Loss 0.008929257870644636\n",
      "Epoch 240, Training Loss 0.008987809020473296\n",
      "Epoch 240, Training Loss 0.009074392046093407\n",
      "Epoch 240, Training Loss 0.00917249889043934\n",
      "Epoch 240, Training Loss 0.009285539537525315\n",
      "Epoch 240, Training Loss 0.009347265725598082\n",
      "Epoch 240, Training Loss 0.00939016281496114\n",
      "Epoch 240, Training Loss 0.009426027906777533\n",
      "Epoch 240, Training Loss 0.009441731550047159\n",
      "Epoch 240, Training Loss 0.009490777883688203\n",
      "Epoch 240, Training Loss 0.00962624105308062\n",
      "Epoch 240, Training Loss 0.009710128797823205\n",
      "Epoch 240, Training Loss 0.009783046267678976\n",
      "Epoch 240, Training Loss 0.009803965244718525\n",
      "Epoch 240, Training Loss 0.009832254884874119\n",
      "Epoch 240, Training Loss 0.00992533329235928\n",
      "Epoch 240, Training Loss 0.010042397024305275\n",
      "Epoch 240, Training Loss 0.010212254407994278\n",
      "Epoch 240, Training Loss 0.010309432883320563\n",
      "Epoch 240, Training Loss 0.010351476500101407\n",
      "Epoch 240, Training Loss 0.01036040263090883\n",
      "Epoch 240, Training Loss 0.010426586238391069\n",
      "Epoch 240, Training Loss 0.01044167042709887\n",
      "Epoch 240, Training Loss 0.010507025867295653\n",
      "Epoch 240, Training Loss 0.010540312793834702\n",
      "Epoch 240, Training Loss 0.010612455160354676\n",
      "Epoch 240, Training Loss 0.010734543775367882\n",
      "Epoch 240, Training Loss 0.010784998821342349\n",
      "Epoch 240, Training Loss 0.010871430196206245\n",
      "Epoch 240, Training Loss 0.010913362201955884\n",
      "Epoch 240, Training Loss 0.010943063336503132\n",
      "Epoch 240, Training Loss 0.01099220267854288\n",
      "Epoch 240, Training Loss 0.0110788724052927\n",
      "Epoch 240, Training Loss 0.011124697110741912\n",
      "Epoch 240, Training Loss 0.011193118364814564\n",
      "Epoch 240, Training Loss 0.011263349886073748\n",
      "Epoch 240, Training Loss 0.011318791090198756\n",
      "Epoch 240, Training Loss 0.011434737435134743\n",
      "Epoch 240, Training Loss 0.011553171435561593\n",
      "Epoch 240, Training Loss 0.011822955834719798\n",
      "Epoch 240, Training Loss 0.011889301844851097\n",
      "Epoch 240, Training Loss 0.011974032477015997\n",
      "Epoch 240, Training Loss 0.012002060077298442\n",
      "Epoch 240, Training Loss 0.012063188799311552\n",
      "Epoch 240, Training Loss 0.01215089159384084\n",
      "Epoch 240, Training Loss 0.012184501841754826\n",
      "Epoch 240, Training Loss 0.01226897906843106\n",
      "Epoch 240, Training Loss 0.01231514684358121\n",
      "Epoch 240, Training Loss 0.012346034897538974\n",
      "Epoch 240, Training Loss 0.0123735568015729\n",
      "Epoch 240, Training Loss 0.01250757892196879\n",
      "Epoch 240, Training Loss 0.012591541963307869\n",
      "Epoch 240, Training Loss 0.012672906104937348\n",
      "Epoch 240, Training Loss 0.01275171728416458\n",
      "Epoch 240, Training Loss 0.012827596774258081\n",
      "Epoch 240, Training Loss 0.01286936162785172\n",
      "Epoch 240, Training Loss 0.012896180190407978\n",
      "Epoch 240, Training Loss 0.012938731561398225\n",
      "Epoch 240, Training Loss 0.012960702280306716\n",
      "Epoch 240, Training Loss 0.013052930254513002\n",
      "Epoch 240, Training Loss 0.013139989183586843\n",
      "Epoch 240, Training Loss 0.013155130531919924\n",
      "Epoch 240, Training Loss 0.013198941012920663\n",
      "Epoch 240, Training Loss 0.013340143215916384\n",
      "Epoch 240, Training Loss 0.013408357825944834\n",
      "Epoch 240, Training Loss 0.01348426841411387\n",
      "Epoch 240, Training Loss 0.013555413126931204\n",
      "Epoch 240, Training Loss 0.013569615019694961\n",
      "Epoch 240, Training Loss 0.01364016489606932\n",
      "Epoch 240, Training Loss 0.013757212612125308\n",
      "Epoch 240, Training Loss 0.013860516179272014\n",
      "Epoch 240, Training Loss 0.013903402026071002\n",
      "Epoch 240, Training Loss 0.013985596507277978\n",
      "Epoch 240, Training Loss 0.014054761380743226\n",
      "Epoch 240, Training Loss 0.01411528504796593\n",
      "Epoch 240, Training Loss 0.014154533882413412\n",
      "Epoch 240, Training Loss 0.014222256211049455\n",
      "Epoch 240, Training Loss 0.014290982033804897\n",
      "Epoch 240, Training Loss 0.014419470649912877\n",
      "Epoch 240, Training Loss 0.014513224992743882\n",
      "Epoch 240, Training Loss 0.01459534708327135\n",
      "Epoch 240, Training Loss 0.014666413969915275\n",
      "Epoch 240, Training Loss 0.014731961700831875\n",
      "Epoch 240, Training Loss 0.014783364104326157\n",
      "Epoch 240, Training Loss 0.01488330551599393\n",
      "Epoch 240, Training Loss 0.014921749923544963\n",
      "Epoch 240, Training Loss 0.014969736435915084\n",
      "Epoch 240, Training Loss 0.015065167759618987\n",
      "Epoch 240, Training Loss 0.015132102732429915\n",
      "Epoch 240, Training Loss 0.01519308119947972\n",
      "Epoch 240, Training Loss 0.015232759530839446\n",
      "Epoch 240, Training Loss 0.015240813797825704\n",
      "Epoch 240, Training Loss 0.01530018326816866\n",
      "Epoch 240, Training Loss 0.015390000646085956\n",
      "Epoch 240, Training Loss 0.015436386765169023\n",
      "Epoch 240, Training Loss 0.015501226438447605\n",
      "Epoch 240, Training Loss 0.015568533921233186\n",
      "Epoch 240, Training Loss 0.015603057601277137\n",
      "Epoch 240, Training Loss 0.015638844685895784\n",
      "Epoch 240, Training Loss 0.015718022572791295\n",
      "Epoch 240, Training Loss 0.015769805789560727\n",
      "Epoch 240, Training Loss 0.015834274906021975\n",
      "Epoch 240, Training Loss 0.015886414936050544\n",
      "Epoch 240, Training Loss 0.015966917497113042\n",
      "Epoch 240, Training Loss 0.016034074305721065\n",
      "Epoch 240, Training Loss 0.01616125937510768\n",
      "Epoch 240, Training Loss 0.016185612188499718\n",
      "Epoch 240, Training Loss 0.016344751311761455\n",
      "Epoch 240, Training Loss 0.016377421933800325\n",
      "Epoch 240, Training Loss 0.016436306425653722\n",
      "Epoch 240, Training Loss 0.01649754526286536\n",
      "Epoch 240, Training Loss 0.016552122199641126\n",
      "Epoch 240, Training Loss 0.01663321923509793\n",
      "Epoch 240, Training Loss 0.016681080816856698\n",
      "Epoch 240, Training Loss 0.016710773302728067\n",
      "Epoch 240, Training Loss 0.016765125693105485\n",
      "Epoch 240, Training Loss 0.016836168117644955\n",
      "Epoch 240, Training Loss 0.01694386151543035\n",
      "Epoch 240, Training Loss 0.01697460955420933\n",
      "Epoch 240, Training Loss 0.017025508652881854\n",
      "Epoch 240, Training Loss 0.017146318454814652\n",
      "Epoch 240, Training Loss 0.01721351626067115\n",
      "Epoch 240, Training Loss 0.01726877079535838\n",
      "Epoch 240, Training Loss 0.017338466822810452\n",
      "Epoch 240, Training Loss 0.017420145832454725\n",
      "Epoch 240, Training Loss 0.0174591027437221\n",
      "Epoch 240, Training Loss 0.017527297407726915\n",
      "Epoch 240, Training Loss 0.017593128870472388\n",
      "Epoch 240, Training Loss 0.017621972685670267\n",
      "Epoch 240, Training Loss 0.0176787667894674\n",
      "Epoch 240, Training Loss 0.017700612287172842\n",
      "Epoch 240, Training Loss 0.01774895506972909\n",
      "Epoch 240, Training Loss 0.01787933143620353\n",
      "Epoch 240, Training Loss 0.017936667503045914\n",
      "Epoch 240, Training Loss 0.017965298898094107\n",
      "Epoch 240, Training Loss 0.017992817305500054\n",
      "Epoch 240, Training Loss 0.0180290832304541\n",
      "Epoch 240, Training Loss 0.018058247420617648\n",
      "Epoch 240, Training Loss 0.018156561594994743\n",
      "Epoch 240, Training Loss 0.018267483695212493\n",
      "Epoch 240, Training Loss 0.018298185737136645\n",
      "Epoch 240, Training Loss 0.018342614063726323\n",
      "Epoch 240, Training Loss 0.018431796351104708\n",
      "Epoch 240, Training Loss 0.018479520463934907\n",
      "Epoch 240, Training Loss 0.018506840287643433\n",
      "Epoch 240, Training Loss 0.018566712719814665\n",
      "Epoch 240, Training Loss 0.018625571982592078\n",
      "Epoch 240, Training Loss 0.018655093769659587\n",
      "Epoch 240, Training Loss 0.018691531386669447\n",
      "Epoch 240, Training Loss 0.018705888722172898\n",
      "Epoch 240, Training Loss 0.018794809648519397\n",
      "Epoch 240, Training Loss 0.01880967366995047\n",
      "Epoch 240, Training Loss 0.018920121534043908\n",
      "Epoch 240, Training Loss 0.018984609219791067\n",
      "Epoch 240, Training Loss 0.019027764607063683\n",
      "Epoch 240, Training Loss 0.0190570279128154\n",
      "Epoch 240, Training Loss 0.01917802935282764\n",
      "Epoch 240, Training Loss 0.01919405903045536\n",
      "Epoch 240, Training Loss 0.019213586836657904\n",
      "Epoch 240, Training Loss 0.019257584096663786\n",
      "Epoch 240, Training Loss 0.0193133444324031\n",
      "Epoch 240, Training Loss 0.019393145300262152\n",
      "Epoch 240, Training Loss 0.01946083681188676\n",
      "Epoch 240, Training Loss 0.019540149785811676\n",
      "Epoch 240, Training Loss 0.019563114961914128\n",
      "Epoch 240, Training Loss 0.019630059524251106\n",
      "Epoch 240, Training Loss 0.01979025441955518\n",
      "Epoch 240, Training Loss 0.01985532973411843\n",
      "Epoch 240, Training Loss 0.019905405153003534\n",
      "Epoch 240, Training Loss 0.01994875871905071\n",
      "Epoch 240, Training Loss 0.020114145335167304\n",
      "Epoch 240, Training Loss 0.02031969277204855\n",
      "Epoch 240, Training Loss 0.020371841491245286\n",
      "Epoch 240, Training Loss 0.02040635801368224\n",
      "Epoch 240, Training Loss 0.020541673038474016\n",
      "Epoch 240, Training Loss 0.02058857928335076\n",
      "Epoch 240, Training Loss 0.020630512589617345\n",
      "Epoch 240, Training Loss 0.020680465850421725\n",
      "Epoch 240, Training Loss 0.02074598712796617\n",
      "Epoch 240, Training Loss 0.02083579398741674\n",
      "Epoch 240, Training Loss 0.020848595090043706\n",
      "Epoch 240, Training Loss 0.021038614964360356\n",
      "Epoch 240, Training Loss 0.02117851086715927\n",
      "Epoch 240, Training Loss 0.021202806569754963\n",
      "Epoch 240, Training Loss 0.021229510496268072\n",
      "Epoch 240, Training Loss 0.021316454579691637\n",
      "Epoch 240, Training Loss 0.02141251620155333\n",
      "Epoch 240, Training Loss 0.02152590607316769\n",
      "Epoch 240, Training Loss 0.02157698709177106\n",
      "Epoch 240, Training Loss 0.021609120681295956\n",
      "Epoch 240, Training Loss 0.0216575049803547\n",
      "Epoch 240, Training Loss 0.021693452957498333\n",
      "Epoch 240, Training Loss 0.02170994536786833\n",
      "Epoch 240, Training Loss 0.021766009862603776\n",
      "Epoch 240, Training Loss 0.021826283044546195\n",
      "Epoch 240, Training Loss 0.021899311772912093\n",
      "Epoch 240, Training Loss 0.022072625806783815\n",
      "Epoch 240, Training Loss 0.022211807797236555\n",
      "Epoch 240, Training Loss 0.022277716123868172\n",
      "Epoch 240, Training Loss 0.022394274883662992\n",
      "Epoch 240, Training Loss 0.022436433855820535\n",
      "Epoch 240, Training Loss 0.022500251284669464\n",
      "Epoch 240, Training Loss 0.022540409995071457\n",
      "Epoch 240, Training Loss 0.022600247284702366\n",
      "Epoch 240, Training Loss 0.02266445989920603\n",
      "Epoch 240, Training Loss 0.02273654892071224\n",
      "Epoch 240, Training Loss 0.02278078480890435\n",
      "Epoch 240, Training Loss 0.022889407852407343\n",
      "Epoch 240, Training Loss 0.022918268919700894\n",
      "Epoch 240, Training Loss 0.023011281501020654\n",
      "Epoch 240, Training Loss 0.023046014538091963\n",
      "Epoch 240, Training Loss 0.02315643095158879\n",
      "Epoch 240, Training Loss 0.023171685859346595\n",
      "Epoch 240, Training Loss 0.023251041178674915\n",
      "Epoch 240, Training Loss 0.023401922733901674\n",
      "Epoch 240, Training Loss 0.023434033076090696\n",
      "Epoch 240, Training Loss 0.02349335531813695\n",
      "Epoch 240, Training Loss 0.02361545821501757\n",
      "Epoch 240, Training Loss 0.02369689965403884\n",
      "Epoch 240, Training Loss 0.023725111844361095\n",
      "Epoch 240, Training Loss 0.02377963036267787\n",
      "Epoch 240, Training Loss 0.023806815713886983\n",
      "Epoch 240, Training Loss 0.02382868248671579\n",
      "Epoch 240, Training Loss 0.023859550651815505\n",
      "Epoch 240, Training Loss 0.023943266613990107\n",
      "Epoch 240, Training Loss 0.024005169791462437\n",
      "Epoch 240, Training Loss 0.02409498609276131\n",
      "Epoch 240, Training Loss 0.02415787128438635\n",
      "Epoch 240, Training Loss 0.02426670699337464\n",
      "Epoch 240, Training Loss 0.0243574026156255\n",
      "Epoch 240, Training Loss 0.02440723036463513\n",
      "Epoch 240, Training Loss 0.024436748259679397\n",
      "Epoch 240, Training Loss 0.02446389630320184\n",
      "Epoch 240, Training Loss 0.024532020296258353\n",
      "Epoch 240, Training Loss 0.02455767472703343\n",
      "Epoch 240, Training Loss 0.024583482280578415\n",
      "Epoch 240, Training Loss 0.024607158410708273\n",
      "Epoch 240, Training Loss 0.024823556560665712\n",
      "Epoch 240, Training Loss 0.02489763022819653\n",
      "Epoch 240, Training Loss 0.024930100430093725\n",
      "Epoch 240, Training Loss 0.024964943389429728\n",
      "Epoch 240, Training Loss 0.024998534605969364\n",
      "Epoch 240, Training Loss 0.02506420204637434\n",
      "Epoch 240, Training Loss 0.0251654292072129\n",
      "Epoch 240, Training Loss 0.025204034290536095\n",
      "Epoch 240, Training Loss 0.02528068210508036\n",
      "Epoch 240, Training Loss 0.025377659500478898\n",
      "Epoch 240, Training Loss 0.02544863980985187\n",
      "Epoch 240, Training Loss 0.02548430423857287\n",
      "Epoch 240, Training Loss 0.025554229490473257\n",
      "Epoch 240, Training Loss 0.025670488759794313\n",
      "Epoch 240, Training Loss 0.02579307777371229\n",
      "Epoch 240, Training Loss 0.0258589214317815\n",
      "Epoch 240, Training Loss 0.025921669683259582\n",
      "Epoch 240, Training Loss 0.02600299072204172\n",
      "Epoch 240, Training Loss 0.026051862429127173\n",
      "Epoch 240, Training Loss 0.026109695593562084\n",
      "Epoch 240, Training Loss 0.026216155967896667\n",
      "Epoch 240, Training Loss 0.026275252544051966\n",
      "Epoch 240, Training Loss 0.02639054459081415\n",
      "Epoch 240, Training Loss 0.026450441442215648\n",
      "Epoch 240, Training Loss 0.026487880342942485\n",
      "Epoch 240, Training Loss 0.02656259126377666\n",
      "Epoch 240, Training Loss 0.026642834540823345\n",
      "Epoch 240, Training Loss 0.026755624755030818\n",
      "Epoch 240, Training Loss 0.0268094780756032\n",
      "Epoch 240, Training Loss 0.02692296775832029\n",
      "Epoch 240, Training Loss 0.02701136377125578\n",
      "Epoch 240, Training Loss 0.02714336351634425\n",
      "Epoch 240, Training Loss 0.02716209901713521\n",
      "Epoch 240, Training Loss 0.027214068812830255\n",
      "Epoch 240, Training Loss 0.02725351307019496\n",
      "Epoch 240, Training Loss 0.02736065752597054\n",
      "Epoch 240, Training Loss 0.027422760120805\n",
      "Epoch 240, Training Loss 0.027434620001684406\n",
      "Epoch 240, Training Loss 0.0275682497089562\n",
      "Epoch 240, Training Loss 0.027653432425583147\n",
      "Epoch 240, Training Loss 0.02768159559165673\n",
      "Epoch 240, Training Loss 0.02773679553797883\n",
      "Epoch 240, Training Loss 0.027762588180835022\n",
      "Epoch 240, Training Loss 0.027833741644749898\n",
      "Epoch 240, Training Loss 0.02787044050607859\n",
      "Epoch 240, Training Loss 0.027903185140870302\n",
      "Epoch 240, Training Loss 0.028014326394866684\n",
      "Epoch 240, Training Loss 0.028105198468207894\n",
      "Epoch 240, Training Loss 0.02814194256418368\n",
      "Epoch 240, Training Loss 0.028261127263007453\n",
      "Epoch 240, Training Loss 0.02833586044030745\n",
      "Epoch 240, Training Loss 0.028406715442729005\n",
      "Epoch 240, Training Loss 0.028447942801720232\n",
      "Epoch 240, Training Loss 0.028501494850158272\n",
      "Epoch 240, Training Loss 0.028601485938715088\n",
      "Epoch 240, Training Loss 0.02863165767460852\n",
      "Epoch 240, Training Loss 0.028662385614922323\n",
      "Epoch 240, Training Loss 0.02878451830459773\n",
      "Epoch 240, Training Loss 0.02881921661119727\n",
      "Epoch 240, Training Loss 0.02884904814281446\n",
      "Epoch 240, Training Loss 0.028905606486231965\n",
      "Epoch 240, Training Loss 0.02892085701725005\n",
      "Epoch 240, Training Loss 0.028985467442261326\n",
      "Epoch 240, Training Loss 0.029039984983999446\n",
      "Epoch 240, Training Loss 0.02919546896865701\n",
      "Epoch 240, Training Loss 0.02928447123209629\n",
      "Epoch 240, Training Loss 0.029359013376319232\n",
      "Epoch 240, Training Loss 0.029444789995684686\n",
      "Epoch 240, Training Loss 0.029472735053633372\n",
      "Epoch 240, Training Loss 0.029575768437193672\n",
      "Epoch 240, Training Loss 0.02965051077527787\n",
      "Epoch 240, Training Loss 0.029694709110447703\n",
      "Epoch 240, Training Loss 0.029926991440913143\n",
      "Epoch 240, Training Loss 0.030089735158878708\n",
      "Epoch 240, Training Loss 0.03019814969092855\n",
      "Epoch 240, Training Loss 0.030313354645929564\n",
      "Epoch 240, Training Loss 0.030350259591556152\n",
      "Epoch 240, Training Loss 0.0304699498341869\n",
      "Epoch 240, Training Loss 0.030513760415227402\n",
      "Epoch 240, Training Loss 0.03061941555520172\n",
      "Epoch 240, Training Loss 0.03064648781145644\n",
      "Epoch 240, Training Loss 0.030668228924812753\n",
      "Epoch 240, Training Loss 0.030718239246393598\n",
      "Epoch 240, Training Loss 0.030780510827446417\n",
      "Epoch 240, Training Loss 0.03080513451517562\n",
      "Epoch 240, Training Loss 0.03083040787066664\n",
      "Epoch 240, Training Loss 0.03088333632837495\n",
      "Epoch 240, Training Loss 0.03103623801039632\n",
      "Epoch 240, Training Loss 0.031126385481427887\n",
      "Epoch 240, Training Loss 0.03133080441259381\n",
      "Epoch 240, Training Loss 0.03151861089281738\n",
      "Epoch 240, Training Loss 0.03157654807002515\n",
      "Epoch 240, Training Loss 0.03161818530146138\n",
      "Epoch 240, Training Loss 0.03164100681807932\n",
      "Epoch 240, Training Loss 0.03167344866108502\n",
      "Epoch 240, Training Loss 0.03170938588395391\n",
      "Epoch 240, Training Loss 0.0317475018323735\n",
      "Epoch 240, Training Loss 0.031801997755994765\n",
      "Epoch 240, Training Loss 0.03184055418902746\n",
      "Epoch 240, Training Loss 0.03189260428330725\n",
      "Epoch 240, Training Loss 0.031911361763072785\n",
      "Epoch 240, Training Loss 0.031992788005577366\n",
      "Epoch 240, Training Loss 0.032048940330462725\n",
      "Epoch 240, Training Loss 0.03209490028491049\n",
      "Epoch 240, Training Loss 0.032175411354116806\n",
      "Epoch 240, Training Loss 0.03221990802692121\n",
      "Epoch 240, Training Loss 0.03226030592763763\n",
      "Epoch 240, Training Loss 0.03229016188563079\n",
      "Epoch 240, Training Loss 0.032341988714973985\n",
      "Epoch 240, Training Loss 0.03243209525545025\n",
      "Epoch 240, Training Loss 0.032507224475888684\n",
      "Epoch 240, Training Loss 0.03266327195119618\n",
      "Epoch 240, Training Loss 0.032673409316436294\n",
      "Epoch 240, Training Loss 0.03273541240981492\n",
      "Epoch 240, Training Loss 0.032869362949377014\n",
      "Epoch 240, Training Loss 0.03291929482549067\n",
      "Epoch 240, Training Loss 0.03298682939854291\n",
      "Epoch 240, Training Loss 0.03302363636236533\n",
      "Epoch 240, Training Loss 0.03307304033816642\n",
      "Epoch 240, Training Loss 0.03323715631588055\n",
      "Epoch 240, Training Loss 0.03332123859509197\n",
      "Epoch 240, Training Loss 0.0333642327992479\n",
      "Epoch 240, Training Loss 0.03338354090740785\n",
      "Epoch 240, Training Loss 0.03348750312684004\n",
      "Epoch 240, Training Loss 0.033559967883412374\n",
      "Epoch 240, Training Loss 0.03361473997871456\n",
      "Epoch 240, Training Loss 0.03372320950405239\n",
      "Epoch 240, Training Loss 0.0337728037554866\n",
      "Epoch 240, Training Loss 0.03380745529349598\n",
      "Epoch 240, Training Loss 0.03400895434022045\n",
      "Epoch 240, Training Loss 0.03405263899977955\n",
      "Epoch 240, Training Loss 0.0340905415324156\n",
      "Epoch 240, Training Loss 0.03419388846918236\n",
      "Epoch 240, Training Loss 0.034439661038105786\n",
      "Epoch 240, Training Loss 0.034528328254080524\n",
      "Epoch 240, Training Loss 0.03458181217067477\n",
      "Epoch 240, Training Loss 0.03469157286638589\n",
      "Epoch 240, Training Loss 0.034773356203213715\n",
      "Epoch 240, Training Loss 0.034874617644345095\n",
      "Epoch 240, Training Loss 0.03488541405845691\n",
      "Epoch 240, Training Loss 0.03504989137980239\n",
      "Epoch 240, Training Loss 0.03510254515689867\n",
      "Epoch 240, Training Loss 0.03523102036767813\n",
      "Epoch 240, Training Loss 0.035294476900692755\n",
      "Epoch 240, Training Loss 0.035328751565088204\n",
      "Epoch 240, Training Loss 0.03536134433951658\n",
      "Epoch 240, Training Loss 0.035492384103019636\n",
      "Epoch 240, Training Loss 0.035546175522796444\n",
      "Epoch 240, Training Loss 0.03560034121991709\n",
      "Epoch 240, Training Loss 0.0357077852614424\n",
      "Epoch 240, Training Loss 0.03576113461978886\n",
      "Epoch 240, Training Loss 0.03588432282426149\n",
      "Epoch 240, Training Loss 0.03591739828102862\n",
      "Epoch 240, Training Loss 0.035963570738992534\n",
      "Epoch 240, Training Loss 0.0360279874203255\n",
      "Epoch 240, Training Loss 0.03605152125818097\n",
      "Epoch 240, Training Loss 0.03614817864066252\n",
      "Epoch 240, Training Loss 0.03618466897862856\n",
      "Epoch 240, Training Loss 0.03623047032241074\n",
      "Epoch 240, Training Loss 0.036312737070438465\n",
      "Epoch 240, Training Loss 0.0364189743179866\n",
      "Epoch 240, Training Loss 0.03644857745643829\n",
      "Epoch 240, Training Loss 0.03664042405686472\n",
      "Epoch 240, Training Loss 0.03672389814194263\n",
      "Epoch 240, Training Loss 0.03674197689120365\n",
      "Epoch 240, Training Loss 0.03681831746457426\n",
      "Epoch 240, Training Loss 0.03691525180476344\n",
      "Epoch 240, Training Loss 0.03713888570706805\n",
      "Epoch 240, Training Loss 0.03726596185696952\n",
      "Epoch 240, Training Loss 0.037531861168977894\n",
      "Epoch 240, Training Loss 0.03756521647031445\n",
      "Epoch 240, Training Loss 0.03764561983004041\n",
      "Epoch 240, Training Loss 0.03776202454765225\n",
      "Epoch 240, Training Loss 0.03780955886122916\n",
      "Epoch 240, Training Loss 0.037876642826596835\n",
      "Epoch 240, Training Loss 0.0379497964379361\n",
      "Epoch 240, Training Loss 0.03800484253739095\n",
      "Epoch 240, Training Loss 0.03811241444879119\n",
      "Epoch 240, Training Loss 0.038172882617763276\n",
      "Epoch 240, Training Loss 0.038220260121390376\n",
      "Epoch 240, Training Loss 0.03826440081460035\n",
      "Epoch 240, Training Loss 0.03829095363104835\n",
      "Epoch 240, Training Loss 0.038372256786531535\n",
      "Epoch 240, Training Loss 0.038442982885333925\n",
      "Epoch 240, Training Loss 0.03857874822245477\n",
      "Epoch 240, Training Loss 0.03862317854031216\n",
      "Epoch 240, Training Loss 0.038718062465476905\n",
      "Epoch 240, Training Loss 0.03878954780416663\n",
      "Epoch 240, Training Loss 0.038858876647570595\n",
      "Epoch 240, Training Loss 0.038919167708048166\n",
      "Epoch 240, Training Loss 0.03894910835208433\n",
      "Epoch 240, Training Loss 0.03897902535100746\n",
      "Epoch 240, Training Loss 0.03901864760293318\n",
      "Epoch 240, Training Loss 0.0391701543781087\n",
      "Epoch 240, Training Loss 0.03926294962542079\n",
      "Epoch 240, Training Loss 0.03934556786554492\n",
      "Epoch 240, Training Loss 0.03938302052412134\n",
      "Epoch 240, Training Loss 0.03947395754708906\n",
      "Epoch 240, Training Loss 0.039555449196544794\n",
      "Epoch 240, Training Loss 0.0396200549435061\n",
      "Epoch 240, Training Loss 0.039654960427695264\n",
      "Epoch 240, Training Loss 0.03969561382997162\n",
      "Epoch 240, Training Loss 0.039763734536007275\n",
      "Epoch 240, Training Loss 0.039831447737205705\n",
      "Epoch 240, Training Loss 0.03997691798909946\n",
      "Epoch 240, Training Loss 0.04001833905365861\n",
      "Epoch 240, Training Loss 0.040037736174581416\n",
      "Epoch 240, Training Loss 0.04010170149078588\n",
      "Epoch 240, Training Loss 0.04014828764652486\n",
      "Epoch 240, Training Loss 0.04020125052267614\n",
      "Epoch 240, Training Loss 0.04027121159834001\n",
      "Epoch 240, Training Loss 0.04033226574309018\n",
      "Epoch 240, Training Loss 0.04039095193231026\n",
      "Epoch 240, Training Loss 0.040444386262527623\n",
      "Epoch 240, Training Loss 0.040498015655995444\n",
      "Epoch 240, Training Loss 0.04055204890999476\n",
      "Epoch 240, Training Loss 0.040692139957028695\n",
      "Epoch 240, Training Loss 0.04079913029440052\n",
      "Epoch 240, Training Loss 0.04086521978351904\n",
      "Epoch 240, Training Loss 0.0408905262497904\n",
      "Epoch 240, Training Loss 0.0410203952431593\n",
      "Epoch 240, Training Loss 0.04116488956427083\n",
      "Epoch 240, Training Loss 0.0412277282612241\n",
      "Epoch 240, Training Loss 0.04128459304073335\n",
      "Epoch 240, Training Loss 0.04140455252371843\n",
      "Epoch 240, Training Loss 0.04146832934595035\n",
      "Epoch 240, Training Loss 0.04149389547495948\n",
      "Epoch 240, Training Loss 0.04154197855428566\n",
      "Epoch 240, Training Loss 0.04164160902330847\n",
      "Epoch 240, Training Loss 0.0416834073949872\n",
      "Epoch 240, Training Loss 0.041725131184639185\n",
      "Epoch 240, Training Loss 0.04183986139855326\n",
      "Epoch 240, Training Loss 0.041857815942491224\n",
      "Epoch 240, Training Loss 0.04188724022889819\n",
      "Epoch 240, Training Loss 0.041978333923665574\n",
      "Epoch 240, Training Loss 0.04208127957711573\n",
      "Epoch 240, Training Loss 0.04216168424212243\n",
      "Epoch 240, Training Loss 0.04228411192524597\n",
      "Epoch 240, Training Loss 0.04232341860887854\n",
      "Epoch 240, Training Loss 0.04237509020985297\n",
      "Epoch 240, Training Loss 0.04252333570774311\n",
      "Epoch 240, Training Loss 0.04258680617486787\n",
      "Epoch 240, Training Loss 0.04262496155324628\n",
      "Epoch 240, Training Loss 0.04264973867101037\n",
      "Epoch 240, Training Loss 0.04270385723511028\n",
      "Epoch 240, Training Loss 0.04274219961877426\n",
      "Epoch 240, Training Loss 0.0428507595705321\n",
      "Epoch 240, Training Loss 0.04290049507335076\n",
      "Epoch 240, Training Loss 0.04295642385699922\n",
      "Epoch 240, Training Loss 0.04305019271035995\n",
      "Epoch 240, Training Loss 0.04317364895470974\n",
      "Epoch 240, Training Loss 0.043332976171784963\n",
      "Epoch 240, Training Loss 0.043406835249255954\n",
      "Epoch 240, Training Loss 0.04344759746145009\n",
      "Epoch 240, Training Loss 0.04359494795596413\n",
      "Epoch 240, Training Loss 0.04378564505185217\n",
      "Epoch 240, Training Loss 0.04389633656517529\n",
      "Epoch 240, Training Loss 0.04391601763348883\n",
      "Epoch 240, Training Loss 0.04400008115286241\n",
      "Epoch 240, Training Loss 0.04404687826388308\n",
      "Epoch 240, Training Loss 0.04417808982900932\n",
      "Epoch 240, Training Loss 0.04432636236621881\n",
      "Epoch 240, Training Loss 0.044376583271505086\n",
      "Epoch 240, Training Loss 0.044422295198792504\n",
      "Epoch 240, Training Loss 0.044489169304552094\n",
      "Epoch 240, Training Loss 0.04456598666267913\n",
      "Epoch 240, Training Loss 0.0446054690852142\n",
      "Epoch 240, Training Loss 0.04464923268091172\n",
      "Epoch 240, Training Loss 0.04473872949331141\n",
      "Epoch 240, Training Loss 0.04482312776479403\n",
      "Epoch 240, Training Loss 0.04488567498874615\n",
      "Epoch 240, Training Loss 0.04492845371677576\n",
      "Epoch 240, Training Loss 0.04495873584654992\n",
      "Epoch 240, Training Loss 0.04503580953990159\n",
      "Epoch 240, Training Loss 0.04531292414800037\n",
      "Epoch 240, Training Loss 0.04538928461975664\n",
      "Epoch 240, Training Loss 0.04547166308838293\n",
      "Epoch 240, Training Loss 0.04552614496356291\n",
      "Epoch 240, Training Loss 0.045579107601524275\n",
      "Epoch 240, Training Loss 0.045638437070252606\n",
      "Epoch 240, Training Loss 0.0456723086148872\n",
      "Epoch 240, Training Loss 0.04573983205017893\n",
      "Epoch 240, Training Loss 0.04584960399798172\n",
      "Epoch 240, Training Loss 0.04591499549536811\n",
      "Epoch 240, Training Loss 0.04596293929075856\n",
      "Epoch 240, Training Loss 0.045993770091006025\n",
      "Epoch 240, Training Loss 0.04604834094441131\n",
      "Epoch 240, Training Loss 0.04613010330147126\n",
      "Epoch 240, Training Loss 0.04622187079740283\n",
      "Epoch 240, Training Loss 0.04631076917192801\n",
      "Epoch 240, Training Loss 0.04633316259933612\n",
      "Epoch 240, Training Loss 0.046411274472022875\n",
      "Epoch 240, Training Loss 0.04650683348457736\n",
      "Epoch 240, Training Loss 0.04663089617951047\n",
      "Epoch 240, Training Loss 0.04669849787448126\n",
      "Epoch 240, Training Loss 0.046811454955016825\n",
      "Epoch 240, Training Loss 0.04685742763356994\n",
      "Epoch 240, Training Loss 0.04689214652216019\n",
      "Epoch 240, Training Loss 0.04693634419039826\n",
      "Epoch 240, Training Loss 0.04697243491475901\n",
      "Epoch 240, Training Loss 0.04707285817510084\n",
      "Epoch 240, Training Loss 0.047124675000467534\n",
      "Epoch 240, Training Loss 0.04717803095667468\n",
      "Epoch 240, Training Loss 0.047204308868612134\n",
      "Epoch 240, Training Loss 0.04723908412722809\n",
      "Epoch 240, Training Loss 0.0472794328417267\n",
      "Epoch 240, Training Loss 0.04729905539749624\n",
      "Epoch 240, Training Loss 0.04735392078702502\n",
      "Epoch 240, Training Loss 0.04738498416781673\n",
      "Epoch 240, Training Loss 0.047494637230268255\n",
      "Epoch 240, Training Loss 0.047561791375913015\n",
      "Epoch 240, Training Loss 0.04766653769749605\n",
      "Epoch 240, Training Loss 0.04779048085920132\n",
      "Epoch 240, Training Loss 0.04785561806026875\n",
      "Epoch 240, Training Loss 0.047964153430827175\n",
      "Epoch 240, Training Loss 0.048023180245919644\n",
      "Epoch 240, Training Loss 0.04814760144526029\n",
      "Epoch 240, Training Loss 0.04824092284457096\n",
      "Epoch 240, Training Loss 0.048326179781175975\n",
      "Epoch 240, Training Loss 0.048487869575452966\n",
      "Epoch 240, Training Loss 0.04851718553070389\n",
      "Epoch 240, Training Loss 0.048550273409075174\n",
      "Epoch 240, Training Loss 0.048627435761477676\n",
      "Epoch 240, Training Loss 0.04867297028967887\n",
      "Epoch 240, Training Loss 0.04873264390949036\n",
      "Epoch 240, Training Loss 0.04881903145740957\n",
      "Epoch 240, Training Loss 0.04893618911240355\n",
      "Epoch 240, Training Loss 0.04905437056482543\n",
      "Epoch 240, Training Loss 0.04907833841567397\n",
      "Epoch 240, Training Loss 0.049124420375760904\n",
      "Epoch 240, Training Loss 0.04922288345694637\n",
      "Epoch 240, Training Loss 0.04924706639805356\n",
      "Epoch 240, Training Loss 0.04930665177802848\n",
      "Epoch 240, Training Loss 0.04937479586002257\n",
      "Epoch 240, Training Loss 0.04944735100490453\n",
      "Epoch 240, Training Loss 0.04946666680302595\n",
      "Epoch 240, Training Loss 0.04948057911639838\n",
      "Epoch 240, Training Loss 0.049520431076297944\n",
      "Epoch 240, Training Loss 0.04956549320243718\n",
      "Epoch 240, Training Loss 0.04964884808775314\n",
      "Epoch 240, Training Loss 0.0497070723647952\n",
      "Epoch 240, Training Loss 0.049722402532230064\n",
      "Epoch 240, Training Loss 0.04984824420095843\n",
      "Epoch 240, Training Loss 0.04991242366895327\n",
      "Epoch 240, Training Loss 0.050004866585740465\n",
      "Epoch 240, Training Loss 0.05004918776497797\n",
      "Epoch 240, Training Loss 0.05010702674601065\n",
      "Epoch 240, Training Loss 0.05014075619130469\n",
      "Epoch 240, Training Loss 0.05022746593396053\n",
      "Epoch 240, Training Loss 0.05040530283230326\n",
      "Epoch 240, Training Loss 0.0504815797613281\n",
      "Epoch 240, Training Loss 0.05059601296194355\n",
      "Epoch 240, Training Loss 0.05067008133689442\n",
      "Epoch 240, Training Loss 0.05078917348464412\n",
      "Epoch 240, Training Loss 0.05083741370738601\n",
      "Epoch 240, Training Loss 0.050891958407538435\n",
      "Epoch 240, Training Loss 0.05092289432814188\n",
      "Epoch 240, Training Loss 0.05101183554826928\n",
      "Epoch 240, Training Loss 0.05107748329989574\n",
      "Epoch 240, Training Loss 0.051116500003977924\n",
      "Epoch 240, Training Loss 0.051187284425963334\n",
      "Epoch 240, Training Loss 0.05124915450932863\n",
      "Epoch 240, Training Loss 0.05133569641384151\n",
      "Epoch 240, Training Loss 0.05149484659864298\n",
      "Epoch 240, Training Loss 0.05154066906987077\n",
      "Epoch 240, Training Loss 0.05162574776419727\n",
      "Epoch 240, Training Loss 0.051701252462456715\n",
      "Epoch 240, Training Loss 0.05188990444066408\n",
      "Epoch 240, Training Loss 0.052065914384234704\n",
      "Epoch 240, Training Loss 0.05212472154534972\n",
      "Epoch 240, Training Loss 0.05216518661085888\n",
      "Epoch 240, Training Loss 0.052280525218986\n",
      "Epoch 240, Training Loss 0.05230844148691944\n",
      "Epoch 240, Training Loss 0.052352322667451275\n",
      "Epoch 240, Training Loss 0.05240339475869656\n",
      "Epoch 240, Training Loss 0.05248108745524017\n",
      "Epoch 240, Training Loss 0.052581304624138396\n",
      "Epoch 240, Training Loss 0.05266634827596433\n",
      "Epoch 240, Training Loss 0.05273667696918197\n",
      "Epoch 240, Training Loss 0.05278420681907984\n",
      "Epoch 240, Training Loss 0.05285142973491736\n",
      "Epoch 240, Training Loss 0.05289621523681485\n",
      "Epoch 240, Training Loss 0.05297809870868369\n",
      "Epoch 240, Training Loss 0.05312776592407671\n",
      "Epoch 240, Training Loss 0.05321664591986791\n",
      "Epoch 240, Training Loss 0.053241615840460145\n",
      "Epoch 240, Training Loss 0.053319389002321436\n",
      "Epoch 240, Training Loss 0.053427366541622355\n",
      "Epoch 240, Training Loss 0.05350314663982738\n",
      "Epoch 240, Training Loss 0.053571194250141375\n",
      "Epoch 240, Training Loss 0.053624865827043455\n",
      "Epoch 240, Training Loss 0.05365304035477607\n",
      "Epoch 240, Training Loss 0.05374503728476308\n",
      "Epoch 240, Training Loss 0.053931061628029284\n",
      "Epoch 240, Training Loss 0.053991308732939614\n",
      "Epoch 240, Training Loss 0.054148470727450514\n",
      "Epoch 240, Training Loss 0.05433639731255772\n",
      "Epoch 240, Training Loss 0.05449470853530671\n",
      "Epoch 240, Training Loss 0.054569449610984345\n",
      "Epoch 240, Training Loss 0.054751339053516\n",
      "Epoch 240, Training Loss 0.05484602697696203\n",
      "Epoch 240, Training Loss 0.055061488030621275\n",
      "Epoch 240, Training Loss 0.055124078257380964\n",
      "Epoch 240, Training Loss 0.055149487216773506\n",
      "Epoch 250, Training Loss 4.959387033034468e-05\n",
      "Epoch 250, Training Loss 0.00015005597468379818\n",
      "Epoch 250, Training Loss 0.00021449778505298488\n",
      "Epoch 250, Training Loss 0.00023703359762954589\n",
      "Epoch 250, Training Loss 0.0002703456341496209\n",
      "Epoch 250, Training Loss 0.00032952911151415854\n",
      "Epoch 250, Training Loss 0.0003912827533567348\n",
      "Epoch 250, Training Loss 0.00045132713006509233\n",
      "Epoch 250, Training Loss 0.0005222636315485705\n",
      "Epoch 250, Training Loss 0.0005649359649061547\n",
      "Epoch 250, Training Loss 0.0005894314373850517\n",
      "Epoch 250, Training Loss 0.0006775546466450557\n",
      "Epoch 250, Training Loss 0.0007057793419379408\n",
      "Epoch 250, Training Loss 0.0008044657118789985\n",
      "Epoch 250, Training Loss 0.0008248506099595438\n",
      "Epoch 250, Training Loss 0.0009010498604887281\n",
      "Epoch 250, Training Loss 0.000943433078925323\n",
      "Epoch 250, Training Loss 0.000985275649125009\n",
      "Epoch 250, Training Loss 0.0010279410559198131\n",
      "Epoch 250, Training Loss 0.0010845999345373925\n",
      "Epoch 250, Training Loss 0.0011512627324942128\n",
      "Epoch 250, Training Loss 0.0012911588544278498\n",
      "Epoch 250, Training Loss 0.0013620026047577334\n",
      "Epoch 250, Training Loss 0.0014781311935628467\n",
      "Epoch 250, Training Loss 0.0015323684429344924\n",
      "Epoch 250, Training Loss 0.0015707868425285116\n",
      "Epoch 250, Training Loss 0.0016650767601511973\n",
      "Epoch 250, Training Loss 0.0017285959514053277\n",
      "Epoch 250, Training Loss 0.0018195265718280811\n",
      "Epoch 250, Training Loss 0.001867623556681606\n",
      "Epoch 250, Training Loss 0.0019766959216436157\n",
      "Epoch 250, Training Loss 0.0020312340191715513\n",
      "Epoch 250, Training Loss 0.0020573855785038465\n",
      "Epoch 250, Training Loss 0.0021431662852082717\n",
      "Epoch 250, Training Loss 0.002243421783627909\n",
      "Epoch 250, Training Loss 0.0023313427124829855\n",
      "Epoch 250, Training Loss 0.0023690523803615205\n",
      "Epoch 250, Training Loss 0.0024130008876552364\n",
      "Epoch 250, Training Loss 0.0024534791155391947\n",
      "Epoch 250, Training Loss 0.002470461954184048\n",
      "Epoch 250, Training Loss 0.002491893468286528\n",
      "Epoch 250, Training Loss 0.002540290339009079\n",
      "Epoch 250, Training Loss 0.002593927530814772\n",
      "Epoch 250, Training Loss 0.0026235295211910594\n",
      "Epoch 250, Training Loss 0.0026563971596376976\n",
      "Epoch 250, Training Loss 0.0027142234927857926\n",
      "Epoch 250, Training Loss 0.002723100434993501\n",
      "Epoch 250, Training Loss 0.0027520356126977583\n",
      "Epoch 250, Training Loss 0.0027719662814160518\n",
      "Epoch 250, Training Loss 0.0027825150173157454\n",
      "Epoch 250, Training Loss 0.0028346517412682704\n",
      "Epoch 250, Training Loss 0.0028621080620905093\n",
      "Epoch 250, Training Loss 0.0028844474251513057\n",
      "Epoch 250, Training Loss 0.002902246426309809\n",
      "Epoch 250, Training Loss 0.0029537566726708124\n",
      "Epoch 250, Training Loss 0.002981922778007967\n",
      "Epoch 250, Training Loss 0.0030019762824572945\n",
      "Epoch 250, Training Loss 0.0030228501153619163\n",
      "Epoch 250, Training Loss 0.003053518767823534\n",
      "Epoch 250, Training Loss 0.0031926605804011112\n",
      "Epoch 250, Training Loss 0.0032220035368609994\n",
      "Epoch 250, Training Loss 0.0032300767089809526\n",
      "Epoch 250, Training Loss 0.003288485221159847\n",
      "Epoch 250, Training Loss 0.0033281140757100585\n",
      "Epoch 250, Training Loss 0.0033583702689841807\n",
      "Epoch 250, Training Loss 0.003373787802098619\n",
      "Epoch 250, Training Loss 0.003390758771739919\n",
      "Epoch 250, Training Loss 0.0034125424258153684\n",
      "Epoch 250, Training Loss 0.003467259527829564\n",
      "Epoch 250, Training Loss 0.003527704782931663\n",
      "Epoch 250, Training Loss 0.0035867380672622746\n",
      "Epoch 250, Training Loss 0.0036408238230830493\n",
      "Epoch 250, Training Loss 0.0036503681853470747\n",
      "Epoch 250, Training Loss 0.0036705779733464997\n",
      "Epoch 250, Training Loss 0.003816507388706631\n",
      "Epoch 250, Training Loss 0.0038442210451511624\n",
      "Epoch 250, Training Loss 0.0039045065367484793\n",
      "Epoch 250, Training Loss 0.003957055210162078\n",
      "Epoch 250, Training Loss 0.004075068443337136\n",
      "Epoch 250, Training Loss 0.004166190244514695\n",
      "Epoch 250, Training Loss 0.004192420137364922\n",
      "Epoch 250, Training Loss 0.0042291226287079434\n",
      "Epoch 250, Training Loss 0.004325808146182457\n",
      "Epoch 250, Training Loss 0.004439538419770691\n",
      "Epoch 250, Training Loss 0.004468433917416712\n",
      "Epoch 250, Training Loss 0.004500038761054845\n",
      "Epoch 250, Training Loss 0.004518383383855719\n",
      "Epoch 250, Training Loss 0.004593648610736632\n",
      "Epoch 250, Training Loss 0.0046237283029005195\n",
      "Epoch 250, Training Loss 0.0046450649054668594\n",
      "Epoch 250, Training Loss 0.004742571463584519\n",
      "Epoch 250, Training Loss 0.004766517901159537\n",
      "Epoch 250, Training Loss 0.0047874152553184415\n",
      "Epoch 250, Training Loss 0.004849401965994588\n",
      "Epoch 250, Training Loss 0.004890093721611344\n",
      "Epoch 250, Training Loss 0.005000904053592545\n",
      "Epoch 250, Training Loss 0.00511788846829625\n",
      "Epoch 250, Training Loss 0.005167775045451529\n",
      "Epoch 250, Training Loss 0.005198487713027869\n",
      "Epoch 250, Training Loss 0.005227045538356466\n",
      "Epoch 250, Training Loss 0.0053287809202566625\n",
      "Epoch 250, Training Loss 0.005349960017358632\n",
      "Epoch 250, Training Loss 0.0053706615984611345\n",
      "Epoch 250, Training Loss 0.005400452933147969\n",
      "Epoch 250, Training Loss 0.005481904795360001\n",
      "Epoch 250, Training Loss 0.005608558491625063\n",
      "Epoch 250, Training Loss 0.005682763687627928\n",
      "Epoch 250, Training Loss 0.005700158885420512\n",
      "Epoch 250, Training Loss 0.005712312118619528\n",
      "Epoch 250, Training Loss 0.005761329245412975\n",
      "Epoch 250, Training Loss 0.0058191421303583684\n",
      "Epoch 250, Training Loss 0.005918266093286941\n",
      "Epoch 250, Training Loss 0.0059534164748209365\n",
      "Epoch 250, Training Loss 0.00601453868948552\n",
      "Epoch 250, Training Loss 0.0060403986305208\n",
      "Epoch 250, Training Loss 0.006127557408807756\n",
      "Epoch 250, Training Loss 0.006223357879244686\n",
      "Epoch 250, Training Loss 0.006277970334900843\n",
      "Epoch 250, Training Loss 0.006321438827582866\n",
      "Epoch 250, Training Loss 0.006350957491980566\n",
      "Epoch 250, Training Loss 0.006356204670551412\n",
      "Epoch 250, Training Loss 0.006379326027782296\n",
      "Epoch 250, Training Loss 0.006398844786340852\n",
      "Epoch 250, Training Loss 0.00648452035601601\n",
      "Epoch 250, Training Loss 0.006605076133166356\n",
      "Epoch 250, Training Loss 0.006687412956667602\n",
      "Epoch 250, Training Loss 0.006780161804583905\n",
      "Epoch 250, Training Loss 0.006800758074778502\n",
      "Epoch 250, Training Loss 0.006841407727945567\n",
      "Epoch 250, Training Loss 0.006860793850448964\n",
      "Epoch 250, Training Loss 0.00692059651679357\n",
      "Epoch 250, Training Loss 0.007012274110322947\n",
      "Epoch 250, Training Loss 0.007040136585445584\n",
      "Epoch 250, Training Loss 0.007144397828499298\n",
      "Epoch 250, Training Loss 0.007206416899657539\n",
      "Epoch 250, Training Loss 0.007293425322941425\n",
      "Epoch 250, Training Loss 0.00736041738034781\n",
      "Epoch 250, Training Loss 0.0074115847809063965\n",
      "Epoch 250, Training Loss 0.00747264534249292\n",
      "Epoch 250, Training Loss 0.007540746893295471\n",
      "Epoch 250, Training Loss 0.007701671202345501\n",
      "Epoch 250, Training Loss 0.007816392603232657\n",
      "Epoch 250, Training Loss 0.007914959983018886\n",
      "Epoch 250, Training Loss 0.007945984686531907\n",
      "Epoch 250, Training Loss 0.008026624330655312\n",
      "Epoch 250, Training Loss 0.008110188699239278\n",
      "Epoch 250, Training Loss 0.008160435776833607\n",
      "Epoch 250, Training Loss 0.008210578603465158\n",
      "Epoch 250, Training Loss 0.008242468901045259\n",
      "Epoch 250, Training Loss 0.008269228001870691\n",
      "Epoch 250, Training Loss 0.00828646296216056\n",
      "Epoch 250, Training Loss 0.008351890059411907\n",
      "Epoch 250, Training Loss 0.008457179825581477\n",
      "Epoch 250, Training Loss 0.008498581806364496\n",
      "Epoch 250, Training Loss 0.00853262898033423\n",
      "Epoch 250, Training Loss 0.008548188337679867\n",
      "Epoch 250, Training Loss 0.00856496326630111\n",
      "Epoch 250, Training Loss 0.008586955010233557\n",
      "Epoch 250, Training Loss 0.00863844636337036\n",
      "Epoch 250, Training Loss 0.008727943461597956\n",
      "Epoch 250, Training Loss 0.008765775218482137\n",
      "Epoch 250, Training Loss 0.00881652983591494\n",
      "Epoch 250, Training Loss 0.008846518872996501\n",
      "Epoch 250, Training Loss 0.00888197524282519\n",
      "Epoch 250, Training Loss 0.008890732784000465\n",
      "Epoch 250, Training Loss 0.009031408187121039\n",
      "Epoch 250, Training Loss 0.009046482933980539\n",
      "Epoch 250, Training Loss 0.009071365450782812\n",
      "Epoch 250, Training Loss 0.009128462575028277\n",
      "Epoch 250, Training Loss 0.009236727800110684\n",
      "Epoch 250, Training Loss 0.009325588835984507\n",
      "Epoch 250, Training Loss 0.009359180466974598\n",
      "Epoch 250, Training Loss 0.009396871265447925\n",
      "Epoch 250, Training Loss 0.00946021914098631\n",
      "Epoch 250, Training Loss 0.00947636916645138\n",
      "Epoch 250, Training Loss 0.00955580603247961\n",
      "Epoch 250, Training Loss 0.009640519324418567\n",
      "Epoch 250, Training Loss 0.00966769319249179\n",
      "Epoch 250, Training Loss 0.00972319458244497\n",
      "Epoch 250, Training Loss 0.0097972805262841\n",
      "Epoch 250, Training Loss 0.009821799710569212\n",
      "Epoch 250, Training Loss 0.009882170388408367\n",
      "Epoch 250, Training Loss 0.009942034722122428\n",
      "Epoch 250, Training Loss 0.0100129205796658\n",
      "Epoch 250, Training Loss 0.010039794018796986\n",
      "Epoch 250, Training Loss 0.010076206494861132\n",
      "Epoch 250, Training Loss 0.010105273794130329\n",
      "Epoch 250, Training Loss 0.010123214320591687\n",
      "Epoch 250, Training Loss 0.010215012552550116\n",
      "Epoch 250, Training Loss 0.01024216583386883\n",
      "Epoch 250, Training Loss 0.01029043917453674\n",
      "Epoch 250, Training Loss 0.010313551690157912\n",
      "Epoch 250, Training Loss 0.010356282584888436\n",
      "Epoch 250, Training Loss 0.010411103409083794\n",
      "Epoch 250, Training Loss 0.01044982910225325\n",
      "Epoch 250, Training Loss 0.01048848998871015\n",
      "Epoch 250, Training Loss 0.010563542612035142\n",
      "Epoch 250, Training Loss 0.01059138679417217\n",
      "Epoch 250, Training Loss 0.010610115734021873\n",
      "Epoch 250, Training Loss 0.010682907249049647\n",
      "Epoch 250, Training Loss 0.010741629042898488\n",
      "Epoch 250, Training Loss 0.010754676167126697\n",
      "Epoch 250, Training Loss 0.010867202514544358\n",
      "Epoch 250, Training Loss 0.010904804101366255\n",
      "Epoch 250, Training Loss 0.01101347570702948\n",
      "Epoch 250, Training Loss 0.01104854710955087\n",
      "Epoch 250, Training Loss 0.011104781933895806\n",
      "Epoch 250, Training Loss 0.011136414924436403\n",
      "Epoch 250, Training Loss 0.011148227375748632\n",
      "Epoch 250, Training Loss 0.011342486344597034\n",
      "Epoch 250, Training Loss 0.011355800167693164\n",
      "Epoch 250, Training Loss 0.011667197336183141\n",
      "Epoch 250, Training Loss 0.011760825676548169\n",
      "Epoch 250, Training Loss 0.011847788643668337\n",
      "Epoch 250, Training Loss 0.012060979023323301\n",
      "Epoch 250, Training Loss 0.012203793524338119\n",
      "Epoch 250, Training Loss 0.012248879612382987\n",
      "Epoch 250, Training Loss 0.01229238027975897\n",
      "Epoch 250, Training Loss 0.01241708806920749\n",
      "Epoch 250, Training Loss 0.012451851044369437\n",
      "Epoch 250, Training Loss 0.012467869479433083\n",
      "Epoch 250, Training Loss 0.012483487020620642\n",
      "Epoch 250, Training Loss 0.012515509146792085\n",
      "Epoch 250, Training Loss 0.012579360879752833\n",
      "Epoch 250, Training Loss 0.012659234771876575\n",
      "Epoch 250, Training Loss 0.012799678339987346\n",
      "Epoch 250, Training Loss 0.012831819432494623\n",
      "Epoch 250, Training Loss 0.012960983555206595\n",
      "Epoch 250, Training Loss 0.013151841743699159\n",
      "Epoch 250, Training Loss 0.013168780237336255\n",
      "Epoch 250, Training Loss 0.013215870631601461\n",
      "Epoch 250, Training Loss 0.013255490108614649\n",
      "Epoch 250, Training Loss 0.013426539337481646\n",
      "Epoch 250, Training Loss 0.013512844786100337\n",
      "Epoch 250, Training Loss 0.013622200806670443\n",
      "Epoch 250, Training Loss 0.013667856020938672\n",
      "Epoch 250, Training Loss 0.013720905036091462\n",
      "Epoch 250, Training Loss 0.013775802657479788\n",
      "Epoch 250, Training Loss 0.013804059837113523\n",
      "Epoch 250, Training Loss 0.01388498065966989\n",
      "Epoch 250, Training Loss 0.013920964663038436\n",
      "Epoch 250, Training Loss 0.014027257932855003\n",
      "Epoch 250, Training Loss 0.014077410106058888\n",
      "Epoch 250, Training Loss 0.014244227614753958\n",
      "Epoch 250, Training Loss 0.014253897822278616\n",
      "Epoch 250, Training Loss 0.014321487088499547\n",
      "Epoch 250, Training Loss 0.014442094438530676\n",
      "Epoch 250, Training Loss 0.014584018013980763\n",
      "Epoch 250, Training Loss 0.014666984106659356\n",
      "Epoch 250, Training Loss 0.014749186086084913\n",
      "Epoch 250, Training Loss 0.014861884968750693\n",
      "Epoch 250, Training Loss 0.014873232041030665\n",
      "Epoch 250, Training Loss 0.01494100757176652\n",
      "Epoch 250, Training Loss 0.014969029081532794\n",
      "Epoch 250, Training Loss 0.014989347810454457\n",
      "Epoch 250, Training Loss 0.015003865354877832\n",
      "Epoch 250, Training Loss 0.015059659399254166\n",
      "Epoch 250, Training Loss 0.015126923240883194\n",
      "Epoch 250, Training Loss 0.015183765949714748\n",
      "Epoch 250, Training Loss 0.015211215541671838\n",
      "Epoch 250, Training Loss 0.015282215949509988\n",
      "Epoch 250, Training Loss 0.015374825732625278\n",
      "Epoch 250, Training Loss 0.015461924601384364\n",
      "Epoch 250, Training Loss 0.015490780828658806\n",
      "Epoch 250, Training Loss 0.015581456643036184\n",
      "Epoch 250, Training Loss 0.01564168958993786\n",
      "Epoch 250, Training Loss 0.015682812907573437\n",
      "Epoch 250, Training Loss 0.01571190077216005\n",
      "Epoch 250, Training Loss 0.015835013451373867\n",
      "Epoch 250, Training Loss 0.015867083696672297\n",
      "Epoch 250, Training Loss 0.015928482559397626\n",
      "Epoch 250, Training Loss 0.0160340530943135\n",
      "Epoch 250, Training Loss 0.01610266555295995\n",
      "Epoch 250, Training Loss 0.016205659215969732\n",
      "Epoch 250, Training Loss 0.0162121190551354\n",
      "Epoch 250, Training Loss 0.0162333143729707\n",
      "Epoch 250, Training Loss 0.016257961389020352\n",
      "Epoch 250, Training Loss 0.016282248297644318\n",
      "Epoch 250, Training Loss 0.01636121772072466\n",
      "Epoch 250, Training Loss 0.016459144741211018\n",
      "Epoch 250, Training Loss 0.016506309965220484\n",
      "Epoch 250, Training Loss 0.01653958862418752\n",
      "Epoch 250, Training Loss 0.016597556775612067\n",
      "Epoch 250, Training Loss 0.016627969512063294\n",
      "Epoch 250, Training Loss 0.016656426544946708\n",
      "Epoch 250, Training Loss 0.01668042803297529\n",
      "Epoch 250, Training Loss 0.016722149421672914\n",
      "Epoch 250, Training Loss 0.016805936459718687\n",
      "Epoch 250, Training Loss 0.016836793291742157\n",
      "Epoch 250, Training Loss 0.016879981758830417\n",
      "Epoch 250, Training Loss 0.016995830045741463\n",
      "Epoch 250, Training Loss 0.017056149112589448\n",
      "Epoch 250, Training Loss 0.01711329047346626\n",
      "Epoch 250, Training Loss 0.01720535474450654\n",
      "Epoch 250, Training Loss 0.017304907392119737\n",
      "Epoch 250, Training Loss 0.01732911058651555\n",
      "Epoch 250, Training Loss 0.017359173107568338\n",
      "Epoch 250, Training Loss 0.017387988994998473\n",
      "Epoch 250, Training Loss 0.017439491857472053\n",
      "Epoch 250, Training Loss 0.017467583876217493\n",
      "Epoch 250, Training Loss 0.01754838588964337\n",
      "Epoch 250, Training Loss 0.017582800061873082\n",
      "Epoch 250, Training Loss 0.017622113088503137\n",
      "Epoch 250, Training Loss 0.017668084137837218\n",
      "Epoch 250, Training Loss 0.017684760000890174\n",
      "Epoch 250, Training Loss 0.01772301898711859\n",
      "Epoch 250, Training Loss 0.01778506964702359\n",
      "Epoch 250, Training Loss 0.01782078310475706\n",
      "Epoch 250, Training Loss 0.01786758340752262\n",
      "Epoch 250, Training Loss 0.017948735912647243\n",
      "Epoch 250, Training Loss 0.017986809458497843\n",
      "Epoch 250, Training Loss 0.018085573935676415\n",
      "Epoch 250, Training Loss 0.018198197912377166\n",
      "Epoch 250, Training Loss 0.018309423819069973\n",
      "Epoch 250, Training Loss 0.0184380430966387\n",
      "Epoch 250, Training Loss 0.018486433278988388\n",
      "Epoch 250, Training Loss 0.018557677226488854\n",
      "Epoch 250, Training Loss 0.018633392446524347\n",
      "Epoch 250, Training Loss 0.01867123495053757\n",
      "Epoch 250, Training Loss 0.018747377588087335\n",
      "Epoch 250, Training Loss 0.018812672023082632\n",
      "Epoch 250, Training Loss 0.0189023884561132\n",
      "Epoch 250, Training Loss 0.018968850907767214\n",
      "Epoch 250, Training Loss 0.019176777709475564\n",
      "Epoch 250, Training Loss 0.019234002184341934\n",
      "Epoch 250, Training Loss 0.01944851272684686\n",
      "Epoch 250, Training Loss 0.019507718665520552\n",
      "Epoch 250, Training Loss 0.019605609643108705\n",
      "Epoch 250, Training Loss 0.01976272055064626\n",
      "Epoch 250, Training Loss 0.019834388028401546\n",
      "Epoch 250, Training Loss 0.01994817630599832\n",
      "Epoch 250, Training Loss 0.01998739831549737\n",
      "Epoch 250, Training Loss 0.02008407431490281\n",
      "Epoch 250, Training Loss 0.02011782628819918\n",
      "Epoch 250, Training Loss 0.020225691478080152\n",
      "Epoch 250, Training Loss 0.02033581508947608\n",
      "Epoch 250, Training Loss 0.02041792867185972\n",
      "Epoch 250, Training Loss 0.020450333724050876\n",
      "Epoch 250, Training Loss 0.020485459633952822\n",
      "Epoch 250, Training Loss 0.020505536597727053\n",
      "Epoch 250, Training Loss 0.020553369101260783\n",
      "Epoch 250, Training Loss 0.02061342741684307\n",
      "Epoch 250, Training Loss 0.0206626486676314\n",
      "Epoch 250, Training Loss 0.020712840809103322\n",
      "Epoch 250, Training Loss 0.020790814469708964\n",
      "Epoch 250, Training Loss 0.020851539850444592\n",
      "Epoch 250, Training Loss 0.020898558020763233\n",
      "Epoch 250, Training Loss 0.02093097071885072\n",
      "Epoch 250, Training Loss 0.020974858659212395\n",
      "Epoch 250, Training Loss 0.021081910516275927\n",
      "Epoch 250, Training Loss 0.02111861666859797\n",
      "Epoch 250, Training Loss 0.021208019195901005\n",
      "Epoch 250, Training Loss 0.021361638782331552\n",
      "Epoch 250, Training Loss 0.02139185795374691\n",
      "Epoch 250, Training Loss 0.021414145608158672\n",
      "Epoch 250, Training Loss 0.02144354594695141\n",
      "Epoch 250, Training Loss 0.02149893697875235\n",
      "Epoch 250, Training Loss 0.021555493783462994\n",
      "Epoch 250, Training Loss 0.021583935017209222\n",
      "Epoch 250, Training Loss 0.021703047506377825\n",
      "Epoch 250, Training Loss 0.021792271219746535\n",
      "Epoch 250, Training Loss 0.0218520284918568\n",
      "Epoch 250, Training Loss 0.02190176749606724\n",
      "Epoch 250, Training Loss 0.021958376159486565\n",
      "Epoch 250, Training Loss 0.021995743086366246\n",
      "Epoch 250, Training Loss 0.022050810308502915\n",
      "Epoch 250, Training Loss 0.02212903226780541\n",
      "Epoch 250, Training Loss 0.022239313675257402\n",
      "Epoch 250, Training Loss 0.022281055086199434\n",
      "Epoch 250, Training Loss 0.02233614358345947\n",
      "Epoch 250, Training Loss 0.02237511539350614\n",
      "Epoch 250, Training Loss 0.022447475332223698\n",
      "Epoch 250, Training Loss 0.02257947878354727\n",
      "Epoch 250, Training Loss 0.02264527166429002\n",
      "Epoch 250, Training Loss 0.022727102391383685\n",
      "Epoch 250, Training Loss 0.022771429749267637\n",
      "Epoch 250, Training Loss 0.022823228619878403\n",
      "Epoch 250, Training Loss 0.022909294982986224\n",
      "Epoch 250, Training Loss 0.022972090143586515\n",
      "Epoch 250, Training Loss 0.02302563943378532\n",
      "Epoch 250, Training Loss 0.02309003042871766\n",
      "Epoch 250, Training Loss 0.023154696228120792\n",
      "Epoch 250, Training Loss 0.023182501756798122\n",
      "Epoch 250, Training Loss 0.02324165490305866\n",
      "Epoch 250, Training Loss 0.023277588352999267\n",
      "Epoch 250, Training Loss 0.02346355840801964\n",
      "Epoch 250, Training Loss 0.023578420140878166\n",
      "Epoch 250, Training Loss 0.023604028581944117\n",
      "Epoch 250, Training Loss 0.023670039339767544\n",
      "Epoch 250, Training Loss 0.02391271532544166\n",
      "Epoch 250, Training Loss 0.024049223834158057\n",
      "Epoch 250, Training Loss 0.024092475812205725\n",
      "Epoch 250, Training Loss 0.024129050026369066\n",
      "Epoch 250, Training Loss 0.024242593063627516\n",
      "Epoch 250, Training Loss 0.024358963167480648\n",
      "Epoch 250, Training Loss 0.024392350900756275\n",
      "Epoch 250, Training Loss 0.02448083663506962\n",
      "Epoch 250, Training Loss 0.02453881085080945\n",
      "Epoch 250, Training Loss 0.024577553810365974\n",
      "Epoch 250, Training Loss 0.024612088594823847\n",
      "Epoch 250, Training Loss 0.024645871163615028\n",
      "Epoch 250, Training Loss 0.02480491762147154\n",
      "Epoch 250, Training Loss 0.0249500054642177\n",
      "Epoch 250, Training Loss 0.024993600974054744\n",
      "Epoch 250, Training Loss 0.025092528182584457\n",
      "Epoch 250, Training Loss 0.025220161944608705\n",
      "Epoch 250, Training Loss 0.02535016638229189\n",
      "Epoch 250, Training Loss 0.025461391441028595\n",
      "Epoch 250, Training Loss 0.025485664258337083\n",
      "Epoch 250, Training Loss 0.025500984782653162\n",
      "Epoch 250, Training Loss 0.025563065640275818\n",
      "Epoch 250, Training Loss 0.025600652740386975\n",
      "Epoch 250, Training Loss 0.025638888726997024\n",
      "Epoch 250, Training Loss 0.025647160158280637\n",
      "Epoch 250, Training Loss 0.025699287992985466\n",
      "Epoch 250, Training Loss 0.025746206104483867\n",
      "Epoch 250, Training Loss 0.02595721352118951\n",
      "Epoch 250, Training Loss 0.026205667212981818\n",
      "Epoch 250, Training Loss 0.02624847057461262\n",
      "Epoch 250, Training Loss 0.026280224458445484\n",
      "Epoch 250, Training Loss 0.02634236232444282\n",
      "Epoch 250, Training Loss 0.026488199998574602\n",
      "Epoch 250, Training Loss 0.02656835140100659\n",
      "Epoch 250, Training Loss 0.026583015862161583\n",
      "Epoch 250, Training Loss 0.02668483380068694\n",
      "Epoch 250, Training Loss 0.026744334551784425\n",
      "Epoch 250, Training Loss 0.026820080079104934\n",
      "Epoch 250, Training Loss 0.026929830780367144\n",
      "Epoch 250, Training Loss 0.026978244657849756\n",
      "Epoch 250, Training Loss 0.02701890444837968\n",
      "Epoch 250, Training Loss 0.02706974985904496\n",
      "Epoch 250, Training Loss 0.027075623664076027\n",
      "Epoch 250, Training Loss 0.02710555751255387\n",
      "Epoch 250, Training Loss 0.02717819757690496\n",
      "Epoch 250, Training Loss 0.027218717540426138\n",
      "Epoch 250, Training Loss 0.02731377705919754\n",
      "Epoch 250, Training Loss 0.02739046450378016\n",
      "Epoch 250, Training Loss 0.027418080657182257\n",
      "Epoch 250, Training Loss 0.02748019317024485\n",
      "Epoch 250, Training Loss 0.027519466833609257\n",
      "Epoch 250, Training Loss 0.027562233037613047\n",
      "Epoch 250, Training Loss 0.027574446944929564\n",
      "Epoch 250, Training Loss 0.027618442532842232\n",
      "Epoch 250, Training Loss 0.02772052698682927\n",
      "Epoch 250, Training Loss 0.027738449195295077\n",
      "Epoch 250, Training Loss 0.027839831322493493\n",
      "Epoch 250, Training Loss 0.027935231595759846\n",
      "Epoch 250, Training Loss 0.028015673704221464\n",
      "Epoch 250, Training Loss 0.02818161274944825\n",
      "Epoch 250, Training Loss 0.028286994152042604\n",
      "Epoch 250, Training Loss 0.02832311603164448\n",
      "Epoch 250, Training Loss 0.028378199869909745\n",
      "Epoch 250, Training Loss 0.02845509836505003\n",
      "Epoch 250, Training Loss 0.028482251753554205\n",
      "Epoch 250, Training Loss 0.028535199075903926\n",
      "Epoch 250, Training Loss 0.02865378569890185\n",
      "Epoch 250, Training Loss 0.02876987291888222\n",
      "Epoch 250, Training Loss 0.028828661010512496\n",
      "Epoch 250, Training Loss 0.028901579075843058\n",
      "Epoch 250, Training Loss 0.028976472283420547\n",
      "Epoch 250, Training Loss 0.029068090710574594\n",
      "Epoch 250, Training Loss 0.029118502834726057\n",
      "Epoch 250, Training Loss 0.029426422868700473\n",
      "Epoch 250, Training Loss 0.029444825494318934\n",
      "Epoch 250, Training Loss 0.02951187890944788\n",
      "Epoch 250, Training Loss 0.029573072076124876\n",
      "Epoch 250, Training Loss 0.029611841889212617\n",
      "Epoch 250, Training Loss 0.029733172069420406\n",
      "Epoch 250, Training Loss 0.029779744157662897\n",
      "Epoch 250, Training Loss 0.029826956024021863\n",
      "Epoch 250, Training Loss 0.029879794541217596\n",
      "Epoch 250, Training Loss 0.029942035181400227\n",
      "Epoch 250, Training Loss 0.02997467788519895\n",
      "Epoch 250, Training Loss 0.030050036330085697\n",
      "Epoch 250, Training Loss 0.03012380045552826\n",
      "Epoch 250, Training Loss 0.03033038710548883\n",
      "Epoch 250, Training Loss 0.03046244245363147\n",
      "Epoch 250, Training Loss 0.030531770196598012\n",
      "Epoch 250, Training Loss 0.030603481315509856\n",
      "Epoch 250, Training Loss 0.030681526290176585\n",
      "Epoch 250, Training Loss 0.030704480819189753\n",
      "Epoch 250, Training Loss 0.03071500779167675\n",
      "Epoch 250, Training Loss 0.030794311633400257\n",
      "Epoch 250, Training Loss 0.030857301557131702\n",
      "Epoch 250, Training Loss 0.03088877793065632\n",
      "Epoch 250, Training Loss 0.03095436728287898\n",
      "Epoch 250, Training Loss 0.03097759739286207\n",
      "Epoch 250, Training Loss 0.031065839916215185\n",
      "Epoch 250, Training Loss 0.03114881490766907\n",
      "Epoch 250, Training Loss 0.0313967707825353\n",
      "Epoch 250, Training Loss 0.03143035366004595\n",
      "Epoch 250, Training Loss 0.031482884030946345\n",
      "Epoch 250, Training Loss 0.03154015344748621\n",
      "Epoch 250, Training Loss 0.031593087364507416\n",
      "Epoch 250, Training Loss 0.03163407954430241\n",
      "Epoch 250, Training Loss 0.0318316312315052\n",
      "Epoch 250, Training Loss 0.03196635942303521\n",
      "Epoch 250, Training Loss 0.032011876596719065\n",
      "Epoch 250, Training Loss 0.03204153640769765\n",
      "Epoch 250, Training Loss 0.03209561469388263\n",
      "Epoch 250, Training Loss 0.03219229837253099\n",
      "Epoch 250, Training Loss 0.03224047903350228\n",
      "Epoch 250, Training Loss 0.03227706109125958\n",
      "Epoch 250, Training Loss 0.03229372476494354\n",
      "Epoch 250, Training Loss 0.03233793228471656\n",
      "Epoch 250, Training Loss 0.03243122787376785\n",
      "Epoch 250, Training Loss 0.03249489826321735\n",
      "Epoch 250, Training Loss 0.032521115460545015\n",
      "Epoch 250, Training Loss 0.03261447032077519\n",
      "Epoch 250, Training Loss 0.03275491944347482\n",
      "Epoch 250, Training Loss 0.03284264349407228\n",
      "Epoch 250, Training Loss 0.03289945974953168\n",
      "Epoch 250, Training Loss 0.033015981361227076\n",
      "Epoch 250, Training Loss 0.03305802558301031\n",
      "Epoch 250, Training Loss 0.033094230054965826\n",
      "Epoch 250, Training Loss 0.03313755170832795\n",
      "Epoch 250, Training Loss 0.03318935662896261\n",
      "Epoch 250, Training Loss 0.03328286324475256\n",
      "Epoch 250, Training Loss 0.033362568484123827\n",
      "Epoch 250, Training Loss 0.033441293019386926\n",
      "Epoch 250, Training Loss 0.033481252650656476\n",
      "Epoch 250, Training Loss 0.03354730331481856\n",
      "Epoch 250, Training Loss 0.03363424886549201\n",
      "Epoch 250, Training Loss 0.033764613436801774\n",
      "Epoch 250, Training Loss 0.03380446995327444\n",
      "Epoch 250, Training Loss 0.03384064096907425\n",
      "Epoch 250, Training Loss 0.03392494863311253\n",
      "Epoch 250, Training Loss 0.033941312071979236\n",
      "Epoch 250, Training Loss 0.03412775734446638\n",
      "Epoch 250, Training Loss 0.0342368144938565\n",
      "Epoch 250, Training Loss 0.034298964645689746\n",
      "Epoch 250, Training Loss 0.03439091951133745\n",
      "Epoch 250, Training Loss 0.03449828935193989\n",
      "Epoch 250, Training Loss 0.034529171184377025\n",
      "Epoch 250, Training Loss 0.03458958079197618\n",
      "Epoch 250, Training Loss 0.03464858384524255\n",
      "Epoch 250, Training Loss 0.0346875517203918\n",
      "Epoch 250, Training Loss 0.03474451795451419\n",
      "Epoch 250, Training Loss 0.03479701492166066\n",
      "Epoch 250, Training Loss 0.03483036784347995\n",
      "Epoch 250, Training Loss 0.03489585747987584\n",
      "Epoch 250, Training Loss 0.034942750462338024\n",
      "Epoch 250, Training Loss 0.03509118201513596\n",
      "Epoch 250, Training Loss 0.03520318440190228\n",
      "Epoch 250, Training Loss 0.035282569051936954\n",
      "Epoch 250, Training Loss 0.03534052418390065\n",
      "Epoch 250, Training Loss 0.03536742465758739\n",
      "Epoch 250, Training Loss 0.03539757541371295\n",
      "Epoch 250, Training Loss 0.035412100755283255\n",
      "Epoch 250, Training Loss 0.03545591415649237\n",
      "Epoch 250, Training Loss 0.03549409433282302\n",
      "Epoch 250, Training Loss 0.03555106786032067\n",
      "Epoch 250, Training Loss 0.03557550080794641\n",
      "Epoch 250, Training Loss 0.03560013428766309\n",
      "Epoch 250, Training Loss 0.035654356959811825\n",
      "Epoch 250, Training Loss 0.03569107141543437\n",
      "Epoch 250, Training Loss 0.03571001156185613\n",
      "Epoch 250, Training Loss 0.035749559700989246\n",
      "Epoch 250, Training Loss 0.03579643660563204\n",
      "Epoch 250, Training Loss 0.03582441211616158\n",
      "Epoch 250, Training Loss 0.03583712742635337\n",
      "Epoch 250, Training Loss 0.0358975093277014\n",
      "Epoch 250, Training Loss 0.03594587676291404\n",
      "Epoch 250, Training Loss 0.03596829737255545\n",
      "Epoch 250, Training Loss 0.036116908129919176\n",
      "Epoch 250, Training Loss 0.036159446795859265\n",
      "Epoch 250, Training Loss 0.036289021700901716\n",
      "Epoch 250, Training Loss 0.036354315592824\n",
      "Epoch 250, Training Loss 0.03637546221987652\n",
      "Epoch 250, Training Loss 0.036402204209138325\n",
      "Epoch 250, Training Loss 0.0364414596390646\n",
      "Epoch 250, Training Loss 0.036476254498328815\n",
      "Epoch 250, Training Loss 0.03651408172007698\n",
      "Epoch 250, Training Loss 0.03659191268586728\n",
      "Epoch 250, Training Loss 0.03662360077926322\n",
      "Epoch 250, Training Loss 0.036659059285655465\n",
      "Epoch 250, Training Loss 0.03671804023966136\n",
      "Epoch 250, Training Loss 0.03688114943623524\n",
      "Epoch 250, Training Loss 0.036942021832551304\n",
      "Epoch 250, Training Loss 0.03698050280701836\n",
      "Epoch 250, Training Loss 0.03703048948586807\n",
      "Epoch 250, Training Loss 0.037134197651935014\n",
      "Epoch 250, Training Loss 0.03715164109985904\n",
      "Epoch 250, Training Loss 0.037170798702956274\n",
      "Epoch 250, Training Loss 0.037182668220999714\n",
      "Epoch 250, Training Loss 0.03725859403426823\n",
      "Epoch 250, Training Loss 0.037295261225865586\n",
      "Epoch 250, Training Loss 0.03731078503693899\n",
      "Epoch 250, Training Loss 0.03738656081914273\n",
      "Epoch 250, Training Loss 0.037459145661543514\n",
      "Epoch 250, Training Loss 0.03753177468341006\n",
      "Epoch 250, Training Loss 0.03755277597412105\n",
      "Epoch 250, Training Loss 0.03757716368709969\n",
      "Epoch 250, Training Loss 0.03767366774196801\n",
      "Epoch 250, Training Loss 0.037763236478194975\n",
      "Epoch 250, Training Loss 0.03781952640063146\n",
      "Epoch 250, Training Loss 0.0378917490180982\n",
      "Epoch 250, Training Loss 0.03792737396445859\n",
      "Epoch 250, Training Loss 0.03804073533903131\n",
      "Epoch 250, Training Loss 0.03806503330919501\n",
      "Epoch 250, Training Loss 0.03812374815742588\n",
      "Epoch 250, Training Loss 0.03819639581050772\n",
      "Epoch 250, Training Loss 0.038254039371005064\n",
      "Epoch 250, Training Loss 0.03828067928397804\n",
      "Epoch 250, Training Loss 0.03832954980487771\n",
      "Epoch 250, Training Loss 0.03838298590722806\n",
      "Epoch 250, Training Loss 0.03845238388763726\n",
      "Epoch 250, Training Loss 0.03850786858047728\n",
      "Epoch 250, Training Loss 0.03853705355509296\n",
      "Epoch 250, Training Loss 0.038659092655186265\n",
      "Epoch 250, Training Loss 0.03886584647635803\n",
      "Epoch 250, Training Loss 0.038930386282822776\n",
      "Epoch 250, Training Loss 0.03904578804938823\n",
      "Epoch 250, Training Loss 0.0391115728554928\n",
      "Epoch 250, Training Loss 0.03916395511990294\n",
      "Epoch 250, Training Loss 0.039250430340766716\n",
      "Epoch 250, Training Loss 0.039312325738956364\n",
      "Epoch 250, Training Loss 0.03945285032022163\n",
      "Epoch 250, Training Loss 0.03948525367411868\n",
      "Epoch 250, Training Loss 0.0395080864285369\n",
      "Epoch 250, Training Loss 0.03954413754429639\n",
      "Epoch 250, Training Loss 0.03960796836894625\n",
      "Epoch 250, Training Loss 0.039752224633527344\n",
      "Epoch 250, Training Loss 0.039773498511161\n",
      "Epoch 250, Training Loss 0.03982091504935165\n",
      "Epoch 250, Training Loss 0.03996197614864067\n",
      "Epoch 250, Training Loss 0.04000506840605775\n",
      "Epoch 250, Training Loss 0.040031380231475545\n",
      "Epoch 250, Training Loss 0.0400997949019789\n",
      "Epoch 250, Training Loss 0.04024761787001187\n",
      "Epoch 250, Training Loss 0.040380526776485084\n",
      "Epoch 250, Training Loss 0.04043757142272809\n",
      "Epoch 250, Training Loss 0.040528969800509414\n",
      "Epoch 250, Training Loss 0.040588039970930544\n",
      "Epoch 250, Training Loss 0.04064314181635351\n",
      "Epoch 250, Training Loss 0.04070276083827228\n",
      "Epoch 250, Training Loss 0.040738347540859636\n",
      "Epoch 250, Training Loss 0.04076568749399803\n",
      "Epoch 250, Training Loss 0.04084249104663749\n",
      "Epoch 250, Training Loss 0.040879398364635645\n",
      "Epoch 250, Training Loss 0.04092233421226196\n",
      "Epoch 250, Training Loss 0.040999439823062485\n",
      "Epoch 250, Training Loss 0.04106269285137601\n",
      "Epoch 250, Training Loss 0.04121334951721094\n",
      "Epoch 250, Training Loss 0.041258438963733636\n",
      "Epoch 250, Training Loss 0.04128780263795248\n",
      "Epoch 250, Training Loss 0.041312446181193146\n",
      "Epoch 250, Training Loss 0.04134256457445471\n",
      "Epoch 250, Training Loss 0.04141920497176497\n",
      "Epoch 250, Training Loss 0.04145076036479448\n",
      "Epoch 250, Training Loss 0.04154809306332808\n",
      "Epoch 250, Training Loss 0.04163469368641925\n",
      "Epoch 250, Training Loss 0.041682101016187724\n",
      "Epoch 250, Training Loss 0.04173774675399427\n",
      "Epoch 250, Training Loss 0.04177743080905769\n",
      "Epoch 250, Training Loss 0.041812627578077986\n",
      "Epoch 250, Training Loss 0.04188557978555122\n",
      "Epoch 250, Training Loss 0.0419527765100255\n",
      "Epoch 250, Training Loss 0.042003521561751245\n",
      "Epoch 250, Training Loss 0.04203052857064206\n",
      "Epoch 250, Training Loss 0.04205850977629252\n",
      "Epoch 250, Training Loss 0.042149500210967174\n",
      "Epoch 250, Training Loss 0.04218728915559571\n",
      "Epoch 250, Training Loss 0.042203719598476956\n",
      "Epoch 250, Training Loss 0.04223437765863298\n",
      "Epoch 250, Training Loss 0.04232051511666716\n",
      "Epoch 250, Training Loss 0.04238174980461045\n",
      "Epoch 250, Training Loss 0.042453100908990675\n",
      "Epoch 250, Training Loss 0.04249701529319691\n",
      "Epoch 250, Training Loss 0.04254141930059609\n",
      "Epoch 250, Training Loss 0.04261282972379318\n",
      "Epoch 250, Training Loss 0.042727621514564544\n",
      "Epoch 250, Training Loss 0.04281571293976206\n",
      "Epoch 250, Training Loss 0.04284436011251033\n",
      "Epoch 250, Training Loss 0.04286734628267205\n",
      "Epoch 250, Training Loss 0.042966310963239475\n",
      "Epoch 250, Training Loss 0.04301148967381538\n",
      "Epoch 250, Training Loss 0.04304298861583462\n",
      "Epoch 250, Training Loss 0.043072322833106455\n",
      "Epoch 250, Training Loss 0.04312999570525497\n",
      "Epoch 250, Training Loss 0.0432231612045013\n",
      "Epoch 250, Training Loss 0.04328944694305606\n",
      "Epoch 250, Training Loss 0.0433135993887082\n",
      "Epoch 250, Training Loss 0.043505655177047145\n",
      "Epoch 250, Training Loss 0.04357381822848621\n",
      "Epoch 250, Training Loss 0.04369948558983348\n",
      "Epoch 250, Training Loss 0.04375152067338471\n",
      "Epoch 250, Training Loss 0.043881473461966344\n",
      "Epoch 250, Training Loss 0.0440130977054148\n",
      "Epoch 250, Training Loss 0.04410479161018968\n",
      "Epoch 250, Training Loss 0.04425467111949173\n",
      "Epoch 250, Training Loss 0.04448963691661482\n",
      "Epoch 250, Training Loss 0.0445098955073463\n",
      "Epoch 250, Training Loss 0.04461109675312191\n",
      "Epoch 250, Training Loss 0.044718769847598795\n",
      "Epoch 250, Training Loss 0.04491249582363421\n",
      "Epoch 250, Training Loss 0.04494938271208321\n",
      "Epoch 250, Training Loss 0.045150470090410705\n",
      "Epoch 250, Training Loss 0.04518689537394668\n",
      "Epoch 250, Training Loss 0.04530404742671382\n",
      "Epoch 250, Training Loss 0.04538088834182362\n",
      "Epoch 250, Training Loss 0.04541439876910728\n",
      "Epoch 250, Training Loss 0.04546814001179622\n",
      "Epoch 250, Training Loss 0.045502869449817886\n",
      "Epoch 250, Training Loss 0.04556224243108021\n",
      "Epoch 250, Training Loss 0.04565363809349649\n",
      "Epoch 250, Training Loss 0.0456758105521188\n",
      "Epoch 250, Training Loss 0.04573293146200935\n",
      "Epoch 250, Training Loss 0.04579689545864168\n",
      "Epoch 250, Training Loss 0.04583633534522141\n",
      "Epoch 250, Training Loss 0.04585670743106633\n",
      "Epoch 250, Training Loss 0.04590998499897187\n",
      "Epoch 250, Training Loss 0.04596279142722202\n",
      "Epoch 250, Training Loss 0.04605596357673083\n",
      "Epoch 250, Training Loss 0.04610501978572582\n",
      "Epoch 250, Training Loss 0.04618175781231917\n",
      "Epoch 250, Training Loss 0.04619950497382418\n",
      "Epoch 250, Training Loss 0.046241289540015336\n",
      "Epoch 250, Training Loss 0.04627987305524156\n",
      "Epoch 250, Training Loss 0.04633866106112347\n",
      "Epoch 250, Training Loss 0.0463819125866103\n",
      "Epoch 250, Training Loss 0.04639752579393232\n",
      "Epoch 250, Training Loss 0.046458707074932465\n",
      "Epoch 250, Training Loss 0.04656379730464495\n",
      "Epoch 250, Training Loss 0.04659607950085893\n",
      "Epoch 250, Training Loss 0.04669593605022315\n",
      "Epoch 250, Training Loss 0.04684407938369419\n",
      "Epoch 250, Training Loss 0.04689021793770535\n",
      "Epoch 250, Training Loss 0.046921431578224156\n",
      "Epoch 250, Training Loss 0.04696914291454722\n",
      "Epoch 250, Training Loss 0.04701778568301226\n",
      "Epoch 250, Training Loss 0.04705885288786724\n",
      "Epoch 250, Training Loss 0.0470893462635148\n",
      "Epoch 250, Training Loss 0.04712978724325957\n",
      "Epoch 250, Training Loss 0.04715816448252086\n",
      "Epoch 250, Training Loss 0.04719911260194505\n",
      "Epoch 250, Training Loss 0.04723402366216016\n",
      "Epoch 250, Training Loss 0.04730028582467695\n",
      "Epoch 250, Training Loss 0.04730991353257023\n",
      "Epoch 250, Training Loss 0.047399367966219935\n",
      "Epoch 250, Training Loss 0.0474755088792747\n",
      "Epoch 250, Training Loss 0.04762966982374811\n",
      "Epoch 250, Training Loss 0.04771774361336418\n",
      "Epoch 250, Training Loss 0.04773545423891786\n",
      "Epoch 250, Training Loss 0.04779104808466914\n",
      "Epoch 250, Training Loss 0.04788582776839871\n",
      "Epoch 250, Training Loss 0.04809313630530387\n",
      "Epoch 250, Training Loss 0.04813347833142961\n",
      "Epoch 250, Training Loss 0.04817989679670338\n",
      "Epoch 250, Training Loss 0.048207259032508484\n",
      "Epoch 250, Training Loss 0.048273861839952865\n",
      "Epoch 250, Training Loss 0.048482968196239505\n",
      "Epoch 250, Training Loss 0.04860479024815304\n",
      "Epoch 250, Training Loss 0.04870470300081003\n",
      "Epoch 250, Training Loss 0.04875279815636499\n",
      "Epoch 250, Training Loss 0.0488016954545751\n",
      "Epoch 250, Training Loss 0.04888022887101278\n",
      "Epoch 250, Training Loss 0.048984190537844356\n",
      "Epoch 250, Training Loss 0.049115650839936895\n",
      "Epoch 250, Training Loss 0.04916822453281935\n",
      "Epoch 250, Training Loss 0.04921453474077594\n",
      "Epoch 250, Training Loss 0.04923871315389281\n",
      "Epoch 250, Training Loss 0.04928843693776756\n",
      "Epoch 250, Training Loss 0.049331972704809685\n",
      "Epoch 250, Training Loss 0.04941303886132567\n",
      "Epoch 250, Training Loss 0.049537604499031856\n",
      "Epoch 250, Training Loss 0.0495959381719389\n",
      "Epoch 250, Training Loss 0.04966675390637077\n",
      "Epoch 250, Training Loss 0.049775155966796575\n",
      "Epoch 250, Training Loss 0.0498190263001596\n",
      "Epoch 250, Training Loss 0.04984902628683163\n",
      "Epoch 250, Training Loss 0.04992274601843751\n",
      "Epoch 250, Training Loss 0.049960322600081944\n",
      "Epoch 250, Training Loss 0.04999959617531608\n",
      "Epoch 250, Training Loss 0.05003457031774875\n",
      "Epoch 250, Training Loss 0.05007111952435273\n",
      "Epoch 250, Training Loss 0.05010034498410857\n",
      "Epoch 250, Training Loss 0.05013101156908171\n",
      "Epoch 250, Training Loss 0.050179090227603036\n",
      "Epoch 250, Training Loss 0.05020346485204576\n",
      "Epoch 250, Training Loss 0.05025387090711814\n",
      "Epoch 250, Training Loss 0.050269409514549175\n",
      "Epoch 260, Training Loss 0.0005569916856868188\n",
      "Epoch 260, Training Loss 0.0008426420676433826\n",
      "Epoch 260, Training Loss 0.0011519456992063986\n",
      "Epoch 260, Training Loss 0.00126789036728537\n",
      "Epoch 260, Training Loss 0.0013532876168065669\n",
      "Epoch 260, Training Loss 0.0014214436940448668\n",
      "Epoch 260, Training Loss 0.0015012713106315764\n",
      "Epoch 260, Training Loss 0.0015262576258357834\n",
      "Epoch 260, Training Loss 0.001588028474517948\n",
      "Epoch 260, Training Loss 0.0016847454711718633\n",
      "Epoch 260, Training Loss 0.0017813494895844509\n",
      "Epoch 260, Training Loss 0.0018318620536600233\n",
      "Epoch 260, Training Loss 0.0019976310312862285\n",
      "Epoch 260, Training Loss 0.0021301597009038987\n",
      "Epoch 260, Training Loss 0.002227742413101751\n",
      "Epoch 260, Training Loss 0.0022944140002665006\n",
      "Epoch 260, Training Loss 0.0023454968529322264\n",
      "Epoch 260, Training Loss 0.0023624322810650938\n",
      "Epoch 260, Training Loss 0.0024731279865307424\n",
      "Epoch 260, Training Loss 0.002539544291270282\n",
      "Epoch 260, Training Loss 0.0025462243227226196\n",
      "Epoch 260, Training Loss 0.0026425251253711446\n",
      "Epoch 260, Training Loss 0.0026702656448744904\n",
      "Epoch 260, Training Loss 0.002759555270156973\n",
      "Epoch 260, Training Loss 0.002786191150574657\n",
      "Epoch 260, Training Loss 0.002862784915063006\n",
      "Epoch 260, Training Loss 0.00290330179640666\n",
      "Epoch 260, Training Loss 0.0029684324615427754\n",
      "Epoch 260, Training Loss 0.003021867868378568\n",
      "Epoch 260, Training Loss 0.0030741900517640974\n",
      "Epoch 260, Training Loss 0.003218097587251831\n",
      "Epoch 260, Training Loss 0.0032616575115153095\n",
      "Epoch 260, Training Loss 0.003314587865215357\n",
      "Epoch 260, Training Loss 0.0033561430402252526\n",
      "Epoch 260, Training Loss 0.0034469527745014414\n",
      "Epoch 260, Training Loss 0.0035443312054990656\n",
      "Epoch 260, Training Loss 0.003591825731832277\n",
      "Epoch 260, Training Loss 0.0035999160000335074\n",
      "Epoch 260, Training Loss 0.003636898095374141\n",
      "Epoch 260, Training Loss 0.0036863279433639916\n",
      "Epoch 260, Training Loss 0.0037197881911778846\n",
      "Epoch 260, Training Loss 0.0038071068901749674\n",
      "Epoch 260, Training Loss 0.0038412496616678014\n",
      "Epoch 260, Training Loss 0.003870568213188816\n",
      "Epoch 260, Training Loss 0.003888029091850952\n",
      "Epoch 260, Training Loss 0.0039115984819334025\n",
      "Epoch 260, Training Loss 0.0039330023731512335\n",
      "Epoch 260, Training Loss 0.003998197511534023\n",
      "Epoch 260, Training Loss 0.004054762024070372\n",
      "Epoch 260, Training Loss 0.004143442788287578\n",
      "Epoch 260, Training Loss 0.004180724574181506\n",
      "Epoch 260, Training Loss 0.004230353553705584\n",
      "Epoch 260, Training Loss 0.0042865191124227195\n",
      "Epoch 260, Training Loss 0.004349522755893371\n",
      "Epoch 260, Training Loss 0.0043794425487842245\n",
      "Epoch 260, Training Loss 0.004422273006899963\n",
      "Epoch 260, Training Loss 0.004445541884673907\n",
      "Epoch 260, Training Loss 0.00448869258436896\n",
      "Epoch 260, Training Loss 0.004554003959431139\n",
      "Epoch 260, Training Loss 0.004633942772598599\n",
      "Epoch 260, Training Loss 0.004666769937814578\n",
      "Epoch 260, Training Loss 0.004739513181273818\n",
      "Epoch 260, Training Loss 0.004771461501917647\n",
      "Epoch 260, Training Loss 0.004790952042116762\n",
      "Epoch 260, Training Loss 0.004844370932863725\n",
      "Epoch 260, Training Loss 0.00488541288482373\n",
      "Epoch 260, Training Loss 0.005041389341425636\n",
      "Epoch 260, Training Loss 0.005086585825733136\n",
      "Epoch 260, Training Loss 0.005137572111442799\n",
      "Epoch 260, Training Loss 0.005166419516162723\n",
      "Epoch 260, Training Loss 0.005330411654175319\n",
      "Epoch 260, Training Loss 0.005351474171604418\n",
      "Epoch 260, Training Loss 0.005482515392829772\n",
      "Epoch 260, Training Loss 0.005537676752385352\n",
      "Epoch 260, Training Loss 0.0055999276299706045\n",
      "Epoch 260, Training Loss 0.00579074370648588\n",
      "Epoch 260, Training Loss 0.005807165186280561\n",
      "Epoch 260, Training Loss 0.005875437376339493\n",
      "Epoch 260, Training Loss 0.005953783923259857\n",
      "Epoch 260, Training Loss 0.0060648915353123944\n",
      "Epoch 260, Training Loss 0.006086251597203638\n",
      "Epoch 260, Training Loss 0.006147769869298048\n",
      "Epoch 260, Training Loss 0.006244711326601942\n",
      "Epoch 260, Training Loss 0.0062759844764895606\n",
      "Epoch 260, Training Loss 0.0064358400940761696\n",
      "Epoch 260, Training Loss 0.006459800684896043\n",
      "Epoch 260, Training Loss 0.006542908519630313\n",
      "Epoch 260, Training Loss 0.006574942221831711\n",
      "Epoch 260, Training Loss 0.006623645457189025\n",
      "Epoch 260, Training Loss 0.0067056884972469125\n",
      "Epoch 260, Training Loss 0.006758375911523238\n",
      "Epoch 260, Training Loss 0.00677544962914894\n",
      "Epoch 260, Training Loss 0.006858284202883082\n",
      "Epoch 260, Training Loss 0.00694768830223957\n",
      "Epoch 260, Training Loss 0.006979707553458717\n",
      "Epoch 260, Training Loss 0.0070703287651795715\n",
      "Epoch 260, Training Loss 0.007100584896524315\n",
      "Epoch 260, Training Loss 0.007176909096720045\n",
      "Epoch 260, Training Loss 0.007213764029609806\n",
      "Epoch 260, Training Loss 0.007230407466678439\n",
      "Epoch 260, Training Loss 0.007276918468734873\n",
      "Epoch 260, Training Loss 0.007489819304726999\n",
      "Epoch 260, Training Loss 0.0075441016196309\n",
      "Epoch 260, Training Loss 0.007727301449698331\n",
      "Epoch 260, Training Loss 0.007766398878725212\n",
      "Epoch 260, Training Loss 0.007804793121097871\n",
      "Epoch 260, Training Loss 0.007830838028513028\n",
      "Epoch 260, Training Loss 0.007908908279893724\n",
      "Epoch 260, Training Loss 0.008009646189115618\n",
      "Epoch 260, Training Loss 0.00805100825879618\n",
      "Epoch 260, Training Loss 0.0080962438967622\n",
      "Epoch 260, Training Loss 0.008166494792389215\n",
      "Epoch 260, Training Loss 0.008194444335454031\n",
      "Epoch 260, Training Loss 0.008218953482416051\n",
      "Epoch 260, Training Loss 0.008265891311275761\n",
      "Epoch 260, Training Loss 0.008286073986116959\n",
      "Epoch 260, Training Loss 0.008412661932437393\n",
      "Epoch 260, Training Loss 0.008452584170271902\n",
      "Epoch 260, Training Loss 0.008499308723880127\n",
      "Epoch 260, Training Loss 0.008570401001569179\n",
      "Epoch 260, Training Loss 0.008628089017832598\n",
      "Epoch 260, Training Loss 0.008654035726690766\n",
      "Epoch 260, Training Loss 0.008710361058440278\n",
      "Epoch 260, Training Loss 0.008767424483576317\n",
      "Epoch 260, Training Loss 0.00886816124948661\n",
      "Epoch 260, Training Loss 0.009003642749975024\n",
      "Epoch 260, Training Loss 0.009138938090752077\n",
      "Epoch 260, Training Loss 0.009268704595763589\n",
      "Epoch 260, Training Loss 0.009320213031881224\n",
      "Epoch 260, Training Loss 0.009342618492643928\n",
      "Epoch 260, Training Loss 0.009388811501633862\n",
      "Epoch 260, Training Loss 0.009446378822302651\n",
      "Epoch 260, Training Loss 0.009481960277566138\n",
      "Epoch 260, Training Loss 0.009583064980204682\n",
      "Epoch 260, Training Loss 0.009608369331349573\n",
      "Epoch 260, Training Loss 0.009743882949366366\n",
      "Epoch 260, Training Loss 0.009896308928013534\n",
      "Epoch 260, Training Loss 0.009982063138471615\n",
      "Epoch 260, Training Loss 0.010023126113073676\n",
      "Epoch 260, Training Loss 0.010172313556808721\n",
      "Epoch 260, Training Loss 0.010206613123959974\n",
      "Epoch 260, Training Loss 0.010335820416212463\n",
      "Epoch 260, Training Loss 0.010413534883314462\n",
      "Epoch 260, Training Loss 0.010460696310576651\n",
      "Epoch 260, Training Loss 0.010476068700747111\n",
      "Epoch 260, Training Loss 0.01072249793068832\n",
      "Epoch 260, Training Loss 0.010787026480297604\n",
      "Epoch 260, Training Loss 0.01082358773454757\n",
      "Epoch 260, Training Loss 0.010842546549104059\n",
      "Epoch 260, Training Loss 0.010920866852259393\n",
      "Epoch 260, Training Loss 0.01097475181398032\n",
      "Epoch 260, Training Loss 0.011010988284369258\n",
      "Epoch 260, Training Loss 0.011077055617061722\n",
      "Epoch 260, Training Loss 0.011086904052573511\n",
      "Epoch 260, Training Loss 0.011101341934379219\n",
      "Epoch 260, Training Loss 0.011168188495856836\n",
      "Epoch 260, Training Loss 0.0112242122934631\n",
      "Epoch 260, Training Loss 0.011246869306596916\n",
      "Epoch 260, Training Loss 0.01132341531400695\n",
      "Epoch 260, Training Loss 0.011462696543906617\n",
      "Epoch 260, Training Loss 0.011549880913318232\n",
      "Epoch 260, Training Loss 0.011578797297838056\n",
      "Epoch 260, Training Loss 0.011633177851791235\n",
      "Epoch 260, Training Loss 0.01164974457384242\n",
      "Epoch 260, Training Loss 0.011719125638003735\n",
      "Epoch 260, Training Loss 0.011818714642925832\n",
      "Epoch 260, Training Loss 0.011849276681580697\n",
      "Epoch 260, Training Loss 0.011939734742736153\n",
      "Epoch 260, Training Loss 0.011969832046662488\n",
      "Epoch 260, Training Loss 0.012009721495273054\n",
      "Epoch 260, Training Loss 0.012057819208987724\n",
      "Epoch 260, Training Loss 0.012083088679601202\n",
      "Epoch 260, Training Loss 0.012119676932678236\n",
      "Epoch 260, Training Loss 0.012213039357820168\n",
      "Epoch 260, Training Loss 0.012281059833538844\n",
      "Epoch 260, Training Loss 0.012335476363697053\n",
      "Epoch 260, Training Loss 0.012368165359706106\n",
      "Epoch 260, Training Loss 0.01242179152808245\n",
      "Epoch 260, Training Loss 0.012473448980575823\n",
      "Epoch 260, Training Loss 0.012520597254990808\n",
      "Epoch 260, Training Loss 0.012548079807668581\n",
      "Epoch 260, Training Loss 0.012597951626576617\n",
      "Epoch 260, Training Loss 0.012655017009633772\n",
      "Epoch 260, Training Loss 0.012682026631467978\n",
      "Epoch 260, Training Loss 0.012703181128315342\n",
      "Epoch 260, Training Loss 0.012845127274641944\n",
      "Epoch 260, Training Loss 0.012907426137967831\n",
      "Epoch 260, Training Loss 0.012979914937430368\n",
      "Epoch 260, Training Loss 0.013066479403292164\n",
      "Epoch 260, Training Loss 0.013125000036228686\n",
      "Epoch 260, Training Loss 0.013165868647858653\n",
      "Epoch 260, Training Loss 0.013254521181806922\n",
      "Epoch 260, Training Loss 0.013325742587013661\n",
      "Epoch 260, Training Loss 0.013455451688255229\n",
      "Epoch 260, Training Loss 0.013493483000651688\n",
      "Epoch 260, Training Loss 0.01351550839367368\n",
      "Epoch 260, Training Loss 0.013570301316302184\n",
      "Epoch 260, Training Loss 0.013603928832985137\n",
      "Epoch 260, Training Loss 0.013682666959364891\n",
      "Epoch 260, Training Loss 0.013820106295220878\n",
      "Epoch 260, Training Loss 0.013867602727073424\n",
      "Epoch 260, Training Loss 0.013918133788977933\n",
      "Epoch 260, Training Loss 0.01396645968863288\n",
      "Epoch 260, Training Loss 0.014015647912007349\n",
      "Epoch 260, Training Loss 0.014115396532756478\n",
      "Epoch 260, Training Loss 0.014178904924479782\n",
      "Epoch 260, Training Loss 0.014229771372078516\n",
      "Epoch 260, Training Loss 0.014326934584194933\n",
      "Epoch 260, Training Loss 0.01434583541141618\n",
      "Epoch 260, Training Loss 0.014365526402721663\n",
      "Epoch 260, Training Loss 0.014415692957832724\n",
      "Epoch 260, Training Loss 0.014443875293430808\n",
      "Epoch 260, Training Loss 0.014463373283019571\n",
      "Epoch 260, Training Loss 0.014472621905705547\n",
      "Epoch 260, Training Loss 0.014489894616715324\n",
      "Epoch 260, Training Loss 0.01452602187702742\n",
      "Epoch 260, Training Loss 0.014649955796963914\n",
      "Epoch 260, Training Loss 0.014757848797899568\n",
      "Epoch 260, Training Loss 0.014785980001268217\n",
      "Epoch 260, Training Loss 0.014850758002413547\n",
      "Epoch 260, Training Loss 0.014900571674399097\n",
      "Epoch 260, Training Loss 0.01502280678573872\n",
      "Epoch 260, Training Loss 0.015109874222956388\n",
      "Epoch 260, Training Loss 0.01518486731309358\n",
      "Epoch 260, Training Loss 0.015344456119863006\n",
      "Epoch 260, Training Loss 0.015446095806582238\n",
      "Epoch 260, Training Loss 0.015506395117958526\n",
      "Epoch 260, Training Loss 0.015554919091584471\n",
      "Epoch 260, Training Loss 0.0156438021457342\n",
      "Epoch 260, Training Loss 0.01568992298317935\n",
      "Epoch 260, Training Loss 0.01574544016988781\n",
      "Epoch 260, Training Loss 0.015793983403550427\n",
      "Epoch 260, Training Loss 0.015842724701656444\n",
      "Epoch 260, Training Loss 0.015873139205476856\n",
      "Epoch 260, Training Loss 0.015919687774846963\n",
      "Epoch 260, Training Loss 0.015994832115581312\n",
      "Epoch 260, Training Loss 0.01605066030210032\n",
      "Epoch 260, Training Loss 0.01608763401791968\n",
      "Epoch 260, Training Loss 0.016153636853546476\n",
      "Epoch 260, Training Loss 0.016223475952153964\n",
      "Epoch 260, Training Loss 0.016273353935417047\n",
      "Epoch 260, Training Loss 0.016332639005544887\n",
      "Epoch 260, Training Loss 0.016413241825507158\n",
      "Epoch 260, Training Loss 0.01653815728798032\n",
      "Epoch 260, Training Loss 0.016652528435358648\n",
      "Epoch 260, Training Loss 0.016696705381073953\n",
      "Epoch 260, Training Loss 0.016772355413293024\n",
      "Epoch 260, Training Loss 0.01682684986013681\n",
      "Epoch 260, Training Loss 0.01687245889414397\n",
      "Epoch 260, Training Loss 0.016898449746739416\n",
      "Epoch 260, Training Loss 0.016995981028970437\n",
      "Epoch 260, Training Loss 0.01703447508244582\n",
      "Epoch 260, Training Loss 0.017137488257973586\n",
      "Epoch 260, Training Loss 0.01716407716972158\n",
      "Epoch 260, Training Loss 0.017273215751480456\n",
      "Epoch 260, Training Loss 0.017314947896834722\n",
      "Epoch 260, Training Loss 0.01745206728646689\n",
      "Epoch 260, Training Loss 0.017564749895997555\n",
      "Epoch 260, Training Loss 0.017633789943655016\n",
      "Epoch 260, Training Loss 0.017660507870971433\n",
      "Epoch 260, Training Loss 0.017755122231843563\n",
      "Epoch 260, Training Loss 0.01780833423082881\n",
      "Epoch 260, Training Loss 0.017835066542434302\n",
      "Epoch 260, Training Loss 0.017910707494853746\n",
      "Epoch 260, Training Loss 0.01807148338300283\n",
      "Epoch 260, Training Loss 0.018135601969654944\n",
      "Epoch 260, Training Loss 0.01824490218709135\n",
      "Epoch 260, Training Loss 0.018331166981455045\n",
      "Epoch 260, Training Loss 0.01856242782017097\n",
      "Epoch 260, Training Loss 0.018762204429858823\n",
      "Epoch 260, Training Loss 0.018888862051493715\n",
      "Epoch 260, Training Loss 0.018912058100318704\n",
      "Epoch 260, Training Loss 0.019046780394266365\n",
      "Epoch 260, Training Loss 0.01915130608529329\n",
      "Epoch 260, Training Loss 0.01920592705385707\n",
      "Epoch 260, Training Loss 0.0192954180448952\n",
      "Epoch 260, Training Loss 0.019346349032433784\n",
      "Epoch 260, Training Loss 0.019438395258205968\n",
      "Epoch 260, Training Loss 0.019477366422757963\n",
      "Epoch 260, Training Loss 0.01957760259921155\n",
      "Epoch 260, Training Loss 0.019609694500737216\n",
      "Epoch 260, Training Loss 0.019659892482625896\n",
      "Epoch 260, Training Loss 0.019711742035287153\n",
      "Epoch 260, Training Loss 0.019747300236069066\n",
      "Epoch 260, Training Loss 0.019798391920816907\n",
      "Epoch 260, Training Loss 0.019815627332476667\n",
      "Epoch 260, Training Loss 0.01983516437925703\n",
      "Epoch 260, Training Loss 0.019879038537950124\n",
      "Epoch 260, Training Loss 0.01998264527322291\n",
      "Epoch 260, Training Loss 0.020001165004913956\n",
      "Epoch 260, Training Loss 0.020020307286444793\n",
      "Epoch 260, Training Loss 0.02011134443492593\n",
      "Epoch 260, Training Loss 0.020136617785653153\n",
      "Epoch 260, Training Loss 0.020155053118320033\n",
      "Epoch 260, Training Loss 0.02017950586667356\n",
      "Epoch 260, Training Loss 0.02026326882848254\n",
      "Epoch 260, Training Loss 0.020324159741682737\n",
      "Epoch 260, Training Loss 0.020375940476513237\n",
      "Epoch 260, Training Loss 0.020548703605690232\n",
      "Epoch 260, Training Loss 0.02062338586751362\n",
      "Epoch 260, Training Loss 0.020738594100007886\n",
      "Epoch 260, Training Loss 0.020821818238114725\n",
      "Epoch 260, Training Loss 0.0208318989129637\n",
      "Epoch 260, Training Loss 0.02090449280181752\n",
      "Epoch 260, Training Loss 0.021186915119690702\n",
      "Epoch 260, Training Loss 0.021406050395730243\n",
      "Epoch 260, Training Loss 0.021523658410691277\n",
      "Epoch 260, Training Loss 0.021600556005473674\n",
      "Epoch 260, Training Loss 0.021673842893598978\n",
      "Epoch 260, Training Loss 0.021797844535692612\n",
      "Epoch 260, Training Loss 0.021820931410169244\n",
      "Epoch 260, Training Loss 0.02190743608083433\n",
      "Epoch 260, Training Loss 0.021979692116346393\n",
      "Epoch 260, Training Loss 0.02207197279721746\n",
      "Epoch 260, Training Loss 0.022153466390302915\n",
      "Epoch 260, Training Loss 0.022179333608040152\n",
      "Epoch 260, Training Loss 0.022289363391664062\n",
      "Epoch 260, Training Loss 0.022347487767091227\n",
      "Epoch 260, Training Loss 0.022363988473616026\n",
      "Epoch 260, Training Loss 0.02242502499231235\n",
      "Epoch 260, Training Loss 0.022604863676826095\n",
      "Epoch 260, Training Loss 0.02273057452212457\n",
      "Epoch 260, Training Loss 0.022791647588681727\n",
      "Epoch 260, Training Loss 0.022870497507117022\n",
      "Epoch 260, Training Loss 0.022976810861107373\n",
      "Epoch 260, Training Loss 0.02302405149128069\n",
      "Epoch 260, Training Loss 0.023045278400244652\n",
      "Epoch 260, Training Loss 0.023060794405834487\n",
      "Epoch 260, Training Loss 0.023084609202988198\n",
      "Epoch 260, Training Loss 0.023140077722732865\n",
      "Epoch 260, Training Loss 0.02316608123750428\n",
      "Epoch 260, Training Loss 0.023281034578204803\n",
      "Epoch 260, Training Loss 0.023355109298535053\n",
      "Epoch 260, Training Loss 0.023398051262878434\n",
      "Epoch 260, Training Loss 0.02344404791762857\n",
      "Epoch 260, Training Loss 0.023505410923243826\n",
      "Epoch 260, Training Loss 0.023578050673184227\n",
      "Epoch 260, Training Loss 0.02363710487058953\n",
      "Epoch 260, Training Loss 0.023800087674065967\n",
      "Epoch 260, Training Loss 0.02384200177269766\n",
      "Epoch 260, Training Loss 0.023912002797574376\n",
      "Epoch 260, Training Loss 0.024034611981414506\n",
      "Epoch 260, Training Loss 0.02408720116616915\n",
      "Epoch 260, Training Loss 0.024147810076799274\n",
      "Epoch 260, Training Loss 0.024313021213640377\n",
      "Epoch 260, Training Loss 0.02445148133620372\n",
      "Epoch 260, Training Loss 0.0244991425383131\n",
      "Epoch 260, Training Loss 0.024571348492013256\n",
      "Epoch 260, Training Loss 0.024614186710356485\n",
      "Epoch 260, Training Loss 0.024655282779065583\n",
      "Epoch 260, Training Loss 0.02469400718124569\n",
      "Epoch 260, Training Loss 0.02475410101570837\n",
      "Epoch 260, Training Loss 0.02492099497383913\n",
      "Epoch 260, Training Loss 0.024978211154932598\n",
      "Epoch 260, Training Loss 0.025023529292358196\n",
      "Epoch 260, Training Loss 0.02503143842070056\n",
      "Epoch 260, Training Loss 0.02507193022064598\n",
      "Epoch 260, Training Loss 0.02511793993296735\n",
      "Epoch 260, Training Loss 0.025187848173343883\n",
      "Epoch 260, Training Loss 0.0252337684644067\n",
      "Epoch 260, Training Loss 0.025255426530466626\n",
      "Epoch 260, Training Loss 0.02529526377499313\n",
      "Epoch 260, Training Loss 0.025350973438209547\n",
      "Epoch 260, Training Loss 0.025374290904344614\n",
      "Epoch 260, Training Loss 0.025421086214649516\n",
      "Epoch 260, Training Loss 0.025450422154034456\n",
      "Epoch 260, Training Loss 0.025633145432771585\n",
      "Epoch 260, Training Loss 0.025705702692780004\n",
      "Epoch 260, Training Loss 0.025734351337541973\n",
      "Epoch 260, Training Loss 0.025747334910318484\n",
      "Epoch 260, Training Loss 0.025777312661961788\n",
      "Epoch 260, Training Loss 0.025832655612746124\n",
      "Epoch 260, Training Loss 0.025857845720861233\n",
      "Epoch 260, Training Loss 0.025887470334515814\n",
      "Epoch 260, Training Loss 0.025954911680034626\n",
      "Epoch 260, Training Loss 0.025973643154225044\n",
      "Epoch 260, Training Loss 0.02599786462255127\n",
      "Epoch 260, Training Loss 0.026047615340941816\n",
      "Epoch 260, Training Loss 0.026120757976489717\n",
      "Epoch 260, Training Loss 0.026182779686792237\n",
      "Epoch 260, Training Loss 0.026258666413095412\n",
      "Epoch 260, Training Loss 0.026306711915580323\n",
      "Epoch 260, Training Loss 0.026372085324824428\n",
      "Epoch 260, Training Loss 0.026470791521694157\n",
      "Epoch 260, Training Loss 0.026511482300732254\n",
      "Epoch 260, Training Loss 0.02662961109412734\n",
      "Epoch 260, Training Loss 0.026741828852216414\n",
      "Epoch 260, Training Loss 0.026772147829970703\n",
      "Epoch 260, Training Loss 0.02688783688215858\n",
      "Epoch 260, Training Loss 0.026917076173602887\n",
      "Epoch 260, Training Loss 0.026950945588819625\n",
      "Epoch 260, Training Loss 0.027033502356890025\n",
      "Epoch 260, Training Loss 0.027072772842800946\n",
      "Epoch 260, Training Loss 0.027087814708733383\n",
      "Epoch 260, Training Loss 0.02712074311419159\n",
      "Epoch 260, Training Loss 0.027137438568365202\n",
      "Epoch 260, Training Loss 0.027206358220547323\n",
      "Epoch 260, Training Loss 0.02723913233134124\n",
      "Epoch 260, Training Loss 0.02737026571300919\n",
      "Epoch 260, Training Loss 0.02739906690173956\n",
      "Epoch 260, Training Loss 0.027547504275899064\n",
      "Epoch 260, Training Loss 0.027612569762746352\n",
      "Epoch 260, Training Loss 0.027684275098407016\n",
      "Epoch 260, Training Loss 0.02775480953054717\n",
      "Epoch 260, Training Loss 0.02782276613325776\n",
      "Epoch 260, Training Loss 0.027928696058647673\n",
      "Epoch 260, Training Loss 0.027963173394079516\n",
      "Epoch 260, Training Loss 0.027983368242807363\n",
      "Epoch 260, Training Loss 0.028027360210233294\n",
      "Epoch 260, Training Loss 0.02804451871632367\n",
      "Epoch 260, Training Loss 0.028086950682708636\n",
      "Epoch 260, Training Loss 0.028121056184625192\n",
      "Epoch 260, Training Loss 0.02822566666173489\n",
      "Epoch 260, Training Loss 0.028279472896924524\n",
      "Epoch 260, Training Loss 0.028338866653111396\n",
      "Epoch 260, Training Loss 0.028366096569653935\n",
      "Epoch 260, Training Loss 0.02837543084722041\n",
      "Epoch 260, Training Loss 0.028399104633561486\n",
      "Epoch 260, Training Loss 0.028436889652820194\n",
      "Epoch 260, Training Loss 0.028473940625539065\n",
      "Epoch 260, Training Loss 0.028513014466618487\n",
      "Epoch 260, Training Loss 0.028556505492066637\n",
      "Epoch 260, Training Loss 0.028594553725474783\n",
      "Epoch 260, Training Loss 0.028614819507159845\n",
      "Epoch 260, Training Loss 0.028720223623544663\n",
      "Epoch 260, Training Loss 0.028764688026379135\n",
      "Epoch 260, Training Loss 0.028883048273680156\n",
      "Epoch 260, Training Loss 0.028951717745465087\n",
      "Epoch 260, Training Loss 0.028998765770507896\n",
      "Epoch 260, Training Loss 0.029024966828086796\n",
      "Epoch 260, Training Loss 0.029056413051531747\n",
      "Epoch 260, Training Loss 0.029089465851673994\n",
      "Epoch 260, Training Loss 0.029133568286819532\n",
      "Epoch 260, Training Loss 0.029221683111794464\n",
      "Epoch 260, Training Loss 0.029252313827748035\n",
      "Epoch 260, Training Loss 0.029305349046940845\n",
      "Epoch 260, Training Loss 0.029331533933806296\n",
      "Epoch 260, Training Loss 0.02944512731488556\n",
      "Epoch 260, Training Loss 0.029499318693643032\n",
      "Epoch 260, Training Loss 0.029527637390586572\n",
      "Epoch 260, Training Loss 0.02962435146226831\n",
      "Epoch 260, Training Loss 0.02963776980786372\n",
      "Epoch 260, Training Loss 0.029666442626520344\n",
      "Epoch 260, Training Loss 0.029765444264635253\n",
      "Epoch 260, Training Loss 0.029797680456848705\n",
      "Epoch 260, Training Loss 0.029839618303015104\n",
      "Epoch 260, Training Loss 0.029889735338442466\n",
      "Epoch 260, Training Loss 0.02996902469345523\n",
      "Epoch 260, Training Loss 0.03007499816948953\n",
      "Epoch 260, Training Loss 0.030130299756212918\n",
      "Epoch 260, Training Loss 0.030188147616965692\n",
      "Epoch 260, Training Loss 0.03021478054144651\n",
      "Epoch 260, Training Loss 0.030292192683614733\n",
      "Epoch 260, Training Loss 0.030341354253537515\n",
      "Epoch 260, Training Loss 0.03038748400404935\n",
      "Epoch 260, Training Loss 0.03046228195947912\n",
      "Epoch 260, Training Loss 0.030571136771298735\n",
      "Epoch 260, Training Loss 0.030649422203450252\n",
      "Epoch 260, Training Loss 0.030737937168430183\n",
      "Epoch 260, Training Loss 0.03080602928691203\n",
      "Epoch 260, Training Loss 0.03081896383663082\n",
      "Epoch 260, Training Loss 0.030854308554936018\n",
      "Epoch 260, Training Loss 0.030912472022092328\n",
      "Epoch 260, Training Loss 0.03098400605632864\n",
      "Epoch 260, Training Loss 0.031069943006090877\n",
      "Epoch 260, Training Loss 0.031109459444527966\n",
      "Epoch 260, Training Loss 0.03112474363178129\n",
      "Epoch 260, Training Loss 0.03123151613971042\n",
      "Epoch 260, Training Loss 0.03140996337589591\n",
      "Epoch 260, Training Loss 0.03144421634357184\n",
      "Epoch 260, Training Loss 0.03152577693471709\n",
      "Epoch 260, Training Loss 0.03158837223015821\n",
      "Epoch 260, Training Loss 0.03166435910579379\n",
      "Epoch 260, Training Loss 0.03169854969391242\n",
      "Epoch 260, Training Loss 0.03182960861741239\n",
      "Epoch 260, Training Loss 0.031856587329341095\n",
      "Epoch 260, Training Loss 0.03193273636943582\n",
      "Epoch 260, Training Loss 0.031954708603232185\n",
      "Epoch 260, Training Loss 0.031996742492336826\n",
      "Epoch 260, Training Loss 0.032023014828248327\n",
      "Epoch 260, Training Loss 0.03206231194855574\n",
      "Epoch 260, Training Loss 0.0320809322404568\n",
      "Epoch 260, Training Loss 0.0321565690150251\n",
      "Epoch 260, Training Loss 0.03219968718274132\n",
      "Epoch 260, Training Loss 0.032241590758142494\n",
      "Epoch 260, Training Loss 0.0323051718406646\n",
      "Epoch 260, Training Loss 0.03232984833986215\n",
      "Epoch 260, Training Loss 0.03236728679517384\n",
      "Epoch 260, Training Loss 0.032386048720754165\n",
      "Epoch 260, Training Loss 0.032423497615929914\n",
      "Epoch 260, Training Loss 0.03252912635493385\n",
      "Epoch 260, Training Loss 0.03259537933284745\n",
      "Epoch 260, Training Loss 0.03268264392462304\n",
      "Epoch 260, Training Loss 0.032713945662064474\n",
      "Epoch 260, Training Loss 0.03278905546526089\n",
      "Epoch 260, Training Loss 0.032830614098788374\n",
      "Epoch 260, Training Loss 0.03294585906016781\n",
      "Epoch 260, Training Loss 0.03298792449990883\n",
      "Epoch 260, Training Loss 0.033024252618632045\n",
      "Epoch 260, Training Loss 0.03307313258138002\n",
      "Epoch 260, Training Loss 0.03314286009753909\n",
      "Epoch 260, Training Loss 0.03332264887650147\n",
      "Epoch 260, Training Loss 0.033362203480108925\n",
      "Epoch 260, Training Loss 0.03340606675828662\n",
      "Epoch 260, Training Loss 0.03344087676166573\n",
      "Epoch 260, Training Loss 0.03349180771585773\n",
      "Epoch 260, Training Loss 0.03359483988941325\n",
      "Epoch 260, Training Loss 0.03366714277211815\n",
      "Epoch 260, Training Loss 0.0336913266802764\n",
      "Epoch 260, Training Loss 0.03371884610713519\n",
      "Epoch 260, Training Loss 0.03373563397661461\n",
      "Epoch 260, Training Loss 0.033801566527160765\n",
      "Epoch 260, Training Loss 0.03387564500612791\n",
      "Epoch 260, Training Loss 0.03392357051329654\n",
      "Epoch 260, Training Loss 0.03396321057944613\n",
      "Epoch 260, Training Loss 0.03402162867638728\n",
      "Epoch 260, Training Loss 0.03403033950256512\n",
      "Epoch 260, Training Loss 0.03407184946436502\n",
      "Epoch 260, Training Loss 0.03411434453678653\n",
      "Epoch 260, Training Loss 0.034164085908604744\n",
      "Epoch 260, Training Loss 0.034250179492056256\n",
      "Epoch 260, Training Loss 0.034324726852638376\n",
      "Epoch 260, Training Loss 0.03441061125770497\n",
      "Epoch 260, Training Loss 0.034423933804506805\n",
      "Epoch 260, Training Loss 0.034478610076005466\n",
      "Epoch 260, Training Loss 0.03449440389678664\n",
      "Epoch 260, Training Loss 0.034518616973682095\n",
      "Epoch 260, Training Loss 0.034593670111497304\n",
      "Epoch 260, Training Loss 0.034617762602365494\n",
      "Epoch 260, Training Loss 0.03470064933016382\n",
      "Epoch 260, Training Loss 0.03476896737178173\n",
      "Epoch 260, Training Loss 0.03483210059533091\n",
      "Epoch 260, Training Loss 0.03491162226173331\n",
      "Epoch 260, Training Loss 0.034956507793868254\n",
      "Epoch 260, Training Loss 0.03499786889173395\n",
      "Epoch 260, Training Loss 0.035098735966583916\n",
      "Epoch 260, Training Loss 0.035182803201720195\n",
      "Epoch 260, Training Loss 0.035313704433967655\n",
      "Epoch 260, Training Loss 0.03540455081138064\n",
      "Epoch 260, Training Loss 0.035535890455988935\n",
      "Epoch 260, Training Loss 0.035672322905901103\n",
      "Epoch 260, Training Loss 0.03572889347798894\n",
      "Epoch 260, Training Loss 0.035776179840626275\n",
      "Epoch 260, Training Loss 0.03580486870613759\n",
      "Epoch 260, Training Loss 0.03582832293854098\n",
      "Epoch 260, Training Loss 0.035913667014545146\n",
      "Epoch 260, Training Loss 0.035946072600281716\n",
      "Epoch 260, Training Loss 0.036036056387202475\n",
      "Epoch 260, Training Loss 0.036064218224176325\n",
      "Epoch 260, Training Loss 0.036118610397033644\n",
      "Epoch 260, Training Loss 0.03630582330798459\n",
      "Epoch 260, Training Loss 0.036409727999788075\n",
      "Epoch 260, Training Loss 0.03647650929722849\n",
      "Epoch 260, Training Loss 0.03656095217691872\n",
      "Epoch 260, Training Loss 0.0366231775157811\n",
      "Epoch 260, Training Loss 0.03664363870371009\n",
      "Epoch 260, Training Loss 0.036760571488367436\n",
      "Epoch 260, Training Loss 0.036895080049738974\n",
      "Epoch 260, Training Loss 0.036923225508774145\n",
      "Epoch 260, Training Loss 0.037029476638030634\n",
      "Epoch 260, Training Loss 0.03706331315921986\n",
      "Epoch 260, Training Loss 0.03712611775690466\n",
      "Epoch 260, Training Loss 0.037218732875474086\n",
      "Epoch 260, Training Loss 0.03725301502856052\n",
      "Epoch 260, Training Loss 0.037332041169423846\n",
      "Epoch 260, Training Loss 0.037368300977660834\n",
      "Epoch 260, Training Loss 0.03742940665718139\n",
      "Epoch 260, Training Loss 0.0375652276259392\n",
      "Epoch 260, Training Loss 0.03759846220786214\n",
      "Epoch 260, Training Loss 0.03769123747580401\n",
      "Epoch 260, Training Loss 0.037714810057631346\n",
      "Epoch 260, Training Loss 0.03779788008686203\n",
      "Epoch 260, Training Loss 0.03796735339407402\n",
      "Epoch 260, Training Loss 0.0380276959607392\n",
      "Epoch 260, Training Loss 0.03806431251792404\n",
      "Epoch 260, Training Loss 0.0381160572956047\n",
      "Epoch 260, Training Loss 0.03813573727300843\n",
      "Epoch 260, Training Loss 0.03822904401887542\n",
      "Epoch 260, Training Loss 0.0383253538155752\n",
      "Epoch 260, Training Loss 0.038459679885360094\n",
      "Epoch 260, Training Loss 0.03849469524655787\n",
      "Epoch 260, Training Loss 0.03853072516341477\n",
      "Epoch 260, Training Loss 0.03857441512508142\n",
      "Epoch 260, Training Loss 0.03862658218966554\n",
      "Epoch 260, Training Loss 0.038679150675716416\n",
      "Epoch 260, Training Loss 0.038724498024753404\n",
      "Epoch 260, Training Loss 0.03875990150451108\n",
      "Epoch 260, Training Loss 0.03878397283220516\n",
      "Epoch 260, Training Loss 0.03879342312195703\n",
      "Epoch 260, Training Loss 0.038926251520238375\n",
      "Epoch 260, Training Loss 0.03898880270767071\n",
      "Epoch 260, Training Loss 0.039054320288507645\n",
      "Epoch 260, Training Loss 0.039080797351034514\n",
      "Epoch 260, Training Loss 0.03921142954300717\n",
      "Epoch 260, Training Loss 0.0392277581573171\n",
      "Epoch 260, Training Loss 0.03926534985687555\n",
      "Epoch 260, Training Loss 0.03939702009956074\n",
      "Epoch 260, Training Loss 0.03946580385010394\n",
      "Epoch 260, Training Loss 0.0395419819970065\n",
      "Epoch 260, Training Loss 0.039619576573595786\n",
      "Epoch 260, Training Loss 0.03970829383803584\n",
      "Epoch 260, Training Loss 0.03977658462119968\n",
      "Epoch 260, Training Loss 0.0399651070622027\n",
      "Epoch 260, Training Loss 0.04002455923208476\n",
      "Epoch 260, Training Loss 0.04003542056009459\n",
      "Epoch 260, Training Loss 0.04010308804788419\n",
      "Epoch 260, Training Loss 0.040134416586455064\n",
      "Epoch 260, Training Loss 0.040195485827682155\n",
      "Epoch 260, Training Loss 0.040217752809591036\n",
      "Epoch 260, Training Loss 0.04025209505561158\n",
      "Epoch 260, Training Loss 0.04032321848854175\n",
      "Epoch 260, Training Loss 0.040331284574511676\n",
      "Epoch 260, Training Loss 0.04034684995329841\n",
      "Epoch 260, Training Loss 0.0403773609835826\n",
      "Epoch 260, Training Loss 0.04042409604136615\n",
      "Epoch 260, Training Loss 0.04047649594612153\n",
      "Epoch 260, Training Loss 0.04056878562104386\n",
      "Epoch 260, Training Loss 0.04065779981303417\n",
      "Epoch 260, Training Loss 0.040780248866557046\n",
      "Epoch 260, Training Loss 0.04084471504375472\n",
      "Epoch 260, Training Loss 0.04087281006607501\n",
      "Epoch 260, Training Loss 0.040894748550742065\n",
      "Epoch 260, Training Loss 0.040921119766433714\n",
      "Epoch 260, Training Loss 0.040990810305990104\n",
      "Epoch 260, Training Loss 0.041113738883453446\n",
      "Epoch 260, Training Loss 0.04114590815144598\n",
      "Epoch 260, Training Loss 0.04119386388015126\n",
      "Epoch 260, Training Loss 0.041304733349924995\n",
      "Epoch 260, Training Loss 0.041340808708654225\n",
      "Epoch 260, Training Loss 0.04137095949574447\n",
      "Epoch 260, Training Loss 0.04139209415375367\n",
      "Epoch 260, Training Loss 0.0414091484743835\n",
      "Epoch 260, Training Loss 0.04149003125324636\n",
      "Epoch 260, Training Loss 0.041595932176632\n",
      "Epoch 260, Training Loss 0.041646916980800384\n",
      "Epoch 260, Training Loss 0.04185328769731\n",
      "Epoch 260, Training Loss 0.04195476040336996\n",
      "Epoch 260, Training Loss 0.04199638097103962\n",
      "Epoch 260, Training Loss 0.04204237830701291\n",
      "Epoch 260, Training Loss 0.042073671829284114\n",
      "Epoch 260, Training Loss 0.042137272529127766\n",
      "Epoch 260, Training Loss 0.04222744841145261\n",
      "Epoch 260, Training Loss 0.042255200187811426\n",
      "Epoch 260, Training Loss 0.04229603580537412\n",
      "Epoch 260, Training Loss 0.04242847109501681\n",
      "Epoch 260, Training Loss 0.04248766122264382\n",
      "Epoch 260, Training Loss 0.04251245538527837\n",
      "Epoch 260, Training Loss 0.042530006754671786\n",
      "Epoch 260, Training Loss 0.04262413514142051\n",
      "Epoch 260, Training Loss 0.04267893219426217\n",
      "Epoch 260, Training Loss 0.042757527320943484\n",
      "Epoch 260, Training Loss 0.042787393661635115\n",
      "Epoch 260, Training Loss 0.042900322931116\n",
      "Epoch 260, Training Loss 0.042951758800293595\n",
      "Epoch 260, Training Loss 0.043020467673454556\n",
      "Epoch 260, Training Loss 0.043037801808880075\n",
      "Epoch 260, Training Loss 0.043102609793372125\n",
      "Epoch 260, Training Loss 0.04315373552975047\n",
      "Epoch 260, Training Loss 0.04318060351909636\n",
      "Epoch 260, Training Loss 0.04331547074684459\n",
      "Epoch 260, Training Loss 0.04336184404654633\n",
      "Epoch 260, Training Loss 0.043487901572504886\n",
      "Epoch 260, Training Loss 0.04359970164075589\n",
      "Epoch 260, Training Loss 0.043685535082707894\n",
      "Epoch 260, Training Loss 0.04373124974441555\n",
      "Epoch 260, Training Loss 0.04374707881849536\n",
      "Epoch 260, Training Loss 0.04382222751810518\n",
      "Epoch 260, Training Loss 0.0438806122017648\n",
      "Epoch 260, Training Loss 0.04401606828262529\n",
      "Epoch 260, Training Loss 0.04403865201901311\n",
      "Epoch 260, Training Loss 0.04410905346972272\n",
      "Epoch 260, Training Loss 0.04412789985506564\n",
      "Epoch 260, Training Loss 0.04415283313966678\n",
      "Epoch 260, Training Loss 0.044183898602238356\n",
      "Epoch 260, Training Loss 0.04420899116681874\n",
      "Epoch 260, Training Loss 0.044246221294083524\n",
      "Epoch 260, Training Loss 0.04427346583786885\n",
      "Epoch 260, Training Loss 0.044333903628098956\n",
      "Epoch 260, Training Loss 0.04437399857982403\n",
      "Epoch 260, Training Loss 0.044391550499436154\n",
      "Epoch 260, Training Loss 0.04441523554501459\n",
      "Epoch 260, Training Loss 0.04449878784213358\n",
      "Epoch 260, Training Loss 0.044542150064002334\n",
      "Epoch 260, Training Loss 0.044564012051595714\n",
      "Epoch 260, Training Loss 0.044597993313651677\n",
      "Epoch 260, Training Loss 0.044616649671078985\n",
      "Epoch 260, Training Loss 0.044661140160473144\n",
      "Epoch 260, Training Loss 0.04472326247266892\n",
      "Epoch 260, Training Loss 0.04475812504456267\n",
      "Epoch 260, Training Loss 0.04481906185219603\n",
      "Epoch 260, Training Loss 0.044878053243684074\n",
      "Epoch 260, Training Loss 0.044900288691629875\n",
      "Epoch 260, Training Loss 0.04491821541974936\n",
      "Epoch 260, Training Loss 0.04495495659789866\n",
      "Epoch 260, Training Loss 0.04500584014753342\n",
      "Epoch 260, Training Loss 0.045022821281929415\n",
      "Epoch 260, Training Loss 0.045159417511228366\n",
      "Epoch 260, Training Loss 0.04519997657600628\n",
      "Epoch 260, Training Loss 0.04528993056060465\n",
      "Epoch 260, Training Loss 0.04532129031158221\n",
      "Epoch 260, Training Loss 0.04534380744054171\n",
      "Epoch 260, Training Loss 0.04537081838671185\n",
      "Epoch 260, Training Loss 0.04545195989698991\n",
      "Epoch 260, Training Loss 0.04549546028330179\n",
      "Epoch 260, Training Loss 0.04553171483468731\n",
      "Epoch 260, Training Loss 0.04557756261121186\n",
      "Epoch 260, Training Loss 0.04562048453425564\n",
      "Epoch 260, Training Loss 0.04567612300726973\n",
      "Epoch 260, Training Loss 0.045778915933345245\n",
      "Epoch 260, Training Loss 0.045878093622391446\n",
      "Epoch 260, Training Loss 0.04593504663697346\n",
      "Epoch 260, Training Loss 0.04599322012239767\n",
      "Epoch 260, Training Loss 0.04608434787832315\n",
      "Epoch 260, Training Loss 0.04614618365281283\n",
      "Epoch 260, Training Loss 0.04620305427273883\n",
      "Epoch 260, Training Loss 0.04622500907817898\n",
      "Epoch 260, Training Loss 0.04633973279427213\n",
      "Epoch 260, Training Loss 0.04640821579193501\n",
      "Epoch 260, Training Loss 0.04643255807971463\n",
      "Epoch 260, Training Loss 0.04650030647585516\n",
      "Epoch 260, Training Loss 0.04663372947238957\n",
      "Epoch 260, Training Loss 0.04667431033630867\n",
      "Epoch 260, Training Loss 0.04669801311989022\n",
      "Epoch 260, Training Loss 0.04670963229080353\n",
      "Epoch 260, Training Loss 0.04675306290652498\n",
      "Epoch 260, Training Loss 0.04685188370132747\n",
      "Epoch 260, Training Loss 0.046882324134502225\n",
      "Epoch 260, Training Loss 0.046911282466648295\n",
      "Epoch 260, Training Loss 0.046934810842808976\n",
      "Epoch 260, Training Loss 0.04699609402621932\n",
      "Epoch 260, Training Loss 0.04704387818196374\n",
      "Epoch 260, Training Loss 0.047110281753070325\n",
      "Epoch 260, Training Loss 0.04715935011927867\n",
      "Epoch 260, Training Loss 0.04722475807558831\n",
      "Epoch 260, Training Loss 0.047268125180350354\n",
      "Epoch 260, Training Loss 0.04738639503934175\n",
      "Epoch 260, Training Loss 0.04743858249774655\n",
      "Epoch 260, Training Loss 0.04749106468759058\n",
      "Epoch 260, Training Loss 0.04755742459848542\n",
      "Epoch 260, Training Loss 0.04759292714381138\n",
      "Epoch 260, Training Loss 0.04764461589445982\n",
      "Epoch 260, Training Loss 0.047665287423140516\n",
      "Epoch 260, Training Loss 0.04768538978510558\n",
      "Epoch 260, Training Loss 0.0477861098586665\n",
      "Epoch 260, Training Loss 0.04802027688113987\n",
      "Epoch 260, Training Loss 0.04805984151968851\n",
      "Epoch 260, Training Loss 0.04815876735624431\n",
      "Epoch 260, Training Loss 0.04842860122268443\n",
      "Epoch 260, Training Loss 0.04855179543907056\n",
      "Epoch 260, Training Loss 0.04857567853480101\n",
      "Epoch 260, Training Loss 0.04862013007061618\n",
      "Epoch 260, Training Loss 0.0486614730755758\n",
      "Epoch 260, Training Loss 0.0487818609764728\n",
      "Epoch 260, Training Loss 0.048822513702289794\n",
      "Epoch 260, Training Loss 0.04900308513520833\n",
      "Epoch 260, Training Loss 0.04904335274604028\n",
      "Epoch 260, Training Loss 0.04918237455853778\n",
      "Epoch 260, Training Loss 0.04922362094413956\n",
      "Epoch 260, Training Loss 0.0492802450184465\n",
      "Epoch 260, Training Loss 0.049377854341221855\n",
      "Epoch 260, Training Loss 0.049533369624034485\n",
      "Epoch 260, Training Loss 0.0497519889235487\n",
      "Epoch 260, Training Loss 0.04982166223721031\n",
      "Epoch 260, Training Loss 0.049877879826132863\n",
      "Epoch 260, Training Loss 0.049922159050598436\n",
      "Epoch 260, Training Loss 0.050085702210145494\n",
      "Epoch 260, Training Loss 0.05020486766322399\n",
      "Epoch 260, Training Loss 0.05025359255957234\n",
      "Epoch 260, Training Loss 0.05044272286124299\n",
      "Epoch 260, Training Loss 0.05056260905795447\n",
      "Epoch 260, Training Loss 0.050591644909008956\n",
      "Epoch 260, Training Loss 0.050633147467866235\n",
      "Epoch 260, Training Loss 0.05067116648013539\n",
      "Epoch 260, Training Loss 0.050720343513347574\n",
      "Epoch 260, Training Loss 0.05076012194640172\n",
      "Epoch 260, Training Loss 0.05082341335187464\n",
      "Epoch 260, Training Loss 0.050857302915576434\n",
      "Epoch 260, Training Loss 0.050936898058804365\n",
      "Epoch 260, Training Loss 0.051015862094028794\n",
      "Epoch 260, Training Loss 0.05105437263977402\n",
      "Epoch 260, Training Loss 0.05113682951099332\n",
      "Epoch 260, Training Loss 0.05115562310987783\n",
      "Epoch 270, Training Loss 0.00012878605814845972\n",
      "Epoch 270, Training Loss 0.00016115962877831494\n",
      "Epoch 270, Training Loss 0.00028572857256054575\n",
      "Epoch 270, Training Loss 0.0003097578168601331\n",
      "Epoch 270, Training Loss 0.00037481036999493914\n",
      "Epoch 270, Training Loss 0.00043700297679895024\n",
      "Epoch 270, Training Loss 0.0004901929098703062\n",
      "Epoch 270, Training Loss 0.0005553571740760828\n",
      "Epoch 270, Training Loss 0.0006344457159338095\n",
      "Epoch 270, Training Loss 0.0006560182241756288\n",
      "Epoch 270, Training Loss 0.0006730832002313851\n",
      "Epoch 270, Training Loss 0.0006859688436055123\n",
      "Epoch 270, Training Loss 0.0007443800902046511\n",
      "Epoch 270, Training Loss 0.0008292112766248186\n",
      "Epoch 270, Training Loss 0.0009089542738616923\n",
      "Epoch 270, Training Loss 0.0009649204342718929\n",
      "Epoch 270, Training Loss 0.0011545658549841713\n",
      "Epoch 270, Training Loss 0.001183818916187567\n",
      "Epoch 270, Training Loss 0.0012150401073267392\n",
      "Epoch 270, Training Loss 0.001253954570769044\n",
      "Epoch 270, Training Loss 0.0012681409919067569\n",
      "Epoch 270, Training Loss 0.0013505510540436144\n",
      "Epoch 270, Training Loss 0.0013950394378389086\n",
      "Epoch 270, Training Loss 0.0014221203356238124\n",
      "Epoch 270, Training Loss 0.0014787610355869432\n",
      "Epoch 270, Training Loss 0.0014924503305493414\n",
      "Epoch 270, Training Loss 0.001517812559462112\n",
      "Epoch 270, Training Loss 0.0015417867032882502\n",
      "Epoch 270, Training Loss 0.0015945221747621855\n",
      "Epoch 270, Training Loss 0.0016097554620212453\n",
      "Epoch 270, Training Loss 0.0016448135735452785\n",
      "Epoch 270, Training Loss 0.0016918141321014717\n",
      "Epoch 270, Training Loss 0.0017257178561938236\n",
      "Epoch 270, Training Loss 0.001764064270031193\n",
      "Epoch 270, Training Loss 0.0019183293892466047\n",
      "Epoch 270, Training Loss 0.001993143255762813\n",
      "Epoch 270, Training Loss 0.0020278584357360592\n",
      "Epoch 270, Training Loss 0.0020523325640165136\n",
      "Epoch 270, Training Loss 0.002091052228599177\n",
      "Epoch 270, Training Loss 0.0021112820578982\n",
      "Epoch 270, Training Loss 0.002130332206378279\n",
      "Epoch 270, Training Loss 0.0022077486502564015\n",
      "Epoch 270, Training Loss 0.00223994501235197\n",
      "Epoch 270, Training Loss 0.0023815250417212846\n",
      "Epoch 270, Training Loss 0.0024466276290300098\n",
      "Epoch 270, Training Loss 0.0024983841447097717\n",
      "Epoch 270, Training Loss 0.002568602527413146\n",
      "Epoch 270, Training Loss 0.002591524486337095\n",
      "Epoch 270, Training Loss 0.002609473045753396\n",
      "Epoch 270, Training Loss 0.0026303014677503834\n",
      "Epoch 270, Training Loss 0.002728046044288084\n",
      "Epoch 270, Training Loss 0.002771868072735989\n",
      "Epoch 270, Training Loss 0.0029001897272399016\n",
      "Epoch 270, Training Loss 0.0029760793356410683\n",
      "Epoch 270, Training Loss 0.002994452307805838\n",
      "Epoch 270, Training Loss 0.003021366341644541\n",
      "Epoch 270, Training Loss 0.003078654503730862\n",
      "Epoch 270, Training Loss 0.003257598413530823\n",
      "Epoch 270, Training Loss 0.0033424377651013375\n",
      "Epoch 270, Training Loss 0.0034788568287401856\n",
      "Epoch 270, Training Loss 0.0035422862795612695\n",
      "Epoch 270, Training Loss 0.0035909605938988878\n",
      "Epoch 270, Training Loss 0.0036060241339228036\n",
      "Epoch 270, Training Loss 0.0036252852079108394\n",
      "Epoch 270, Training Loss 0.0036713422819152665\n",
      "Epoch 270, Training Loss 0.003719746779478953\n",
      "Epoch 270, Training Loss 0.0037389433206728353\n",
      "Epoch 270, Training Loss 0.0037450057619949208\n",
      "Epoch 270, Training Loss 0.003785902427633286\n",
      "Epoch 270, Training Loss 0.003822106561359123\n",
      "Epoch 270, Training Loss 0.003839826236938691\n",
      "Epoch 270, Training Loss 0.00394761234776729\n",
      "Epoch 270, Training Loss 0.004121396930583412\n",
      "Epoch 270, Training Loss 0.00418735697300976\n",
      "Epoch 270, Training Loss 0.004272451568895099\n",
      "Epoch 270, Training Loss 0.004286291489206121\n",
      "Epoch 270, Training Loss 0.004307033291057972\n",
      "Epoch 270, Training Loss 0.004361205967143178\n",
      "Epoch 270, Training Loss 0.0043787093769968545\n",
      "Epoch 270, Training Loss 0.004417116846055592\n",
      "Epoch 270, Training Loss 0.004454040989789474\n",
      "Epoch 270, Training Loss 0.004603277414899005\n",
      "Epoch 270, Training Loss 0.004685270816178235\n",
      "Epoch 270, Training Loss 0.004841370369329134\n",
      "Epoch 270, Training Loss 0.004878202929040012\n",
      "Epoch 270, Training Loss 0.004926844658997015\n",
      "Epoch 270, Training Loss 0.0050148631123316184\n",
      "Epoch 270, Training Loss 0.005047349989904886\n",
      "Epoch 270, Training Loss 0.005087840093938096\n",
      "Epoch 270, Training Loss 0.005115991705418815\n",
      "Epoch 270, Training Loss 0.005160572868950494\n",
      "Epoch 270, Training Loss 0.0051908950813238505\n",
      "Epoch 270, Training Loss 0.005328329462829568\n",
      "Epoch 270, Training Loss 0.005360420421123162\n",
      "Epoch 270, Training Loss 0.005396522869191626\n",
      "Epoch 270, Training Loss 0.005443543664363148\n",
      "Epoch 270, Training Loss 0.005535605048541637\n",
      "Epoch 270, Training Loss 0.005653647779155989\n",
      "Epoch 270, Training Loss 0.00571948127604335\n",
      "Epoch 270, Training Loss 0.00574394130586263\n",
      "Epoch 270, Training Loss 0.005810155916144438\n",
      "Epoch 270, Training Loss 0.0058248130589142405\n",
      "Epoch 270, Training Loss 0.005887525338951089\n",
      "Epoch 270, Training Loss 0.005983728455150943\n",
      "Epoch 270, Training Loss 0.006044612098794878\n",
      "Epoch 270, Training Loss 0.006126487577010108\n",
      "Epoch 270, Training Loss 0.0062471107523793075\n",
      "Epoch 270, Training Loss 0.00630949755542247\n",
      "Epoch 270, Training Loss 0.006389633970944892\n",
      "Epoch 270, Training Loss 0.006396471983050485\n",
      "Epoch 270, Training Loss 0.006440177069776847\n",
      "Epoch 270, Training Loss 0.006487451132286883\n",
      "Epoch 270, Training Loss 0.00657967552411682\n",
      "Epoch 270, Training Loss 0.006619894244086445\n",
      "Epoch 270, Training Loss 0.006643403772279011\n",
      "Epoch 270, Training Loss 0.006701674572575618\n",
      "Epoch 270, Training Loss 0.006739628013542584\n",
      "Epoch 270, Training Loss 0.006866197066638819\n",
      "Epoch 270, Training Loss 0.0068854920886447435\n",
      "Epoch 270, Training Loss 0.006980354147975135\n",
      "Epoch 270, Training Loss 0.007100949026382221\n",
      "Epoch 270, Training Loss 0.00718141789786289\n",
      "Epoch 270, Training Loss 0.0072822229480585254\n",
      "Epoch 270, Training Loss 0.007421175380597067\n",
      "Epoch 270, Training Loss 0.007620501144410438\n",
      "Epoch 270, Training Loss 0.007703243176598111\n",
      "Epoch 270, Training Loss 0.00782131179369262\n",
      "Epoch 270, Training Loss 0.0078929443813889\n",
      "Epoch 270, Training Loss 0.007945158740962901\n",
      "Epoch 270, Training Loss 0.007959037269477534\n",
      "Epoch 270, Training Loss 0.008149788825108154\n",
      "Epoch 270, Training Loss 0.008158392837995191\n",
      "Epoch 270, Training Loss 0.00827483808600327\n",
      "Epoch 270, Training Loss 0.00857360091815462\n",
      "Epoch 270, Training Loss 0.00859048230218632\n",
      "Epoch 270, Training Loss 0.008625357864958131\n",
      "Epoch 270, Training Loss 0.00867164010941849\n",
      "Epoch 270, Training Loss 0.00868961136118816\n",
      "Epoch 270, Training Loss 0.008757373257933179\n",
      "Epoch 270, Training Loss 0.008797893774054964\n",
      "Epoch 270, Training Loss 0.008816876433804022\n",
      "Epoch 270, Training Loss 0.008891486014356203\n",
      "Epoch 270, Training Loss 0.00893536557043281\n",
      "Epoch 270, Training Loss 0.008975547652029434\n",
      "Epoch 270, Training Loss 0.009059646785559367\n",
      "Epoch 270, Training Loss 0.009280419340976478\n",
      "Epoch 270, Training Loss 0.00932820446853576\n",
      "Epoch 270, Training Loss 0.009451437690902663\n",
      "Epoch 270, Training Loss 0.009480330008713294\n",
      "Epoch 270, Training Loss 0.009558957157647022\n",
      "Epoch 270, Training Loss 0.00960695793759316\n",
      "Epoch 270, Training Loss 0.00961418231458539\n",
      "Epoch 270, Training Loss 0.009636275551241377\n",
      "Epoch 270, Training Loss 0.009658726906437246\n",
      "Epoch 270, Training Loss 0.00970969421793814\n",
      "Epoch 270, Training Loss 0.009798321741945146\n",
      "Epoch 270, Training Loss 0.009854041089963577\n",
      "Epoch 270, Training Loss 0.009948557345172786\n",
      "Epoch 270, Training Loss 0.009976185460472504\n",
      "Epoch 270, Training Loss 0.010002798479421975\n",
      "Epoch 270, Training Loss 0.010014536328461316\n",
      "Epoch 270, Training Loss 0.010046297334172689\n",
      "Epoch 270, Training Loss 0.010083992991720319\n",
      "Epoch 270, Training Loss 0.010106594239830818\n",
      "Epoch 270, Training Loss 0.01015097240362402\n",
      "Epoch 270, Training Loss 0.01017018011949785\n",
      "Epoch 270, Training Loss 0.010220778203281143\n",
      "Epoch 270, Training Loss 0.010255970565787972\n",
      "Epoch 270, Training Loss 0.01028263050338725\n",
      "Epoch 270, Training Loss 0.010319407095613383\n",
      "Epoch 270, Training Loss 0.010409965310865046\n",
      "Epoch 270, Training Loss 0.010505881427270372\n",
      "Epoch 270, Training Loss 0.010556173289332853\n",
      "Epoch 270, Training Loss 0.010713400023863139\n",
      "Epoch 270, Training Loss 0.010910608184040355\n",
      "Epoch 270, Training Loss 0.010970149222580368\n",
      "Epoch 270, Training Loss 0.01098373458218163\n",
      "Epoch 270, Training Loss 0.011043187266553912\n",
      "Epoch 270, Training Loss 0.011077761519080995\n",
      "Epoch 270, Training Loss 0.011147088633226158\n",
      "Epoch 270, Training Loss 0.011233468692454384\n",
      "Epoch 270, Training Loss 0.011295842557021266\n",
      "Epoch 270, Training Loss 0.011352485829435498\n",
      "Epoch 270, Training Loss 0.011406363016637543\n",
      "Epoch 270, Training Loss 0.01145692067482816\n",
      "Epoch 270, Training Loss 0.011524496187963296\n",
      "Epoch 270, Training Loss 0.01155488072034648\n",
      "Epoch 270, Training Loss 0.011592093533586206\n",
      "Epoch 270, Training Loss 0.011599998680584114\n",
      "Epoch 270, Training Loss 0.011667909317404565\n",
      "Epoch 270, Training Loss 0.01171070921575398\n",
      "Epoch 270, Training Loss 0.011735250997116498\n",
      "Epoch 270, Training Loss 0.011773959774038067\n",
      "Epoch 270, Training Loss 0.01183092034400424\n",
      "Epoch 270, Training Loss 0.01193153226981535\n",
      "Epoch 270, Training Loss 0.011952965753748442\n",
      "Epoch 270, Training Loss 0.011967893206345304\n",
      "Epoch 270, Training Loss 0.012025961630961018\n",
      "Epoch 270, Training Loss 0.012112931581809545\n",
      "Epoch 270, Training Loss 0.012154322420068257\n",
      "Epoch 270, Training Loss 0.01218771371904694\n",
      "Epoch 270, Training Loss 0.012242228621640778\n",
      "Epoch 270, Training Loss 0.012277759392471874\n",
      "Epoch 270, Training Loss 0.012434026374079078\n",
      "Epoch 270, Training Loss 0.012648329348362925\n",
      "Epoch 270, Training Loss 0.012743676476695043\n",
      "Epoch 270, Training Loss 0.012890310705546528\n",
      "Epoch 270, Training Loss 0.012914368258717725\n",
      "Epoch 270, Training Loss 0.012952426619961134\n",
      "Epoch 270, Training Loss 0.012977134272017899\n",
      "Epoch 270, Training Loss 0.012986480791355148\n",
      "Epoch 270, Training Loss 0.012997288612382071\n",
      "Epoch 270, Training Loss 0.013044419622791887\n",
      "Epoch 270, Training Loss 0.013060229827466486\n",
      "Epoch 270, Training Loss 0.013072758141900306\n",
      "Epoch 270, Training Loss 0.01309564259305806\n",
      "Epoch 270, Training Loss 0.013116770362614862\n",
      "Epoch 270, Training Loss 0.013167005864179234\n",
      "Epoch 270, Training Loss 0.013245242253026885\n",
      "Epoch 270, Training Loss 0.013259614579012747\n",
      "Epoch 270, Training Loss 0.0133020955172446\n",
      "Epoch 270, Training Loss 0.013318602292848479\n",
      "Epoch 270, Training Loss 0.013363735318598349\n",
      "Epoch 270, Training Loss 0.013450793885623516\n",
      "Epoch 270, Training Loss 0.01348588224964888\n",
      "Epoch 270, Training Loss 0.0135180552810421\n",
      "Epoch 270, Training Loss 0.013569479945766004\n",
      "Epoch 270, Training Loss 0.013601275031690669\n",
      "Epoch 270, Training Loss 0.013644102950701896\n",
      "Epoch 270, Training Loss 0.013668347990302287\n",
      "Epoch 270, Training Loss 0.013693918761632898\n",
      "Epoch 270, Training Loss 0.01379550289293594\n",
      "Epoch 270, Training Loss 0.01387191012613666\n",
      "Epoch 270, Training Loss 0.01389239160546466\n",
      "Epoch 270, Training Loss 0.013912454730404727\n",
      "Epoch 270, Training Loss 0.013940770252276679\n",
      "Epoch 270, Training Loss 0.013980447878594251\n",
      "Epoch 270, Training Loss 0.014011938207666092\n",
      "Epoch 270, Training Loss 0.014029273587633924\n",
      "Epoch 270, Training Loss 0.014073465839433282\n",
      "Epoch 270, Training Loss 0.014147412942369918\n",
      "Epoch 270, Training Loss 0.014162500147753016\n",
      "Epoch 270, Training Loss 0.014218609126837319\n",
      "Epoch 270, Training Loss 0.014233196208425\n",
      "Epoch 270, Training Loss 0.01428290942143125\n",
      "Epoch 270, Training Loss 0.01430710552014944\n",
      "Epoch 270, Training Loss 0.014352271811503088\n",
      "Epoch 270, Training Loss 0.01436088877834875\n",
      "Epoch 270, Training Loss 0.014372533047333588\n",
      "Epoch 270, Training Loss 0.014399047812942387\n",
      "Epoch 270, Training Loss 0.014420901635385421\n",
      "Epoch 270, Training Loss 0.014436596281030941\n",
      "Epoch 270, Training Loss 0.014512656638255853\n",
      "Epoch 270, Training Loss 0.014535899431852009\n",
      "Epoch 270, Training Loss 0.014634310892518715\n",
      "Epoch 270, Training Loss 0.014683871078031027\n",
      "Epoch 270, Training Loss 0.014770397585947686\n",
      "Epoch 270, Training Loss 0.014797454164542086\n",
      "Epoch 270, Training Loss 0.014879750143236403\n",
      "Epoch 270, Training Loss 0.014895409861903476\n",
      "Epoch 270, Training Loss 0.014959224346725036\n",
      "Epoch 270, Training Loss 0.014996268838296271\n",
      "Epoch 270, Training Loss 0.015048404437992388\n",
      "Epoch 270, Training Loss 0.015129451996639676\n",
      "Epoch 270, Training Loss 0.01518048027046787\n",
      "Epoch 270, Training Loss 0.015246510188288206\n",
      "Epoch 270, Training Loss 0.015424643413942602\n",
      "Epoch 270, Training Loss 0.015584582455165665\n",
      "Epoch 270, Training Loss 0.015723376500937143\n",
      "Epoch 270, Training Loss 0.01574693145432874\n",
      "Epoch 270, Training Loss 0.01579569388940797\n",
      "Epoch 270, Training Loss 0.015884046647054575\n",
      "Epoch 270, Training Loss 0.015933053522154003\n",
      "Epoch 270, Training Loss 0.01603429414548666\n",
      "Epoch 270, Training Loss 0.016082258058397476\n",
      "Epoch 270, Training Loss 0.016117099000264885\n",
      "Epoch 270, Training Loss 0.01614230773244958\n",
      "Epoch 270, Training Loss 0.01622416658500862\n",
      "Epoch 270, Training Loss 0.01628543321898355\n",
      "Epoch 270, Training Loss 0.01630382504626213\n",
      "Epoch 270, Training Loss 0.016349660227303883\n",
      "Epoch 270, Training Loss 0.01647789894407877\n",
      "Epoch 270, Training Loss 0.01649904689546722\n",
      "Epoch 270, Training Loss 0.016530296393096104\n",
      "Epoch 270, Training Loss 0.016659631621916696\n",
      "Epoch 270, Training Loss 0.016746263076310688\n",
      "Epoch 270, Training Loss 0.01675852671529993\n",
      "Epoch 270, Training Loss 0.01677102377683477\n",
      "Epoch 270, Training Loss 0.01678149532729193\n",
      "Epoch 270, Training Loss 0.016822244586718396\n",
      "Epoch 270, Training Loss 0.016847554630602297\n",
      "Epoch 270, Training Loss 0.01686309823287589\n",
      "Epoch 270, Training Loss 0.016886722994968295\n",
      "Epoch 270, Training Loss 0.016914603855970607\n",
      "Epoch 270, Training Loss 0.016929460295816632\n",
      "Epoch 270, Training Loss 0.01694903823980571\n",
      "Epoch 270, Training Loss 0.016978013643023114\n",
      "Epoch 270, Training Loss 0.0170084313671712\n",
      "Epoch 270, Training Loss 0.01705043523958253\n",
      "Epoch 270, Training Loss 0.017088918993725916\n",
      "Epoch 270, Training Loss 0.01713301846102509\n",
      "Epoch 270, Training Loss 0.017154849467255994\n",
      "Epoch 270, Training Loss 0.01718637696824625\n",
      "Epoch 270, Training Loss 0.01726808715218683\n",
      "Epoch 270, Training Loss 0.01737641502121735\n",
      "Epoch 270, Training Loss 0.01740617613674944\n",
      "Epoch 270, Training Loss 0.017438344723518812\n",
      "Epoch 270, Training Loss 0.017481078981490962\n",
      "Epoch 270, Training Loss 0.017486937869283015\n",
      "Epoch 270, Training Loss 0.017515534262322937\n",
      "Epoch 270, Training Loss 0.017574818346344525\n",
      "Epoch 270, Training Loss 0.017590690115966912\n",
      "Epoch 270, Training Loss 0.01766653630711005\n",
      "Epoch 270, Training Loss 0.01774623972671035\n",
      "Epoch 270, Training Loss 0.017761243737119313\n",
      "Epoch 270, Training Loss 0.017832886081074587\n",
      "Epoch 270, Training Loss 0.017897878653224546\n",
      "Epoch 270, Training Loss 0.017921476797593754\n",
      "Epoch 270, Training Loss 0.018068385564143318\n",
      "Epoch 270, Training Loss 0.018128694326895504\n",
      "Epoch 270, Training Loss 0.0182724874835852\n",
      "Epoch 270, Training Loss 0.01838752483863317\n",
      "Epoch 270, Training Loss 0.01843546592818616\n",
      "Epoch 270, Training Loss 0.018497665761672246\n",
      "Epoch 270, Training Loss 0.0185596224032533\n",
      "Epoch 270, Training Loss 0.018655919718801442\n",
      "Epoch 270, Training Loss 0.01867325702929855\n",
      "Epoch 270, Training Loss 0.018684768159647504\n",
      "Epoch 270, Training Loss 0.01871084724255192\n",
      "Epoch 270, Training Loss 0.018735710677959958\n",
      "Epoch 270, Training Loss 0.018810282024266697\n",
      "Epoch 270, Training Loss 0.018832594309897754\n",
      "Epoch 270, Training Loss 0.018876570923601652\n",
      "Epoch 270, Training Loss 0.018900531938399575\n",
      "Epoch 270, Training Loss 0.018914706582236852\n",
      "Epoch 270, Training Loss 0.018935729112342724\n",
      "Epoch 270, Training Loss 0.01899425743404976\n",
      "Epoch 270, Training Loss 0.01902174652265885\n",
      "Epoch 270, Training Loss 0.01914548748970756\n",
      "Epoch 270, Training Loss 0.019175798109377664\n",
      "Epoch 270, Training Loss 0.01919274461929641\n",
      "Epoch 270, Training Loss 0.019302451769914238\n",
      "Epoch 270, Training Loss 0.019317709034561273\n",
      "Epoch 270, Training Loss 0.019416451865040204\n",
      "Epoch 270, Training Loss 0.019474498652483876\n",
      "Epoch 270, Training Loss 0.019606212960303668\n",
      "Epoch 270, Training Loss 0.01977282530173202\n",
      "Epoch 270, Training Loss 0.019783139455101222\n",
      "Epoch 270, Training Loss 0.019907187458480257\n",
      "Epoch 270, Training Loss 0.02002106291358657\n",
      "Epoch 270, Training Loss 0.020243710843022063\n",
      "Epoch 270, Training Loss 0.020347692660720604\n",
      "Epoch 270, Training Loss 0.020438221759636842\n",
      "Epoch 270, Training Loss 0.02053599579550345\n",
      "Epoch 270, Training Loss 0.02086185884740575\n",
      "Epoch 270, Training Loss 0.020952436308402694\n",
      "Epoch 270, Training Loss 0.021197530798271032\n",
      "Epoch 270, Training Loss 0.021249096419023887\n",
      "Epoch 270, Training Loss 0.021330993991735797\n",
      "Epoch 270, Training Loss 0.02140018287474466\n",
      "Epoch 270, Training Loss 0.02149610108722125\n",
      "Epoch 270, Training Loss 0.02155894640109042\n",
      "Epoch 270, Training Loss 0.021613515820592413\n",
      "Epoch 270, Training Loss 0.021646254269591988\n",
      "Epoch 270, Training Loss 0.02173938908997704\n",
      "Epoch 270, Training Loss 0.021770770680588074\n",
      "Epoch 270, Training Loss 0.02180738208096122\n",
      "Epoch 270, Training Loss 0.021825284911486345\n",
      "Epoch 270, Training Loss 0.021846990572655444\n",
      "Epoch 270, Training Loss 0.0218782553239666\n",
      "Epoch 270, Training Loss 0.022006419773125434\n",
      "Epoch 270, Training Loss 0.022043537646246232\n",
      "Epoch 270, Training Loss 0.022094126855073226\n",
      "Epoch 270, Training Loss 0.022161323250845417\n",
      "Epoch 270, Training Loss 0.022217507798062718\n",
      "Epoch 270, Training Loss 0.02223545947657598\n",
      "Epoch 270, Training Loss 0.022304502720742123\n",
      "Epoch 270, Training Loss 0.022406900327776553\n",
      "Epoch 270, Training Loss 0.02257065822029739\n",
      "Epoch 270, Training Loss 0.02265723583424259\n",
      "Epoch 270, Training Loss 0.022684296140509187\n",
      "Epoch 270, Training Loss 0.022810289641018108\n",
      "Epoch 270, Training Loss 0.022880928905304434\n",
      "Epoch 270, Training Loss 0.023135074249961796\n",
      "Epoch 270, Training Loss 0.023259947362267757\n",
      "Epoch 270, Training Loss 0.023367227062278086\n",
      "Epoch 270, Training Loss 0.023544053444662667\n",
      "Epoch 270, Training Loss 0.02362295672716692\n",
      "Epoch 270, Training Loss 0.02377219896411042\n",
      "Epoch 270, Training Loss 0.023954379188892483\n",
      "Epoch 270, Training Loss 0.024011333184816953\n",
      "Epoch 270, Training Loss 0.024037045917337966\n",
      "Epoch 270, Training Loss 0.024070164445987748\n",
      "Epoch 270, Training Loss 0.024139047331174317\n",
      "Epoch 270, Training Loss 0.02415831219000013\n",
      "Epoch 270, Training Loss 0.024196587570840516\n",
      "Epoch 270, Training Loss 0.024222622864910632\n",
      "Epoch 270, Training Loss 0.024296381330960775\n",
      "Epoch 270, Training Loss 0.02435856316205295\n",
      "Epoch 270, Training Loss 0.02438554624000283\n",
      "Epoch 270, Training Loss 0.024441274172385865\n",
      "Epoch 270, Training Loss 0.024533302057534456\n",
      "Epoch 270, Training Loss 0.02458184933089806\n",
      "Epoch 270, Training Loss 0.024654534146370713\n",
      "Epoch 270, Training Loss 0.02469525427517036\n",
      "Epoch 270, Training Loss 0.02470979143334243\n",
      "Epoch 270, Training Loss 0.024742476465871268\n",
      "Epoch 270, Training Loss 0.024791025930582106\n",
      "Epoch 270, Training Loss 0.024806165384114398\n",
      "Epoch 270, Training Loss 0.024921966376035568\n",
      "Epoch 270, Training Loss 0.024949111151592353\n",
      "Epoch 270, Training Loss 0.02498023543754578\n",
      "Epoch 270, Training Loss 0.024996056912176292\n",
      "Epoch 270, Training Loss 0.02506402493847529\n",
      "Epoch 270, Training Loss 0.025131039004629988\n",
      "Epoch 270, Training Loss 0.025153129528302825\n",
      "Epoch 270, Training Loss 0.025183696845246246\n",
      "Epoch 270, Training Loss 0.025262781023464697\n",
      "Epoch 270, Training Loss 0.025284311241086793\n",
      "Epoch 270, Training Loss 0.025318584721678358\n",
      "Epoch 270, Training Loss 0.025369781024201447\n",
      "Epoch 270, Training Loss 0.02543867531392123\n",
      "Epoch 270, Training Loss 0.025464136505980627\n",
      "Epoch 270, Training Loss 0.025483115183194276\n",
      "Epoch 270, Training Loss 0.02551776644966715\n",
      "Epoch 270, Training Loss 0.025536941426336918\n",
      "Epoch 270, Training Loss 0.025626140825278924\n",
      "Epoch 270, Training Loss 0.02568556398839292\n",
      "Epoch 270, Training Loss 0.025745191223100018\n",
      "Epoch 270, Training Loss 0.025808726992372355\n",
      "Epoch 270, Training Loss 0.025849330565318122\n",
      "Epoch 270, Training Loss 0.025880425753991316\n",
      "Epoch 270, Training Loss 0.025923159978616876\n",
      "Epoch 270, Training Loss 0.025970976771143697\n",
      "Epoch 270, Training Loss 0.02600539328001649\n",
      "Epoch 270, Training Loss 0.02602288657513535\n",
      "Epoch 270, Training Loss 0.02605107772376036\n",
      "Epoch 270, Training Loss 0.026077878257721815\n",
      "Epoch 270, Training Loss 0.02608809588105439\n",
      "Epoch 270, Training Loss 0.026108973486887295\n",
      "Epoch 270, Training Loss 0.026144793367875584\n",
      "Epoch 270, Training Loss 0.02620683093209896\n",
      "Epoch 270, Training Loss 0.026271214614600857\n",
      "Epoch 270, Training Loss 0.02636837229118361\n",
      "Epoch 270, Training Loss 0.026430786910045252\n",
      "Epoch 270, Training Loss 0.026475318315703315\n",
      "Epoch 270, Training Loss 0.02650013737394796\n",
      "Epoch 270, Training Loss 0.02653778422399974\n",
      "Epoch 270, Training Loss 0.02657195122655281\n",
      "Epoch 270, Training Loss 0.02659729387625442\n",
      "Epoch 270, Training Loss 0.026642925366806937\n",
      "Epoch 270, Training Loss 0.02670341005186786\n",
      "Epoch 270, Training Loss 0.026738674728833425\n",
      "Epoch 270, Training Loss 0.02682478836787593\n",
      "Epoch 270, Training Loss 0.026950878659239435\n",
      "Epoch 270, Training Loss 0.027098055627635297\n",
      "Epoch 270, Training Loss 0.027137980149711107\n",
      "Epoch 270, Training Loss 0.02724312711626177\n",
      "Epoch 270, Training Loss 0.02735344370912827\n",
      "Epoch 270, Training Loss 0.02743086343049965\n",
      "Epoch 270, Training Loss 0.027487857761504628\n",
      "Epoch 270, Training Loss 0.027590890878292224\n",
      "Epoch 270, Training Loss 0.027652828016882892\n",
      "Epoch 270, Training Loss 0.027686871386959654\n",
      "Epoch 270, Training Loss 0.027754439268489858\n",
      "Epoch 270, Training Loss 0.02779473936366265\n",
      "Epoch 270, Training Loss 0.02784065881153316\n",
      "Epoch 270, Training Loss 0.02794382953539948\n",
      "Epoch 270, Training Loss 0.027992970606698024\n",
      "Epoch 270, Training Loss 0.028057662783252536\n",
      "Epoch 270, Training Loss 0.028097154697655793\n",
      "Epoch 270, Training Loss 0.028149185350874576\n",
      "Epoch 270, Training Loss 0.028173446768413647\n",
      "Epoch 270, Training Loss 0.028260560328726803\n",
      "Epoch 270, Training Loss 0.028349348745258796\n",
      "Epoch 270, Training Loss 0.02836953243240714\n",
      "Epoch 270, Training Loss 0.028422417344234865\n",
      "Epoch 270, Training Loss 0.028449284075937994\n",
      "Epoch 270, Training Loss 0.028474036741125232\n",
      "Epoch 270, Training Loss 0.028521765932164458\n",
      "Epoch 270, Training Loss 0.028559001997503858\n",
      "Epoch 270, Training Loss 0.028600524192737876\n",
      "Epoch 270, Training Loss 0.02863103821468742\n",
      "Epoch 270, Training Loss 0.028662940443200453\n",
      "Epoch 270, Training Loss 0.028749356764461605\n",
      "Epoch 270, Training Loss 0.028766952396687265\n",
      "Epoch 270, Training Loss 0.028838441922755725\n",
      "Epoch 270, Training Loss 0.028880136138747643\n",
      "Epoch 270, Training Loss 0.028983029847740746\n",
      "Epoch 270, Training Loss 0.029018756236804798\n",
      "Epoch 270, Training Loss 0.02904899986079701\n",
      "Epoch 270, Training Loss 0.029089426811155686\n",
      "Epoch 270, Training Loss 0.029147903655257906\n",
      "Epoch 270, Training Loss 0.029211869590756154\n",
      "Epoch 270, Training Loss 0.029246669039940058\n",
      "Epoch 270, Training Loss 0.0292923711585667\n",
      "Epoch 270, Training Loss 0.029361960224454742\n",
      "Epoch 270, Training Loss 0.029414418147510047\n",
      "Epoch 270, Training Loss 0.029467520202793505\n",
      "Epoch 270, Training Loss 0.0295539160873598\n",
      "Epoch 270, Training Loss 0.02960224692230982\n",
      "Epoch 270, Training Loss 0.029674584547395976\n",
      "Epoch 270, Training Loss 0.02972629789829902\n",
      "Epoch 270, Training Loss 0.029753031731938082\n",
      "Epoch 270, Training Loss 0.029798059720698447\n",
      "Epoch 270, Training Loss 0.029825782607002256\n",
      "Epoch 270, Training Loss 0.029938600908564712\n",
      "Epoch 270, Training Loss 0.02998394560416603\n",
      "Epoch 270, Training Loss 0.030014764094758596\n",
      "Epoch 270, Training Loss 0.03004152028490325\n",
      "Epoch 270, Training Loss 0.03009363409022198\n",
      "Epoch 270, Training Loss 0.03011316291825927\n",
      "Epoch 270, Training Loss 0.030169487864141118\n",
      "Epoch 270, Training Loss 0.030255699704122514\n",
      "Epoch 270, Training Loss 0.030269844137141694\n",
      "Epoch 270, Training Loss 0.030382920874763863\n",
      "Epoch 270, Training Loss 0.030478133368627418\n",
      "Epoch 270, Training Loss 0.030576471495382545\n",
      "Epoch 270, Training Loss 0.030630327517028582\n",
      "Epoch 270, Training Loss 0.03064347198709388\n",
      "Epoch 270, Training Loss 0.03073184742994816\n",
      "Epoch 270, Training Loss 0.030785487918302303\n",
      "Epoch 270, Training Loss 0.03081154201984825\n",
      "Epoch 270, Training Loss 0.030864797035932465\n",
      "Epoch 270, Training Loss 0.030898350173050104\n",
      "Epoch 270, Training Loss 0.030919614592162165\n",
      "Epoch 270, Training Loss 0.030935328552628035\n",
      "Epoch 270, Training Loss 0.031088368213304397\n",
      "Epoch 270, Training Loss 0.031173978614218324\n",
      "Epoch 270, Training Loss 0.031181755556089\n",
      "Epoch 270, Training Loss 0.03121142539724498\n",
      "Epoch 270, Training Loss 0.031279447530765475\n",
      "Epoch 270, Training Loss 0.03144158794225463\n",
      "Epoch 270, Training Loss 0.03146749143155239\n",
      "Epoch 270, Training Loss 0.031532445326423665\n",
      "Epoch 270, Training Loss 0.031583340699805895\n",
      "Epoch 270, Training Loss 0.031665971554468014\n",
      "Epoch 270, Training Loss 0.03180305543474739\n",
      "Epoch 270, Training Loss 0.03187409797361802\n",
      "Epoch 270, Training Loss 0.0319627404464242\n",
      "Epoch 270, Training Loss 0.03207353077045239\n",
      "Epoch 270, Training Loss 0.032181133341658716\n",
      "Epoch 270, Training Loss 0.03228054536194505\n",
      "Epoch 270, Training Loss 0.032332370995574865\n",
      "Epoch 270, Training Loss 0.03236265179863119\n",
      "Epoch 270, Training Loss 0.03240732400787666\n",
      "Epoch 270, Training Loss 0.03246038623304223\n",
      "Epoch 270, Training Loss 0.03265743585401084\n",
      "Epoch 270, Training Loss 0.03272716827688219\n",
      "Epoch 270, Training Loss 0.03276383427752996\n",
      "Epoch 270, Training Loss 0.032794657627196834\n",
      "Epoch 270, Training Loss 0.03281536820949629\n",
      "Epoch 270, Training Loss 0.03282932581825425\n",
      "Epoch 270, Training Loss 0.03285599415681189\n",
      "Epoch 270, Training Loss 0.032865667718646054\n",
      "Epoch 270, Training Loss 0.03291845458197643\n",
      "Epoch 270, Training Loss 0.032982339251838876\n",
      "Epoch 270, Training Loss 0.03299655846696433\n",
      "Epoch 270, Training Loss 0.03307683515729254\n",
      "Epoch 270, Training Loss 0.03335543799504657\n",
      "Epoch 270, Training Loss 0.03343652576015181\n",
      "Epoch 270, Training Loss 0.03356640920688486\n",
      "Epoch 270, Training Loss 0.033634541669974816\n",
      "Epoch 270, Training Loss 0.033671852467400606\n",
      "Epoch 270, Training Loss 0.033701787706907564\n",
      "Epoch 270, Training Loss 0.03375018684996192\n",
      "Epoch 270, Training Loss 0.03378535377854944\n",
      "Epoch 270, Training Loss 0.033901503385232803\n",
      "Epoch 270, Training Loss 0.03402447298852741\n",
      "Epoch 270, Training Loss 0.0340589187185392\n",
      "Epoch 270, Training Loss 0.034093163108996224\n",
      "Epoch 270, Training Loss 0.03419880425293104\n",
      "Epoch 270, Training Loss 0.03422021845300846\n",
      "Epoch 270, Training Loss 0.034335340546501225\n",
      "Epoch 270, Training Loss 0.03442837688964708\n",
      "Epoch 270, Training Loss 0.03454529555440616\n",
      "Epoch 270, Training Loss 0.0345591870868993\n",
      "Epoch 270, Training Loss 0.03462178905165809\n",
      "Epoch 270, Training Loss 0.034651915431427566\n",
      "Epoch 270, Training Loss 0.03482458812465696\n",
      "Epoch 270, Training Loss 0.03484276503079173\n",
      "Epoch 270, Training Loss 0.034950020416375356\n",
      "Epoch 270, Training Loss 0.035011945264366316\n",
      "Epoch 270, Training Loss 0.03508497842306466\n",
      "Epoch 270, Training Loss 0.03511793283647988\n",
      "Epoch 270, Training Loss 0.035237732227258095\n",
      "Epoch 270, Training Loss 0.03526268166113559\n",
      "Epoch 270, Training Loss 0.03534399736446359\n",
      "Epoch 270, Training Loss 0.03546955803578333\n",
      "Epoch 270, Training Loss 0.03563291434010448\n",
      "Epoch 270, Training Loss 0.03572461598128785\n",
      "Epoch 270, Training Loss 0.03579055220948632\n",
      "Epoch 270, Training Loss 0.03581813535929355\n",
      "Epoch 270, Training Loss 0.035843921940216364\n",
      "Epoch 270, Training Loss 0.03587892740938688\n",
      "Epoch 270, Training Loss 0.03589655814127868\n",
      "Epoch 270, Training Loss 0.03596526454202831\n",
      "Epoch 270, Training Loss 0.03601534110898881\n",
      "Epoch 270, Training Loss 0.0360833120173858\n",
      "Epoch 270, Training Loss 0.03610360166570053\n",
      "Epoch 270, Training Loss 0.03624480104852763\n",
      "Epoch 270, Training Loss 0.036352533700011305\n",
      "Epoch 270, Training Loss 0.03644133818959889\n",
      "Epoch 270, Training Loss 0.03654652571512858\n",
      "Epoch 270, Training Loss 0.03662451901211096\n",
      "Epoch 270, Training Loss 0.03673812516016976\n",
      "Epoch 270, Training Loss 0.03700956633216832\n",
      "Epoch 270, Training Loss 0.037037442769983886\n",
      "Epoch 270, Training Loss 0.03705635554004756\n",
      "Epoch 270, Training Loss 0.037098736176505456\n",
      "Epoch 270, Training Loss 0.037276315162806294\n",
      "Epoch 270, Training Loss 0.03735248437282317\n",
      "Epoch 270, Training Loss 0.03741232754098123\n",
      "Epoch 270, Training Loss 0.03755869338696208\n",
      "Epoch 270, Training Loss 0.03762547506074257\n",
      "Epoch 270, Training Loss 0.037707983400039086\n",
      "Epoch 270, Training Loss 0.0378638241741368\n",
      "Epoch 270, Training Loss 0.03794024403616691\n",
      "Epoch 270, Training Loss 0.03796753124691203\n",
      "Epoch 270, Training Loss 0.03805294410263657\n",
      "Epoch 270, Training Loss 0.03812079265763712\n",
      "Epoch 270, Training Loss 0.03821969176928425\n",
      "Epoch 270, Training Loss 0.03825092183299782\n",
      "Epoch 270, Training Loss 0.0382635999450703\n",
      "Epoch 270, Training Loss 0.038377981487056595\n",
      "Epoch 270, Training Loss 0.03844221499081596\n",
      "Epoch 270, Training Loss 0.03854845518239147\n",
      "Epoch 270, Training Loss 0.038591070869423046\n",
      "Epoch 270, Training Loss 0.038682821758753615\n",
      "Epoch 270, Training Loss 0.038717556920478985\n",
      "Epoch 270, Training Loss 0.038822439162756135\n",
      "Epoch 270, Training Loss 0.038924502293981825\n",
      "Epoch 270, Training Loss 0.038942182683707464\n",
      "Epoch 270, Training Loss 0.0389937924315243\n",
      "Epoch 270, Training Loss 0.039038943040453834\n",
      "Epoch 270, Training Loss 0.039069373417250294\n",
      "Epoch 270, Training Loss 0.03913055219725632\n",
      "Epoch 270, Training Loss 0.03918059012028472\n",
      "Epoch 270, Training Loss 0.03920249259778206\n",
      "Epoch 270, Training Loss 0.03921318016029285\n",
      "Epoch 270, Training Loss 0.0392577310498861\n",
      "Epoch 270, Training Loss 0.039266025418263226\n",
      "Epoch 270, Training Loss 0.03929432993814292\n",
      "Epoch 270, Training Loss 0.039300567884941386\n",
      "Epoch 270, Training Loss 0.03939941296558779\n",
      "Epoch 270, Training Loss 0.03943578645472636\n",
      "Epoch 270, Training Loss 0.03953707883196414\n",
      "Epoch 270, Training Loss 0.039605618285401094\n",
      "Epoch 270, Training Loss 0.03962966002037992\n",
      "Epoch 270, Training Loss 0.03964942475409269\n",
      "Epoch 270, Training Loss 0.03966666874535325\n",
      "Epoch 270, Training Loss 0.039757144337286575\n",
      "Epoch 270, Training Loss 0.03980826520982682\n",
      "Epoch 270, Training Loss 0.03989965486985719\n",
      "Epoch 270, Training Loss 0.040000739945646596\n",
      "Epoch 270, Training Loss 0.04003513544855063\n",
      "Epoch 270, Training Loss 0.04009657435576477\n",
      "Epoch 270, Training Loss 0.04032330301202014\n",
      "Epoch 270, Training Loss 0.04038971095152981\n",
      "Epoch 270, Training Loss 0.040437291983676994\n",
      "Epoch 270, Training Loss 0.04047806152735677\n",
      "Epoch 270, Training Loss 0.04054282171666013\n",
      "Epoch 270, Training Loss 0.040641623761152365\n",
      "Epoch 270, Training Loss 0.04068437110527855\n",
      "Epoch 270, Training Loss 0.04075374456641771\n",
      "Epoch 270, Training Loss 0.04080857558990531\n",
      "Epoch 270, Training Loss 0.040850569767987024\n",
      "Epoch 270, Training Loss 0.04092111510446157\n",
      "Epoch 270, Training Loss 0.040988134910993256\n",
      "Epoch 270, Training Loss 0.04101058703316066\n",
      "Epoch 270, Training Loss 0.04117410206724235\n",
      "Epoch 270, Training Loss 0.04131342180768776\n",
      "Epoch 270, Training Loss 0.04140666618590693\n",
      "Epoch 270, Training Loss 0.041430304498623706\n",
      "Epoch 270, Training Loss 0.0415473902583732\n",
      "Epoch 270, Training Loss 0.0416151327616952\n",
      "Epoch 270, Training Loss 0.04164383307461391\n",
      "Epoch 270, Training Loss 0.0417172313355805\n",
      "Epoch 270, Training Loss 0.0418992726289479\n",
      "Epoch 270, Training Loss 0.04190898668068602\n",
      "Epoch 270, Training Loss 0.041949008131766566\n",
      "Epoch 270, Training Loss 0.0420878191842981\n",
      "Epoch 270, Training Loss 0.04213208964813853\n",
      "Epoch 270, Training Loss 0.04215904301666009\n",
      "Epoch 270, Training Loss 0.042183702223269684\n",
      "Epoch 270, Training Loss 0.04224215263781874\n",
      "Epoch 270, Training Loss 0.04236396669607867\n",
      "Epoch 270, Training Loss 0.04238365270326014\n",
      "Epoch 270, Training Loss 0.042516573252456495\n",
      "Epoch 270, Training Loss 0.042557890370935963\n",
      "Epoch 270, Training Loss 0.04258634518627125\n",
      "Epoch 270, Training Loss 0.04266799799502467\n",
      "Epoch 270, Training Loss 0.04267675334663914\n",
      "Epoch 270, Training Loss 0.0427912941328762\n",
      "Epoch 270, Training Loss 0.042934860809065424\n",
      "Epoch 270, Training Loss 0.04314101593511756\n",
      "Epoch 270, Training Loss 0.043224695902031936\n",
      "Epoch 270, Training Loss 0.043262864735758744\n",
      "Epoch 270, Training Loss 0.043360344102115506\n",
      "Epoch 270, Training Loss 0.04361254737957779\n",
      "Epoch 270, Training Loss 0.04368253670932958\n",
      "Epoch 270, Training Loss 0.043734441912683\n",
      "Epoch 270, Training Loss 0.043785772482862174\n",
      "Epoch 270, Training Loss 0.043889268402658076\n",
      "Epoch 270, Training Loss 0.044007966612868224\n",
      "Epoch 270, Training Loss 0.04404011494396707\n",
      "Epoch 270, Training Loss 0.04409061748501095\n",
      "Epoch 270, Training Loss 0.044142051867883454\n",
      "Epoch 270, Training Loss 0.04416711121926184\n",
      "Epoch 270, Training Loss 0.04419736786509681\n",
      "Epoch 270, Training Loss 0.044233891268587094\n",
      "Epoch 270, Training Loss 0.044250055458015566\n",
      "Epoch 270, Training Loss 0.04433168132272561\n",
      "Epoch 270, Training Loss 0.044356152216386995\n",
      "Epoch 270, Training Loss 0.04439243251372062\n",
      "Epoch 270, Training Loss 0.04440884078350252\n",
      "Epoch 270, Training Loss 0.04445715715556079\n",
      "Epoch 270, Training Loss 0.044498652678287925\n",
      "Epoch 270, Training Loss 0.04458785015265541\n",
      "Epoch 270, Training Loss 0.04462430741080581\n",
      "Epoch 270, Training Loss 0.044711574155818244\n",
      "Epoch 270, Training Loss 0.044753910360329066\n",
      "Epoch 270, Training Loss 0.04487011808535213\n",
      "Epoch 270, Training Loss 0.04505132445636803\n",
      "Epoch 270, Training Loss 0.04513033424311168\n",
      "Epoch 270, Training Loss 0.04520067704193618\n",
      "Epoch 270, Training Loss 0.045264208542845207\n",
      "Epoch 270, Training Loss 0.04528575130831212\n",
      "Epoch 270, Training Loss 0.045314132692077964\n",
      "Epoch 270, Training Loss 0.04533402898641842\n",
      "Epoch 270, Training Loss 0.04535552977329439\n",
      "Epoch 270, Training Loss 0.045456221006820195\n",
      "Epoch 270, Training Loss 0.04547696192975125\n",
      "Epoch 270, Training Loss 0.045492809633855394\n",
      "Epoch 270, Training Loss 0.04551499351844801\n",
      "Epoch 270, Training Loss 0.04564781600009183\n",
      "Epoch 270, Training Loss 0.04570260089095634\n",
      "Epoch 270, Training Loss 0.04586376348519912\n",
      "Epoch 270, Training Loss 0.04591103836708247\n",
      "Epoch 270, Training Loss 0.046029965010950406\n",
      "Epoch 270, Training Loss 0.046053211046311326\n",
      "Epoch 270, Training Loss 0.04610645955504702\n",
      "Epoch 270, Training Loss 0.04612244323224706\n",
      "Epoch 270, Training Loss 0.0461702336285673\n",
      "Epoch 270, Training Loss 0.04627258383580829\n",
      "Epoch 270, Training Loss 0.04645984217434016\n",
      "Epoch 270, Training Loss 0.046518544998744034\n",
      "Epoch 270, Training Loss 0.046543843969178704\n",
      "Epoch 270, Training Loss 0.04657244967782627\n",
      "Epoch 270, Training Loss 0.04658940605833517\n",
      "Epoch 270, Training Loss 0.046610668774389324\n",
      "Epoch 270, Training Loss 0.04665126639258717\n",
      "Epoch 270, Training Loss 0.04666020163594533\n",
      "Epoch 270, Training Loss 0.04671757588939517\n",
      "Epoch 270, Training Loss 0.04690580360372277\n",
      "Epoch 270, Training Loss 0.04704509509484405\n",
      "Epoch 270, Training Loss 0.047131457156442166\n",
      "Epoch 270, Training Loss 0.047271817720130734\n",
      "Epoch 270, Training Loss 0.04732443300573646\n",
      "Epoch 270, Training Loss 0.04743991569018044\n",
      "Epoch 270, Training Loss 0.04753267992516536\n",
      "Epoch 270, Training Loss 0.04756851533851813\n",
      "Epoch 270, Training Loss 0.04760660325436641\n",
      "Epoch 270, Training Loss 0.047696477492980636\n",
      "Epoch 270, Training Loss 0.04780574943251012\n",
      "Epoch 270, Training Loss 0.04798481339002814\n",
      "Epoch 270, Training Loss 0.04804501381924238\n",
      "Epoch 270, Training Loss 0.048164270803942096\n",
      "Epoch 270, Training Loss 0.04821463083119496\n",
      "Epoch 270, Training Loss 0.048261557293631845\n",
      "Epoch 270, Training Loss 0.048308215273158324\n",
      "Epoch 270, Training Loss 0.048354103013187114\n",
      "Epoch 270, Training Loss 0.04847287650093855\n",
      "Epoch 270, Training Loss 0.048609792381105826\n",
      "Epoch 270, Training Loss 0.04871427889465523\n",
      "Epoch 270, Training Loss 0.0487911402369323\n",
      "Epoch 270, Training Loss 0.048815273722667064\n",
      "Epoch 270, Training Loss 0.04889462993758947\n",
      "Epoch 270, Training Loss 0.0489427621126213\n",
      "Epoch 270, Training Loss 0.04903361097673702\n",
      "Epoch 270, Training Loss 0.04916967084045377\n",
      "Epoch 270, Training Loss 0.04917858938848519\n",
      "Epoch 280, Training Loss 5.83601643895859e-05\n",
      "Epoch 280, Training Loss 9.942383927000148e-05\n",
      "Epoch 280, Training Loss 0.00016552815332894434\n",
      "Epoch 280, Training Loss 0.00020664144316902551\n",
      "Epoch 280, Training Loss 0.0002508813996449151\n",
      "Epoch 280, Training Loss 0.00029128415467184217\n",
      "Epoch 280, Training Loss 0.00041250071828932407\n",
      "Epoch 280, Training Loss 0.00044731588800773597\n",
      "Epoch 280, Training Loss 0.0004871725307210632\n",
      "Epoch 280, Training Loss 0.0004979701572195496\n",
      "Epoch 280, Training Loss 0.0005390889921208934\n",
      "Epoch 280, Training Loss 0.0005542702444107331\n",
      "Epoch 280, Training Loss 0.0005946810978948308\n",
      "Epoch 280, Training Loss 0.0006605114268562983\n",
      "Epoch 280, Training Loss 0.0007572008506454471\n",
      "Epoch 280, Training Loss 0.0008305705478772178\n",
      "Epoch 280, Training Loss 0.000877110961624576\n",
      "Epoch 280, Training Loss 0.00098878641843872\n",
      "Epoch 280, Training Loss 0.0010760873008300276\n",
      "Epoch 280, Training Loss 0.0011389585726363275\n",
      "Epoch 280, Training Loss 0.001174354044448994\n",
      "Epoch 280, Training Loss 0.001217975421711002\n",
      "Epoch 280, Training Loss 0.0012471613205035628\n",
      "Epoch 280, Training Loss 0.0012794515651548305\n",
      "Epoch 280, Training Loss 0.0013207546875947881\n",
      "Epoch 280, Training Loss 0.001330766828059007\n",
      "Epoch 280, Training Loss 0.0014910736726239666\n",
      "Epoch 280, Training Loss 0.0015736816991644596\n",
      "Epoch 280, Training Loss 0.0016048148434008936\n",
      "Epoch 280, Training Loss 0.0016382045989089152\n",
      "Epoch 280, Training Loss 0.0017523469029427946\n",
      "Epoch 280, Training Loss 0.0018350803602934646\n",
      "Epoch 280, Training Loss 0.0018597482965635064\n",
      "Epoch 280, Training Loss 0.0019157289436844457\n",
      "Epoch 280, Training Loss 0.0019389310282419251\n",
      "Epoch 280, Training Loss 0.0020013423457913232\n",
      "Epoch 280, Training Loss 0.0020310917233243167\n",
      "Epoch 280, Training Loss 0.0021094966077190988\n",
      "Epoch 280, Training Loss 0.002170462317316962\n",
      "Epoch 280, Training Loss 0.0021943781617790687\n",
      "Epoch 280, Training Loss 0.0022779425684734227\n",
      "Epoch 280, Training Loss 0.0023116655683483155\n",
      "Epoch 280, Training Loss 0.0023471108766372705\n",
      "Epoch 280, Training Loss 0.002422883795798206\n",
      "Epoch 280, Training Loss 0.002456032041022006\n",
      "Epoch 280, Training Loss 0.002488196664311163\n",
      "Epoch 280, Training Loss 0.002555887085025954\n",
      "Epoch 280, Training Loss 0.0026032114627740116\n",
      "Epoch 280, Training Loss 0.002638066281585971\n",
      "Epoch 280, Training Loss 0.002672459350189056\n",
      "Epoch 280, Training Loss 0.002708165740827694\n",
      "Epoch 280, Training Loss 0.002763842438559627\n",
      "Epoch 280, Training Loss 0.0028624689656659924\n",
      "Epoch 280, Training Loss 0.002919406173965129\n",
      "Epoch 280, Training Loss 0.002982499048142406\n",
      "Epoch 280, Training Loss 0.003097331588444731\n",
      "Epoch 280, Training Loss 0.003192426330741028\n",
      "Epoch 280, Training Loss 0.0032973183959703464\n",
      "Epoch 280, Training Loss 0.003341829355405953\n",
      "Epoch 280, Training Loss 0.0034250291219205045\n",
      "Epoch 280, Training Loss 0.003484232950231532\n",
      "Epoch 280, Training Loss 0.003542429306651549\n",
      "Epoch 280, Training Loss 0.003705083217490893\n",
      "Epoch 280, Training Loss 0.0037877748963301597\n",
      "Epoch 280, Training Loss 0.003946593511239876\n",
      "Epoch 280, Training Loss 0.004107464313307001\n",
      "Epoch 280, Training Loss 0.00417935149744153\n",
      "Epoch 280, Training Loss 0.004188286719958076\n",
      "Epoch 280, Training Loss 0.0043044300860656266\n",
      "Epoch 280, Training Loss 0.0044357820689116065\n",
      "Epoch 280, Training Loss 0.0044728332750565945\n",
      "Epoch 280, Training Loss 0.004579770438911398\n",
      "Epoch 280, Training Loss 0.00462018019195808\n",
      "Epoch 280, Training Loss 0.004652038974630291\n",
      "Epoch 280, Training Loss 0.0046800568067442505\n",
      "Epoch 280, Training Loss 0.004760399399458638\n",
      "Epoch 280, Training Loss 0.00479914222324691\n",
      "Epoch 280, Training Loss 0.004818486903086686\n",
      "Epoch 280, Training Loss 0.0049675330620906925\n",
      "Epoch 280, Training Loss 0.0050808338506761795\n",
      "Epoch 280, Training Loss 0.005143915815755268\n",
      "Epoch 280, Training Loss 0.0051853278719717665\n",
      "Epoch 280, Training Loss 0.0052382869751947695\n",
      "Epoch 280, Training Loss 0.00529794096339332\n",
      "Epoch 280, Training Loss 0.005400790359534304\n",
      "Epoch 280, Training Loss 0.005517154975491755\n",
      "Epoch 280, Training Loss 0.005542790110501678\n",
      "Epoch 280, Training Loss 0.005608019391027138\n",
      "Epoch 280, Training Loss 0.005646228833752863\n",
      "Epoch 280, Training Loss 0.005675247832870735\n",
      "Epoch 280, Training Loss 0.0057202648744225275\n",
      "Epoch 280, Training Loss 0.0057707721268629554\n",
      "Epoch 280, Training Loss 0.005834174223953996\n",
      "Epoch 280, Training Loss 0.005883833167769605\n",
      "Epoch 280, Training Loss 0.005975688641528835\n",
      "Epoch 280, Training Loss 0.006020566804067749\n",
      "Epoch 280, Training Loss 0.006043583148128122\n",
      "Epoch 280, Training Loss 0.006069911237153442\n",
      "Epoch 280, Training Loss 0.0061048879448915155\n",
      "Epoch 280, Training Loss 0.0061302239657677424\n",
      "Epoch 280, Training Loss 0.0061509931332591325\n",
      "Epoch 280, Training Loss 0.006174205560022798\n",
      "Epoch 280, Training Loss 0.006188902351886148\n",
      "Epoch 280, Training Loss 0.006224925423164844\n",
      "Epoch 280, Training Loss 0.006354588353672944\n",
      "Epoch 280, Training Loss 0.00645965587912256\n",
      "Epoch 280, Training Loss 0.006525221869678181\n",
      "Epoch 280, Training Loss 0.00658445111285333\n",
      "Epoch 280, Training Loss 0.006617037260838215\n",
      "Epoch 280, Training Loss 0.006676551107617328\n",
      "Epoch 280, Training Loss 0.006707075054940703\n",
      "Epoch 280, Training Loss 0.0068633380825952875\n",
      "Epoch 280, Training Loss 0.006898233424657789\n",
      "Epoch 280, Training Loss 0.006991831524430982\n",
      "Epoch 280, Training Loss 0.007012859466211761\n",
      "Epoch 280, Training Loss 0.007115590710626425\n",
      "Epoch 280, Training Loss 0.007331387974116999\n",
      "Epoch 280, Training Loss 0.007342539008949761\n",
      "Epoch 280, Training Loss 0.007363426099505152\n",
      "Epoch 280, Training Loss 0.007417245430376409\n",
      "Epoch 280, Training Loss 0.007433019637408883\n",
      "Epoch 280, Training Loss 0.00747646255802144\n",
      "Epoch 280, Training Loss 0.007508670550930645\n",
      "Epoch 280, Training Loss 0.007546594503985913\n",
      "Epoch 280, Training Loss 0.007610091419719026\n",
      "Epoch 280, Training Loss 0.00778300954383867\n",
      "Epoch 280, Training Loss 0.00783086994893925\n",
      "Epoch 280, Training Loss 0.007966535779840463\n",
      "Epoch 280, Training Loss 0.008001992085357875\n",
      "Epoch 280, Training Loss 0.00804053729249979\n",
      "Epoch 280, Training Loss 0.008157819234992347\n",
      "Epoch 280, Training Loss 0.008177008965145558\n",
      "Epoch 280, Training Loss 0.008231865665506181\n",
      "Epoch 280, Training Loss 0.008268509864630869\n",
      "Epoch 280, Training Loss 0.008328085612205554\n",
      "Epoch 280, Training Loss 0.00837155836372329\n",
      "Epoch 280, Training Loss 0.008431998064236529\n",
      "Epoch 280, Training Loss 0.008484567526866065\n",
      "Epoch 280, Training Loss 0.008523507872024728\n",
      "Epoch 280, Training Loss 0.008585622514505177\n",
      "Epoch 280, Training Loss 0.008652105879233887\n",
      "Epoch 280, Training Loss 0.008676878202234959\n",
      "Epoch 280, Training Loss 0.008709638667128542\n",
      "Epoch 280, Training Loss 0.008742634844763772\n",
      "Epoch 280, Training Loss 0.008761725761830007\n",
      "Epoch 280, Training Loss 0.008799730568452527\n",
      "Epoch 280, Training Loss 0.008821607926441237\n",
      "Epoch 280, Training Loss 0.008859987615409981\n",
      "Epoch 280, Training Loss 0.00890231406366181\n",
      "Epoch 280, Training Loss 0.009116310306498423\n",
      "Epoch 280, Training Loss 0.009163921150495595\n",
      "Epoch 280, Training Loss 0.009203464540121766\n",
      "Epoch 280, Training Loss 0.009231078206821133\n",
      "Epoch 280, Training Loss 0.009311168766020776\n",
      "Epoch 280, Training Loss 0.009335811578018395\n",
      "Epoch 280, Training Loss 0.00934960137602523\n",
      "Epoch 280, Training Loss 0.009486779655732424\n",
      "Epoch 280, Training Loss 0.009553873124877777\n",
      "Epoch 280, Training Loss 0.009602443016755878\n",
      "Epoch 280, Training Loss 0.00967880690891934\n",
      "Epoch 280, Training Loss 0.009736635180933357\n",
      "Epoch 280, Training Loss 0.009812138783519187\n",
      "Epoch 280, Training Loss 0.009831171135639633\n",
      "Epoch 280, Training Loss 0.009874770547027517\n",
      "Epoch 280, Training Loss 0.009962365022310248\n",
      "Epoch 280, Training Loss 0.009975191016974467\n",
      "Epoch 280, Training Loss 0.010064897598470074\n",
      "Epoch 280, Training Loss 0.01013610011998021\n",
      "Epoch 280, Training Loss 0.010228483308263866\n",
      "Epoch 280, Training Loss 0.01033135371275551\n",
      "Epoch 280, Training Loss 0.010350580439757546\n",
      "Epoch 280, Training Loss 0.010365799211723077\n",
      "Epoch 280, Training Loss 0.01044946340093737\n",
      "Epoch 280, Training Loss 0.010498799902298833\n",
      "Epoch 280, Training Loss 0.010579124335413966\n",
      "Epoch 280, Training Loss 0.010667651819655925\n",
      "Epoch 280, Training Loss 0.010764697651781351\n",
      "Epoch 280, Training Loss 0.010860485745870205\n",
      "Epoch 280, Training Loss 0.010915000243541187\n",
      "Epoch 280, Training Loss 0.010974587576673425\n",
      "Epoch 280, Training Loss 0.010995598888034216\n",
      "Epoch 280, Training Loss 0.011067919091909857\n",
      "Epoch 280, Training Loss 0.011097492797233526\n",
      "Epoch 280, Training Loss 0.011137321697967247\n",
      "Epoch 280, Training Loss 0.011165012073729311\n",
      "Epoch 280, Training Loss 0.011250548569075858\n",
      "Epoch 280, Training Loss 0.011280579556165564\n",
      "Epoch 280, Training Loss 0.011360463352226045\n",
      "Epoch 280, Training Loss 0.011376710712809658\n",
      "Epoch 280, Training Loss 0.011560140748667862\n",
      "Epoch 280, Training Loss 0.0117238628555356\n",
      "Epoch 280, Training Loss 0.011760495652508971\n",
      "Epoch 280, Training Loss 0.011772785373294101\n",
      "Epoch 280, Training Loss 0.011800602354142634\n",
      "Epoch 280, Training Loss 0.011844465327437234\n",
      "Epoch 280, Training Loss 0.011904352241555406\n",
      "Epoch 280, Training Loss 0.011960120137442675\n",
      "Epoch 280, Training Loss 0.012033348669038366\n",
      "Epoch 280, Training Loss 0.012108748649482798\n",
      "Epoch 280, Training Loss 0.012126717146586084\n",
      "Epoch 280, Training Loss 0.012200163588609042\n",
      "Epoch 280, Training Loss 0.012286331906528843\n",
      "Epoch 280, Training Loss 0.012375826956323156\n",
      "Epoch 280, Training Loss 0.012436392659302372\n",
      "Epoch 280, Training Loss 0.012459144050426915\n",
      "Epoch 280, Training Loss 0.012490148953212153\n",
      "Epoch 280, Training Loss 0.012514956059086773\n",
      "Epoch 280, Training Loss 0.012531737317605054\n",
      "Epoch 280, Training Loss 0.012579088420261774\n",
      "Epoch 280, Training Loss 0.012637551201631307\n",
      "Epoch 280, Training Loss 0.012697225159672481\n",
      "Epoch 280, Training Loss 0.012754785391809347\n",
      "Epoch 280, Training Loss 0.012816429323376254\n",
      "Epoch 280, Training Loss 0.01285584510513641\n",
      "Epoch 280, Training Loss 0.012899466925431662\n",
      "Epoch 280, Training Loss 0.012920076734341129\n",
      "Epoch 280, Training Loss 0.012993173389707494\n",
      "Epoch 280, Training Loss 0.013023479445658796\n",
      "Epoch 280, Training Loss 0.013058293255312783\n",
      "Epoch 280, Training Loss 0.013086199562143906\n",
      "Epoch 280, Training Loss 0.013189547994743337\n",
      "Epoch 280, Training Loss 0.013223137982223002\n",
      "Epoch 280, Training Loss 0.013253192080880332\n",
      "Epoch 280, Training Loss 0.013297464574098854\n",
      "Epoch 280, Training Loss 0.013340758306838934\n",
      "Epoch 280, Training Loss 0.013425878436683351\n",
      "Epoch 280, Training Loss 0.01349209069841258\n",
      "Epoch 280, Training Loss 0.013583446992799411\n",
      "Epoch 280, Training Loss 0.01360214671984677\n",
      "Epoch 280, Training Loss 0.013701052920025823\n",
      "Epoch 280, Training Loss 0.013707489318921781\n",
      "Epoch 280, Training Loss 0.013752742120853204\n",
      "Epoch 280, Training Loss 0.013827470034156042\n",
      "Epoch 280, Training Loss 0.013897012014661337\n",
      "Epoch 280, Training Loss 0.01395955506552611\n",
      "Epoch 280, Training Loss 0.013999298941987136\n",
      "Epoch 280, Training Loss 0.014075733881438976\n",
      "Epoch 280, Training Loss 0.014112698731829633\n",
      "Epoch 280, Training Loss 0.014198171154088567\n",
      "Epoch 280, Training Loss 0.014240004568267852\n",
      "Epoch 280, Training Loss 0.014275336660125561\n",
      "Epoch 280, Training Loss 0.014316630983829993\n",
      "Epoch 280, Training Loss 0.014323114257310624\n",
      "Epoch 280, Training Loss 0.014397547382008656\n",
      "Epoch 280, Training Loss 0.01447625525350518\n",
      "Epoch 280, Training Loss 0.014542613997269431\n",
      "Epoch 280, Training Loss 0.014557127869995239\n",
      "Epoch 280, Training Loss 0.014618979193522688\n",
      "Epoch 280, Training Loss 0.014688762460413682\n",
      "Epoch 280, Training Loss 0.014782472881024031\n",
      "Epoch 280, Training Loss 0.014872161093313256\n",
      "Epoch 280, Training Loss 0.014961835128538634\n",
      "Epoch 280, Training Loss 0.015001107226995289\n",
      "Epoch 280, Training Loss 0.015067745948715322\n",
      "Epoch 280, Training Loss 0.015099876315530647\n",
      "Epoch 280, Training Loss 0.015142476534509026\n",
      "Epoch 280, Training Loss 0.015156032827794742\n",
      "Epoch 280, Training Loss 0.015173278385154008\n",
      "Epoch 280, Training Loss 0.015249370602662187\n",
      "Epoch 280, Training Loss 0.015332716246209372\n",
      "Epoch 280, Training Loss 0.015366235465082862\n",
      "Epoch 280, Training Loss 0.015399467272093266\n",
      "Epoch 280, Training Loss 0.015426597534738424\n",
      "Epoch 280, Training Loss 0.015443562778294125\n",
      "Epoch 280, Training Loss 0.015476433870494556\n",
      "Epoch 280, Training Loss 0.015489301105360965\n",
      "Epoch 280, Training Loss 0.015544555506661839\n",
      "Epoch 280, Training Loss 0.015603639930367584\n",
      "Epoch 280, Training Loss 0.015623287837880919\n",
      "Epoch 280, Training Loss 0.015645748896933997\n",
      "Epoch 280, Training Loss 0.015653163014822984\n",
      "Epoch 280, Training Loss 0.015685842175970372\n",
      "Epoch 280, Training Loss 0.01575259134159102\n",
      "Epoch 280, Training Loss 0.0158034310261707\n",
      "Epoch 280, Training Loss 0.015931051840191075\n",
      "Epoch 280, Training Loss 0.015989352195092555\n",
      "Epoch 280, Training Loss 0.01611362087666569\n",
      "Epoch 280, Training Loss 0.016258083480407897\n",
      "Epoch 280, Training Loss 0.016319672847428666\n",
      "Epoch 280, Training Loss 0.01637267586334473\n",
      "Epoch 280, Training Loss 0.01640180951517904\n",
      "Epoch 280, Training Loss 0.01647092229650949\n",
      "Epoch 280, Training Loss 0.0166210306110456\n",
      "Epoch 280, Training Loss 0.016695993555866925\n",
      "Epoch 280, Training Loss 0.016796099099442553\n",
      "Epoch 280, Training Loss 0.016875215533339535\n",
      "Epoch 280, Training Loss 0.01694275881937893\n",
      "Epoch 280, Training Loss 0.017069759472604373\n",
      "Epoch 280, Training Loss 0.017145351534161496\n",
      "Epoch 280, Training Loss 0.017226083367310293\n",
      "Epoch 280, Training Loss 0.017288327489829506\n",
      "Epoch 280, Training Loss 0.017311265328875208\n",
      "Epoch 280, Training Loss 0.017432721530604163\n",
      "Epoch 280, Training Loss 0.017514988593042582\n",
      "Epoch 280, Training Loss 0.01758060159276018\n",
      "Epoch 280, Training Loss 0.017589980828077972\n",
      "Epoch 280, Training Loss 0.017641327114027862\n",
      "Epoch 280, Training Loss 0.017758191890700166\n",
      "Epoch 280, Training Loss 0.01779019864171248\n",
      "Epoch 280, Training Loss 0.01781415803681897\n",
      "Epoch 280, Training Loss 0.017851318224378483\n",
      "Epoch 280, Training Loss 0.017871781958557684\n",
      "Epoch 280, Training Loss 0.017905610748102218\n",
      "Epoch 280, Training Loss 0.017967784552194197\n",
      "Epoch 280, Training Loss 0.018008507358248504\n",
      "Epoch 280, Training Loss 0.01805692966289037\n",
      "Epoch 280, Training Loss 0.018116125364161437\n",
      "Epoch 280, Training Loss 0.01817712144297369\n",
      "Epoch 280, Training Loss 0.018237819479506874\n",
      "Epoch 280, Training Loss 0.0183056944021312\n",
      "Epoch 280, Training Loss 0.018326815776288738\n",
      "Epoch 280, Training Loss 0.018364896760810445\n",
      "Epoch 280, Training Loss 0.018501204265819866\n",
      "Epoch 280, Training Loss 0.018589759370564576\n",
      "Epoch 280, Training Loss 0.018601818654514716\n",
      "Epoch 280, Training Loss 0.01862350071344496\n",
      "Epoch 280, Training Loss 0.018659141113567153\n",
      "Epoch 280, Training Loss 0.018767455034196148\n",
      "Epoch 280, Training Loss 0.018796987482644332\n",
      "Epoch 280, Training Loss 0.018867393063567103\n",
      "Epoch 280, Training Loss 0.018894389325328876\n",
      "Epoch 280, Training Loss 0.018948048316275754\n",
      "Epoch 280, Training Loss 0.019035472952620223\n",
      "Epoch 280, Training Loss 0.0190873255922228\n",
      "Epoch 280, Training Loss 0.019176718144132125\n",
      "Epoch 280, Training Loss 0.019235114699177196\n",
      "Epoch 280, Training Loss 0.01932801126295229\n",
      "Epoch 280, Training Loss 0.019387718653330183\n",
      "Epoch 280, Training Loss 0.01945030700791713\n",
      "Epoch 280, Training Loss 0.01951256916617684\n",
      "Epoch 280, Training Loss 0.01952923513958445\n",
      "Epoch 280, Training Loss 0.01954985907077408\n",
      "Epoch 280, Training Loss 0.019600096439747403\n",
      "Epoch 280, Training Loss 0.01962610614269286\n",
      "Epoch 280, Training Loss 0.019688933520861294\n",
      "Epoch 280, Training Loss 0.01973266169295439\n",
      "Epoch 280, Training Loss 0.01974590734490539\n",
      "Epoch 280, Training Loss 0.019776449004030975\n",
      "Epoch 280, Training Loss 0.019789072455090408\n",
      "Epoch 280, Training Loss 0.01982328140050592\n",
      "Epoch 280, Training Loss 0.01994235018658859\n",
      "Epoch 280, Training Loss 0.020006767787334637\n",
      "Epoch 280, Training Loss 0.0201112335307233\n",
      "Epoch 280, Training Loss 0.020124225028078344\n",
      "Epoch 280, Training Loss 0.02014967790254585\n",
      "Epoch 280, Training Loss 0.020174791122955817\n",
      "Epoch 280, Training Loss 0.020216240384437315\n",
      "Epoch 280, Training Loss 0.020301636395370946\n",
      "Epoch 280, Training Loss 0.020334130604429852\n",
      "Epoch 280, Training Loss 0.020391761745704944\n",
      "Epoch 280, Training Loss 0.02042147153369187\n",
      "Epoch 280, Training Loss 0.02046837317673942\n",
      "Epoch 280, Training Loss 0.020499312748794286\n",
      "Epoch 280, Training Loss 0.020548799528938046\n",
      "Epoch 280, Training Loss 0.020668953060484527\n",
      "Epoch 280, Training Loss 0.020706000872423202\n",
      "Epoch 280, Training Loss 0.020761001134610466\n",
      "Epoch 280, Training Loss 0.020824344694147557\n",
      "Epoch 280, Training Loss 0.02088680734638782\n",
      "Epoch 280, Training Loss 0.020908971242082623\n",
      "Epoch 280, Training Loss 0.020959472987572173\n",
      "Epoch 280, Training Loss 0.020976451931811888\n",
      "Epoch 280, Training Loss 0.02101962898960313\n",
      "Epoch 280, Training Loss 0.021050334778254676\n",
      "Epoch 280, Training Loss 0.021070829858226926\n",
      "Epoch 280, Training Loss 0.021140418566830094\n",
      "Epoch 280, Training Loss 0.021196740473408606\n",
      "Epoch 280, Training Loss 0.021265083172446703\n",
      "Epoch 280, Training Loss 0.021275435475503927\n",
      "Epoch 280, Training Loss 0.021360696651889464\n",
      "Epoch 280, Training Loss 0.02143856016394999\n",
      "Epoch 280, Training Loss 0.02146691249807358\n",
      "Epoch 280, Training Loss 0.021522079483674995\n",
      "Epoch 280, Training Loss 0.021545067213980668\n",
      "Epoch 280, Training Loss 0.021579851904917326\n",
      "Epoch 280, Training Loss 0.02165643344073535\n",
      "Epoch 280, Training Loss 0.02168758760523194\n",
      "Epoch 280, Training Loss 0.021777317519811795\n",
      "Epoch 280, Training Loss 0.021866819620027643\n",
      "Epoch 280, Training Loss 0.02189118577681882\n",
      "Epoch 280, Training Loss 0.021911585035369448\n",
      "Epoch 280, Training Loss 0.021973773783496808\n",
      "Epoch 280, Training Loss 0.02201508759113643\n",
      "Epoch 280, Training Loss 0.022194006824461016\n",
      "Epoch 280, Training Loss 0.022309954150739456\n",
      "Epoch 280, Training Loss 0.022323494506737842\n",
      "Epoch 280, Training Loss 0.02236452271037585\n",
      "Epoch 280, Training Loss 0.022425082915931788\n",
      "Epoch 280, Training Loss 0.022438502027307784\n",
      "Epoch 280, Training Loss 0.0224945179455916\n",
      "Epoch 280, Training Loss 0.022540484112032386\n",
      "Epoch 280, Training Loss 0.02262019992703596\n",
      "Epoch 280, Training Loss 0.022696567920829785\n",
      "Epoch 280, Training Loss 0.022706826381823596\n",
      "Epoch 280, Training Loss 0.02274786315077101\n",
      "Epoch 280, Training Loss 0.02281816117465496\n",
      "Epoch 280, Training Loss 0.022879855712051585\n",
      "Epoch 280, Training Loss 0.022905701171139926\n",
      "Epoch 280, Training Loss 0.02305116543970297\n",
      "Epoch 280, Training Loss 0.023101419743979373\n",
      "Epoch 280, Training Loss 0.02319895169314216\n",
      "Epoch 280, Training Loss 0.02324026084655081\n",
      "Epoch 280, Training Loss 0.023323438051716446\n",
      "Epoch 280, Training Loss 0.02334621550796358\n",
      "Epoch 280, Training Loss 0.023358967660180748\n",
      "Epoch 280, Training Loss 0.023405157849002066\n",
      "Epoch 280, Training Loss 0.023435631035672276\n",
      "Epoch 280, Training Loss 0.02345824600828578\n",
      "Epoch 280, Training Loss 0.02349349255899868\n",
      "Epoch 280, Training Loss 0.02351076277139623\n",
      "Epoch 280, Training Loss 0.02358388439382967\n",
      "Epoch 280, Training Loss 0.0237016203548864\n",
      "Epoch 280, Training Loss 0.02377662302025825\n",
      "Epoch 280, Training Loss 0.023947674307086126\n",
      "Epoch 280, Training Loss 0.024053657453631045\n",
      "Epoch 280, Training Loss 0.02409686236058736\n",
      "Epoch 280, Training Loss 0.024135321407290675\n",
      "Epoch 280, Training Loss 0.024146454774386362\n",
      "Epoch 280, Training Loss 0.02420045398628277\n",
      "Epoch 280, Training Loss 0.024284468362312715\n",
      "Epoch 280, Training Loss 0.024360344512983585\n",
      "Epoch 280, Training Loss 0.0245042927408367\n",
      "Epoch 280, Training Loss 0.024541010169069403\n",
      "Epoch 280, Training Loss 0.02458301410817391\n",
      "Epoch 280, Training Loss 0.024605826340148896\n",
      "Epoch 280, Training Loss 0.024654452887880603\n",
      "Epoch 280, Training Loss 0.02472765161239011\n",
      "Epoch 280, Training Loss 0.02474654702317265\n",
      "Epoch 280, Training Loss 0.024825067358220095\n",
      "Epoch 280, Training Loss 0.024877453924340133\n",
      "Epoch 280, Training Loss 0.024929600080613363\n",
      "Epoch 280, Training Loss 0.024959353240602233\n",
      "Epoch 280, Training Loss 0.02499649114311313\n",
      "Epoch 280, Training Loss 0.02501059777777442\n",
      "Epoch 280, Training Loss 0.02504951760287175\n",
      "Epoch 280, Training Loss 0.02506426574729021\n",
      "Epoch 280, Training Loss 0.02509998842059156\n",
      "Epoch 280, Training Loss 0.025114829282817025\n",
      "Epoch 280, Training Loss 0.025137390576950882\n",
      "Epoch 280, Training Loss 0.02517782727880475\n",
      "Epoch 280, Training Loss 0.025326390026608848\n",
      "Epoch 280, Training Loss 0.025357595425756538\n",
      "Epoch 280, Training Loss 0.02538806391770349\n",
      "Epoch 280, Training Loss 0.025475708384285953\n",
      "Epoch 280, Training Loss 0.025488500323746822\n",
      "Epoch 280, Training Loss 0.025602297833584763\n",
      "Epoch 280, Training Loss 0.025800822802897916\n",
      "Epoch 280, Training Loss 0.025911383178380445\n",
      "Epoch 280, Training Loss 0.025928447165948047\n",
      "Epoch 280, Training Loss 0.025983290270428217\n",
      "Epoch 280, Training Loss 0.026013327529058432\n",
      "Epoch 280, Training Loss 0.026034853397332646\n",
      "Epoch 280, Training Loss 0.02605682895387835\n",
      "Epoch 280, Training Loss 0.026086883974330656\n",
      "Epoch 280, Training Loss 0.026146224795190423\n",
      "Epoch 280, Training Loss 0.026233474085645754\n",
      "Epoch 280, Training Loss 0.026276367020976667\n",
      "Epoch 280, Training Loss 0.0262954304604541\n",
      "Epoch 280, Training Loss 0.02632351784402376\n",
      "Epoch 280, Training Loss 0.02635734074551355\n",
      "Epoch 280, Training Loss 0.026390076067079517\n",
      "Epoch 280, Training Loss 0.026417511920242207\n",
      "Epoch 280, Training Loss 0.026465256779414158\n",
      "Epoch 280, Training Loss 0.026526852334608966\n",
      "Epoch 280, Training Loss 0.02656234020267225\n",
      "Epoch 280, Training Loss 0.026616871697575692\n",
      "Epoch 280, Training Loss 0.026680411887652886\n",
      "Epoch 280, Training Loss 0.026731029050448513\n",
      "Epoch 280, Training Loss 0.026795208980531796\n",
      "Epoch 280, Training Loss 0.026902359400582894\n",
      "Epoch 280, Training Loss 0.0269721907199077\n",
      "Epoch 280, Training Loss 0.02700044994797472\n",
      "Epoch 280, Training Loss 0.027021026124467936\n",
      "Epoch 280, Training Loss 0.027089115961090378\n",
      "Epoch 280, Training Loss 0.02710914941947631\n",
      "Epoch 280, Training Loss 0.027212486361794155\n",
      "Epoch 280, Training Loss 0.02723235777124305\n",
      "Epoch 280, Training Loss 0.027277722141331496\n",
      "Epoch 280, Training Loss 0.02729987446576014\n",
      "Epoch 280, Training Loss 0.02737813678872593\n",
      "Epoch 280, Training Loss 0.027408366204689606\n",
      "Epoch 280, Training Loss 0.027459099280226336\n",
      "Epoch 280, Training Loss 0.027502335242384\n",
      "Epoch 280, Training Loss 0.027681986347574482\n",
      "Epoch 280, Training Loss 0.02772302302124593\n",
      "Epoch 280, Training Loss 0.027745623228466496\n",
      "Epoch 280, Training Loss 0.027813811165988064\n",
      "Epoch 280, Training Loss 0.027846457552202903\n",
      "Epoch 280, Training Loss 0.027975638557816176\n",
      "Epoch 280, Training Loss 0.02805859678069992\n",
      "Epoch 280, Training Loss 0.028205285021496933\n",
      "Epoch 280, Training Loss 0.028255490144319318\n",
      "Epoch 280, Training Loss 0.02833555256852599\n",
      "Epoch 280, Training Loss 0.028371512000222723\n",
      "Epoch 280, Training Loss 0.028461272946184935\n",
      "Epoch 280, Training Loss 0.028481261415259383\n",
      "Epoch 280, Training Loss 0.028617144808830583\n",
      "Epoch 280, Training Loss 0.028674012279885884\n",
      "Epoch 280, Training Loss 0.02883216205750928\n",
      "Epoch 280, Training Loss 0.028968037304985324\n",
      "Epoch 280, Training Loss 0.02900429238991626\n",
      "Epoch 280, Training Loss 0.029102274041641934\n",
      "Epoch 280, Training Loss 0.029150488373139858\n",
      "Epoch 280, Training Loss 0.02928706093588868\n",
      "Epoch 280, Training Loss 0.029311967314556812\n",
      "Epoch 280, Training Loss 0.029352076710118434\n",
      "Epoch 280, Training Loss 0.02937146114981007\n",
      "Epoch 280, Training Loss 0.029430013571572884\n",
      "Epoch 280, Training Loss 0.02945683087648638\n",
      "Epoch 280, Training Loss 0.029480394078394794\n",
      "Epoch 280, Training Loss 0.029487845417031127\n",
      "Epoch 280, Training Loss 0.029686451904461397\n",
      "Epoch 280, Training Loss 0.029763286474191814\n",
      "Epoch 280, Training Loss 0.029817369176417856\n",
      "Epoch 280, Training Loss 0.029860307996654335\n",
      "Epoch 280, Training Loss 0.029886682485075443\n",
      "Epoch 280, Training Loss 0.029938522367226078\n",
      "Epoch 280, Training Loss 0.029964725161209474\n",
      "Epoch 280, Training Loss 0.030031587743340894\n",
      "Epoch 280, Training Loss 0.030065092380227677\n",
      "Epoch 280, Training Loss 0.03008485629813998\n",
      "Epoch 280, Training Loss 0.03012367575477018\n",
      "Epoch 280, Training Loss 0.030130405857315874\n",
      "Epoch 280, Training Loss 0.030175779664488818\n",
      "Epoch 280, Training Loss 0.030216453884563902\n",
      "Epoch 280, Training Loss 0.030229517746397563\n",
      "Epoch 280, Training Loss 0.030248792209040823\n",
      "Epoch 280, Training Loss 0.030383724958071356\n",
      "Epoch 280, Training Loss 0.030498105290052875\n",
      "Epoch 280, Training Loss 0.03066477228177573\n",
      "Epoch 280, Training Loss 0.030727986351746464\n",
      "Epoch 280, Training Loss 0.030835012817764868\n",
      "Epoch 280, Training Loss 0.030895858794055365\n",
      "Epoch 280, Training Loss 0.030985161500728078\n",
      "Epoch 280, Training Loss 0.03104196013013363\n",
      "Epoch 280, Training Loss 0.031111352055755152\n",
      "Epoch 280, Training Loss 0.031173450968173498\n",
      "Epoch 280, Training Loss 0.031200104088777356\n",
      "Epoch 280, Training Loss 0.03127469687694041\n",
      "Epoch 280, Training Loss 0.03129521680964857\n",
      "Epoch 280, Training Loss 0.0313401776526714\n",
      "Epoch 280, Training Loss 0.031414057981446766\n",
      "Epoch 280, Training Loss 0.03154938827897601\n",
      "Epoch 280, Training Loss 0.03158752397810349\n",
      "Epoch 280, Training Loss 0.03163700092576749\n",
      "Epoch 280, Training Loss 0.03168584476939648\n",
      "Epoch 280, Training Loss 0.03174881846763079\n",
      "Epoch 280, Training Loss 0.03179210793677136\n",
      "Epoch 280, Training Loss 0.03181747097076605\n",
      "Epoch 280, Training Loss 0.031913497407213234\n",
      "Epoch 280, Training Loss 0.03194014457306917\n",
      "Epoch 280, Training Loss 0.03200473059790537\n",
      "Epoch 280, Training Loss 0.03206054507421277\n",
      "Epoch 280, Training Loss 0.03217426657288447\n",
      "Epoch 280, Training Loss 0.03234061265609625\n",
      "Epoch 280, Training Loss 0.03241449021710478\n",
      "Epoch 280, Training Loss 0.03247617636545254\n",
      "Epoch 280, Training Loss 0.03257273232666752\n",
      "Epoch 280, Training Loss 0.032612292842148706\n",
      "Epoch 280, Training Loss 0.032626646033147605\n",
      "Epoch 280, Training Loss 0.03266179829638194\n",
      "Epoch 280, Training Loss 0.032710726964204094\n",
      "Epoch 280, Training Loss 0.0327188965121327\n",
      "Epoch 280, Training Loss 0.0327538845862936\n",
      "Epoch 280, Training Loss 0.03282016270277102\n",
      "Epoch 280, Training Loss 0.032838381081343154\n",
      "Epoch 280, Training Loss 0.03285350546936321\n",
      "Epoch 280, Training Loss 0.03288418139138109\n",
      "Epoch 280, Training Loss 0.032951213683828216\n",
      "Epoch 280, Training Loss 0.03295931037065223\n",
      "Epoch 280, Training Loss 0.033010473136035036\n",
      "Epoch 280, Training Loss 0.033107440565567456\n",
      "Epoch 280, Training Loss 0.033180451858431444\n",
      "Epoch 280, Training Loss 0.03325916322179215\n",
      "Epoch 280, Training Loss 0.03327481734895093\n",
      "Epoch 280, Training Loss 0.03331932081969555\n",
      "Epoch 280, Training Loss 0.03342500150315654\n",
      "Epoch 280, Training Loss 0.033514893405935\n",
      "Epoch 280, Training Loss 0.03357440011178041\n",
      "Epoch 280, Training Loss 0.0336912637927791\n",
      "Epoch 280, Training Loss 0.03377591892826797\n",
      "Epoch 280, Training Loss 0.0338070120137242\n",
      "Epoch 280, Training Loss 0.03384522875458898\n",
      "Epoch 280, Training Loss 0.033898047754502926\n",
      "Epoch 280, Training Loss 0.03399343923379755\n",
      "Epoch 280, Training Loss 0.03403680336118564\n",
      "Epoch 280, Training Loss 0.034154321854848346\n",
      "Epoch 280, Training Loss 0.03420438471159724\n",
      "Epoch 280, Training Loss 0.03423685447998878\n",
      "Epoch 280, Training Loss 0.03433888263540709\n",
      "Epoch 280, Training Loss 0.03441881404563194\n",
      "Epoch 280, Training Loss 0.03447898441527868\n",
      "Epoch 280, Training Loss 0.034558105298563305\n",
      "Epoch 280, Training Loss 0.03457797351507637\n",
      "Epoch 280, Training Loss 0.034600879555768176\n",
      "Epoch 280, Training Loss 0.034648344999112554\n",
      "Epoch 280, Training Loss 0.03473960387382342\n",
      "Epoch 280, Training Loss 0.03479865365995147\n",
      "Epoch 280, Training Loss 0.034907316452587774\n",
      "Epoch 280, Training Loss 0.03496113716972434\n",
      "Epoch 280, Training Loss 0.03501031479841132\n",
      "Epoch 280, Training Loss 0.035099304523533374\n",
      "Epoch 280, Training Loss 0.03515011573965421\n",
      "Epoch 280, Training Loss 0.03520193458010759\n",
      "Epoch 280, Training Loss 0.03524326576608355\n",
      "Epoch 280, Training Loss 0.035323771175897446\n",
      "Epoch 280, Training Loss 0.03543422576647414\n",
      "Epoch 280, Training Loss 0.03551951437281049\n",
      "Epoch 280, Training Loss 0.035554334716013894\n",
      "Epoch 280, Training Loss 0.03564428298405426\n",
      "Epoch 280, Training Loss 0.035661727865881586\n",
      "Epoch 280, Training Loss 0.03575017452752099\n",
      "Epoch 280, Training Loss 0.03579900722862567\n",
      "Epoch 280, Training Loss 0.03583262703271911\n",
      "Epoch 280, Training Loss 0.03589379349830644\n",
      "Epoch 280, Training Loss 0.035924344982766095\n",
      "Epoch 280, Training Loss 0.0359703047942999\n",
      "Epoch 280, Training Loss 0.03607186317250914\n",
      "Epoch 280, Training Loss 0.036095404408543426\n",
      "Epoch 280, Training Loss 0.03613528700443008\n",
      "Epoch 280, Training Loss 0.03615731857848042\n",
      "Epoch 280, Training Loss 0.03619259358574267\n",
      "Epoch 280, Training Loss 0.036209349340075615\n",
      "Epoch 280, Training Loss 0.036265543238629044\n",
      "Epoch 280, Training Loss 0.03630740790808921\n",
      "Epoch 280, Training Loss 0.03633572060026495\n",
      "Epoch 280, Training Loss 0.036388085934135804\n",
      "Epoch 280, Training Loss 0.03641172927027792\n",
      "Epoch 280, Training Loss 0.036451132111179896\n",
      "Epoch 280, Training Loss 0.03656194864562653\n",
      "Epoch 280, Training Loss 0.036603573972330716\n",
      "Epoch 280, Training Loss 0.03692808307652526\n",
      "Epoch 280, Training Loss 0.03694952898920821\n",
      "Epoch 280, Training Loss 0.036986396307973644\n",
      "Epoch 280, Training Loss 0.03700381279756289\n",
      "Epoch 280, Training Loss 0.03721519779292461\n",
      "Epoch 280, Training Loss 0.03756261287826349\n",
      "Epoch 280, Training Loss 0.03760442774221206\n",
      "Epoch 280, Training Loss 0.037733792811465425\n",
      "Epoch 280, Training Loss 0.03780600352896392\n",
      "Epoch 280, Training Loss 0.037835955475111635\n",
      "Epoch 280, Training Loss 0.037917985005037444\n",
      "Epoch 280, Training Loss 0.03795653617645488\n",
      "Epoch 280, Training Loss 0.038057899753422574\n",
      "Epoch 280, Training Loss 0.03807842718434932\n",
      "Epoch 280, Training Loss 0.03811727522853333\n",
      "Epoch 280, Training Loss 0.03819480485549611\n",
      "Epoch 280, Training Loss 0.038246017745565006\n",
      "Epoch 280, Training Loss 0.03834809218604794\n",
      "Epoch 280, Training Loss 0.0384084313608887\n",
      "Epoch 280, Training Loss 0.038476753289766054\n",
      "Epoch 280, Training Loss 0.038569769139051475\n",
      "Epoch 280, Training Loss 0.03861026443562487\n",
      "Epoch 280, Training Loss 0.03881977091286131\n",
      "Epoch 280, Training Loss 0.038996191915062726\n",
      "Epoch 280, Training Loss 0.03908957473878913\n",
      "Epoch 280, Training Loss 0.039113826138060304\n",
      "Epoch 280, Training Loss 0.03912501173901379\n",
      "Epoch 280, Training Loss 0.03920793789200713\n",
      "Epoch 280, Training Loss 0.039274063295401306\n",
      "Epoch 280, Training Loss 0.03938003919144516\n",
      "Epoch 280, Training Loss 0.03943446939370459\n",
      "Epoch 280, Training Loss 0.039600670466895034\n",
      "Epoch 280, Training Loss 0.03968729832938508\n",
      "Epoch 280, Training Loss 0.03975829042439513\n",
      "Epoch 280, Training Loss 0.03990284486464165\n",
      "Epoch 280, Training Loss 0.04007769234614242\n",
      "Epoch 280, Training Loss 0.040247078456789674\n",
      "Epoch 280, Training Loss 0.04040254520602486\n",
      "Epoch 280, Training Loss 0.04043844603109257\n",
      "Epoch 280, Training Loss 0.04062604004591037\n",
      "Epoch 280, Training Loss 0.04064837605763903\n",
      "Epoch 280, Training Loss 0.040804813945275326\n",
      "Epoch 280, Training Loss 0.040859764268564636\n",
      "Epoch 280, Training Loss 0.04088976424094528\n",
      "Epoch 280, Training Loss 0.040914586638612555\n",
      "Epoch 280, Training Loss 0.0409668022987035\n",
      "Epoch 280, Training Loss 0.041058095358431225\n",
      "Epoch 280, Training Loss 0.04110143238933438\n",
      "Epoch 280, Training Loss 0.04116011804024525\n",
      "Epoch 280, Training Loss 0.04121814487690149\n",
      "Epoch 280, Training Loss 0.04124960534886726\n",
      "Epoch 280, Training Loss 0.04128003999640894\n",
      "Epoch 280, Training Loss 0.04129908979832268\n",
      "Epoch 280, Training Loss 0.04137180486391477\n",
      "Epoch 280, Training Loss 0.041377366000019455\n",
      "Epoch 280, Training Loss 0.041421177710080165\n",
      "Epoch 280, Training Loss 0.04158597898876766\n",
      "Epoch 280, Training Loss 0.0416220574654029\n",
      "Epoch 280, Training Loss 0.041708720132203586\n",
      "Epoch 280, Training Loss 0.04178444573255565\n",
      "Epoch 280, Training Loss 0.041882196206675694\n",
      "Epoch 280, Training Loss 0.0420543187383629\n",
      "Epoch 280, Training Loss 0.04235243929616745\n",
      "Epoch 280, Training Loss 0.04244934778798686\n",
      "Epoch 280, Training Loss 0.04252091669322699\n",
      "Epoch 280, Training Loss 0.042590687740975255\n",
      "Epoch 280, Training Loss 0.04261605509335313\n",
      "Epoch 280, Training Loss 0.04269050008467282\n",
      "Epoch 280, Training Loss 0.04274393200440823\n",
      "Epoch 280, Training Loss 0.04291436733211131\n",
      "Epoch 280, Training Loss 0.04294875917641822\n",
      "Epoch 280, Training Loss 0.0430077502058576\n",
      "Epoch 280, Training Loss 0.04309084069083833\n",
      "Epoch 280, Training Loss 0.04316631219971477\n",
      "Epoch 280, Training Loss 0.043281863358996024\n",
      "Epoch 280, Training Loss 0.04338591799611592\n",
      "Epoch 280, Training Loss 0.0435308664120124\n",
      "Epoch 280, Training Loss 0.04356247462127406\n",
      "Epoch 280, Training Loss 0.04367659491655962\n",
      "Epoch 280, Training Loss 0.04376032080648996\n",
      "Epoch 280, Training Loss 0.04385429926697746\n",
      "Epoch 280, Training Loss 0.04394606097965785\n",
      "Epoch 280, Training Loss 0.04399061624241798\n",
      "Epoch 280, Training Loss 0.044152145744408566\n",
      "Epoch 280, Training Loss 0.044178682523529474\n",
      "Epoch 280, Training Loss 0.04427391003288538\n",
      "Epoch 280, Training Loss 0.044326349877326005\n",
      "Epoch 280, Training Loss 0.04440691615895504\n",
      "Epoch 280, Training Loss 0.04448536598502332\n",
      "Epoch 280, Training Loss 0.04452441681933868\n",
      "Epoch 280, Training Loss 0.04454361241610954\n",
      "Epoch 280, Training Loss 0.0446144610922069\n",
      "Epoch 280, Training Loss 0.044636872967424066\n",
      "Epoch 280, Training Loss 0.044702795428245345\n",
      "Epoch 280, Training Loss 0.04480997051524423\n",
      "Epoch 280, Training Loss 0.04493934651265097\n",
      "Epoch 280, Training Loss 0.04499275889128561\n",
      "Epoch 280, Training Loss 0.04503082603220935\n",
      "Epoch 280, Training Loss 0.04512518750208304\n",
      "Epoch 280, Training Loss 0.04515712495412096\n",
      "Epoch 280, Training Loss 0.04534692961908401\n",
      "Epoch 280, Training Loss 0.04540474067949463\n",
      "Epoch 280, Training Loss 0.04544418691859945\n",
      "Epoch 280, Training Loss 0.04560058024566611\n",
      "Epoch 280, Training Loss 0.045655938993205726\n",
      "Epoch 280, Training Loss 0.04570212265085953\n",
      "Epoch 280, Training Loss 0.04573070800379681\n",
      "Epoch 280, Training Loss 0.04592106805142501\n",
      "Epoch 280, Training Loss 0.0459824893468767\n",
      "Epoch 280, Training Loss 0.04609244028963815\n",
      "Epoch 280, Training Loss 0.04614884209464235\n",
      "Epoch 280, Training Loss 0.04627992936274242\n",
      "Epoch 280, Training Loss 0.046339930117349416\n",
      "Epoch 280, Training Loss 0.046380513444152255\n",
      "Epoch 280, Training Loss 0.046436288857313296\n",
      "Epoch 280, Training Loss 0.04644879399825012\n",
      "Epoch 280, Training Loss 0.046472256260035594\n",
      "Epoch 280, Training Loss 0.046489823390455806\n",
      "Epoch 280, Training Loss 0.04654181989676812\n",
      "Epoch 280, Training Loss 0.04659999886056042\n",
      "Epoch 280, Training Loss 0.04665877448533045\n",
      "Epoch 280, Training Loss 0.04667823026409311\n",
      "Epoch 280, Training Loss 0.04678203784110372\n",
      "Epoch 280, Training Loss 0.0468120116530858\n",
      "Epoch 280, Training Loss 0.04685086739318602\n",
      "Epoch 280, Training Loss 0.04693733538260393\n",
      "Epoch 280, Training Loss 0.04696458771043574\n",
      "Epoch 280, Training Loss 0.046995098252430596\n",
      "Epoch 280, Training Loss 0.04703100989132052\n",
      "Epoch 280, Training Loss 0.04704309217846188\n",
      "Epoch 280, Training Loss 0.047059325931255544\n",
      "Epoch 280, Training Loss 0.04711227313451031\n",
      "Epoch 280, Training Loss 0.04713038782662977\n",
      "Epoch 280, Training Loss 0.04718021294125892\n",
      "Epoch 280, Training Loss 0.047332898327780655\n",
      "Epoch 280, Training Loss 0.047603380833002154\n",
      "Epoch 280, Training Loss 0.04762504343658\n",
      "Epoch 280, Training Loss 0.04767453674789127\n",
      "Epoch 280, Training Loss 0.047769534708860584\n",
      "Epoch 280, Training Loss 0.047788964923890426\n",
      "Epoch 280, Training Loss 0.0478617744889501\n",
      "Epoch 280, Training Loss 0.04793139004155689\n",
      "Epoch 280, Training Loss 0.04799131100254176\n",
      "Epoch 280, Training Loss 0.0480320028153241\n",
      "Epoch 280, Training Loss 0.048084980954685254\n",
      "Epoch 280, Training Loss 0.04811686381359425\n",
      "Epoch 280, Training Loss 0.04817988612162678\n",
      "Epoch 280, Training Loss 0.04819156393132475\n",
      "Epoch 280, Training Loss 0.048245426741383414\n",
      "Epoch 280, Training Loss 0.048289570278580995\n",
      "Epoch 280, Training Loss 0.048338885843048775\n",
      "Epoch 290, Training Loss 0.00031122010763343945\n",
      "Epoch 290, Training Loss 0.0004257336830544045\n",
      "Epoch 290, Training Loss 0.000522002930302754\n",
      "Epoch 290, Training Loss 0.0005726068163924205\n",
      "Epoch 290, Training Loss 0.0005994464373192214\n",
      "Epoch 290, Training Loss 0.0006326616853666123\n",
      "Epoch 290, Training Loss 0.0006849043800131134\n",
      "Epoch 290, Training Loss 0.0007121973668637178\n",
      "Epoch 290, Training Loss 0.0007569976841740291\n",
      "Epoch 290, Training Loss 0.0008010754714269773\n",
      "Epoch 290, Training Loss 0.0008578648304809694\n",
      "Epoch 290, Training Loss 0.000877478244521505\n",
      "Epoch 290, Training Loss 0.0009456760431056284\n",
      "Epoch 290, Training Loss 0.0010495509135315332\n",
      "Epoch 290, Training Loss 0.001059815301881422\n",
      "Epoch 290, Training Loss 0.0010952673414174247\n",
      "Epoch 290, Training Loss 0.0012859591209065273\n",
      "Epoch 290, Training Loss 0.0013996559812132355\n",
      "Epoch 290, Training Loss 0.001452921844466263\n",
      "Epoch 290, Training Loss 0.001466175086338959\n",
      "Epoch 290, Training Loss 0.00151460618971635\n",
      "Epoch 290, Training Loss 0.0015922890775991827\n",
      "Epoch 290, Training Loss 0.0016517619747678032\n",
      "Epoch 290, Training Loss 0.0016812632489196784\n",
      "Epoch 290, Training Loss 0.00175656774855407\n",
      "Epoch 290, Training Loss 0.001814863801745655\n",
      "Epoch 290, Training Loss 0.0018452555298462243\n",
      "Epoch 290, Training Loss 0.0018871040643214266\n",
      "Epoch 290, Training Loss 0.0019320515925279054\n",
      "Epoch 290, Training Loss 0.0019518891801995695\n",
      "Epoch 290, Training Loss 0.002046875974825581\n",
      "Epoch 290, Training Loss 0.00209511591650336\n",
      "Epoch 290, Training Loss 0.0021225254639716403\n",
      "Epoch 290, Training Loss 0.0021712999276416686\n",
      "Epoch 290, Training Loss 0.002205276371115614\n",
      "Epoch 290, Training Loss 0.002221453416844844\n",
      "Epoch 290, Training Loss 0.002238792155290504\n",
      "Epoch 290, Training Loss 0.0022553977654184527\n",
      "Epoch 290, Training Loss 0.0022603719979953354\n",
      "Epoch 290, Training Loss 0.0023197108037684047\n",
      "Epoch 290, Training Loss 0.0023920953617957625\n",
      "Epoch 290, Training Loss 0.0024324458722463425\n",
      "Epoch 290, Training Loss 0.002489653040233361\n",
      "Epoch 290, Training Loss 0.0025072100690192046\n",
      "Epoch 290, Training Loss 0.002538569519877472\n",
      "Epoch 290, Training Loss 0.002569390187525879\n",
      "Epoch 290, Training Loss 0.0025832587561291425\n",
      "Epoch 290, Training Loss 0.0025992166778534805\n",
      "Epoch 290, Training Loss 0.002612197238718495\n",
      "Epoch 290, Training Loss 0.0026282816083239547\n",
      "Epoch 290, Training Loss 0.00264057019889793\n",
      "Epoch 290, Training Loss 0.002730294101564285\n",
      "Epoch 290, Training Loss 0.0027748606068884853\n",
      "Epoch 290, Training Loss 0.0028273756717048263\n",
      "Epoch 290, Training Loss 0.002944548437467126\n",
      "Epoch 290, Training Loss 0.003029782726974858\n",
      "Epoch 290, Training Loss 0.003048495898652069\n",
      "Epoch 290, Training Loss 0.0030936001558237902\n",
      "Epoch 290, Training Loss 0.003203971370402009\n",
      "Epoch 290, Training Loss 0.003222903889982635\n",
      "Epoch 290, Training Loss 0.003248457471146951\n",
      "Epoch 290, Training Loss 0.0033049560479505363\n",
      "Epoch 290, Training Loss 0.003343430050598729\n",
      "Epoch 290, Training Loss 0.0033820604082539945\n",
      "Epoch 290, Training Loss 0.0034149278442392036\n",
      "Epoch 290, Training Loss 0.0034184108111326157\n",
      "Epoch 290, Training Loss 0.0034560506274361556\n",
      "Epoch 290, Training Loss 0.0035308644177315905\n",
      "Epoch 290, Training Loss 0.0035796266765502826\n",
      "Epoch 290, Training Loss 0.0037797086457114506\n",
      "Epoch 290, Training Loss 0.0038292739761259663\n",
      "Epoch 290, Training Loss 0.00391229916848671\n",
      "Epoch 290, Training Loss 0.004018103224796998\n",
      "Epoch 290, Training Loss 0.004035838106420854\n",
      "Epoch 290, Training Loss 0.0040842336956816525\n",
      "Epoch 290, Training Loss 0.004107092674809706\n",
      "Epoch 290, Training Loss 0.004173554578626914\n",
      "Epoch 290, Training Loss 0.004195814979348398\n",
      "Epoch 290, Training Loss 0.0042249152226649855\n",
      "Epoch 290, Training Loss 0.004330991510994961\n",
      "Epoch 290, Training Loss 0.004380097139633887\n",
      "Epoch 290, Training Loss 0.004438002966284809\n",
      "Epoch 290, Training Loss 0.004490857732796665\n",
      "Epoch 290, Training Loss 0.004549188123446653\n",
      "Epoch 290, Training Loss 0.004630585163867439\n",
      "Epoch 290, Training Loss 0.004677873941750535\n",
      "Epoch 290, Training Loss 0.0047473189741959005\n",
      "Epoch 290, Training Loss 0.004811607090139385\n",
      "Epoch 290, Training Loss 0.0048221376140380415\n",
      "Epoch 290, Training Loss 0.004926905220272043\n",
      "Epoch 290, Training Loss 0.0049553041919813395\n",
      "Epoch 290, Training Loss 0.004966298875379879\n",
      "Epoch 290, Training Loss 0.005027516532744116\n",
      "Epoch 290, Training Loss 0.005267613359888935\n",
      "Epoch 290, Training Loss 0.005318768984338035\n",
      "Epoch 290, Training Loss 0.005335127146107137\n",
      "Epoch 290, Training Loss 0.005354236248973519\n",
      "Epoch 290, Training Loss 0.005390668280429833\n",
      "Epoch 290, Training Loss 0.005447741376076494\n",
      "Epoch 290, Training Loss 0.005480145811355766\n",
      "Epoch 290, Training Loss 0.005530505085928494\n",
      "Epoch 290, Training Loss 0.00554745998251421\n",
      "Epoch 290, Training Loss 0.005624836510526078\n",
      "Epoch 290, Training Loss 0.005669863436959417\n",
      "Epoch 290, Training Loss 0.005803357947634443\n",
      "Epoch 290, Training Loss 0.005879856031233221\n",
      "Epoch 290, Training Loss 0.005933564884857277\n",
      "Epoch 290, Training Loss 0.005967278550069332\n",
      "Epoch 290, Training Loss 0.0059926370084357194\n",
      "Epoch 290, Training Loss 0.006027373694576552\n",
      "Epoch 290, Training Loss 0.006055340163416856\n",
      "Epoch 290, Training Loss 0.006127020784491754\n",
      "Epoch 290, Training Loss 0.006148405479984191\n",
      "Epoch 290, Training Loss 0.006181669952359784\n",
      "Epoch 290, Training Loss 0.006223733491345268\n",
      "Epoch 290, Training Loss 0.006281336068885539\n",
      "Epoch 290, Training Loss 0.006389864560519166\n",
      "Epoch 290, Training Loss 0.006408187555675598\n",
      "Epoch 290, Training Loss 0.006418832257101812\n",
      "Epoch 290, Training Loss 0.006441250141850575\n",
      "Epoch 290, Training Loss 0.006463327831850695\n",
      "Epoch 290, Training Loss 0.006508901404143046\n",
      "Epoch 290, Training Loss 0.0065819081713986176\n",
      "Epoch 290, Training Loss 0.006605413238294042\n",
      "Epoch 290, Training Loss 0.006626820245035984\n",
      "Epoch 290, Training Loss 0.0066680538064926445\n",
      "Epoch 290, Training Loss 0.006852838018720927\n",
      "Epoch 290, Training Loss 0.0069023869711967644\n",
      "Epoch 290, Training Loss 0.006948201567865908\n",
      "Epoch 290, Training Loss 0.006996534675147737\n",
      "Epoch 290, Training Loss 0.007100921329956435\n",
      "Epoch 290, Training Loss 0.007130790864774848\n",
      "Epoch 290, Training Loss 0.007207395414502629\n",
      "Epoch 290, Training Loss 0.007273308057135538\n",
      "Epoch 290, Training Loss 0.007311085287583968\n",
      "Epoch 290, Training Loss 0.0073590629964547655\n",
      "Epoch 290, Training Loss 0.007407139630361801\n",
      "Epoch 290, Training Loss 0.007430919220663912\n",
      "Epoch 290, Training Loss 0.007497086578830028\n",
      "Epoch 290, Training Loss 0.007574015681374618\n",
      "Epoch 290, Training Loss 0.0076347313725444324\n",
      "Epoch 290, Training Loss 0.007692169889633823\n",
      "Epoch 290, Training Loss 0.007743892977740663\n",
      "Epoch 290, Training Loss 0.007749865142106915\n",
      "Epoch 290, Training Loss 0.007796351372412003\n",
      "Epoch 290, Training Loss 0.007815861034497638\n",
      "Epoch 290, Training Loss 0.007888052468140469\n",
      "Epoch 290, Training Loss 0.007911253006845389\n",
      "Epoch 290, Training Loss 0.007956678563159654\n",
      "Epoch 290, Training Loss 0.008028865071035007\n",
      "Epoch 290, Training Loss 0.008037174769910648\n",
      "Epoch 290, Training Loss 0.008053127233513042\n",
      "Epoch 290, Training Loss 0.00807407609862335\n",
      "Epoch 290, Training Loss 0.008095129843517814\n",
      "Epoch 290, Training Loss 0.008140406030935266\n",
      "Epoch 290, Training Loss 0.008150681870265405\n",
      "Epoch 290, Training Loss 0.008203296703310287\n",
      "Epoch 290, Training Loss 0.008208256739470393\n",
      "Epoch 290, Training Loss 0.008334584048856288\n",
      "Epoch 290, Training Loss 0.008347051998819499\n",
      "Epoch 290, Training Loss 0.008365462612136818\n",
      "Epoch 290, Training Loss 0.00839716855072133\n",
      "Epoch 290, Training Loss 0.008414720654930643\n",
      "Epoch 290, Training Loss 0.008434990871711956\n",
      "Epoch 290, Training Loss 0.008482824585250463\n",
      "Epoch 290, Training Loss 0.00867322297192767\n",
      "Epoch 290, Training Loss 0.008698101815841425\n",
      "Epoch 290, Training Loss 0.008773754830198252\n",
      "Epoch 290, Training Loss 0.008790344410144802\n",
      "Epoch 290, Training Loss 0.00884170952738951\n",
      "Epoch 290, Training Loss 0.008880966284033625\n",
      "Epoch 290, Training Loss 0.008897515474115986\n",
      "Epoch 290, Training Loss 0.00893076074362049\n",
      "Epoch 290, Training Loss 0.008946246362131689\n",
      "Epoch 290, Training Loss 0.008980389888686558\n",
      "Epoch 290, Training Loss 0.009013199206332074\n",
      "Epoch 290, Training Loss 0.009056524156242647\n",
      "Epoch 290, Training Loss 0.009074853319326974\n",
      "Epoch 290, Training Loss 0.00913270492442524\n",
      "Epoch 290, Training Loss 0.009168564080548903\n",
      "Epoch 290, Training Loss 0.009246908736241328\n",
      "Epoch 290, Training Loss 0.009262788660295517\n",
      "Epoch 290, Training Loss 0.009282453834195918\n",
      "Epoch 290, Training Loss 0.00932334999487166\n",
      "Epoch 290, Training Loss 0.009368266663192522\n",
      "Epoch 290, Training Loss 0.009494369168934957\n",
      "Epoch 290, Training Loss 0.009587374985661081\n",
      "Epoch 290, Training Loss 0.009684542151730117\n",
      "Epoch 290, Training Loss 0.009719306586996254\n",
      "Epoch 290, Training Loss 0.009751971980766452\n",
      "Epoch 290, Training Loss 0.009828790367873924\n",
      "Epoch 290, Training Loss 0.009891074782599459\n",
      "Epoch 290, Training Loss 0.010003750379818975\n",
      "Epoch 290, Training Loss 0.010053740359980927\n",
      "Epoch 290, Training Loss 0.010151416814202428\n",
      "Epoch 290, Training Loss 0.010180514458867024\n",
      "Epoch 290, Training Loss 0.010225663972122934\n",
      "Epoch 290, Training Loss 0.010378449522268475\n",
      "Epoch 290, Training Loss 0.010393524784809146\n",
      "Epoch 290, Training Loss 0.010440974950652255\n",
      "Epoch 290, Training Loss 0.010473004717956228\n",
      "Epoch 290, Training Loss 0.010498612077639954\n",
      "Epoch 290, Training Loss 0.010524031960422082\n",
      "Epoch 290, Training Loss 0.010553498687508428\n",
      "Epoch 290, Training Loss 0.010570400801208584\n",
      "Epoch 290, Training Loss 0.01060314611960536\n",
      "Epoch 290, Training Loss 0.010614171287740398\n",
      "Epoch 290, Training Loss 0.010634934838713549\n",
      "Epoch 290, Training Loss 0.01067073618614441\n",
      "Epoch 290, Training Loss 0.010745916141035117\n",
      "Epoch 290, Training Loss 0.010768996920231777\n",
      "Epoch 290, Training Loss 0.01084786951911095\n",
      "Epoch 290, Training Loss 0.01092368378804144\n",
      "Epoch 290, Training Loss 0.0109511512085138\n",
      "Epoch 290, Training Loss 0.010972825881174725\n",
      "Epoch 290, Training Loss 0.01110070208242387\n",
      "Epoch 290, Training Loss 0.011151634170399868\n",
      "Epoch 290, Training Loss 0.011202796845270507\n",
      "Epoch 290, Training Loss 0.01129494405403981\n",
      "Epoch 290, Training Loss 0.011352705232718068\n",
      "Epoch 290, Training Loss 0.011500177318420823\n",
      "Epoch 290, Training Loss 0.011565651143770046\n",
      "Epoch 290, Training Loss 0.011612492148428584\n",
      "Epoch 290, Training Loss 0.011660701663726392\n",
      "Epoch 290, Training Loss 0.011695951780142344\n",
      "Epoch 290, Training Loss 0.011749370475573576\n",
      "Epoch 290, Training Loss 0.011796976541481017\n",
      "Epoch 290, Training Loss 0.011812836872032651\n",
      "Epoch 290, Training Loss 0.011838293364604988\n",
      "Epoch 290, Training Loss 0.0118942139831031\n",
      "Epoch 290, Training Loss 0.011923240411364475\n",
      "Epoch 290, Training Loss 0.011953233342851177\n",
      "Epoch 290, Training Loss 0.011967873484815669\n",
      "Epoch 290, Training Loss 0.012076793150985829\n",
      "Epoch 290, Training Loss 0.012106305262778916\n",
      "Epoch 290, Training Loss 0.012126320964106551\n",
      "Epoch 290, Training Loss 0.012146163435862459\n",
      "Epoch 290, Training Loss 0.012210864682689004\n",
      "Epoch 290, Training Loss 0.012226251773941129\n",
      "Epoch 290, Training Loss 0.012249962675035038\n",
      "Epoch 290, Training Loss 0.012263908281403087\n",
      "Epoch 290, Training Loss 0.01229713585577867\n",
      "Epoch 290, Training Loss 0.012359351262121517\n",
      "Epoch 290, Training Loss 0.012401850979246409\n",
      "Epoch 290, Training Loss 0.012432222649374085\n",
      "Epoch 290, Training Loss 0.012452378918481109\n",
      "Epoch 290, Training Loss 0.012519159286980852\n",
      "Epoch 290, Training Loss 0.012534005288153773\n",
      "Epoch 290, Training Loss 0.012558428017432085\n",
      "Epoch 290, Training Loss 0.012609951040300224\n",
      "Epoch 290, Training Loss 0.012719147416460507\n",
      "Epoch 290, Training Loss 0.012784278014903444\n",
      "Epoch 290, Training Loss 0.012825125294244465\n",
      "Epoch 290, Training Loss 0.012905049882710094\n",
      "Epoch 290, Training Loss 0.012970164565302793\n",
      "Epoch 290, Training Loss 0.013002225621234235\n",
      "Epoch 290, Training Loss 0.013098877496764902\n",
      "Epoch 290, Training Loss 0.013129830507494871\n",
      "Epoch 290, Training Loss 0.013186455315426754\n",
      "Epoch 290, Training Loss 0.01326326601852751\n",
      "Epoch 290, Training Loss 0.013298538753457958\n",
      "Epoch 290, Training Loss 0.01336637686144875\n",
      "Epoch 290, Training Loss 0.01338690422806422\n",
      "Epoch 290, Training Loss 0.013396519465524408\n",
      "Epoch 290, Training Loss 0.01344506599573547\n",
      "Epoch 290, Training Loss 0.01347670845908906\n",
      "Epoch 290, Training Loss 0.013497184455285178\n",
      "Epoch 290, Training Loss 0.013557934417220219\n",
      "Epoch 290, Training Loss 0.013664447488691991\n",
      "Epoch 290, Training Loss 0.013742585671836954\n",
      "Epoch 290, Training Loss 0.013790717022731672\n",
      "Epoch 290, Training Loss 0.01384411818262123\n",
      "Epoch 290, Training Loss 0.013891673152027724\n",
      "Epoch 290, Training Loss 0.013906837020026486\n",
      "Epoch 290, Training Loss 0.014016964080412407\n",
      "Epoch 290, Training Loss 0.014185096892883138\n",
      "Epoch 290, Training Loss 0.014247809010950843\n",
      "Epoch 290, Training Loss 0.014305380190296285\n",
      "Epoch 290, Training Loss 0.014397571140261906\n",
      "Epoch 290, Training Loss 0.014426033110822292\n",
      "Epoch 290, Training Loss 0.01446075789937678\n",
      "Epoch 290, Training Loss 0.014532363233382783\n",
      "Epoch 290, Training Loss 0.014559939842494896\n",
      "Epoch 290, Training Loss 0.014625635746594928\n",
      "Epoch 290, Training Loss 0.01468260122327339\n",
      "Epoch 290, Training Loss 0.014776873152793558\n",
      "Epoch 290, Training Loss 0.014837463670398307\n",
      "Epoch 290, Training Loss 0.014929630172550869\n",
      "Epoch 290, Training Loss 0.014999820405969878\n",
      "Epoch 290, Training Loss 0.0150623106501301\n",
      "Epoch 290, Training Loss 0.01509611497518828\n",
      "Epoch 290, Training Loss 0.015179472861697187\n",
      "Epoch 290, Training Loss 0.015209169051421762\n",
      "Epoch 290, Training Loss 0.01524146416467493\n",
      "Epoch 290, Training Loss 0.015271161793066604\n",
      "Epoch 290, Training Loss 0.015327777135331193\n",
      "Epoch 290, Training Loss 0.01533841788399574\n",
      "Epoch 290, Training Loss 0.015366017094949055\n",
      "Epoch 290, Training Loss 0.015398051247329396\n",
      "Epoch 290, Training Loss 0.015421192200444734\n",
      "Epoch 290, Training Loss 0.015456088247549389\n",
      "Epoch 290, Training Loss 0.015461505357952564\n",
      "Epoch 290, Training Loss 0.01549452773886054\n",
      "Epoch 290, Training Loss 0.015502244479420698\n",
      "Epoch 290, Training Loss 0.015546871051478112\n",
      "Epoch 290, Training Loss 0.015574001726191824\n",
      "Epoch 290, Training Loss 0.01566409397527309\n",
      "Epoch 290, Training Loss 0.01568535369251619\n",
      "Epoch 290, Training Loss 0.01569577903770234\n",
      "Epoch 290, Training Loss 0.015724339499793318\n",
      "Epoch 290, Training Loss 0.01574888765035421\n",
      "Epoch 290, Training Loss 0.01579450900354387\n",
      "Epoch 290, Training Loss 0.015842256254142584\n",
      "Epoch 290, Training Loss 0.015854817512623794\n",
      "Epoch 290, Training Loss 0.015907332901378423\n",
      "Epoch 290, Training Loss 0.01593653802209727\n",
      "Epoch 290, Training Loss 0.01596551544992896\n",
      "Epoch 290, Training Loss 0.015977400332412985\n",
      "Epoch 290, Training Loss 0.01600466486271423\n",
      "Epoch 290, Training Loss 0.016023647511744742\n",
      "Epoch 290, Training Loss 0.01603280550614476\n",
      "Epoch 290, Training Loss 0.016096219455566056\n",
      "Epoch 290, Training Loss 0.016127850066589386\n",
      "Epoch 290, Training Loss 0.016189177572379445\n",
      "Epoch 290, Training Loss 0.01622866281984693\n",
      "Epoch 290, Training Loss 0.016273782587946033\n",
      "Epoch 290, Training Loss 0.016311304323980227\n",
      "Epoch 290, Training Loss 0.01638558200593857\n",
      "Epoch 290, Training Loss 0.016410434796639225\n",
      "Epoch 290, Training Loss 0.016437107946633188\n",
      "Epoch 290, Training Loss 0.016472934549340925\n",
      "Epoch 290, Training Loss 0.01650455160735799\n",
      "Epoch 290, Training Loss 0.016549533677866202\n",
      "Epoch 290, Training Loss 0.01658574447614591\n",
      "Epoch 290, Training Loss 0.016607328019726572\n",
      "Epoch 290, Training Loss 0.016658022103573925\n",
      "Epoch 290, Training Loss 0.016699932838963996\n",
      "Epoch 290, Training Loss 0.016730509924075907\n",
      "Epoch 290, Training Loss 0.016766538597581447\n",
      "Epoch 290, Training Loss 0.016801604614628818\n",
      "Epoch 290, Training Loss 0.016884149292179042\n",
      "Epoch 290, Training Loss 0.016912174150895548\n",
      "Epoch 290, Training Loss 0.016942333122191337\n",
      "Epoch 290, Training Loss 0.01698177009094459\n",
      "Epoch 290, Training Loss 0.016992079528153438\n",
      "Epoch 290, Training Loss 0.017021836992234106\n",
      "Epoch 290, Training Loss 0.017031012882438044\n",
      "Epoch 290, Training Loss 0.01705006386339188\n",
      "Epoch 290, Training Loss 0.017185789222717095\n",
      "Epoch 290, Training Loss 0.017209379630677802\n",
      "Epoch 290, Training Loss 0.017245184353259786\n",
      "Epoch 290, Training Loss 0.01730067688731186\n",
      "Epoch 290, Training Loss 0.017458227569656565\n",
      "Epoch 290, Training Loss 0.017471909479540595\n",
      "Epoch 290, Training Loss 0.017510887977960125\n",
      "Epoch 290, Training Loss 0.017546968522083844\n",
      "Epoch 290, Training Loss 0.017614763670141243\n",
      "Epoch 290, Training Loss 0.01773533876925764\n",
      "Epoch 290, Training Loss 0.017770893759239476\n",
      "Epoch 290, Training Loss 0.017791613404665268\n",
      "Epoch 290, Training Loss 0.017852660003558868\n",
      "Epoch 290, Training Loss 0.017877784561808874\n",
      "Epoch 290, Training Loss 0.01795497709287383\n",
      "Epoch 290, Training Loss 0.01802893992587257\n",
      "Epoch 290, Training Loss 0.018049953330922727\n",
      "Epoch 290, Training Loss 0.018098381757045454\n",
      "Epoch 290, Training Loss 0.018115843918360294\n",
      "Epoch 290, Training Loss 0.018141804916231567\n",
      "Epoch 290, Training Loss 0.018150393847528548\n",
      "Epoch 290, Training Loss 0.018195866517808357\n",
      "Epoch 290, Training Loss 0.018286739677295584\n",
      "Epoch 290, Training Loss 0.018299190849161057\n",
      "Epoch 290, Training Loss 0.018370671433580045\n",
      "Epoch 290, Training Loss 0.018405675623670716\n",
      "Epoch 290, Training Loss 0.018466348659790233\n",
      "Epoch 290, Training Loss 0.018613393832941343\n",
      "Epoch 290, Training Loss 0.018687830668638276\n",
      "Epoch 290, Training Loss 0.01873732158375899\n",
      "Epoch 290, Training Loss 0.018805728751279966\n",
      "Epoch 290, Training Loss 0.018828974579415663\n",
      "Epoch 290, Training Loss 0.018870829011473203\n",
      "Epoch 290, Training Loss 0.019004236115976368\n",
      "Epoch 290, Training Loss 0.019069132492746537\n",
      "Epoch 290, Training Loss 0.019074008902511024\n",
      "Epoch 290, Training Loss 0.019089034309520213\n",
      "Epoch 290, Training Loss 0.019120480409106407\n",
      "Epoch 290, Training Loss 0.019211860322405388\n",
      "Epoch 290, Training Loss 0.01926076291319545\n",
      "Epoch 290, Training Loss 0.019273443828763254\n",
      "Epoch 290, Training Loss 0.019322609209724704\n",
      "Epoch 290, Training Loss 0.019338164095054653\n",
      "Epoch 290, Training Loss 0.019361662278261485\n",
      "Epoch 290, Training Loss 0.019411058893994144\n",
      "Epoch 290, Training Loss 0.019489450992354194\n",
      "Epoch 290, Training Loss 0.01951431129318293\n",
      "Epoch 290, Training Loss 0.01952694292963885\n",
      "Epoch 290, Training Loss 0.01955542174975395\n",
      "Epoch 290, Training Loss 0.019581454323695215\n",
      "Epoch 290, Training Loss 0.01962718473928397\n",
      "Epoch 290, Training Loss 0.01969827035718295\n",
      "Epoch 290, Training Loss 0.019706741040286697\n",
      "Epoch 290, Training Loss 0.019790032033539375\n",
      "Epoch 290, Training Loss 0.0198570271159577\n",
      "Epoch 290, Training Loss 0.019923110102491495\n",
      "Epoch 290, Training Loss 0.019957616308923152\n",
      "Epoch 290, Training Loss 0.019969214301180007\n",
      "Epoch 290, Training Loss 0.01998418145884505\n",
      "Epoch 290, Training Loss 0.020019922570427857\n",
      "Epoch 290, Training Loss 0.02010269004129864\n",
      "Epoch 290, Training Loss 0.020143818865885097\n",
      "Epoch 290, Training Loss 0.020213257567242474\n",
      "Epoch 290, Training Loss 0.02028067784248125\n",
      "Epoch 290, Training Loss 0.020294917256111645\n",
      "Epoch 290, Training Loss 0.020316777700234368\n",
      "Epoch 290, Training Loss 0.0203617843185954\n",
      "Epoch 290, Training Loss 0.0204057715126954\n",
      "Epoch 290, Training Loss 0.02043631437229817\n",
      "Epoch 290, Training Loss 0.020457845835653528\n",
      "Epoch 290, Training Loss 0.020492744195582273\n",
      "Epoch 290, Training Loss 0.020528489959014157\n",
      "Epoch 290, Training Loss 0.020566173659427964\n",
      "Epoch 290, Training Loss 0.020593519638771252\n",
      "Epoch 290, Training Loss 0.020639141234914626\n",
      "Epoch 290, Training Loss 0.020678267725493252\n",
      "Epoch 290, Training Loss 0.020708757254373655\n",
      "Epoch 290, Training Loss 0.020730025648875425\n",
      "Epoch 290, Training Loss 0.020746264029699174\n",
      "Epoch 290, Training Loss 0.020794517128987958\n",
      "Epoch 290, Training Loss 0.020830554617902317\n",
      "Epoch 290, Training Loss 0.020852495034289598\n",
      "Epoch 290, Training Loss 0.020884641945776543\n",
      "Epoch 290, Training Loss 0.020906006366543262\n",
      "Epoch 290, Training Loss 0.02091880547671157\n",
      "Epoch 290, Training Loss 0.02093786189554121\n",
      "Epoch 290, Training Loss 0.020975950066252705\n",
      "Epoch 290, Training Loss 0.02100928021949785\n",
      "Epoch 290, Training Loss 0.02104305197684871\n",
      "Epoch 290, Training Loss 0.02108787253553815\n",
      "Epoch 290, Training Loss 0.021175517373696884\n",
      "Epoch 290, Training Loss 0.021296124409317323\n",
      "Epoch 290, Training Loss 0.021373736850150368\n",
      "Epoch 290, Training Loss 0.021421866085918624\n",
      "Epoch 290, Training Loss 0.021431001220041376\n",
      "Epoch 290, Training Loss 0.021447848974038727\n",
      "Epoch 290, Training Loss 0.02151418976304705\n",
      "Epoch 290, Training Loss 0.021674531021291184\n",
      "Epoch 290, Training Loss 0.021722747515553555\n",
      "Epoch 290, Training Loss 0.021746379906868996\n",
      "Epoch 290, Training Loss 0.0217770669118637\n",
      "Epoch 290, Training Loss 0.021807100042662658\n",
      "Epoch 290, Training Loss 0.021816316569257345\n",
      "Epoch 290, Training Loss 0.021853914964334357\n",
      "Epoch 290, Training Loss 0.021867758500368317\n",
      "Epoch 290, Training Loss 0.021939412272675797\n",
      "Epoch 290, Training Loss 0.021993438147551492\n",
      "Epoch 290, Training Loss 0.022035364879776375\n",
      "Epoch 290, Training Loss 0.022078170918661842\n",
      "Epoch 290, Training Loss 0.022099008480005938\n",
      "Epoch 290, Training Loss 0.022115836664319724\n",
      "Epoch 290, Training Loss 0.022150191732103485\n",
      "Epoch 290, Training Loss 0.02218657724745095\n",
      "Epoch 290, Training Loss 0.022243875885129813\n",
      "Epoch 290, Training Loss 0.022262429967379708\n",
      "Epoch 290, Training Loss 0.02232637204577589\n",
      "Epoch 290, Training Loss 0.022352201174563536\n",
      "Epoch 290, Training Loss 0.022364894003200027\n",
      "Epoch 290, Training Loss 0.022390149134780515\n",
      "Epoch 290, Training Loss 0.022422928009372768\n",
      "Epoch 290, Training Loss 0.02245646322031727\n",
      "Epoch 290, Training Loss 0.022542330389961486\n",
      "Epoch 290, Training Loss 0.02256594176339867\n",
      "Epoch 290, Training Loss 0.022593109612412694\n",
      "Epoch 290, Training Loss 0.02269613709839066\n",
      "Epoch 290, Training Loss 0.022811223577727063\n",
      "Epoch 290, Training Loss 0.022867268102619884\n",
      "Epoch 290, Training Loss 0.022899203570535803\n",
      "Epoch 290, Training Loss 0.022931580130449113\n",
      "Epoch 290, Training Loss 0.022975795072219942\n",
      "Epoch 290, Training Loss 0.02305600162038146\n",
      "Epoch 290, Training Loss 0.023161413473174776\n",
      "Epoch 290, Training Loss 0.023232705134879367\n",
      "Epoch 290, Training Loss 0.02335592925839145\n",
      "Epoch 290, Training Loss 0.023375477073737955\n",
      "Epoch 290, Training Loss 0.023429312220419687\n",
      "Epoch 290, Training Loss 0.02348784974102131\n",
      "Epoch 290, Training Loss 0.02354914005583776\n",
      "Epoch 290, Training Loss 0.02356855799932309\n",
      "Epoch 290, Training Loss 0.023648471307114262\n",
      "Epoch 290, Training Loss 0.023684420822964757\n",
      "Epoch 290, Training Loss 0.023747983388602734\n",
      "Epoch 290, Training Loss 0.023774987910790822\n",
      "Epoch 290, Training Loss 0.023844124492057753\n",
      "Epoch 290, Training Loss 0.023884534297506217\n",
      "Epoch 290, Training Loss 0.023933166028250515\n",
      "Epoch 290, Training Loss 0.023990034571160442\n",
      "Epoch 290, Training Loss 0.024016881792727485\n",
      "Epoch 290, Training Loss 0.024071746834498996\n",
      "Epoch 290, Training Loss 0.024119288422395965\n",
      "Epoch 290, Training Loss 0.024144900831705927\n",
      "Epoch 290, Training Loss 0.024164600263271108\n",
      "Epoch 290, Training Loss 0.024201424586374667\n",
      "Epoch 290, Training Loss 0.024221682557812357\n",
      "Epoch 290, Training Loss 0.02430214311170113\n",
      "Epoch 290, Training Loss 0.024314594135888854\n",
      "Epoch 290, Training Loss 0.024330745365403954\n",
      "Epoch 290, Training Loss 0.024377180156214616\n",
      "Epoch 290, Training Loss 0.024409193707549053\n",
      "Epoch 290, Training Loss 0.02443230358640785\n",
      "Epoch 290, Training Loss 0.024488515477827596\n",
      "Epoch 290, Training Loss 0.0245490557468875\n",
      "Epoch 290, Training Loss 0.02460552742728568\n",
      "Epoch 290, Training Loss 0.024610638376706472\n",
      "Epoch 290, Training Loss 0.024626800907142175\n",
      "Epoch 290, Training Loss 0.024673630250022387\n",
      "Epoch 290, Training Loss 0.024707999697331427\n",
      "Epoch 290, Training Loss 0.024723329350276067\n",
      "Epoch 290, Training Loss 0.024743945938070565\n",
      "Epoch 290, Training Loss 0.024766019359707375\n",
      "Epoch 290, Training Loss 0.0248689470631654\n",
      "Epoch 290, Training Loss 0.02491643736400949\n",
      "Epoch 290, Training Loss 0.02501970144879559\n",
      "Epoch 290, Training Loss 0.025147807417566056\n",
      "Epoch 290, Training Loss 0.025184320185876564\n",
      "Epoch 290, Training Loss 0.02521979914325506\n",
      "Epoch 290, Training Loss 0.02523150168302114\n",
      "Epoch 290, Training Loss 0.02528341296021743\n",
      "Epoch 290, Training Loss 0.02529919313390732\n",
      "Epoch 290, Training Loss 0.02532439026862497\n",
      "Epoch 290, Training Loss 0.025331524971877333\n",
      "Epoch 290, Training Loss 0.025381164927192775\n",
      "Epoch 290, Training Loss 0.0253994740632689\n",
      "Epoch 290, Training Loss 0.025480452556367914\n",
      "Epoch 290, Training Loss 0.02554925768683805\n",
      "Epoch 290, Training Loss 0.02560247400182459\n",
      "Epoch 290, Training Loss 0.02565131085696142\n",
      "Epoch 290, Training Loss 0.025746882074367244\n",
      "Epoch 290, Training Loss 0.025874555128200166\n",
      "Epoch 290, Training Loss 0.025968230801665455\n",
      "Epoch 290, Training Loss 0.026047971888619195\n",
      "Epoch 290, Training Loss 0.026174652328848115\n",
      "Epoch 290, Training Loss 0.026220765749059256\n",
      "Epoch 290, Training Loss 0.026386402597974822\n",
      "Epoch 290, Training Loss 0.026417754705437956\n",
      "Epoch 290, Training Loss 0.02646920699542841\n",
      "Epoch 290, Training Loss 0.02650182223592973\n",
      "Epoch 290, Training Loss 0.02653411326898962\n",
      "Epoch 290, Training Loss 0.02657864589894386\n",
      "Epoch 290, Training Loss 0.026591803365841965\n",
      "Epoch 290, Training Loss 0.026646913047713203\n",
      "Epoch 290, Training Loss 0.026750924400961423\n",
      "Epoch 290, Training Loss 0.026790082184931317\n",
      "Epoch 290, Training Loss 0.026815244660634174\n",
      "Epoch 290, Training Loss 0.026931003292860543\n",
      "Epoch 290, Training Loss 0.027063394603117005\n",
      "Epoch 290, Training Loss 0.027102171402315006\n",
      "Epoch 290, Training Loss 0.027207622384947852\n",
      "Epoch 290, Training Loss 0.027302929764146656\n",
      "Epoch 290, Training Loss 0.027309382137249382\n",
      "Epoch 290, Training Loss 0.027369094739222655\n",
      "Epoch 290, Training Loss 0.027443771417874394\n",
      "Epoch 290, Training Loss 0.027504876239911255\n",
      "Epoch 290, Training Loss 0.027571150202356526\n",
      "Epoch 290, Training Loss 0.027673976656044725\n",
      "Epoch 290, Training Loss 0.027700976138134176\n",
      "Epoch 290, Training Loss 0.027762677768826523\n",
      "Epoch 290, Training Loss 0.027826342951444448\n",
      "Epoch 290, Training Loss 0.027887658862149358\n",
      "Epoch 290, Training Loss 0.0279379167773849\n",
      "Epoch 290, Training Loss 0.028030342010952666\n",
      "Epoch 290, Training Loss 0.028076776582628603\n",
      "Epoch 290, Training Loss 0.02810268063643647\n",
      "Epoch 290, Training Loss 0.028123671575413678\n",
      "Epoch 290, Training Loss 0.02818382831106958\n",
      "Epoch 290, Training Loss 0.02821007522258936\n",
      "Epoch 290, Training Loss 0.028245058663956383\n",
      "Epoch 290, Training Loss 0.028264587794494866\n",
      "Epoch 290, Training Loss 0.028364594956107266\n",
      "Epoch 290, Training Loss 0.028374598534834927\n",
      "Epoch 290, Training Loss 0.028421350294495443\n",
      "Epoch 290, Training Loss 0.02845105155290145\n",
      "Epoch 290, Training Loss 0.02846574668100823\n",
      "Epoch 290, Training Loss 0.028512856954603892\n",
      "Epoch 290, Training Loss 0.028586069456018184\n",
      "Epoch 290, Training Loss 0.02865567740560283\n",
      "Epoch 290, Training Loss 0.028741309824792664\n",
      "Epoch 290, Training Loss 0.0287827378016232\n",
      "Epoch 290, Training Loss 0.02880738765451476\n",
      "Epoch 290, Training Loss 0.02885042117429835\n",
      "Epoch 290, Training Loss 0.028868906598304735\n",
      "Epoch 290, Training Loss 0.028883622977715892\n",
      "Epoch 290, Training Loss 0.02896454239018795\n",
      "Epoch 290, Training Loss 0.028978474754387574\n",
      "Epoch 290, Training Loss 0.02899617325726544\n",
      "Epoch 290, Training Loss 0.02906960747538187\n",
      "Epoch 290, Training Loss 0.02916721037655707\n",
      "Epoch 290, Training Loss 0.029196775754761248\n",
      "Epoch 290, Training Loss 0.029293903467052466\n",
      "Epoch 290, Training Loss 0.029316681661688344\n",
      "Epoch 290, Training Loss 0.02937751742439521\n",
      "Epoch 290, Training Loss 0.029406462372649853\n",
      "Epoch 290, Training Loss 0.029438177486309957\n",
      "Epoch 290, Training Loss 0.029499733549616564\n",
      "Epoch 290, Training Loss 0.02967505931647023\n",
      "Epoch 290, Training Loss 0.029705877959504343\n",
      "Epoch 290, Training Loss 0.02974796870930592\n",
      "Epoch 290, Training Loss 0.02981308507351943\n",
      "Epoch 290, Training Loss 0.02984732812058533\n",
      "Epoch 290, Training Loss 0.029884592885427805\n",
      "Epoch 290, Training Loss 0.029934334666932672\n",
      "Epoch 290, Training Loss 0.029992245166869762\n",
      "Epoch 290, Training Loss 0.03003563193182754\n",
      "Epoch 290, Training Loss 0.030092612652660673\n",
      "Epoch 290, Training Loss 0.030107156470622818\n",
      "Epoch 290, Training Loss 0.03025597455801771\n",
      "Epoch 290, Training Loss 0.0303675170762755\n",
      "Epoch 290, Training Loss 0.030415805079914687\n",
      "Epoch 290, Training Loss 0.03054390361715975\n",
      "Epoch 290, Training Loss 0.030597164707086847\n",
      "Epoch 290, Training Loss 0.030694456941917386\n",
      "Epoch 290, Training Loss 0.030878524469268864\n",
      "Epoch 290, Training Loss 0.0309847310951213\n",
      "Epoch 290, Training Loss 0.031103843288934407\n",
      "Epoch 290, Training Loss 0.031158186294629103\n",
      "Epoch 290, Training Loss 0.03126409543080308\n",
      "Epoch 290, Training Loss 0.031346217873851626\n",
      "Epoch 290, Training Loss 0.03149134596116612\n",
      "Epoch 290, Training Loss 0.031571142998931316\n",
      "Epoch 290, Training Loss 0.031762540296953926\n",
      "Epoch 290, Training Loss 0.03178559311027246\n",
      "Epoch 290, Training Loss 0.03187496485391065\n",
      "Epoch 290, Training Loss 0.03193986275033249\n",
      "Epoch 290, Training Loss 0.03199304352738344\n",
      "Epoch 290, Training Loss 0.032007298049400265\n",
      "Epoch 290, Training Loss 0.03204622254301996\n",
      "Epoch 290, Training Loss 0.032135916586198354\n",
      "Epoch 290, Training Loss 0.03215011688428061\n",
      "Epoch 290, Training Loss 0.032200430064340646\n",
      "Epoch 290, Training Loss 0.03235526220894435\n",
      "Epoch 290, Training Loss 0.03244660749895703\n",
      "Epoch 290, Training Loss 0.03255711915031018\n",
      "Epoch 290, Training Loss 0.032586329287308674\n",
      "Epoch 290, Training Loss 0.032621284855691636\n",
      "Epoch 290, Training Loss 0.032655812718350524\n",
      "Epoch 290, Training Loss 0.0327662988452722\n",
      "Epoch 290, Training Loss 0.03280152359684391\n",
      "Epoch 290, Training Loss 0.03289158787290135\n",
      "Epoch 290, Training Loss 0.03294568587644764\n",
      "Epoch 290, Training Loss 0.03298825470199499\n",
      "Epoch 290, Training Loss 0.03299442257391541\n",
      "Epoch 290, Training Loss 0.0330361283182993\n",
      "Epoch 290, Training Loss 0.03308991923787729\n",
      "Epoch 290, Training Loss 0.033171158625155954\n",
      "Epoch 290, Training Loss 0.0332242873291273\n",
      "Epoch 290, Training Loss 0.03335323923415102\n",
      "Epoch 290, Training Loss 0.0333670987432008\n",
      "Epoch 290, Training Loss 0.03341199770924704\n",
      "Epoch 290, Training Loss 0.033457634187496414\n",
      "Epoch 290, Training Loss 0.033474364914023856\n",
      "Epoch 290, Training Loss 0.03351663556390578\n",
      "Epoch 290, Training Loss 0.03358401508961359\n",
      "Epoch 290, Training Loss 0.03368423212512546\n",
      "Epoch 290, Training Loss 0.03373515601460453\n",
      "Epoch 290, Training Loss 0.03382591547175308\n",
      "Epoch 290, Training Loss 0.0338948294216686\n",
      "Epoch 290, Training Loss 0.03399689082839929\n",
      "Epoch 290, Training Loss 0.03400930490813997\n",
      "Epoch 290, Training Loss 0.03403782778148018\n",
      "Epoch 290, Training Loss 0.034087483152447036\n",
      "Epoch 290, Training Loss 0.03413722240437499\n",
      "Epoch 290, Training Loss 0.03424787752227881\n",
      "Epoch 290, Training Loss 0.03434018859618327\n",
      "Epoch 290, Training Loss 0.034357753904450616\n",
      "Epoch 290, Training Loss 0.03440395762245921\n",
      "Epoch 290, Training Loss 0.03441775827001675\n",
      "Epoch 290, Training Loss 0.03446960696519431\n",
      "Epoch 290, Training Loss 0.034529770036702\n",
      "Epoch 290, Training Loss 0.034582024226393886\n",
      "Epoch 290, Training Loss 0.03463996060328353\n",
      "Epoch 290, Training Loss 0.03469505871865241\n",
      "Epoch 290, Training Loss 0.03477481508310741\n",
      "Epoch 290, Training Loss 0.0347890599631964\n",
      "Epoch 290, Training Loss 0.03484203583022575\n",
      "Epoch 290, Training Loss 0.03488572653028114\n",
      "Epoch 290, Training Loss 0.03492568799799494\n",
      "Epoch 290, Training Loss 0.03495512768576193\n",
      "Epoch 290, Training Loss 0.03509059439942031\n",
      "Epoch 290, Training Loss 0.03511560485279069\n",
      "Epoch 290, Training Loss 0.03516255817352258\n",
      "Epoch 290, Training Loss 0.03517604746993946\n",
      "Epoch 290, Training Loss 0.03520450120151062\n",
      "Epoch 290, Training Loss 0.03523881053444846\n",
      "Epoch 290, Training Loss 0.035291601156415726\n",
      "Epoch 290, Training Loss 0.03531904626563858\n",
      "Epoch 290, Training Loss 0.035338592108772576\n",
      "Epoch 290, Training Loss 0.03538374980623403\n",
      "Epoch 290, Training Loss 0.03547425547679005\n",
      "Epoch 290, Training Loss 0.03552798740148944\n",
      "Epoch 290, Training Loss 0.03558244492889613\n",
      "Epoch 290, Training Loss 0.03562001658000929\n",
      "Epoch 290, Training Loss 0.03563791535773298\n",
      "Epoch 290, Training Loss 0.03568191942997048\n",
      "Epoch 290, Training Loss 0.035706697664845286\n",
      "Epoch 290, Training Loss 0.03573102048298111\n",
      "Epoch 290, Training Loss 0.035766304338998765\n",
      "Epoch 290, Training Loss 0.03579777573435412\n",
      "Epoch 290, Training Loss 0.03581517477653197\n",
      "Epoch 290, Training Loss 0.03590483370098902\n",
      "Epoch 290, Training Loss 0.035919697955846215\n",
      "Epoch 290, Training Loss 0.03595026242642013\n",
      "Epoch 290, Training Loss 0.03599447041780919\n",
      "Epoch 290, Training Loss 0.03605806615853877\n",
      "Epoch 290, Training Loss 0.03611647697163226\n",
      "Epoch 290, Training Loss 0.03622470442932147\n",
      "Epoch 290, Training Loss 0.03629792749207667\n",
      "Epoch 290, Training Loss 0.03637660261245964\n",
      "Epoch 290, Training Loss 0.03641877201257174\n",
      "Epoch 290, Training Loss 0.03646940944056548\n",
      "Epoch 290, Training Loss 0.03653318594362291\n",
      "Epoch 290, Training Loss 0.036589501214230345\n",
      "Epoch 290, Training Loss 0.036649085098372586\n",
      "Epoch 290, Training Loss 0.03668572261388821\n",
      "Epoch 290, Training Loss 0.03669860489759356\n",
      "Epoch 290, Training Loss 0.036751508897961215\n",
      "Epoch 290, Training Loss 0.03688539310639529\n",
      "Epoch 290, Training Loss 0.037005624364075416\n",
      "Epoch 290, Training Loss 0.03705213927721028\n",
      "Epoch 290, Training Loss 0.03707338326439128\n",
      "Epoch 290, Training Loss 0.03711774824675449\n",
      "Epoch 290, Training Loss 0.0371705347432724\n",
      "Epoch 290, Training Loss 0.03719141581000479\n",
      "Epoch 290, Training Loss 0.03722407513469591\n",
      "Epoch 290, Training Loss 0.0374143255959072\n",
      "Epoch 290, Training Loss 0.03750319167187966\n",
      "Epoch 290, Training Loss 0.03755729086637554\n",
      "Epoch 290, Training Loss 0.0376352527413441\n",
      "Epoch 290, Training Loss 0.03771373834830168\n",
      "Epoch 290, Training Loss 0.03775857824801236\n",
      "Epoch 290, Training Loss 0.03783159934477333\n",
      "Epoch 290, Training Loss 0.03786740333134847\n",
      "Epoch 290, Training Loss 0.037929871004632174\n",
      "Epoch 290, Training Loss 0.037976770704050006\n",
      "Epoch 290, Training Loss 0.03804356538776375\n",
      "Epoch 290, Training Loss 0.03812413337483259\n",
      "Epoch 290, Training Loss 0.0381564867493394\n",
      "Epoch 290, Training Loss 0.03821530857818949\n",
      "Epoch 290, Training Loss 0.03824944448981272\n",
      "Epoch 290, Training Loss 0.03829589250969136\n",
      "Epoch 290, Training Loss 0.038443349970533225\n",
      "Epoch 290, Training Loss 0.03845235677745641\n",
      "Epoch 290, Training Loss 0.03852076133917969\n",
      "Epoch 290, Training Loss 0.038586907612774377\n",
      "Epoch 290, Training Loss 0.03865387679918495\n",
      "Epoch 290, Training Loss 0.03872802217617212\n",
      "Epoch 290, Training Loss 0.03876841085417496\n",
      "Epoch 290, Training Loss 0.03882830170319056\n",
      "Epoch 290, Training Loss 0.03885410241830303\n",
      "Epoch 290, Training Loss 0.038885146415199315\n",
      "Epoch 290, Training Loss 0.03891771402248108\n",
      "Epoch 290, Training Loss 0.03896257504963738\n",
      "Epoch 290, Training Loss 0.039003889581374344\n",
      "Epoch 290, Training Loss 0.03902470331658107\n",
      "Epoch 290, Training Loss 0.039040033954441014\n",
      "Epoch 290, Training Loss 0.03920787991003117\n",
      "Epoch 290, Training Loss 0.039287670264187294\n",
      "Epoch 290, Training Loss 0.039414626871333326\n",
      "Epoch 290, Training Loss 0.039503215341721576\n",
      "Epoch 290, Training Loss 0.0395906592809769\n",
      "Epoch 290, Training Loss 0.03964248246601437\n",
      "Epoch 290, Training Loss 0.0397432935567063\n",
      "Epoch 290, Training Loss 0.039789529573267604\n",
      "Epoch 290, Training Loss 0.03981131457311608\n",
      "Epoch 290, Training Loss 0.039896911968860554\n",
      "Epoch 290, Training Loss 0.039912655417238124\n",
      "Epoch 290, Training Loss 0.03996638914741709\n",
      "Epoch 290, Training Loss 0.03999810047623942\n",
      "Epoch 290, Training Loss 0.040031494745446365\n",
      "Epoch 290, Training Loss 0.04006283923321406\n",
      "Epoch 290, Training Loss 0.04015557265595135\n",
      "Epoch 290, Training Loss 0.04027488498646013\n",
      "Epoch 290, Training Loss 0.040303664054492934\n",
      "Epoch 290, Training Loss 0.040338375940299626\n",
      "Epoch 290, Training Loss 0.04039856703723293\n",
      "Epoch 300, Training Loss 2.717861043446509e-05\n",
      "Epoch 300, Training Loss 8.365393513837434e-05\n",
      "Epoch 300, Training Loss 0.00010736395492959206\n",
      "Epoch 300, Training Loss 0.00015607808985749778\n",
      "Epoch 300, Training Loss 0.00018917610797354633\n",
      "Epoch 300, Training Loss 0.00030004268135789717\n",
      "Epoch 300, Training Loss 0.0003307860849610985\n",
      "Epoch 300, Training Loss 0.0003564233755897683\n",
      "Epoch 300, Training Loss 0.00042178161213617495\n",
      "Epoch 300, Training Loss 0.0004964342812443024\n",
      "Epoch 300, Training Loss 0.0005359950825533904\n",
      "Epoch 300, Training Loss 0.0005456809320098832\n",
      "Epoch 300, Training Loss 0.0006298227015349185\n",
      "Epoch 300, Training Loss 0.0006892939504293149\n",
      "Epoch 300, Training Loss 0.0007678558400896428\n",
      "Epoch 300, Training Loss 0.0007992292107666469\n",
      "Epoch 300, Training Loss 0.0008416629540483894\n",
      "Epoch 300, Training Loss 0.0008819797796809384\n",
      "Epoch 300, Training Loss 0.0008932836252548125\n",
      "Epoch 300, Training Loss 0.000946094398089039\n",
      "Epoch 300, Training Loss 0.0010119208969566447\n",
      "Epoch 300, Training Loss 0.0010622161937177258\n",
      "Epoch 300, Training Loss 0.0010931327042248472\n",
      "Epoch 300, Training Loss 0.0012423663091657641\n",
      "Epoch 300, Training Loss 0.00142136856596774\n",
      "Epoch 300, Training Loss 0.0014509760299483148\n",
      "Epoch 300, Training Loss 0.0015121415808863575\n",
      "Epoch 300, Training Loss 0.001536231701764876\n",
      "Epoch 300, Training Loss 0.0015518356244439435\n",
      "Epoch 300, Training Loss 0.001600513887970382\n",
      "Epoch 300, Training Loss 0.0016261993739468134\n",
      "Epoch 300, Training Loss 0.0016340212414131673\n",
      "Epoch 300, Training Loss 0.00169255103037009\n",
      "Epoch 300, Training Loss 0.001740180900977815\n",
      "Epoch 300, Training Loss 0.0017731168618201829\n",
      "Epoch 300, Training Loss 0.0018706722107603956\n",
      "Epoch 300, Training Loss 0.0019064501822328248\n",
      "Epoch 300, Training Loss 0.0019665184398622386\n",
      "Epoch 300, Training Loss 0.0019998846479150874\n",
      "Epoch 300, Training Loss 0.002023789561841913\n",
      "Epoch 300, Training Loss 0.002120028463694865\n",
      "Epoch 300, Training Loss 0.002159619500831989\n",
      "Epoch 300, Training Loss 0.0022206322624660135\n",
      "Epoch 300, Training Loss 0.002296900583307266\n",
      "Epoch 300, Training Loss 0.0023024227271270952\n",
      "Epoch 300, Training Loss 0.0023470037858552106\n",
      "Epoch 300, Training Loss 0.002403166229048234\n",
      "Epoch 300, Training Loss 0.00242994787454929\n",
      "Epoch 300, Training Loss 0.0024831574582887808\n",
      "Epoch 300, Training Loss 0.0025640301569543607\n",
      "Epoch 300, Training Loss 0.0025806905888735563\n",
      "Epoch 300, Training Loss 0.002599130592444707\n",
      "Epoch 300, Training Loss 0.002673914223132879\n",
      "Epoch 300, Training Loss 0.0026846938770111947\n",
      "Epoch 300, Training Loss 0.0027674794217328663\n",
      "Epoch 300, Training Loss 0.0028024898190527696\n",
      "Epoch 300, Training Loss 0.002815694746482746\n",
      "Epoch 300, Training Loss 0.0028681208854755553\n",
      "Epoch 300, Training Loss 0.0029123394429693213\n",
      "Epoch 300, Training Loss 0.003004741905581044\n",
      "Epoch 300, Training Loss 0.003033345341896805\n",
      "Epoch 300, Training Loss 0.0030627871447902583\n",
      "Epoch 300, Training Loss 0.0031091028692253183\n",
      "Epoch 300, Training Loss 0.0031414514180044153\n",
      "Epoch 300, Training Loss 0.003191592324825237\n",
      "Epoch 300, Training Loss 0.003269075619085404\n",
      "Epoch 300, Training Loss 0.003326355697005949\n",
      "Epoch 300, Training Loss 0.003364291128433307\n",
      "Epoch 300, Training Loss 0.003430950653660671\n",
      "Epoch 300, Training Loss 0.0034619522980078483\n",
      "Epoch 300, Training Loss 0.003516148683517371\n",
      "Epoch 300, Training Loss 0.003586184493649532\n",
      "Epoch 300, Training Loss 0.003607522451516498\n",
      "Epoch 300, Training Loss 0.0036228275101612825\n",
      "Epoch 300, Training Loss 0.0036494139614074357\n",
      "Epoch 300, Training Loss 0.0037231926640729084\n",
      "Epoch 300, Training Loss 0.0038464231806023576\n",
      "Epoch 300, Training Loss 0.0038853588810576424\n",
      "Epoch 300, Training Loss 0.003922658004795613\n",
      "Epoch 300, Training Loss 0.0039764897167787455\n",
      "Epoch 300, Training Loss 0.003984863719786219\n",
      "Epoch 300, Training Loss 0.004030819744100351\n",
      "Epoch 300, Training Loss 0.0040810692655232255\n",
      "Epoch 300, Training Loss 0.004105916730297344\n",
      "Epoch 300, Training Loss 0.004134983955727666\n",
      "Epoch 300, Training Loss 0.00420088275952641\n",
      "Epoch 300, Training Loss 0.004232911495468043\n",
      "Epoch 300, Training Loss 0.004244121528752243\n",
      "Epoch 300, Training Loss 0.004281183479644377\n",
      "Epoch 300, Training Loss 0.004296178081551629\n",
      "Epoch 300, Training Loss 0.004305163369802258\n",
      "Epoch 300, Training Loss 0.00433066458491337\n",
      "Epoch 300, Training Loss 0.004405636337997816\n",
      "Epoch 300, Training Loss 0.004418276495698011\n",
      "Epoch 300, Training Loss 0.004456098110455534\n",
      "Epoch 300, Training Loss 0.004493843745011503\n",
      "Epoch 300, Training Loss 0.004509918294046693\n",
      "Epoch 300, Training Loss 0.004543800759688973\n",
      "Epoch 300, Training Loss 0.004567356418122721\n",
      "Epoch 300, Training Loss 0.004604613251240967\n",
      "Epoch 300, Training Loss 0.00467030697828516\n",
      "Epoch 300, Training Loss 0.004690961090519148\n",
      "Epoch 300, Training Loss 0.004813608946874166\n",
      "Epoch 300, Training Loss 0.004831556524948009\n",
      "Epoch 300, Training Loss 0.004903515690075391\n",
      "Epoch 300, Training Loss 0.004956805215114751\n",
      "Epoch 300, Training Loss 0.004996516754912674\n",
      "Epoch 300, Training Loss 0.005011987784291472\n",
      "Epoch 300, Training Loss 0.005044898351706812\n",
      "Epoch 300, Training Loss 0.005093323419351712\n",
      "Epoch 300, Training Loss 0.0051122035157135535\n",
      "Epoch 300, Training Loss 0.005131691500134861\n",
      "Epoch 300, Training Loss 0.005154278318700202\n",
      "Epoch 300, Training Loss 0.005173994147259256\n",
      "Epoch 300, Training Loss 0.0051859572108196635\n",
      "Epoch 300, Training Loss 0.0052071823000126635\n",
      "Epoch 300, Training Loss 0.005288734116241374\n",
      "Epoch 300, Training Loss 0.0053284858601153505\n",
      "Epoch 300, Training Loss 0.005400198865491335\n",
      "Epoch 300, Training Loss 0.005523516159371265\n",
      "Epoch 300, Training Loss 0.0055366285750285136\n",
      "Epoch 300, Training Loss 0.005548202045633436\n",
      "Epoch 300, Training Loss 0.0056687207699603285\n",
      "Epoch 300, Training Loss 0.0057551796319406205\n",
      "Epoch 300, Training Loss 0.005879703319638663\n",
      "Epoch 300, Training Loss 0.005899698729567287\n",
      "Epoch 300, Training Loss 0.006077225466527979\n",
      "Epoch 300, Training Loss 0.006118818094520389\n",
      "Epoch 300, Training Loss 0.006158260810796333\n",
      "Epoch 300, Training Loss 0.006177508552103778\n",
      "Epoch 300, Training Loss 0.006231432691302217\n",
      "Epoch 300, Training Loss 0.006267225116138797\n",
      "Epoch 300, Training Loss 0.006284635322278037\n",
      "Epoch 300, Training Loss 0.006315217380795409\n",
      "Epoch 300, Training Loss 0.0063492921633583966\n",
      "Epoch 300, Training Loss 0.006370558788109085\n",
      "Epoch 300, Training Loss 0.0064483723803268524\n",
      "Epoch 300, Training Loss 0.00648178075275877\n",
      "Epoch 300, Training Loss 0.006551347843245091\n",
      "Epoch 300, Training Loss 0.0066652891340444\n",
      "Epoch 300, Training Loss 0.006690870676323048\n",
      "Epoch 300, Training Loss 0.00671367481936846\n",
      "Epoch 300, Training Loss 0.006784526148901495\n",
      "Epoch 300, Training Loss 0.0068686904226098675\n",
      "Epoch 300, Training Loss 0.006883288189397215\n",
      "Epoch 300, Training Loss 0.006907215350262268\n",
      "Epoch 300, Training Loss 0.00692651146913276\n",
      "Epoch 300, Training Loss 0.0069857650934277895\n",
      "Epoch 300, Training Loss 0.006997180149873809\n",
      "Epoch 300, Training Loss 0.007019876430997306\n",
      "Epoch 300, Training Loss 0.007123835592070961\n",
      "Epoch 300, Training Loss 0.007213036839366721\n",
      "Epoch 300, Training Loss 0.007258379033973912\n",
      "Epoch 300, Training Loss 0.007280418249156774\n",
      "Epoch 300, Training Loss 0.007314814476158156\n",
      "Epoch 300, Training Loss 0.007341106322205738\n",
      "Epoch 300, Training Loss 0.007354350333028208\n",
      "Epoch 300, Training Loss 0.007362038939786346\n",
      "Epoch 300, Training Loss 0.007408428055298565\n",
      "Epoch 300, Training Loss 0.007486960623780137\n",
      "Epoch 300, Training Loss 0.0075505309496456015\n",
      "Epoch 300, Training Loss 0.007595314526968561\n",
      "Epoch 300, Training Loss 0.007664408529405971\n",
      "Epoch 300, Training Loss 0.007704645661456163\n",
      "Epoch 300, Training Loss 0.0077291928163832025\n",
      "Epoch 300, Training Loss 0.0077383893391932065\n",
      "Epoch 300, Training Loss 0.0077420056474816696\n",
      "Epoch 300, Training Loss 0.00778006182690544\n",
      "Epoch 300, Training Loss 0.007868702613327014\n",
      "Epoch 300, Training Loss 0.007887961647218413\n",
      "Epoch 300, Training Loss 0.007971784947297115\n",
      "Epoch 300, Training Loss 0.007986753880667984\n",
      "Epoch 300, Training Loss 0.00802463914453507\n",
      "Epoch 300, Training Loss 0.008145769292850743\n",
      "Epoch 300, Training Loss 0.008176347478400068\n",
      "Epoch 300, Training Loss 0.008267069377762544\n",
      "Epoch 300, Training Loss 0.008285888105683276\n",
      "Epoch 300, Training Loss 0.008308169621940883\n",
      "Epoch 300, Training Loss 0.008383310713764766\n",
      "Epoch 300, Training Loss 0.008420409774645934\n",
      "Epoch 300, Training Loss 0.008478861661208674\n",
      "Epoch 300, Training Loss 0.008563488623594193\n",
      "Epoch 300, Training Loss 0.008579273349093393\n",
      "Epoch 300, Training Loss 0.008637006502346042\n",
      "Epoch 300, Training Loss 0.008683466889045161\n",
      "Epoch 300, Training Loss 0.00874169286199943\n",
      "Epoch 300, Training Loss 0.00880189345318281\n",
      "Epoch 300, Training Loss 0.00889982481825325\n",
      "Epoch 300, Training Loss 0.008980341280079292\n",
      "Epoch 300, Training Loss 0.009061511582754495\n",
      "Epoch 300, Training Loss 0.009093512452857765\n",
      "Epoch 300, Training Loss 0.009155893825170824\n",
      "Epoch 300, Training Loss 0.009199421908206228\n",
      "Epoch 300, Training Loss 0.009219600145569277\n",
      "Epoch 300, Training Loss 0.009287479222225755\n",
      "Epoch 300, Training Loss 0.009314616754427171\n",
      "Epoch 300, Training Loss 0.0093425178925371\n",
      "Epoch 300, Training Loss 0.009371494350935835\n",
      "Epoch 300, Training Loss 0.009416166150494648\n",
      "Epoch 300, Training Loss 0.009552326825711771\n",
      "Epoch 300, Training Loss 0.009709960760196189\n",
      "Epoch 300, Training Loss 0.009782534426726077\n",
      "Epoch 300, Training Loss 0.00979887475759801\n",
      "Epoch 300, Training Loss 0.009857231406304423\n",
      "Epoch 300, Training Loss 0.00988083887342697\n",
      "Epoch 300, Training Loss 0.009920990199941542\n",
      "Epoch 300, Training Loss 0.009928205454855434\n",
      "Epoch 300, Training Loss 0.00995019634368608\n",
      "Epoch 300, Training Loss 0.009992555928919131\n",
      "Epoch 300, Training Loss 0.010039734367705177\n",
      "Epoch 300, Training Loss 0.010188493117227998\n",
      "Epoch 300, Training Loss 0.010273492513367396\n",
      "Epoch 300, Training Loss 0.010347907621380123\n",
      "Epoch 300, Training Loss 0.010409857626990093\n",
      "Epoch 300, Training Loss 0.010433809085608557\n",
      "Epoch 300, Training Loss 0.010473544913632294\n",
      "Epoch 300, Training Loss 0.01050455059768046\n",
      "Epoch 300, Training Loss 0.010536121034785115\n",
      "Epoch 300, Training Loss 0.010552988670494817\n",
      "Epoch 300, Training Loss 0.010598738565254966\n",
      "Epoch 300, Training Loss 0.010646915653377505\n",
      "Epoch 300, Training Loss 0.010697351610812042\n",
      "Epoch 300, Training Loss 0.010709153082760057\n",
      "Epoch 300, Training Loss 0.010740440509751287\n",
      "Epoch 300, Training Loss 0.010767353669432995\n",
      "Epoch 300, Training Loss 0.010774853496390688\n",
      "Epoch 300, Training Loss 0.010798654838196\n",
      "Epoch 300, Training Loss 0.01083705388247738\n",
      "Epoch 300, Training Loss 0.01085378663004626\n",
      "Epoch 300, Training Loss 0.010882888283447155\n",
      "Epoch 300, Training Loss 0.010986164735053735\n",
      "Epoch 300, Training Loss 0.011022582891947397\n",
      "Epoch 300, Training Loss 0.011076514457907442\n",
      "Epoch 300, Training Loss 0.011094568945143534\n",
      "Epoch 300, Training Loss 0.011132746179828712\n",
      "Epoch 300, Training Loss 0.011149553227169281\n",
      "Epoch 300, Training Loss 0.01117299136269809\n",
      "Epoch 300, Training Loss 0.011230944255676568\n",
      "Epoch 300, Training Loss 0.011288381891463266\n",
      "Epoch 300, Training Loss 0.011352070216613505\n",
      "Epoch 300, Training Loss 0.011471947876598372\n",
      "Epoch 300, Training Loss 0.011608984947795302\n",
      "Epoch 300, Training Loss 0.01164710550280788\n",
      "Epoch 300, Training Loss 0.011699104486295329\n",
      "Epoch 300, Training Loss 0.011707054995967414\n",
      "Epoch 300, Training Loss 0.011713297672622153\n",
      "Epoch 300, Training Loss 0.011726271938127667\n",
      "Epoch 300, Training Loss 0.011788763954420871\n",
      "Epoch 300, Training Loss 0.011813204445521279\n",
      "Epoch 300, Training Loss 0.011846520712294275\n",
      "Epoch 300, Training Loss 0.011961508247539249\n",
      "Epoch 300, Training Loss 0.012041309820440457\n",
      "Epoch 300, Training Loss 0.01211471686585118\n",
      "Epoch 300, Training Loss 0.012160698071603313\n",
      "Epoch 300, Training Loss 0.012245810217321719\n",
      "Epoch 300, Training Loss 0.012301989133729388\n",
      "Epoch 300, Training Loss 0.012344245096821043\n",
      "Epoch 300, Training Loss 0.01238731043401372\n",
      "Epoch 300, Training Loss 0.012487705292589867\n",
      "Epoch 300, Training Loss 0.012532193833590506\n",
      "Epoch 300, Training Loss 0.012603951042251246\n",
      "Epoch 300, Training Loss 0.012638515059757606\n",
      "Epoch 300, Training Loss 0.012656256056907575\n",
      "Epoch 300, Training Loss 0.012698237058323096\n",
      "Epoch 300, Training Loss 0.01274237215883382\n",
      "Epoch 300, Training Loss 0.012772582050370018\n",
      "Epoch 300, Training Loss 0.012816405388862466\n",
      "Epoch 300, Training Loss 0.012830310125413644\n",
      "Epoch 300, Training Loss 0.012883196480672264\n",
      "Epoch 300, Training Loss 0.012955999509800616\n",
      "Epoch 300, Training Loss 0.012974615678634218\n",
      "Epoch 300, Training Loss 0.012983272692350589\n",
      "Epoch 300, Training Loss 0.013061585316263006\n",
      "Epoch 300, Training Loss 0.013107977351983604\n",
      "Epoch 300, Training Loss 0.01316958874204408\n",
      "Epoch 300, Training Loss 0.013259411312501563\n",
      "Epoch 300, Training Loss 0.01327028787513847\n",
      "Epoch 300, Training Loss 0.013277865988571587\n",
      "Epoch 300, Training Loss 0.013304034745215874\n",
      "Epoch 300, Training Loss 0.01333130528981252\n",
      "Epoch 300, Training Loss 0.013414062337492547\n",
      "Epoch 300, Training Loss 0.013441534524145144\n",
      "Epoch 300, Training Loss 0.013502457497708138\n",
      "Epoch 300, Training Loss 0.013532073765188037\n",
      "Epoch 300, Training Loss 0.0136171817378191\n",
      "Epoch 300, Training Loss 0.013626783899843807\n",
      "Epoch 300, Training Loss 0.0136795443955504\n",
      "Epoch 300, Training Loss 0.013683801239994747\n",
      "Epoch 300, Training Loss 0.01370562458127413\n",
      "Epoch 300, Training Loss 0.013740054009568013\n",
      "Epoch 300, Training Loss 0.013761394015868263\n",
      "Epoch 300, Training Loss 0.013792982593516979\n",
      "Epoch 300, Training Loss 0.01382680420639932\n",
      "Epoch 300, Training Loss 0.01385954309128644\n",
      "Epoch 300, Training Loss 0.01389036578354914\n",
      "Epoch 300, Training Loss 0.01390591036954709\n",
      "Epoch 300, Training Loss 0.013941485338899143\n",
      "Epoch 300, Training Loss 0.013993052041034222\n",
      "Epoch 300, Training Loss 0.014061711418330479\n",
      "Epoch 300, Training Loss 0.01408456957570332\n",
      "Epoch 300, Training Loss 0.014147670119595916\n",
      "Epoch 300, Training Loss 0.014160238370142134\n",
      "Epoch 300, Training Loss 0.014208209433514253\n",
      "Epoch 300, Training Loss 0.014261830314074445\n",
      "Epoch 300, Training Loss 0.014280969955270057\n",
      "Epoch 300, Training Loss 0.014301866866395715\n",
      "Epoch 300, Training Loss 0.014337488027918332\n",
      "Epoch 300, Training Loss 0.014386999241583754\n",
      "Epoch 300, Training Loss 0.014398326501226449\n",
      "Epoch 300, Training Loss 0.014422186382967607\n",
      "Epoch 300, Training Loss 0.014456382044531462\n",
      "Epoch 300, Training Loss 0.014476113070798156\n",
      "Epoch 300, Training Loss 0.014522102222565914\n",
      "Epoch 300, Training Loss 0.014553523085930425\n",
      "Epoch 300, Training Loss 0.014748029962844213\n",
      "Epoch 300, Training Loss 0.014833592401836496\n",
      "Epoch 300, Training Loss 0.014922638120644195\n",
      "Epoch 300, Training Loss 0.014987948709679648\n",
      "Epoch 300, Training Loss 0.015074832292864352\n",
      "Epoch 300, Training Loss 0.0151739211180164\n",
      "Epoch 300, Training Loss 0.015197641160839315\n",
      "Epoch 300, Training Loss 0.015271338064323568\n",
      "Epoch 300, Training Loss 0.01533593525073927\n",
      "Epoch 300, Training Loss 0.01539453494367297\n",
      "Epoch 300, Training Loss 0.01545675158860338\n",
      "Epoch 300, Training Loss 0.015573487199647615\n",
      "Epoch 300, Training Loss 0.01558706624959798\n",
      "Epoch 300, Training Loss 0.01563511845474715\n",
      "Epoch 300, Training Loss 0.01563764605587801\n",
      "Epoch 300, Training Loss 0.015654513453397793\n",
      "Epoch 300, Training Loss 0.015732404647890448\n",
      "Epoch 300, Training Loss 0.01577114155265929\n",
      "Epoch 300, Training Loss 0.015798341457272314\n",
      "Epoch 300, Training Loss 0.015821544635121514\n",
      "Epoch 300, Training Loss 0.01583561996805489\n",
      "Epoch 300, Training Loss 0.015872874581243108\n",
      "Epoch 300, Training Loss 0.01590103101606964\n",
      "Epoch 300, Training Loss 0.015911260333336185\n",
      "Epoch 300, Training Loss 0.015962366990702193\n",
      "Epoch 300, Training Loss 0.016000910492404305\n",
      "Epoch 300, Training Loss 0.016022047710955105\n",
      "Epoch 300, Training Loss 0.01605886659618286\n",
      "Epoch 300, Training Loss 0.016219150955619675\n",
      "Epoch 300, Training Loss 0.0162592431681607\n",
      "Epoch 300, Training Loss 0.01628072202250914\n",
      "Epoch 300, Training Loss 0.016309667435234123\n",
      "Epoch 300, Training Loss 0.016391062960767037\n",
      "Epoch 300, Training Loss 0.016427661691818512\n",
      "Epoch 300, Training Loss 0.016543511569838198\n",
      "Epoch 300, Training Loss 0.016625078773135533\n",
      "Epoch 300, Training Loss 0.016660344231364026\n",
      "Epoch 300, Training Loss 0.016707730447978277\n",
      "Epoch 300, Training Loss 0.01685465004740526\n",
      "Epoch 300, Training Loss 0.01689540115518548\n",
      "Epoch 300, Training Loss 0.016943087071880622\n",
      "Epoch 300, Training Loss 0.016971250457088926\n",
      "Epoch 300, Training Loss 0.016989144278080452\n",
      "Epoch 300, Training Loss 0.017060418299439807\n",
      "Epoch 300, Training Loss 0.01709385122687025\n",
      "Epoch 300, Training Loss 0.01710447639076854\n",
      "Epoch 300, Training Loss 0.017134586368780343\n",
      "Epoch 300, Training Loss 0.01721913529062153\n",
      "Epoch 300, Training Loss 0.01726392130700924\n",
      "Epoch 300, Training Loss 0.01729959828357505\n",
      "Epoch 300, Training Loss 0.017346147872398004\n",
      "Epoch 300, Training Loss 0.017358008427354875\n",
      "Epoch 300, Training Loss 0.01738755573647197\n",
      "Epoch 300, Training Loss 0.017478702928694655\n",
      "Epoch 300, Training Loss 0.017528075196654024\n",
      "Epoch 300, Training Loss 0.0175717347319407\n",
      "Epoch 300, Training Loss 0.017582703667009234\n",
      "Epoch 300, Training Loss 0.017591515323147178\n",
      "Epoch 300, Training Loss 0.01761790254281934\n",
      "Epoch 300, Training Loss 0.01766230309646472\n",
      "Epoch 300, Training Loss 0.01770414679568461\n",
      "Epoch 300, Training Loss 0.01773119375617017\n",
      "Epoch 300, Training Loss 0.017789268507110073\n",
      "Epoch 300, Training Loss 0.01780211834934876\n",
      "Epoch 300, Training Loss 0.017861192130729023\n",
      "Epoch 300, Training Loss 0.017871600696626487\n",
      "Epoch 300, Training Loss 0.01789913623941505\n",
      "Epoch 300, Training Loss 0.0179427789632574\n",
      "Epoch 300, Training Loss 0.01796981996423124\n",
      "Epoch 300, Training Loss 0.018028746982512382\n",
      "Epoch 300, Training Loss 0.018065285189506\n",
      "Epoch 300, Training Loss 0.018101442678500435\n",
      "Epoch 300, Training Loss 0.01812640990516471\n",
      "Epoch 300, Training Loss 0.018137406494082582\n",
      "Epoch 300, Training Loss 0.018150177463303174\n",
      "Epoch 300, Training Loss 0.01824712011155665\n",
      "Epoch 300, Training Loss 0.018275612437040033\n",
      "Epoch 300, Training Loss 0.018301171063066786\n",
      "Epoch 300, Training Loss 0.0184191334760412\n",
      "Epoch 300, Training Loss 0.018442614523865416\n",
      "Epoch 300, Training Loss 0.01846953164464544\n",
      "Epoch 300, Training Loss 0.01852541930242287\n",
      "Epoch 300, Training Loss 0.018557440811682424\n",
      "Epoch 300, Training Loss 0.018622838139957975\n",
      "Epoch 300, Training Loss 0.018682817700426176\n",
      "Epoch 300, Training Loss 0.01869538974652872\n",
      "Epoch 300, Training Loss 0.01872222868385046\n",
      "Epoch 300, Training Loss 0.018742244928251103\n",
      "Epoch 300, Training Loss 0.01881745968268865\n",
      "Epoch 300, Training Loss 0.018849888299008275\n",
      "Epoch 300, Training Loss 0.018888494489914583\n",
      "Epoch 300, Training Loss 0.018929767176446975\n",
      "Epoch 300, Training Loss 0.01907763742994222\n",
      "Epoch 300, Training Loss 0.019123707375729746\n",
      "Epoch 300, Training Loss 0.01916249069656767\n",
      "Epoch 300, Training Loss 0.019222797353720993\n",
      "Epoch 300, Training Loss 0.019245423266397374\n",
      "Epoch 300, Training Loss 0.019308085455094725\n",
      "Epoch 300, Training Loss 0.0193612890697949\n",
      "Epoch 300, Training Loss 0.019382435999348607\n",
      "Epoch 300, Training Loss 0.019455299080918777\n",
      "Epoch 300, Training Loss 0.01946758777272347\n",
      "Epoch 300, Training Loss 0.019485501265820222\n",
      "Epoch 300, Training Loss 0.01957583664011334\n",
      "Epoch 300, Training Loss 0.019600970478242507\n",
      "Epoch 300, Training Loss 0.019625491191706885\n",
      "Epoch 300, Training Loss 0.01964857994073816\n",
      "Epoch 300, Training Loss 0.01968004082715439\n",
      "Epoch 300, Training Loss 0.019711831330305057\n",
      "Epoch 300, Training Loss 0.0197623834720172\n",
      "Epoch 300, Training Loss 0.019795907025947176\n",
      "Epoch 300, Training Loss 0.019825371390189545\n",
      "Epoch 300, Training Loss 0.019864661738757627\n",
      "Epoch 300, Training Loss 0.019923640553818067\n",
      "Epoch 300, Training Loss 0.019947769378176165\n",
      "Epoch 300, Training Loss 0.020024578895091134\n",
      "Epoch 300, Training Loss 0.02011692510677211\n",
      "Epoch 300, Training Loss 0.020146597258370282\n",
      "Epoch 300, Training Loss 0.020161238628203796\n",
      "Epoch 300, Training Loss 0.020176077558709038\n",
      "Epoch 300, Training Loss 0.020197266073840314\n",
      "Epoch 300, Training Loss 0.020240163096510198\n",
      "Epoch 300, Training Loss 0.020256417249079287\n",
      "Epoch 300, Training Loss 0.020285930597078044\n",
      "Epoch 300, Training Loss 0.020317114118481878\n",
      "Epoch 300, Training Loss 0.02032936468560849\n",
      "Epoch 300, Training Loss 0.020354109026058137\n",
      "Epoch 300, Training Loss 0.020377627665015018\n",
      "Epoch 300, Training Loss 0.020404777223425333\n",
      "Epoch 300, Training Loss 0.02044595140889954\n",
      "Epoch 300, Training Loss 0.020487948197542745\n",
      "Epoch 300, Training Loss 0.02050238439296384\n",
      "Epoch 300, Training Loss 0.020524312540188508\n",
      "Epoch 300, Training Loss 0.020561666226090715\n",
      "Epoch 300, Training Loss 0.02061187748468536\n",
      "Epoch 300, Training Loss 0.020658430027063277\n",
      "Epoch 300, Training Loss 0.02069119473411928\n",
      "Epoch 300, Training Loss 0.02075550240069113\n",
      "Epoch 300, Training Loss 0.020788655397803772\n",
      "Epoch 300, Training Loss 0.02083492544694039\n",
      "Epoch 300, Training Loss 0.02084894967503617\n",
      "Epoch 300, Training Loss 0.020913308633424704\n",
      "Epoch 300, Training Loss 0.021035223264995094\n",
      "Epoch 300, Training Loss 0.021078686450805774\n",
      "Epoch 300, Training Loss 0.02117066080038867\n",
      "Epoch 300, Training Loss 0.02118111237385751\n",
      "Epoch 300, Training Loss 0.02121348629939034\n",
      "Epoch 300, Training Loss 0.021291487518569355\n",
      "Epoch 300, Training Loss 0.02132089016304049\n",
      "Epoch 300, Training Loss 0.02134805991519195\n",
      "Epoch 300, Training Loss 0.021400780737986003\n",
      "Epoch 300, Training Loss 0.021482845882068165\n",
      "Epoch 300, Training Loss 0.021537018867800264\n",
      "Epoch 300, Training Loss 0.021581428415496904\n",
      "Epoch 300, Training Loss 0.02162788473867604\n",
      "Epoch 300, Training Loss 0.021724739790205724\n",
      "Epoch 300, Training Loss 0.02178305575607435\n",
      "Epoch 300, Training Loss 0.02182349840791234\n",
      "Epoch 300, Training Loss 0.021868535347849778\n",
      "Epoch 300, Training Loss 0.021947134052143644\n",
      "Epoch 300, Training Loss 0.022032929993475146\n",
      "Epoch 300, Training Loss 0.022086304785740918\n",
      "Epoch 300, Training Loss 0.02214959212769385\n",
      "Epoch 300, Training Loss 0.022287222120286825\n",
      "Epoch 300, Training Loss 0.02229920809057153\n",
      "Epoch 300, Training Loss 0.02233815453815586\n",
      "Epoch 300, Training Loss 0.022447368283124873\n",
      "Epoch 300, Training Loss 0.02249665720843236\n",
      "Epoch 300, Training Loss 0.022570400563714184\n",
      "Epoch 300, Training Loss 0.022599847632986697\n",
      "Epoch 300, Training Loss 0.02264292135922824\n",
      "Epoch 300, Training Loss 0.022709793064033475\n",
      "Epoch 300, Training Loss 0.022743588256890244\n",
      "Epoch 300, Training Loss 0.02282898119041015\n",
      "Epoch 300, Training Loss 0.0228781480005111\n",
      "Epoch 300, Training Loss 0.02301607332954569\n",
      "Epoch 300, Training Loss 0.023084075774287668\n",
      "Epoch 300, Training Loss 0.023136051901070702\n",
      "Epoch 300, Training Loss 0.02316831938429352\n",
      "Epoch 300, Training Loss 0.02327507125544826\n",
      "Epoch 300, Training Loss 0.023357778426159718\n",
      "Epoch 300, Training Loss 0.02343821282679563\n",
      "Epoch 300, Training Loss 0.023491746920477743\n",
      "Epoch 300, Training Loss 0.023584580382980082\n",
      "Epoch 300, Training Loss 0.02363002713819694\n",
      "Epoch 300, Training Loss 0.023661817655639003\n",
      "Epoch 300, Training Loss 0.02368376743765381\n",
      "Epoch 300, Training Loss 0.023706913271280543\n",
      "Epoch 300, Training Loss 0.02377235028199623\n",
      "Epoch 300, Training Loss 0.023783988374716524\n",
      "Epoch 300, Training Loss 0.023880818416304943\n",
      "Epoch 300, Training Loss 0.023932777402941568\n",
      "Epoch 300, Training Loss 0.024041687884508535\n",
      "Epoch 300, Training Loss 0.02406444675956026\n",
      "Epoch 300, Training Loss 0.024092091943549415\n",
      "Epoch 300, Training Loss 0.024099156445087602\n",
      "Epoch 300, Training Loss 0.02410785326153002\n",
      "Epoch 300, Training Loss 0.02412326400265898\n",
      "Epoch 300, Training Loss 0.024210281067473048\n",
      "Epoch 300, Training Loss 0.02425948914635898\n",
      "Epoch 300, Training Loss 0.024301477264889213\n",
      "Epoch 300, Training Loss 0.024321740988613394\n",
      "Epoch 300, Training Loss 0.024338213363638544\n",
      "Epoch 300, Training Loss 0.02441478758702612\n",
      "Epoch 300, Training Loss 0.024589217607351138\n",
      "Epoch 300, Training Loss 0.024719860479759666\n",
      "Epoch 300, Training Loss 0.024738826372129534\n",
      "Epoch 300, Training Loss 0.024766630693183987\n",
      "Epoch 300, Training Loss 0.024833587050685644\n",
      "Epoch 300, Training Loss 0.024888958155518146\n",
      "Epoch 300, Training Loss 0.024956596636539683\n",
      "Epoch 300, Training Loss 0.02509747143320339\n",
      "Epoch 300, Training Loss 0.02511134330907365\n",
      "Epoch 300, Training Loss 0.025213636741366074\n",
      "Epoch 300, Training Loss 0.025292096904876743\n",
      "Epoch 300, Training Loss 0.02531157168047622\n",
      "Epoch 300, Training Loss 0.025346496119818953\n",
      "Epoch 300, Training Loss 0.02535983514221733\n",
      "Epoch 300, Training Loss 0.02540051234919397\n",
      "Epoch 300, Training Loss 0.02545152775123906\n",
      "Epoch 300, Training Loss 0.02554280882525017\n",
      "Epoch 300, Training Loss 0.025571663697223985\n",
      "Epoch 300, Training Loss 0.025631384507221792\n",
      "Epoch 300, Training Loss 0.02564986461125638\n",
      "Epoch 300, Training Loss 0.02566883713721543\n",
      "Epoch 300, Training Loss 0.02577472335951941\n",
      "Epoch 300, Training Loss 0.025812578339920476\n",
      "Epoch 300, Training Loss 0.02588066588039212\n",
      "Epoch 300, Training Loss 0.02589816869000721\n",
      "Epoch 300, Training Loss 0.025930512513098358\n",
      "Epoch 300, Training Loss 0.02602298169746957\n",
      "Epoch 300, Training Loss 0.0260551000261665\n",
      "Epoch 300, Training Loss 0.026060692580712155\n",
      "Epoch 300, Training Loss 0.026143316251566382\n",
      "Epoch 300, Training Loss 0.026179863786077142\n",
      "Epoch 300, Training Loss 0.02624852252978822\n",
      "Epoch 300, Training Loss 0.026305343239399058\n",
      "Epoch 300, Training Loss 0.026379710666554243\n",
      "Epoch 300, Training Loss 0.026459867499716332\n",
      "Epoch 300, Training Loss 0.026506665244322183\n",
      "Epoch 300, Training Loss 0.0265350538500063\n",
      "Epoch 300, Training Loss 0.026641941327444466\n",
      "Epoch 300, Training Loss 0.026751175242335633\n",
      "Epoch 300, Training Loss 0.026795390260327234\n",
      "Epoch 300, Training Loss 0.026827849079128305\n",
      "Epoch 300, Training Loss 0.026854345539842003\n",
      "Epoch 300, Training Loss 0.02687231296628161\n",
      "Epoch 300, Training Loss 0.02693661426365509\n",
      "Epoch 300, Training Loss 0.02695999803709919\n",
      "Epoch 300, Training Loss 0.02703997216906751\n",
      "Epoch 300, Training Loss 0.02713871723853166\n",
      "Epoch 300, Training Loss 0.027150882788531273\n",
      "Epoch 300, Training Loss 0.02726930137330671\n",
      "Epoch 300, Training Loss 0.0273288029818879\n",
      "Epoch 300, Training Loss 0.02738782032654928\n",
      "Epoch 300, Training Loss 0.02740913563493706\n",
      "Epoch 300, Training Loss 0.02746136038911903\n",
      "Epoch 300, Training Loss 0.027490375679619896\n",
      "Epoch 300, Training Loss 0.02752820172470987\n",
      "Epoch 300, Training Loss 0.027540270064640646\n",
      "Epoch 300, Training Loss 0.027563610544685476\n",
      "Epoch 300, Training Loss 0.0276253259808654\n",
      "Epoch 300, Training Loss 0.027653423101638857\n",
      "Epoch 300, Training Loss 0.027708469806096096\n",
      "Epoch 300, Training Loss 0.027778189076120247\n",
      "Epoch 300, Training Loss 0.027791014732554783\n",
      "Epoch 300, Training Loss 0.027807091539630385\n",
      "Epoch 300, Training Loss 0.027865059705346328\n",
      "Epoch 300, Training Loss 0.027915373614267505\n",
      "Epoch 300, Training Loss 0.027925618536546445\n",
      "Epoch 300, Training Loss 0.028020806058579126\n",
      "Epoch 300, Training Loss 0.028055725776997713\n",
      "Epoch 300, Training Loss 0.028088487997350982\n",
      "Epoch 300, Training Loss 0.028101401911486332\n",
      "Epoch 300, Training Loss 0.028136134615091755\n",
      "Epoch 300, Training Loss 0.02819196901360379\n",
      "Epoch 300, Training Loss 0.028248608841791825\n",
      "Epoch 300, Training Loss 0.028304096893109667\n",
      "Epoch 300, Training Loss 0.02837429111859168\n",
      "Epoch 300, Training Loss 0.028416486967312138\n",
      "Epoch 300, Training Loss 0.028489662101492286\n",
      "Epoch 300, Training Loss 0.02851156481479764\n",
      "Epoch 300, Training Loss 0.028592419744495428\n",
      "Epoch 300, Training Loss 0.028630385397459424\n",
      "Epoch 300, Training Loss 0.02864236121132608\n",
      "Epoch 300, Training Loss 0.02866281174502009\n",
      "Epoch 300, Training Loss 0.028685939947829184\n",
      "Epoch 300, Training Loss 0.028716238031563972\n",
      "Epoch 300, Training Loss 0.028748630778864026\n",
      "Epoch 300, Training Loss 0.02883966632670892\n",
      "Epoch 300, Training Loss 0.02896462321517241\n",
      "Epoch 300, Training Loss 0.029011711927816805\n",
      "Epoch 300, Training Loss 0.029047506501126433\n",
      "Epoch 300, Training Loss 0.02905433895030652\n",
      "Epoch 300, Training Loss 0.029077885457483667\n",
      "Epoch 300, Training Loss 0.0291196506111966\n",
      "Epoch 300, Training Loss 0.029141835579553337\n",
      "Epoch 300, Training Loss 0.02918843648401196\n",
      "Epoch 300, Training Loss 0.02927932810798154\n",
      "Epoch 300, Training Loss 0.029345274111494193\n",
      "Epoch 300, Training Loss 0.02936353057425212\n",
      "Epoch 300, Training Loss 0.029391542115770376\n",
      "Epoch 300, Training Loss 0.029464059507550523\n",
      "Epoch 300, Training Loss 0.02948274962125284\n",
      "Epoch 300, Training Loss 0.029496523879749506\n",
      "Epoch 300, Training Loss 0.02952810137918519\n",
      "Epoch 300, Training Loss 0.02957409072626868\n",
      "Epoch 300, Training Loss 0.029615388251245593\n",
      "Epoch 300, Training Loss 0.029628756308994825\n",
      "Epoch 300, Training Loss 0.02973975140965827\n",
      "Epoch 300, Training Loss 0.029773847474490325\n",
      "Epoch 300, Training Loss 0.029850384673637237\n",
      "Epoch 300, Training Loss 0.02986369172618498\n",
      "Epoch 300, Training Loss 0.02987371208420133\n",
      "Epoch 300, Training Loss 0.029947661145059083\n",
      "Epoch 300, Training Loss 0.030028386080227887\n",
      "Epoch 300, Training Loss 0.030051662418110026\n",
      "Epoch 300, Training Loss 0.030064694489926443\n",
      "Epoch 300, Training Loss 0.030117197850090274\n",
      "Epoch 300, Training Loss 0.03015283000407755\n",
      "Epoch 300, Training Loss 0.030198540440296084\n",
      "Epoch 300, Training Loss 0.03028720044388014\n",
      "Epoch 300, Training Loss 0.030301652216921795\n",
      "Epoch 300, Training Loss 0.03038199834437455\n",
      "Epoch 300, Training Loss 0.030428671839592213\n",
      "Epoch 300, Training Loss 0.03046028659669468\n",
      "Epoch 300, Training Loss 0.03053832862257024\n",
      "Epoch 300, Training Loss 0.030569449616739016\n",
      "Epoch 300, Training Loss 0.030620614578232855\n",
      "Epoch 300, Training Loss 0.030684613560200157\n",
      "Epoch 300, Training Loss 0.030697633322361674\n",
      "Epoch 300, Training Loss 0.030707495988530044\n",
      "Epoch 300, Training Loss 0.03073945089512507\n",
      "Epoch 300, Training Loss 0.030760984697505413\n",
      "Epoch 300, Training Loss 0.030779994471603647\n",
      "Epoch 300, Training Loss 0.03084557894569681\n",
      "Epoch 300, Training Loss 0.030877649271979814\n",
      "Epoch 300, Training Loss 0.030979812404860165\n",
      "Epoch 300, Training Loss 0.031113736257623988\n",
      "Epoch 300, Training Loss 0.031155420074244138\n",
      "Epoch 300, Training Loss 0.031196944394132214\n",
      "Epoch 300, Training Loss 0.031215872434551453\n",
      "Epoch 300, Training Loss 0.031249319903476312\n",
      "Epoch 300, Training Loss 0.0313300291371658\n",
      "Epoch 300, Training Loss 0.03134613267867766\n",
      "Epoch 300, Training Loss 0.031423597236918975\n",
      "Epoch 300, Training Loss 0.03144184585131914\n",
      "Epoch 300, Training Loss 0.031477504311000826\n",
      "Epoch 300, Training Loss 0.03155514040409261\n",
      "Epoch 300, Training Loss 0.031578889134509104\n",
      "Epoch 300, Training Loss 0.031685867238446803\n",
      "Epoch 300, Training Loss 0.031707638575721654\n",
      "Epoch 300, Training Loss 0.031780715099276255\n",
      "Epoch 300, Training Loss 0.03179137629059041\n",
      "Epoch 300, Training Loss 0.03183596160815424\n",
      "Epoch 300, Training Loss 0.03189633422009552\n",
      "Epoch 300, Training Loss 0.03195638055472499\n",
      "Epoch 300, Training Loss 0.03199649419959472\n",
      "Epoch 300, Training Loss 0.03206119413637673\n",
      "Epoch 300, Training Loss 0.03210350855127396\n",
      "Epoch 300, Training Loss 0.032140198406641896\n",
      "Epoch 300, Training Loss 0.032187205701208935\n",
      "Epoch 300, Training Loss 0.03223580612903437\n",
      "Epoch 300, Training Loss 0.03224895840815609\n",
      "Epoch 300, Training Loss 0.03226323836171033\n",
      "Epoch 300, Training Loss 0.032318317945903674\n",
      "Epoch 300, Training Loss 0.032355474279550336\n",
      "Epoch 300, Training Loss 0.03240877121705991\n",
      "Epoch 300, Training Loss 0.032418099978743384\n",
      "Epoch 300, Training Loss 0.03243973435681609\n",
      "Epoch 300, Training Loss 0.03250252969844072\n",
      "Epoch 300, Training Loss 0.032535252331629815\n",
      "Epoch 300, Training Loss 0.032568368447415744\n",
      "Epoch 300, Training Loss 0.03267506604937984\n",
      "Epoch 300, Training Loss 0.032710426092824284\n",
      "Epoch 300, Training Loss 0.03281121938596563\n",
      "Epoch 300, Training Loss 0.03289061843219887\n",
      "Epoch 300, Training Loss 0.03290255492924691\n",
      "Epoch 300, Training Loss 0.03297226840172849\n",
      "Epoch 300, Training Loss 0.03325522104468759\n",
      "Epoch 300, Training Loss 0.03332251772794234\n",
      "Epoch 300, Training Loss 0.03340012972574214\n",
      "Epoch 300, Training Loss 0.03350272010464002\n",
      "Epoch 300, Training Loss 0.0335408844580145\n",
      "Epoch 300, Training Loss 0.03362069195708084\n",
      "Epoch 300, Training Loss 0.03365044226593641\n",
      "Epoch 300, Training Loss 0.0338080782583817\n",
      "Epoch 300, Training Loss 0.03382530535244957\n",
      "Epoch 300, Training Loss 0.033922187405188216\n",
      "Epoch 300, Training Loss 0.03400720213123066\n",
      "Epoch 300, Training Loss 0.03423155256597054\n",
      "Epoch 300, Training Loss 0.034271072374794945\n",
      "Epoch 300, Training Loss 0.034290028538297664\n",
      "Epoch 300, Training Loss 0.03442102044468264\n",
      "Epoch 300, Training Loss 0.03444183032835841\n",
      "Epoch 300, Training Loss 0.034485352566213254\n",
      "Epoch 300, Training Loss 0.0345358005331834\n",
      "Epoch 300, Training Loss 0.03457097351417669\n",
      "Epoch 300, Training Loss 0.0345951838923804\n",
      "Epoch 300, Training Loss 0.03462765704068686\n",
      "Epoch 300, Training Loss 0.034701280877032246\n",
      "Epoch 300, Training Loss 0.034758505409064194\n",
      "Epoch 300, Training Loss 0.03482459475050497\n",
      "Epoch 300, Training Loss 0.034865283700244505\n",
      "Epoch 300, Training Loss 0.03488207748517051\n",
      "Epoch 300, Training Loss 0.03489611510668531\n",
      "Epoch 300, Training Loss 0.034938093931044995\n",
      "Epoch 300, Training Loss 0.035044495567746095\n",
      "Epoch 300, Training Loss 0.0350874497655236\n",
      "Epoch 300, Training Loss 0.03516942384483679\n",
      "Epoch 300, Training Loss 0.035222298061937246\n",
      "Epoch 300, Training Loss 0.03529670262766425\n",
      "Epoch 300, Training Loss 0.03532606866710898\n",
      "Epoch 300, Training Loss 0.03538628103440184\n",
      "Epoch 300, Training Loss 0.035451784061834864\n",
      "Epoch 300, Training Loss 0.0354774191468249\n",
      "Epoch 300, Training Loss 0.03550412980220316\n",
      "Epoch 300, Training Loss 0.03551686090557738\n",
      "Epoch 300, Training Loss 0.03554275949769046\n",
      "Epoch 300, Training Loss 0.03558663203192832\n",
      "Epoch 300, Training Loss 0.03564997793049039\n",
      "Epoch 300, Training Loss 0.03569978160326324\n",
      "Epoch 300, Training Loss 0.03577679340058314\n",
      "Epoch 300, Training Loss 0.03585971053094243\n",
      "Epoch 300, Training Loss 0.035896900742340955\n",
      "Epoch 300, Training Loss 0.035905762678583905\n",
      "Epoch 300, Training Loss 0.03592020071044926\n",
      "Epoch 300, Training Loss 0.035968671934536234\n",
      "Epoch 300, Training Loss 0.03599868581244422\n",
      "Epoch 300, Training Loss 0.036044130085722145\n",
      "Epoch 300, Training Loss 0.036108973784410324\n",
      "Epoch 300, Training Loss 0.03613230612788759\n",
      "Epoch 300, Training Loss 0.03616519371950356\n",
      "Epoch 300, Training Loss 0.03619469636086556\n",
      "Epoch 300, Training Loss 0.0362283934528232\n",
      "Epoch 300, Training Loss 0.03633031135489283\n",
      "Epoch 300, Training Loss 0.036373206981769796\n",
      "Epoch 300, Training Loss 0.03644599576714113\n",
      "Epoch 300, Training Loss 0.0364682390205705\n",
      "Epoch 300, Training Loss 0.0365333766122694\n",
      "Epoch 300, Training Loss 0.03655006847930286\n",
      "Epoch 300, Training Loss 0.036598078429560796\n",
      "Epoch 300, Training Loss 0.036610429287742816\n",
      "Epoch 300, Training Loss 0.03665525533432794\n",
      "Epoch 300, Training Loss 0.03668990181080158\n",
      "Epoch 300, Training Loss 0.036775733423455026\n",
      "Epoch 300, Training Loss 0.03683435278134826\n",
      "Epoch 300, Training Loss 0.036846938922339124\n",
      "Epoch 300, Training Loss 0.03688857464841507\n",
      "Epoch 300, Training Loss 0.03691853180377627\n",
      "Epoch 300, Training Loss 0.03696044455901684\n",
      "Epoch 300, Training Loss 0.037012045236561646\n",
      "Epoch 300, Training Loss 0.037070315712919966\n",
      "Epoch 300, Training Loss 0.03712420275642077\n",
      "Epoch 300, Training Loss 0.037178553546161945\n",
      "Epoch 300, Training Loss 0.03720487360580517\n",
      "Epoch 300, Training Loss 0.03724143021296986\n",
      "Epoch 300, Training Loss 0.037272880385603634\n",
      "Epoch 300, Training Loss 0.03729427092151283\n",
      "Epoch 300, Training Loss 0.03735343245682223\n",
      "Epoch 300, Training Loss 0.03738369776562447\n",
      "Epoch 300, Training Loss 0.03743034542199996\n",
      "Epoch 300, Training Loss 0.037449517870283756\n",
      "Epoch 300, Training Loss 0.03746746087392025\n",
      "Epoch 300, Training Loss 0.03755194386479247\n",
      "Epoch 300, Training Loss 0.03761998376728076\n",
      "Epoch 300, Training Loss 0.037669317038786954\n",
      "Epoch 300, Training Loss 0.037674009111588413\n"
     ]
    }
   ],
   "source": [
    "training_loop(n_epochs = n_epochs, optimizer = optimizer, model = model, loss_fn = loss_fn, train_loader = train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "6ccd7090-49b0-47c3-aa00-ba51789e9880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.98\n",
      "Accuracy val: 0.65\n"
     ]
    }
   ],
   "source": [
    "validate(model = model, train_loader = train_loader, val_loader = val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6719e7b7-7b6c-45c4-9834-52f82a4f631c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314cc62d-e1e3-40b7-b4b6-5096a8f7e711",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet10()\n",
    "model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c347afff-9e43-4798-b2c6-ca73f956fbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop(n_epochs = n_epochs, optimizer = optimizer, model = model, loss_fn = loss_fn, train_loader = train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e35817-c8bf-4291-9669-3538ac72fcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(model = model, train_loader = train_loader, val_loader = val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7150a05d-d780-40f1-b007-b58b20b725ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
