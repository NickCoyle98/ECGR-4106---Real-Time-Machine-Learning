{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4fed83bd-1f33-4252-80af-31d4f18d8ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cda78dd6-707c-46c2-9fc2-5bb0b6109f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>stories</th>\n",
       "      <th>mainroad</th>\n",
       "      <th>guestroom</th>\n",
       "      <th>basement</th>\n",
       "      <th>hotwaterheating</th>\n",
       "      <th>airconditioning</th>\n",
       "      <th>parking</th>\n",
       "      <th>prefarea</th>\n",
       "      <th>furnishingstatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13300000</td>\n",
       "      <td>7420</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12250000</td>\n",
       "      <td>8960</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12250000</td>\n",
       "      <td>9960</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>semi-furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12215000</td>\n",
       "      <td>7500</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>yes</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11410000</td>\n",
       "      <td>7420</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      price  area  bedrooms  bathrooms  stories mainroad guestroom basement  \\\n",
       "0  13300000  7420         4          2        3      yes        no       no   \n",
       "1  12250000  8960         4          4        4      yes        no       no   \n",
       "2  12250000  9960         3          2        2      yes        no      yes   \n",
       "3  12215000  7500         4          2        2      yes        no      yes   \n",
       "4  11410000  7420         4          1        2      yes       yes      yes   \n",
       "\n",
       "  hotwaterheating airconditioning  parking prefarea furnishingstatus  \n",
       "0              no             yes        2      yes        furnished  \n",
       "1              no             yes        3       no        furnished  \n",
       "2              no              no        2      yes   semi-furnished  \n",
       "3              no             yes        3      yes        furnished  \n",
       "4              no             yes        2       no        furnished  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"https://raw.githubusercontent.com/NickCoyle98/Homework/main/Homework%201/Housing.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c0b95c6-af07-4c7d-9ca4-3581da418daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,[1,2,3,4,10]].values\n",
    "Y = data.iloc[:,[0]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce4fa44c-a366-4a95-8e04-2292064d22f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n",
    "scaler = StandardScaler()\n",
    "Y_test = scaler.fit_transform(Y_test)\n",
    "Y_train = scaler.fit_transform(Y_train)\n",
    "X_train =  scaler.fit_transform(X_train)\n",
    "X_test =  scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "257e09cc-60fd-46f8-a9cf-1ff49e3a9a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "Y_train = torch.from_numpy(Y_train).float()\n",
    "Y_test = torch.from_numpy(Y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09fd6bdc-65cb-4a09-93a0-a297c7aac6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_model = nn.Sequential(\n",
    "    nn.Linear(5,8),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(8,1))\n",
    "optimizer = optim.SGD(seq_model.parameters(), lr = 1e-3)\n",
    "seq_model = seq_model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "654d203e-50bd-4380-bb64-427cdadcbd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, seq_model, loss_fn, t_u_train, t_u_val, t_c_train, t_c_val):\n",
    "        for epoch in range(1, n_epochs+1):\n",
    " \n",
    "            t_p_train = seq_model(t_u_train.float())\n",
    "            loss_train = loss_fn(t_p_train, t_c_train)\n",
    "            t_p_val = seq_model(t_u_val)\n",
    "            loss_val = loss_fn(t_p_val, t_c_val)\n",
    "            optimizer.zero_grad()\n",
    "            loss_train.backward()\n",
    "\n",
    "            if epoch == 1 or epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Training Loss {loss_train.item()} : .4f, \"f\" Validation Loss {loss_val.item() : .4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "858b6851-7c17-4d43-8651-47703a361354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss 1.5726133584976196 : .4f,  Validation Loss  1.6006\n",
      "Epoch 10, Training Loss 1.5726133584976196 : .4f,  Validation Loss  1.6006\n",
      "Epoch 20, Training Loss 1.5726133584976196 : .4f,  Validation Loss  1.6006\n",
      "Epoch 30, Training Loss 1.5726133584976196 : .4f,  Validation Loss  1.6006\n",
      "Epoch 40, Training Loss 1.5726133584976196 : .4f,  Validation Loss  1.6006\n",
      "Epoch 50, Training Loss 1.5726133584976196 : .4f,  Validation Loss  1.6006\n",
      "Epoch 60, Training Loss 1.5726133584976196 : .4f,  Validation Loss  1.6006\n",
      "Epoch 70, Training Loss 1.5726133584976196 : .4f,  Validation Loss  1.6006\n",
      "Epoch 80, Training Loss 1.5726133584976196 : .4f,  Validation Loss  1.6006\n",
      "Epoch 90, Training Loss 1.5726133584976196 : .4f,  Validation Loss  1.6006\n",
      "Epoch 100, Training Loss 1.5726133584976196 : .4f,  Validation Loss  1.6006\n",
      "Epoch 110, Training Loss 1.5726133584976196 : .4f,  Validation Loss  1.6006\n",
      "Epoch 120, Training Loss 1.5726133584976196 : .4f,  Validation Loss  1.6006\n",
      "Epoch 130, Training Loss 1.5726133584976196 : .4f,  Validation Loss  1.6006\n",
      "Epoch 140, Training Loss 1.5726133584976196 : .4f,  Validation Loss  1.6006\n",
      "Epoch 150, Training Loss 1.5726133584976196 : .4f,  Validation Loss  1.6006\n",
      "Epoch 160, Training Loss 1.5726133584976196 : .4f,  Validation Loss  1.6006\n",
      "Epoch 170, Training Loss 1.5726133584976196 : .4f,  Validation Loss  1.6006\n",
      "Epoch 180, Training Loss 1.5726133584976196 : .4f,  Validation Loss  1.6006\n",
      "Epoch 190, Training Loss 1.5726133584976196 : .4f,  Validation Loss  1.6006\n",
      "Epoch 200, Training Loss 1.5726133584976196 : .4f,  Validation Loss  1.6006\n"
     ]
    }
   ],
   "source": [
    "training_loop(n_epochs = 200, optimizer = optimizer, seq_model = seq_model, loss_fn = nn.MSELoss(), t_u_train = X_train, t_u_val = X_test, t_c_train = Y_train, t_c_val = Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dbad5398-c61f-4a57-a6c9-c7baf2e887a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss 1.0254840850830078 : .4f,  Validation Loss  1.0409\n",
      "Epoch 10, Training Loss 1.0254840850830078 : .4f,  Validation Loss  1.0409\n",
      "Epoch 20, Training Loss 1.0254840850830078 : .4f,  Validation Loss  1.0409\n",
      "Epoch 30, Training Loss 1.0254840850830078 : .4f,  Validation Loss  1.0409\n",
      "Epoch 40, Training Loss 1.0254840850830078 : .4f,  Validation Loss  1.0409\n",
      "Epoch 50, Training Loss 1.0254840850830078 : .4f,  Validation Loss  1.0409\n",
      "Epoch 60, Training Loss 1.0254840850830078 : .4f,  Validation Loss  1.0409\n",
      "Epoch 70, Training Loss 1.0254840850830078 : .4f,  Validation Loss  1.0409\n",
      "Epoch 80, Training Loss 1.0254840850830078 : .4f,  Validation Loss  1.0409\n",
      "Epoch 90, Training Loss 1.0254840850830078 : .4f,  Validation Loss  1.0409\n",
      "Epoch 100, Training Loss 1.0254840850830078 : .4f,  Validation Loss  1.0409\n",
      "Epoch 110, Training Loss 1.0254840850830078 : .4f,  Validation Loss  1.0409\n",
      "Epoch 120, Training Loss 1.0254840850830078 : .4f,  Validation Loss  1.0409\n",
      "Epoch 130, Training Loss 1.0254840850830078 : .4f,  Validation Loss  1.0409\n",
      "Epoch 140, Training Loss 1.0254840850830078 : .4f,  Validation Loss  1.0409\n",
      "Epoch 150, Training Loss 1.0254840850830078 : .4f,  Validation Loss  1.0409\n",
      "Epoch 160, Training Loss 1.0254840850830078 : .4f,  Validation Loss  1.0409\n",
      "Epoch 170, Training Loss 1.0254840850830078 : .4f,  Validation Loss  1.0409\n",
      "Epoch 180, Training Loss 1.0254840850830078 : .4f,  Validation Loss  1.0409\n",
      "Epoch 190, Training Loss 1.0254840850830078 : .4f,  Validation Loss  1.0409\n",
      "Epoch 200, Training Loss 1.0254840850830078 : .4f,  Validation Loss  1.0409\n"
     ]
    }
   ],
   "source": [
    "seq_model = nn.Sequential(\n",
    "        nn.Linear(5,8),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(8,4),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(4,1))\n",
    "seq_model = seq_model.float()\n",
    "training_loop(n_epochs = 200, optimizer = optimizer, seq_model = seq_model, loss_fn = nn.MSELoss(), t_u_train = X_train, t_u_val = X_test, t_c_train = Y_train, t_c_val = Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ccdadfd5-a9c3-4066-80b3-de2fd0bee819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_path = '/p1ch7/'\n",
    "cifar10 = datasets.CIFAR10(data_path, train = True, download = True, transform =  transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))]))\n",
    "cifar10_val = datasets.CIFAR10(data_path, train = False, download = True, transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "91b18e67-4058-4723-a488-468b5ce2aad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size = 64, shuffle = True)\n",
    "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size = 10000, shuffle = False)\n",
    "correct = 0\n",
    "total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7ebf1ddd-5dee-4000-a2c0-24740b71afe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "        nn.Linear(3072, 512),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(512,10),\n",
    "        nn.LogSoftmax(dim = 1))\n",
    "model1 = nn.Sequential(\n",
    "        nn.Linear(3072, 512),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(512, 1024),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(1024, 10),\n",
    "        nn.LogSoftmax(dim = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "05523c02-1905-4bec-84d3-45f89c7d999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "optimizer = optim.SGD(model1.parameters(), lr = learning_rate)\n",
    "loss_fn = nn.NLLLoss()\n",
    "n_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "61a960bc-5543-41c8-b9f2-474047ffcbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1.901732\n",
      "Epoch: 1, Loss: 1.891352\n",
      "Epoch: 10, Loss: 2.065657\n",
      "Epoch: 20, Loss: 1.564264\n",
      "Epoch: 30, Loss: 1.072452\n",
      "Epoch: 40, Loss: 1.424730\n",
      "Epoch: 50, Loss: 1.231876\n",
      "Epoch: 60, Loss: 1.542061\n",
      "Epoch: 70, Loss: 1.647483\n",
      "Epoch: 80, Loss: 1.271078\n",
      "Epoch: 90, Loss: 1.494857\n",
      "Epoch: 100, Loss: 0.986407\n",
      "Epoch: 110, Loss: 1.233557\n",
      "Epoch: 120, Loss: 0.983006\n",
      "Epoch: 130, Loss: 0.814825\n",
      "Epoch: 140, Loss: 0.435624\n",
      "Epoch: 150, Loss: 0.768838\n",
      "Epoch: 160, Loss: 1.001828\n",
      "Epoch: 170, Loss: 0.628968\n",
      "Epoch: 180, Loss: 0.436515\n",
      "Epoch: 190, Loss: 0.298698\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model1(imgs.view(batch_size, -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch == 1 or epoch % 10 == 0:\n",
    "        print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "133c931c-734f-4e71-8261-9d58f6f7f733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: %f 0.4754\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model1(imgs.view(batch_size, -1))\n",
    "        _, predicted = torch.max(outputs, dim = 1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "print(\"Accuracy: %f\", correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c11944-e32b-4f17-af6f-89eba69c2ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503e35bf-9669-4548-bb24-b3a60491c786",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fcea52-426b-46d8-aae2-306164d2c4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
